[
    {
        "title": "About object storage",
        "content": "About object storage\nVirtuozzo Hybrid Infrastructure allows you to export cluster disk space to customers in the form of an S3-like object-based storage.\nVirtuozzo Hybrid Infrastructure is implemented as an Amazon S3-like API, which is one of the most common object storage APIs. End users can work with Virtuozzo Hybrid Infrastructure as they work with Amazon S3. You can use the usual applications for S3 and continue working with them after the data migration from Amazon S3 to Virtuozzo Hybrid Infrastructure.\nObject storage is a storage architecture that enables managing data as objects (like in a key-value storage), as opposed to files in file systems or blocks in a block storage. Except for the data, each object has metadata that describes it as well as a unique identifier that allows finding the object in the storage. Object storage is optimized for storing billions of objects, in particular for application storage, static web content hosting, online storage services, big data, and backups. All of these uses are enabled by object storage thanks to a combination of very high scalability, data availability, and consistency.\nCompared to other types of storage, the key difference of object storage is that parts of an object cannot be modified; so if the object changes, a new version of it is spawned instead. This approach is extremely important for maintaining data availability and consistency. First of all, changing an object as a whole eliminates the issue of conflicts. That is, the object with the latest timestamp is considered to be the current version and that is it. As a result, objects are always consistent, that is, their state is relevant and appropriate.\nAnother feature of object storage is eventual consistency. Eventual consistency does not guarantee that reads are to return the new state after the write has been completed. Readers can observe the old state for an undefined period of time, until the write is propagated to all the replicas (copies).\nIn Virtuozzo Hybrid Infrastructure, object storage offers eventual consistency to improve performance and availability of geographically distant datacenters, as they may not be able to perform data update synchronously (for example, due to network latency) and the update itself may also be slow, as awaiting acknowledges from all the data replicas over long distances can take hundreds of milliseconds. So eventual consistency helps hide communication latencies on writes at the cost of the probable old state observed by readers. However, many use cases can easily tolerate it.\nNote that within the same cluster:\n\nOperations on objects are strongly consistent.\nOperations on buckets (such as updating policies) are eventually consistent.\n\nSee also\n\nProvisioning object storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-object-storage.html"
    },
    {
        "title": "Adding external DNS servers",
        "content": "Adding external DNS servers\nVirtuozzo Hybrid Infrastructure features a built-in DNS server that enables discovery of all of its internal services. For resolving external domain names, you can add DNS servers that already exist in your network infrastructure.\nLimitations\n\nSpecify a DNS server that belongs to a public network to be able to reach external locations like the updates repository, as well as any public networks.\n\nTo add external DNS servers\n\nAdmin panel\n\nGo to Settings > System settings > Cluster DNS.\n\nClick Add and select the IP address type:\n\nSelect Static, and then specify a static DNS IP address.\nSelect DHCP, and then select a DHCP-provided DNS IP address from the list.\n\nClick Add multiple times to specify multiple external DNS servers.\n\nClick Save to save your changes.\n\nCommand-line interface\nUse the following command:vinfra cluster settings dns set --nameservers <nameservers>\r\n\n\n--nameservers <nameservers>\n\nA comma-separated list of DNS servers\n\nFor example, to set the external DNS server to 8.8.8.8, run:# vinfra cluster settings dns set --nameservers 8.8.8.8\r\n+------------------+---------------+\r\n| Field            | Value         |\r\n+------------------+---------------+\r\n| dhcp_nameservers | - 10.10.0.10  |\r\n|                  | - 10.10.0.11  |\r\n|                  | - 10.37.130.2 |\r\n| nameservers      | - 8.8.8.8     |\r\n+------------------+---------------+\r\n\nThe added DNS server will appear in the vinfra cluster settings dns show output:# vinfra cluster settings dns show\r\n+------------------+-----------------------------------+\r\n| Field            | Value                             |\r\n+------------------+-----------------------------------+\r\n| dhcp_nameservers | 10.10.0.10,10.10.0.11,10.37.130.2 |\r\n| nameservers      | 10.10.0.11,10.10.0.10             |\r\n+------------------+-----------------------------------+\r\n\n\nWhat's next\n\nConfiguring NVMe performance",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster settings dns set --nameservers <nameservers>\r\n\n\n--nameservers <nameservers>\n\nA comma-separated list of DNS servers\n\nFor example, to set the external DNS server to 8.8.8.8, run:# vinfra cluster settings dns set --nameservers 8.8.8.8\r\n+------------------+---------------+\r\n| Field            | Value         |\r\n+------------------+---------------+\r\n| dhcp_nameservers | - 10.10.0.10  |\r\n|                  | - 10.10.0.11  |\r\n|                  | - 10.37.130.2 |\r\n| nameservers      | - 8.8.8.8     |\r\n+------------------+---------------+\r\n\nThe added DNS server will appear in the vinfra cluster settings dns show output:# vinfra cluster settings dns show\r\n+------------------+-----------------------------------+\r\n| Field            | Value                             |\r\n+------------------+-----------------------------------+\r\n| dhcp_nameservers | 10.10.0.10,10.10.0.11,10.37.130.2 |\r\n| nameservers      | 10.10.0.11,10.10.0.10             |\r\n+------------------+-----------------------------------+\r\n\n",
                "title": "To add external DNS servers"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to Settings > System settings > Cluster DNS.\n\nClick Add and select the IP address type:\n\nSelect Static, and then specify a static DNS IP address.\nSelect DHCP, and then select a DHCP-provided DNS IP address from the list.\n\nClick Add multiple times to specify multiple external DNS servers.\n\n\n\n\n\nClick Save to save your changes.\n\n",
                "title": "To add external DNS servers"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/adding-external-dns-servers.html"
    },
    {
        "title": "About file storage",
        "content": "About file storage\nFile storage is a storage architecture that uses the Network File System (NFS) protocol to manage data as files. Virtuozzo Hybrid Infrastructure allows you to organize nodes into a highly available NFS cluster in which you can create NFS shares. An NFS share is an access point for a volume and it can be assigned an IP address or a DNS name. The volume, in turn, can be assigned a redundancy scheme, a tier, and a failure domain. In each share, you can create multiple NFS exports that are actual exported directories for user data. Each export has, among other properties, a path that, combined with share\u00e2\u0080\u0099s IP address, uniquely identifies the export on the network and allows you to mount it using standard tools.\nOn the technical side, NFS volumes are based on object storage. Aside from offering high availability and scalability, object storage eliminates the limit on the amount of files and the size of data you can keep in the NFS cluster. Each share is perfect for keeping billions of files of any size. However, such scalability implies I/O overhead that is wasted on file size changes and rewrites. For this reason, an NFS cluster makes a perfect cold and warm file storage, but is not recommended for hot and high performance, and data that is often rewritten (like running virtual machines). Integration of Virtuozzo Hybrid Infrastructure with solutions from VMware, for example, is best done via iSCSI to achieve better performance.\n\nVirtuozzo Hybrid Infrastructure only supports NFS version 4 and newer. Starting with Virtuozzo Hybrid Infrastructure 4.0, pNFS is no longer supported.\n\nSee also\n\nProvisioning file storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-file-storage.html"
    },
    {
        "title": "About the storage cluster",
        "content": "About the storage cluster\nThe storage cluster provides the most efficient usage of the hardware with erasure coding, integrated SSD caching, automatic load balancing, and RDMA/InfiniBand support. The cluster space can be used for:\n\niSCSI block storage (hot data and virtual machines)\nS3 object storage (protected with geo-replication or cross-region replication between datacenters)\nFile storage (NFS)\n\nIn addition, Virtuozzo Hybrid Infrastructure is integrated with Acronis Cyber Protection solutions for storing backups in the cluster, sending them to cloud services (like Google Cloud, Microsoft Azure, and AWS S3), or storing them on NAS via the NFS protocol. Geo-replication is available for Backup Gateways set up on different storage backends: a local storage cluster, NFS share, or public cloud.\nData storage policies can be customized to meet various use cases: each data volume can have a specific redundancy mode, storage tier, and failure domain. Moreover, the data can be encrypted with the AES-256 standard.\n\nSee also\n\nDeploying the storage cluster\n\nData redundancy\n\nFailure domains\n\nStorage tiers\n\nStorage policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-the-storage-cluster.html"
    },
    {
        "title": "About the compute cluster",
        "content": "About the compute cluster\nThe compute cluster provides virtualization management for virtual machines and software-defined networks. In Virtuozzo Hybrid Infrastructure, virtual machines are based on open-source technology and can run both Windows and Linux guests. They are also highly available and can be migrated live with no downtime. Moreover, VM volume snapshots are application consistent.\nVirtuozzo Hybrid Infrastructure provides for the most efficient usage of the compute resources with Kubernetes as a Service, Load Balancer as a Service, and metering for account compute resources. To allocate space to virtual machines and select different redundancy modes, storage policies are applied to VM volumes. Storage policies can also limit bandwidth and IOPS in order to provide predictable performance levels for VM disks. Besides, to distribute the workload between nodes, the virtual machine placement can be based on the node characteristics.\nIn Virtuozzo Hybrid Infrastructure, administrators can create multiple domains, tenants, and users, and allocate resources according to tenant quotas. Moreover, they can use white-labeling to build their own public or private cloud storage solutions. At that, independent end users of compute resources are isolated and secure in their own self-service portals.\nVirtuozzo Hybrid Infrastructure provides secure and isolated virtual networking for virtual machines using VXLAN encapsulation. The distributed virtual switching and routing simplify VM network configuration, and the built-in firewall makes it more secure. The integrated DHCP, IP, and DNS management provides for enhanced configuration of the network.\n\nSee also\n\nProvisioning compute resources",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-the-compute-cluster.html"
    },
    {
        "title": "Accessing cluster information objects via SNMP",
        "content": "Accessing cluster information objects via SNMP\nYou can access cluster information objects with SNMP tools of your choice, for example, the free Net-SNMP suite for Linux.\nPrerequisites\n\nThe SNMP access is enabled, as described in Enabling SNMP access.\n\nTo obtain storage cluster information on the management node\nPlace the MIB file in /usr/share/snmp/mibs and run the snmpwalk command. For example:# snmpwalk  -M /usr/share/snmp/mibs -m VSTORAGE-MIB -v 2c -c public localhost:161 VSTORAGE-MIB:cluster\r\n\nTypical output may look like the following:VSTORAGE-MIB::clusterName.0 = STRING: \"cluster1\"VSTORAGE-MIB::healthStatus.0 = STRING: \"healthy\"VSTORAGE-MIB::usedLogicalSpace.0 = Counter64: 173732322VSTORAGE-MIB::totalLogicalSpace.0 = Counter64: 1337665179648VSTORAGE-MIB::freeLogicalSpace.0 = Counter64: 1318963253248VSTORAGE-MIB::licenseStatus.0 = STRING: \"unknown\"VSTORAGE-MIB::licenseCapacity.0 = Counter64: 1099511627776VSTORAGE-MIB::licenseExpirationStatus.0 = STRING: \"None\"VSTORAGE-MIB::ioReadOpS.0 = Counter64: 0VSTORAGE-MIB::ioWriteOpS.0 = Counter64: 0VSTORAGE-MIB::ioReads.0 = Counter64: 0VSTORAGE-MIB::ioWrites.0 = Counter64: 0VSTORAGE-MIB::csActive.0 = Counter64: 11VSTORAGE-MIB::csTotal.0 = Counter64: 11VSTORAGE-MIB::mdsAvail.0 = Counter64: 4VSTORAGE-MIB::mdsTotal.0 = Counter64: 4<...>\r\n\nSee also\n\nCluster objects and traps\n\nListening to SNMP traps\n\nMonitoring the cluster with Zabbix",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/accessing-cluster-information-objects-via-snmp.html"
    },
    {
        "title": "Adding nodes to backup storage",
        "content": "Adding nodes to backup storage\nYou can add more nodes  that will serve as targets for backups from Acronis Cyber Protect and/or Acronis Cyber Protect Cloud for high availability and scalability of your backup storage.\nPrerequisites\n\nThe backup storage cluster is created and registered in the Cloud Management Panel, as described in Provisioning backup storage space.\n\nTo add nodes to backup storage\n\nAdmin panel\n\nGo to the Storage services > Backup storage > Nodes screen.\nClick  Add node. \nSelect nodes to join the backup storage cluster and click Add.\n\nThe nodes will be added to your backup storage and will run Backup Gateway.\n\nCommand-line interface\nUse the following command:vinfra service backup node add --nodes <nodes>\r\n\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n\nFor example, to add the node with the ID 2f3f6091-0d44-45aa-94e3-ebc2b65c0eeb to the backup cluster, run:# vinfra service backup node add --nodes 2f3f6091-0d44-45aa-94e3-ebc2b65c0eeb\nThe added node will appear in the vinfra service backup node list output:# vinfra service backup node list\r\n+--------------------------------------+------------------------+-----------+\r\n| id                                   | host                   | is_online |\r\n+--------------------------------------+------------------------+-----------+\r\n| 2f3f6091-0d44-45aa-94e3-ebc2b65c0eeb | node003.vstoragedomain | True      |\r\n| 74cbd22b-fb1b-4441-ae52-532078c54f9a | node001.vstoragedomain | True      |\r\n| eeb06dce-4cfd-4c89-bc7f-4689ea5c7058 | node002.vstoragedomain | True      |\r\n+--------------------------------------+------------------------+-----------+\r\n\n\nSee also\n\nChanging the redundancy scheme for backup storage\n\nManaging registrations for backup storage\n\nManaging geo-replication for backup storage\n\nMonitoring backup storage\n\nReleasing nodes from backup storage",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup node add --nodes <nodes>\r\n\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n\nFor example, to add the node with the ID 2f3f6091-0d44-45aa-94e3-ebc2b65c0eeb to the backup cluster, run:# vinfra service backup node add --nodes 2f3f6091-0d44-45aa-94e3-ebc2b65c0eeb\nThe added node will appear in the vinfra service backup node list output:# vinfra service backup node list\r\n+--------------------------------------+------------------------+-----------+\r\n| id                                   | host                   | is_online |\r\n+--------------------------------------+------------------------+-----------+\r\n| 2f3f6091-0d44-45aa-94e3-ebc2b65c0eeb | node003.vstoragedomain | True      |\r\n| 74cbd22b-fb1b-4441-ae52-532078c54f9a | node001.vstoragedomain | True      |\r\n| eeb06dce-4cfd-4c89-bc7f-4689ea5c7058 | node002.vstoragedomain | True      |\r\n+--------------------------------------+------------------------+-----------+\r\n\n",
                "title": "To add nodes to backup storage"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > Backup storage > Nodes screen.\nClick  Add node. \nSelect nodes to join the backup storage cluster and click Add.\n\nThe nodes will be added to your backup storage and will run Backup Gateway.\n",
                "title": "To add nodes to backup storage"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/adding-nodes-to-backup-storage.html"
    },
    {
        "title": "About backup storage",
        "content": "About backup storage\nBackup storage uses Backup Gateway as a storage access point. It is intended for service providers who use Acronis Cyber Protect and/or Acronis Cyber Protect Cloud and want to store their clients\u00e2\u0080\u0099 backed-up data in the local cluster, in the cloud (like Google Cloud, Microsoft Azure, and AWS S3), or on NAS (via the NFS protocol). \nBackup storage enables a service provider to easily configure storage for the proprietary deduplication-friendly data format used by Acronis. In addition,  the backup storage data can be geo-replicated.\nBackup storage supports the following backup destinations:\n\nVirtuozzo Hybrid Infrastructure storage clusters with erasure coding providing for data redundancy\nNFS shares\nPublic clouds, including a number of S3 solutions, as well as Microsoft Azure, OpenStack Swift, and Google Cloud Platform\n\nWhile your choice should depend on the scenario and requirements, it is recommended to keep Acronis backup data in the Virtuozzo Hybrid Infrastructure local storage cluster. In this case, you can have the best performance due to WAN optimizations and data locality. Keeping backups in an NFS share or a public cloud implies the unavoidable data transfer and other overhead, which reduces overall performance. Besides, with external backup destinations, redundancy has to be provided by the external storage. Backup storage does not provide data redundancy or perform data deduplication itself.\nBackup storage architecture\nBackup Gateway, the backup storage access point, runs as a  service on the Virtuozzo Hybrid Infrastructure nodes. It is recommended to deploy it on two or more nodes for high availability.\n\nSee also\n\nProvisioning backup storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-backup-storage.html"
    },
    {
        "title": "About block storage",
        "content": "About block storage\nVirtuozzo Hybrid Infrastructure can be used as a block storage backend over iSCSI. Block storage is optimized for data that must be frequently accessed and edited. It is ideal for hot data and virtual machines.\nBlock storage enables managing data as blocks, as opposed to files in file systems or objects in S3 storage. Those blocks can be stored in different operating systems in a SAN-like manner.\nVirtuozzo Hybrid Infrastructure allows you to create groups of redundant targets running on different storage nodes. To each target group, you can attach multiple storage volumes with their own redundancy provided by the storage layer. Targets export these volumes as LUNs.\nYou can create multiple target groups on same nodes. A volume, however, may only be attached to one target group at any moment in time.\nEach node in a target group can host a single target for that group. If one of the nodes in a target group fails along with its targets, healthy targets from the same group continue to provide access to the LUNs previously serviced by the failed targets.\nYou can configure access to the target group by using CHAP or ACL. \nSample block storage\nThe figure below shows a typical setup for exporting Virtuozzo Hybrid Infrastructure disk space via iSCSI.\n\nThe figure shows two volumes located on redundant storage provided by Virtuozzo Hybrid Infrastructure. The volumes are attached as LUNs to a group of two targets running on Virtuozzo Hybrid Infrastructure nodes. Each target has two portals, one per network interface, with the iSCSI traffic type. This makes a total of four discoverable endpoints with different IP addresses. Each target provides access to all LUNs attached to the group.\nTargets work in the ALUA mode, so one path to the volume is preferred and considered Active/Optimized while the other is Standby. The Active/Optimized path is normally chosen by the initiator (Explicit ALUA). If the initiator cannot do so (either does not support it or times out), the path is chosen by the storage itself (Implicit ALUA).\nNetwork interfaces eth0 and eth1 on each node are connected to different switches for redundancy. The initiator, for example, VMware ESXi, is connected to both switches as well and provides volumes as iSCSI disks 1 and 2 to a VM via different network paths.\nIf the Active/Optimized path becomes unavailable for some reason (for example, the node with the target or network switch fails), the Standby path through the other target will be used instead to connect to the volume. When the Active/Optimized path is restored, it will be used again.\nSee also\n\nProvisioning block storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-block-storage.html"
    },
    {
        "title": "About the infrastructure",
        "content": "About the infrastructure\nVirtuozzo Hybrid Infrastructure installed on bare-metal servers unites them into a single cluster, which can be easily scaled by adding disks or nodes. The cluster is managed via a highly available web-based admin panel and the command line. The admin panel provides extensive monitoring of all components. The overview dashboards are integrated with Prometheus, Grafana, SNMP, and Zabbix, to provide an insight into the infrastructure status. In addition, the alerts system keeps the administrator informed about misconfiguration, failures, and other issues.\nClustering helps avoid data loss with replication and erasure coding. With high availability enabled, the cluster and services have no single point of failure. The storage cluster is self-healing: if a node or disk fails, the cluster will automatically try to restore the lost data. Besides, with non-disruptive rolling updates the data stays available even when updating the nodes. In case of a node maintenance or applying hotfixes, the workload is migrated to other available nodes.\nThis section describes the major infrastructure components and their architecture: the storage and compute clusters, as well as backup, block, object, and file storage. \n\nSee also\n\nInstallation",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/about-the-infrastructure.html"
    },
    {
        "title": "Adding  identity providers",
        "content": "Adding  identity providers\nBefore connecting to an identity provider and importing its users, you need to create domain groups for these users and assign respective roles to these groups.\nPrerequisites\n\nLocal domain groups are created, as described in Creating domain groups.\nThe redirection URL (redirect_uri) for the cluster must be https://<url>:8800/api/v2/login/idp/.\n\nTo add an identity provider\n\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, and then click Add.\n\nIn the Add identity provider window, specify the following parameters:\n\nA custom name of the identity provider that will be shown on the login screen.\nThe unique issuer ID provided by the OIDC provider. It usually appears as a URN.\nThe client identifier and secret to access the OIDC provider.\nThe metadata URL of the OIDC provider's discovery endpoint. The metadata URL is typically the issuer endpoint concatenated with the path /.well-known/openid-configuration. For example, if the issuer ID is https://idp.example.com/adfs/, the metadata URL will be https://idp.example.com/adfs/.well-known/openid-configuration.\n\nScopes that define what user identity data will be shared by the OIDC provider during authentication.\n\nThe scopes allatclaims and openid are mandatory for Microsoft AD FS providers.\n\nIn the Mapping section, you can either create mapping rules manually or automatically from a mapping file:\n\nTo manually create mapping rules\n\nSelect Create mapping rules, and then click Add.\n\nIn the Add rule window, create Mapping conditions by clicking Add and specifying the required parameters:\n\nIn Attribute, specify a user attribute that you obtain from the identity provider during authentication.\nIn Condition, specify a condition to apply to the attribute. With the condition Exists, all users with this attribute will be mapped. The condition Contains maps users if this attribute contains any of the specified values. The condition Does not contain maps users if this attribute does not contain any of the specified values.\nIn Value, specify the desired attribute value as a string, comma-separated list, or regular expression.\n\nSelect an existing domain group to assign federated users to.\nIf you have a mapping rule with the condition Exists, select attributes that the name of a local user will consist of. For example, with the mapping attribute email and mapping condition Exists, names of local users may be composed of their emails.\n\nTo automatically create mapping rules\n\nSelect Upload a mapping file in the JSON format with already configured mapping rules.\n\nWrite mapping rules in the JSON format in the Mapping data field. Alternatively, click Upload, and then browse a JSON file on your local server to load the mapping data from.\nA mapping file may look as follows:# cat mapping.json\r\n[\r\n    {\r\n        \"local\": [\r\n            {\r\n                \"user\": {\r\n                    \"name\": \"{0}\"\r\n                },\r\n                \"group\": {\r\n                    \"name\":\"users\"\r\n                }\r\n            }\r\n        ],\r\n        \"remote\": [{\"type\": \"email\"}]\r\n    }\r\n]\nIn this example, all users that have the attribute email will be mapped to the group users within the default domain. For details on creating a mapping file, refer to the OpenStack documentation.\n\nClick Add.\n\nCommand-line interface\nUse the following command:vinfra domain idp create --domain <domain> --issuer <issuer> --scope <scope>\r\n                         [--metadata-url <metadata-url>] [--client-id <client-id>]\r\n                         [--client-secret <client-secret>] [--mapping <path>]\r\n                         [--enable] [--disable] [--verify-ssl | --dont-verify-ssl]\r\n                         [--request-timeout <seconds>] <name>\n\n--domain <domain>\n\nDomain name or ID\n--issuer <issuer>\n\nIdentity provider issuer\n--scope <scope>\n\nScope that define what user identity data will be shared by the identity provider during authentication.\n\nThe scopes allatclaims and openid are mandatory for Microsoft AD FS providers.\n\n--metadata-url <metadata-url>\n\nMetadata URL of the identity provider's discovery endpoint\n--client-id <client-id>\n\nClient ID to access the identity provider\n--client-secret <client-secret>\n\nClient secret to access the identity provider\n--mapping <path>\n\nPath to the mapping configuration file.\nA mapping file may look as follows:# cat mapping.json\r\n[\r\n    {\r\n        \"local\": [\r\n            {\r\n                \"user\": {\r\n                    \"name\": \"{0}\"\r\n                },\r\n                \"group\": {\r\n                    \"name\":\"users\"\r\n                }\r\n            }\r\n        ],\r\n        \"remote\": [{\"type\": \"email\"}]\r\n    }\r\n]\nIn this example, all users that have the attribute email will be mapped to the group users within the default domain. For details on creating a mapping file, refer to the OpenStack documentation.\n\n--enable\n\nEnable identity provider\n--disable\n\nDisable identity provider\n--verify-ssl\n\nEnable identity provider endpoints SSL verification\n--dont-verify-ssl\n\nDisable identity provider endpoints SSL verification\n--request-timeout <seconds>\n\nIdentity provider API request timeout (default: 5)\n<name>\n\nIdentity provider name\n\nFor example, to add an identity provider with the name My ADFS within the mydomain domain, run:# vinfra domain idp create --domain mydomain --issuer https://idp.example.com/adfs/ \\\r\n--scope \"allatclaims openid email\" --client-id xxx --client-secret xxx --mapping mapping.json \"My ADFS\"\nThe added identity provider will appear in the vinfra domain idp list output:# vinfra domain idp list --domain mydomain\r\n+-------------+---------+-------------------------------+--------------------------+-----------+\r\n| id          | name    | issuer                        | scope                    | domain_id |\r\n+-------------+---------+-------------------------------+--------------------------+-----------+\r\n| df5a54ce<\u00e2\u0080\u00a6> | My ADFS | https://idp.example.com/adfs/ | allatclaims openid email | 36f454<\u00e2\u0080\u00a6> |\r\n+-------------+---------+-------------------------------+--------------------------+-----------+\n\nSee also\n\nEditing and deleting identity providers\n\nManaging user assignment to domain groups\n\nManaging project assignment to domain groups\n\nWhat's next\n\nSigning in through identity providers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain idp create --domain <domain> --issuer <issuer> --scope <scope>\r\n                         [--metadata-url <metadata-url>] [--client-id <client-id>]\r\n                         [--client-secret <client-secret>] [--mapping <path>]\r\n                         [--enable] [--disable] [--verify-ssl | --dont-verify-ssl]\r\n                         [--request-timeout <seconds>] <name>\n\n--domain <domain>\n\nDomain name or ID\n--issuer <issuer>\n\nIdentity provider issuer\n--scope <scope>\n\n\nScope that define what user identity data will be shared by the identity provider during authentication.\n\nThe scopes allatclaims and openid are mandatory for Microsoft AD FS providers.\n\n\n--metadata-url <metadata-url>\n\nMetadata URL of the identity provider's discovery endpoint\n--client-id <client-id>\n\nClient ID to access the identity provider\n--client-secret <client-secret>\n\nClient secret to access the identity provider\n--mapping <path>\n\n\nPath to the mapping configuration file.\nA mapping file may look as follows:# cat mapping.json\r\n[\r\n    {\r\n        \"local\": [\r\n            {\r\n                \"user\": {\r\n                    \"name\": \"{0}\"\r\n                },\r\n                \"group\": {\r\n                    \"name\":\"users\"\r\n                }\r\n            }\r\n        ],\r\n        \"remote\": [{\"type\": \"email\"}]\r\n    }\r\n]\nIn this example, all users that have the attribute email will be mapped to the group users within the default domain. For details on creating a mapping file, refer to the OpenStack documentation.\n\n--enable\n\nEnable identity provider\n--disable\n\nDisable identity provider\n--verify-ssl\n\nEnable identity provider endpoints SSL verification\n--dont-verify-ssl\n\nDisable identity provider endpoints SSL verification\n--request-timeout <seconds>\n\nIdentity provider API request timeout (default: 5)\n<name>\n\nIdentity provider name\n\nFor example, to add an identity provider with the name My ADFS within the mydomain domain, run:# vinfra domain idp create --domain mydomain --issuer https://idp.example.com/adfs/ \\\r\n--scope \"allatclaims openid email\" --client-id xxx --client-secret xxx --mapping mapping.json \"My ADFS\"\nThe added identity provider will appear in the vinfra domain idp list output:# vinfra domain idp list --domain mydomain\r\n+-------------+---------+-------------------------------+--------------------------+-----------+\r\n| id          | name    | issuer                        | scope                    | domain_id |\r\n+-------------+---------+-------------------------------+--------------------------+-----------+\r\n| df5a54ce<\u00e2\u0080\u00a6> | My ADFS | https://idp.example.com/adfs/ | allatclaims openid email | 36f454<\u00e2\u0080\u00a6> |\r\n+-------------+---------+-------------------------------+--------------------------+-----------+\n",
                "title": "To add an identity provider"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, and then click Add.\n\nIn the Add identity provider window, specify the following parameters:\n\nA custom name of the identity provider that will be shown on the login screen.\nThe unique issuer ID provided by the OIDC provider. It usually appears as a URN.\nThe client identifier and secret to access the OIDC provider.\nThe metadata URL of the OIDC provider's discovery endpoint. The metadata URL is typically the issuer endpoint concatenated with the path /.well-known/openid-configuration. For example, if the issuer ID is https://idp.example.com/adfs/, the metadata URL will be https://idp.example.com/adfs/.well-known/openid-configuration.\n\nScopes that define what user identity data will be shared by the OIDC provider during authentication.\n\nThe scopes allatclaims and openid are mandatory for Microsoft AD FS providers.\n\n\n\n\n\n\n\n\n\nIn the Mapping section, you can either create mapping rules manually or automatically from a mapping file:\n\n\nTo manually create mapping rules\n\n\nSelect Create mapping rules, and then click Add.\n\nIn the Add rule window, create Mapping conditions by clicking Add and specifying the required parameters:\n\nIn Attribute, specify a user attribute that you obtain from the identity provider during authentication.\nIn Condition, specify a condition to apply to the attribute. With the condition Exists, all users with this attribute will be mapped. The condition Contains maps users if this attribute contains any of the specified values. The condition Does not contain maps users if this attribute does not contain any of the specified values.\nIn Value, specify the desired attribute value as a string, comma-separated list, or regular expression.\n\n\nSelect an existing domain group to assign federated users to.\nIf you have a mapping rule with the condition Exists, select attributes that the name of a local user will consist of. For example, with the mapping attribute email and mapping condition Exists, names of local users may be composed of their emails.\n\n\n\n\n\n\n\n\n\nTo automatically create mapping rules\n\n\nSelect Upload a mapping file in the JSON format with already configured mapping rules.\n\nWrite mapping rules in the JSON format in the Mapping data field. Alternatively, click Upload, and then browse a JSON file on your local server to load the mapping data from.\nA mapping file may look as follows:# cat mapping.json\r\n[\r\n    {\r\n        \"local\": [\r\n            {\r\n                \"user\": {\r\n                    \"name\": \"{0}\"\r\n                },\r\n                \"group\": {\r\n                    \"name\":\"users\"\r\n                }\r\n            }\r\n        ],\r\n        \"remote\": [{\"type\": \"email\"}]\r\n    }\r\n]\nIn this example, all users that have the attribute email will be mapped to the group users within the default domain. For details on creating a mapping file, refer to the OpenStack documentation.\n\n\n\n\n\n\n\n\nClick Add.\n\n\n",
                "title": "To add an identity provider"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/adding-identity-providers.html"
    },
    {
        "title": "Adding registrations",
        "content": "Adding registrations\nLimitations\n\nWith geo-replication enabled, new registrations are not automatically replicated to the secondary backup storage. To import such registrations, follow the instructions from Importing registrations to the secondary cluster.\n\nPrerequisites\n\nThe backup storage cluster is created and registered in the Cloud Management Panel, as described in Provisioning backup storage space.\nEnsure that two-factor authentication (2FA) is disabled for your partner account. You can also disable it for a specific user within a 2FA-enabled tenant, as described in the Acronis Cyber Protect Cloud documentation, and specify the user credentials.\n\nIf you have enabled login control for the Acronis Cyber Protect Cloud web interface, ensure that the public IP address of your backup storage cluster is specified among the allowed IP addresses, as instructed in the Acronis Cyber Protect Cloud documentation.\n\nTo add more registrations\n\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Registrations tab, and then click Add registration.\n\nIn the Add registration window, specify a registration name and a unique DNS name for backup storage (for example, newregistration.example.com). Backup agents will use this DNS name and the TCP port 44445 to upload backup data. Then, click Next.\n\nConfigure your DNS server according to the example suggested in the admin panel.\nEach time you change the network configuration of nodes in the backup storage cluster, adjust the DNS records accordingly.\n\nSpecify the following information for your Acronis product:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\nClick Add to create the registration.\n\nCommand-line interface\nUse the following command:vinfra service backup registration add --name <name> --address <address> --username <username>\r\n                                       --account-server <account_server> [--stdin] [--location <location>]\r\n                                       [--primary-storage-id] [--failback-storage-id]\n\n--name <name>\n\n       Backup registration name.\n--address <address>\n\nBackup registration domain name.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--account-server <account-server>\n\nURL of the cloud management portal or the hostname/IP address and port of the local management server.\n--stdin\n\nUse for setting registration password from stdin.\n--location <location>\n\nBackup registration location.\n--primary-storage-id\n\nThe ID of the replica storage.\n--failback-storage-id\n\nThe ID of the failback storage which will become primary after failback procedure is completed.\n\nFor example, to create the backup storage registration registration2 in Acronis Cyber Protect, run:# vinfra service backup registration add --name registration2 --address backupstorage2.example.com \\\r\n--username account@example.com --account-server https://cloud.acronis.com --stdin\nSpecify the registration password when prompted.\nThe new backup storage registration will appear in the vinfra service backup registration list output:# vinfra service backup registration list\r\n+-------------+---------------+----------------------------+------+\r\n| id          | name          | address                    | type |\r\n+-------------+---------------+----------------------------+------+\r\n| be526718<\u00e2\u0080\u00a6> | registration1 | backupstorage.example.com  | ABC  |\r\n| 028adb6b<\u00e2\u0080\u00a6> | registration3 | backupstorage3.example.com | ABC  |\r\n| 300d379f<\u00e2\u0080\u00a6> | registration2 | backupstorage2.example.com | ABC  |\r\n+-------------+---------------+----------------------------+------+\r\n\n\nSee also\n\nUpdating registration certificates\n\nDeleting registrations",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup registration add --name <name> --address <address> --username <username>\r\n                                       --account-server <account_server> [--stdin] [--location <location>]\r\n                                       [--primary-storage-id] [--failback-storage-id]\n\n--name <name>\n\n       Backup registration name.\n--address <address>\n\nBackup registration domain name.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--account-server <account-server>\n\nURL of the cloud management portal or the hostname/IP address and port of the local management server.\n--stdin\n\nUse for setting registration password from stdin.\n--location <location>\n\nBackup registration location.\n--primary-storage-id\n\nThe ID of the replica storage.\n--failback-storage-id\n\nThe ID of the failback storage which will become primary after failback procedure is completed.\n\nFor example, to create the backup storage registration registration2 in Acronis Cyber Protect, run:# vinfra service backup registration add --name registration2 --address backupstorage2.example.com \\\r\n--username account@example.com --account-server https://cloud.acronis.com --stdin\nSpecify the registration password when prompted.\nThe new backup storage registration will appear in the vinfra service backup registration list output:# vinfra service backup registration list\r\n+-------------+---------------+----------------------------+------+\r\n| id          | name          | address                    | type |\r\n+-------------+---------------+----------------------------+------+\r\n| be526718<\u00e2\u0080\u00a6> | registration1 | backupstorage.example.com  | ABC  |\r\n| 028adb6b<\u00e2\u0080\u00a6> | registration3 | backupstorage3.example.com | ABC  |\r\n| 300d379f<\u00e2\u0080\u00a6> | registration2 | backupstorage2.example.com | ABC  |\r\n+-------------+---------------+----------------------------+------+\r\n\n",
                "title": "To add more registrations"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Registrations tab, and then click Add registration.\n\nIn the Add registration window, specify a registration name and a unique DNS name for backup storage (for example, newregistration.example.com). Backup agents will use this DNS name and the TCP port 44445 to upload backup data. Then, click Next.\n\n\nConfigure your DNS server according to the example suggested in the admin panel.\nEach time you change the network configuration of nodes in the backup storage cluster, adjust the DNS records accordingly.\n\n\n\n\n\n\n\n\nSpecify the following information for your Acronis product:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\n\n\n\n\n\nClick Add to create the registration.\n\n",
                "title": "To add more registrations"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/adding-registrations.html"
    },
    {
        "title": "Adding S3 users",
        "content": "Adding S3 users\nPrerequisites\n\nThe S3 cluster is created by following the instructions in Creating the S3 cluster.\n\nTo add an S3 user\n\nOn the Storage services > S3 > Users screen, click Add user.\n\nSpecify a valid email address as login for the user, and then click Add.\n\nWhat's next\n\nManaging object storage\n\nMonitoring object storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/adding-s3-users.html"
    },
    {
        "title": "Assigning QoS policies",
        "content": "Assigning QoS policies\nAlongside the default QoS policy, you can assign QoS policies to specific network ports, floating IP addresses, and entire networks.\nPrerequisites\n\nA QoS policy is created, as described in Creating QoS policies.\n\nTo assign a QoS policy to a network port\nFind out the ID of the required network port, and then set the policy with openstack port set --qos-policy <qos-policy>. For example:# openstack --insecure port list\r\n+--------------------------------------+-----+----------------------------+\r\n| ID                                   | <\u00e2\u0080\u00a6> | Fixed IP Addresses         |\r\n+--------------------------------------+-----+----------------------------+\r\n| c0ea690f-4993-4467-afd5-5389016a0658 |     | ip_address='10.136.18.133' |\r\n+--------------------------------------+-----+----------------------------+\r\n# openstack --insecure port set --qos-policy policy1 c0ea690f-4993-4467-afd5-5389016a0658\n To assign a QoS policy to a  floating IP address\nFind out the ID of the required floating IP, and then set the policy with openstack floating ip set --qos-policy <qos-policy>. For example:# openstack --insecure floating ip list\r\n+--------------------------------------+---------------------+-----+\r\n| ID                                   | Floating IP Address | <\u00e2\u0080\u00a6> |\r\n+--------------------------------------+---------------------+-----+\r\n| 866203a2-4e1c-459f-807f-14ed563409f1 | 10.136.18.135       |     |\r\n+--------------------------------------+---------------------+-----+\r\n# openstack --insecure floating ip set --qos-policy policy1 866203a2-4e1c-459f-807f-14ed563409f1\nTo assign a QoS policy to all ports in a network\nFind out the ID or name of the required network, and then set the policy with openstack network set --qos-policy <qos-policy>. For example:# openstack --insecure network list\r\n+--------------------------------------+----------+-----+\r\n| ID                                   | Name     | <\u00e2\u0080\u00a6> |\r\n+--------------------------------------+----------+-----+\r\n| c6ee561e-9cf7-489b-bbab-7bca557ee7a5 | public   |     |\r\n+--------------------------------------+----------+-----+\r\n# openstack --insecure network set --qos-policy policy1 public\nSee also\n\nSetting the default QoS policy\n\nModifying QoS policy rules\n\nUnassigning QoS policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/assigning-qos-policies.html"
    },
    {
        "title": "Admin panel requirements",
        "content": "Admin panel requirements\n\nThe admin panel requires a Full HD monitor to be displayed correctly. \nThe admin panel has been tested to work at resolutions 1280x720 and higher in the following web browsers: latest Firefox, Chrome, Safari.\nThe admin and self-service panels only support TLS versions 1.2 and 1.3.\n\nSee also\n\nHardware recommendations\n\nServer requirements\n\nDisk requirements\n\nNetwork requirements and recommendations",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/admin-panel-requirements.html"
    },
    {
        "title": "Adding swap space",
        "content": "Adding swap space\nTo support the RAM overcommitment ratio, you need to add swap space. The swap size depends on the chosen RAM overcommitment ratio and  can be calculated by using the following formula:(total RAM \u00e2\u0080\u0093 RAM used for system) * (RAM overcommitment ratio \u00e2\u0080\u0093 1)\nTo better understand how the minimum swap size is calculated, consider the following example:\n\nThe total amount of physical RAM on a compute node is 24 GiB\n8 GiB of RAM is reserved for the system\nThe desired RAM overcommitment ratio is 1.5\n\nAccording to the formula, the minimum required swap size will be 8 GiB. After calculating the required swap size, proceed to configuring swap space by creating a swap file.\nLimitations\n\nAfter the swap file is created, its size cannot be modified.\n\nPrerequisites\n\nTo be able to create a swap file, the root directory must have 100 GiB of free space after the swap file creation. For example, to create a swap file of 8 GiB, ensure that at least 108 GiB is available in the root directory.\n\nTo create a swap file\nOn each node in the compute cluster, execute the configure-swap.sh script specifying the desired swap size:# /usr/libexec/vstorage-ui-agent/bin/configure-swap.sh -s 8192\nThe script creates a swap file, prepares the swap space, and adds the swap mount point to /etc/fstab.\nTo check that the swap file is successfully created, run:# swapon -s\r\nFilename               Type         Size    Used    Priority\r\n/dev/sda3              partition    8258556    0    -2\r\n/swapfile0             file         8389628    0    -3\nTo increase the swap size\nAdd another swap file by running:# /usr/libexec/vstorage-ui-agent/bin/configure-swap.sh -s 8192 --append\nWhat's next\n\nEnabling and disabling RAM overcommitment",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/adding-swap-space.html"
    },
    {
        "title": "Attaching external iSCSI storage",
        "content": "Attaching external iSCSI storage\nAs the Pure Storage iSCSI Driver is based on the iSCSI storage protocol, compute nodes must be configured to support multipathing.\nTo enable multipathing\nOn each compute node, configure and start the multipathing service:\n\nEdit the configuration file /etc/multipath.conf as follows:defaults {\r\n    user_friendly_names yes\r\n    find_multipaths yes\r\n    polling_interval 10\r\n}\r\ndevices {\r\n  device {\r\n    vendor \"PURE\"\r\n    product \"FlashArray\"\r\n    fast_io_fail_tmo 10\r\n    path_grouping_policy \"group_by_prio\"\r\n    failback \"immediate\"\r\n    prio \"alua\"\r\n    hardware_handler \"1 alua\"\r\n    max_sectors_kb 4096\r\n  }\r\n}\r\n\r\nblacklist {\r\n  device {\r\n    vendor \"QEMU\"\r\n    product \".*\"\r\n  }\r\n}\nFor more details about the multipathing configuration, refer to Linux Recommended Settings.\n\nLoad the kernel module:# modprobe dm-multipath\n\nLaunch and enable the multipathing service:\r\n# systemctl start multipathd; systemctl enable multipathd\n\nTo attach an external iSCSI storage\nUse the following command:vinfra service compute storage add <storage_name> --pure --params san_ip=<ip_address>,image_volume_cache_enabled=True\r\n                                   --secret-params pure_api_token=<token> --enable\r\n\nWhere:\n\n<storage_name> is a custom name of your external storage. The name may only contain letters, numbers, and underscores, and must be 3 to 64 characters long.\nsan_ip=<ip_address> is the IP address of the external storage to connect to.\nimage_volume_cache_enabled should be set to True to enable image-volume caching for the external storage, to improve the performance of creating a volume from an image.\n--secret-params is intended to be used for sensitive data, like passwords or tokens. For example, pure_api_token=<token> is the API token for a Pure Storage solution.\n\nWhen the --pure option is specified, it automatically sets the following parameters required for Pure Storage:\n\nvolume_backend_name to the external storage name specified by <storage_name>.\nvolume_driver to the name of the OpenStack volume driver, which is cinder.volume.drivers.pure.PureISCSIDriver.\nuse_multipath_for_image_xfer to True to allow multipathing.\n\nFor example, to add the external storage pure_storage with the IP address 10.10.10.11, run:# vinfra service compute storage add pure_storage --pure --params san_ip=10.10.10.11,image_volume_cache_enabled=True \\\r\n--secret-params pure_api_token=7acb5db8-d312-4f66-b076-f556d6fa1232 --enable\r\n\n\nEnsure that the data specified is valid. An incorrectly configured storage will lead to the critical state of the cinder-volume service and the node itself. However, all other operations on the node will not be affected.\n\nThe added external storage will appear in the vinfra service compute storage list output:# vinfra service compute storage list\r\n+--------------+-----------------------------------------------------------+-----------------------+---------+------------+\r\n| name         | params                                                    | secret_params         | enabled | configured |\r\n+--------------+-----------------------------------------------------------+-----------------------+---------+------------+\r\n| pure_storage | san_ip: 10.10.10.11                                       | pure_ip_token: ****** | True    | True       |\r\n|              | use_multipath_for_image_xfer: True                        |                       |         |            |\r\n|              | image_volume_cache_enabled: True                          |                       |         |            |\r\n|              | volume_backend_name: pure-storage                         |                       |         |            |\r\n|              | volume_driver: cinder.volume.drivers.pure.PureISCSIDriver |                       |         |            |\r\n+--------------+-----------------------------------------------------------+-----------------------+---------+------------+\nSee also\n\nAttaching external NFS storage\n\nDetaching external storages\n\nWhat's next\n\nCreating external storage policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/attaching-external-iscsi-storage.html"
    },
    {
        "title": "Assigning users to multiple domains",
        "content": "Assigning users to multiple domains\nBy using the vinfra tool, system administrators are able to create special service users that can be used by third-party applications to access the compute API with administrator privileges. These users cannot log in to the admin or self-service panels. Service users are similar to system administrators with the Compute permission: they exist only within the Default domain and can view and manage all objects in the compute cluster, including compute nodes. You can assign service users to domains, thus giving them ability to create compute objects in projects of these assigned domains (for example, to create a VM from a backup).\nService users can view virtual machines in all existing projects by specifying the all_tenants query parameter for the GET /servers request (refer to the OpenStack API documentation).\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo assign a service user to a domain\nUse the following command:vinfra domain user create --domain default --assign-domain <domain> compute <username>\n\n--assign-domain <domain>\n\nID or name of the domain to assign the service user to\n<username>\n\nService user name\n\nFor example, to create the service user my-service-user and assign it to the domains mydomain and mydomain2, run:# vinfra domain user create my-service-user --domain default --assign-domain mydomain compute \\\r\n--assign-domain mydomain2 compute\nTo check that the created service user is successfully assigned to the two domains, use the OpenStack client. For example, if the management node IP address is 10.136.16.227, run:# openstack --insecure --os-username my-service-user --os-user-domain-name \\\r\nDefault --os-auth-url=https://10.136.16.227:5000/v3 federation domain list\r\nPassword:\r\n+----------------------------------+---------+-----------+-------------+\r\n| ID                               | Enabled | Name      | Description |\r\n+----------------------------------+---------+-----------+-------------+\r\n| 2929ff42b1e64884a05dea3011862aed | True    | mydomain  |             |\r\n| 7e0d54797152424a9331ae904e220b88 | True    | mydomain2 |             |\r\n+----------------------------------+---------+-----------+-------------+\r\n\nYou can also view the list of all projects within the assigned domains by using this command:openstack --insecure --os-username <username> --os-user-domain-name Default --os-auth-url=https://<MN_IP_address>:5000/v3 federation project list\nTo unassign a service user from a domain\nUse the --unassign-domain <domain> option for the vinfra domain user set command. vinfra domain user set --domain default --unassign-domain <domain> <username>\n\n--unassign-domain <domain>\n\nID or name of the domain to unassign the service user from\n<username>\n\nService user name\n\nFor example, to unassign the service user my-service-user from the domain mydomain, run:# vinfra domain user set my-service-user --domain default --unassign-domain mydomain\nSee also\n\nConfiguring multitenancy\n\nManaging self-service users",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/assigning-users-to-multiple-domains.html"
    },
    {
        "title": "Attaching external NFS storage",
        "content": "Attaching external NFS storage\nThe Generic NFS Reference Driver allows you to mount multiple NFS shares to your compute cluster and use them as external NFS storages.\nTo attach an external NFS storage\nUse the following command:vinfra service compute storage add <storage_name> --nfs --params nas_host=<nas_ip_address>,nas_share_path=<share_path>\r\n                                   [--nfs-mount-options <options>] --enable\r\n\nWhere:\n\n<storage_name> is a custom name of your external storage. The name may only contain letters, numbers, and underscores, and must be 3 to 64 characters long.\nnas_host=<nas_ip_address> is the IP address of the external storage to connect to.\nnas_share_path=<share_path> is the root export path of the NFS share.\n\n--nfs-mount-options <options> is a comma-separated list of mount options for NFS compute storages, with additional flags separated by spaces. For example: lookupcache=none,nfsvers=4,minorversion=0,timeo=150,retrans=3 -m -s. To see a full list of mount options, refer to the mount and nfs man pages.\nNote that if nfs_shares_config is used, these mount options are applied to every share listed in the configuration file, unless they are overwritten in the configuration file.\n\nSetting the lookupcache policy to all is not recommended as it leads to issues with accessing volume .info files.\n\nWhen the --nfs option is specified, it automatically sets the following parameters required for NFS storage:\n\nvolume_backend_name to the external storage name specified by <storage_name>.\nvolume_driver to the name of the OpenStack volume driver, which is cinder.volume.drivers.nfs.NfsDriver.\nnfs_mount_options to lookupcache=pos to ensure that the NFS client revalidates negative entries in the directory entry cache.\nnfs_mount_point_base to the directory to mount the NFS share to, which is /mnt/compute/storages.\nnas_secure_file_permissions to False to create volumes with open permissions.\nnfs_qcow2_volumes to True to create volumes as QCOW2 files.\nnfs_snapshot_support to True and nas_secure_file_operations to False to enable support for snapshots.\nnfs_sparsed_volumes to True to create volumes as sparsed files, which take no space.\n\nFor example, to add the external storage nfs_storage with the IP address 10.10.10.12 and the share path /myshare, run:# vinfra service compute storage add nfs_storage --nfs --params nas_host=10.10.10.12,nas_share_path=/myshare --enable\r\n\n\nEnsure that the data specified is valid. An incorrectly configured storage will lead to the critical state of the cinder-volume service and the node itself. However, all other operations on the node will not be affected.\n\nThe added external storage will appear in the vinfra service compute storage list output:# vinfra service compute storage list\r\n+-------------+----------------------------------------------------+---------------+---------+------------+\r\n| name        | params                                             | secret_params | enabled | configured |\r\n+-------------+----------------------------------------------------+---------------+---------+------------+\r\n| nfs_storage | nas_host: 10.10.10.12                              |               | True    | True       |\r\n|             | nas_secure_file_operations: 'False'                |               |         |            |\r\n|             | nas_secure_file_permissions: 'False'               |               |         |            |\r\n|             | nas_share_path: /myshare                           |               |         |            |\r\n|             | nfs_mount_options: lookupcache=pos                 |               |         |            |\r\n|             | nfs_mount_point_base: /mnt/compute/storages        |               |         |            |\r\n|             | nfs_qcow2_volumes: 'True'                          |               |         |            |\r\n|             | nfs_snapshot_support: 'True'                       |               |         |            |\r\n|             | nfs_sparsed_volumes: 'True'                        |               |         |            |\r\n|             | volume_backend_name: nfs-storage                   |               |         |            |\r\n|             | volume_driver: cinder.volume.drivers.nfs.NfsDriver |               |         |            |\r\n+-------------+----------------------------------------------------+---------------+---------+------------+\nSee also\n\nAttaching external iSCSI storage\n\nDetaching external storages\n\nWhat's next\n\nCreating external storage policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/attaching-external-nfs-storage.html"
    },
    {
        "title": "Attaching an IPMI virtual drive",
        "content": "Attaching an IPMI virtual drive\nTo attach an IPMI virtual drive\n\nDownload IPMIView, if required, and launch it.\nEnsure that the server  is turned on.\n\nIn the IPMIView window, click File > New\u00e2\u0080\u00a6 > System and enter the server\u00e2\u0080\u0099s IPMI IP address. Optionally, change the system name and add a description.\n\nIn the IPMI Domain section, double-click the newly added system name.\n\nAfter the connection is established, navigate to the system KVM console tab and click Launch KVM console.\n\nOn the system Login tab, specify the login ID and password to access the server. Then, click Login.\n\nIn the Java iKVM Viewer window, attach the image to the server as a virtual CD-ROM:\n\nClick Virtual Media > Virtual Storage.\nIn the Virtual Storage window, select ISO File in Local Drive Type, click Open Image, and then select the distribution image from your local machine.\n\nClick OK.\n\nReboot the server by clicking Power Control > Set Power Reset.\n\nWhat's next\n\nInstalling in the attended mode",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/attaching-an-ipmi-virtual-drive.html"
    },
    {
        "title": "Attaching and detaching volumes",
        "content": "Attaching and detaching volumes\nLimitations\n\nYou can only attach and detach non-boot volumes.\n\nPrerequisites\n\nA volume is created, as described in Creating and deleting volumes.\nTo be able to use volumes attached to VMs, they must be initialized inside the guest OS by standard means.\n\nTo attach a volume to a virtual machine\n\nAdmin panel\n\nOn the Compute > Storage > Volumes screen, click an unused volume.\nOn the volume right pane, click Attach.\n\nIn the Attach volume window, select the VM from the drop-down list, and then click Done.\n\nCommand-line interface\nUse the following command:vinfra service compute server volume attach --server <server> <volume>\r\n\n\n--server <server>\n\nVirtual machine ID or name\n<volume>\n\nVolume ID or name\n\nFor example, to attach the available volume with the ID e4cb5363-1fb2-41f5-b24b-18f98a388cba to the virtual machine myvm, run:# vinfra service compute server volume attach e4cb5363-1fb2-41f5-b24b-18f98a388cba --server myvm\r\n+--------+--------------------------------------+\r\n| Field  | Value                                |\r\n+--------+--------------------------------------+\r\n| device | /dev/vdb                             |\r\n| id     | e4cb5363-1fb2-41f5-b24b-18f98a388cba |\r\n+--------+--------------------------------------+\r\n\nThe name of the new device will be shown in the command output. To see all of the VM volumes, run:# vinfra service compute server volume list --server myvm\r\n+--------------------------------------+----------+\r\n| id                                   | device   |\r\n+--------------------------------------+----------+\r\n| e4cb5363-1fb2-41f5-b24b-18f98a388cba | /dev/vdb |\r\n| b325cc6e-8de1-4b6c-9807-5a497e3da7e3 | /dev/vda |\r\n+--------------------------------------+----------+\r\n\n\nTo detach a volume from a virtual machine\n\nAdmin panel\n\nOn the Compute > Storage > Volumes screen, click a volume that is in use.\nIf the VM is stopped, click Detach on the volume right pane.\n\nIf the VM is running, click Force detach on the volume right pane.\n\nThere is a risk of data loss.\n\nCommand-line interface\nUse the following command:vinfra service compute server volume detach [--force] --server <server> <volume>\r\n\n\n--server <server>\n\nVirtual machine ID or name\n--force\n\nDetach a volume without checking if either the volume or server exists. When specifying the volume and server, use their IDs. No name lookup is performed.\n<volume>\n\nVolume ID or name\n\nFor example, to detach the volume with the ID e4cb5363-1fb2-41f5-b24b-18f98a388cba from the virtual machine myvm, run:# vinfra service compute server volume detach e4cb5363-1fb2-41f5-b24b-18f98a388cba \\\r\n--server 871fef54-519b-4111-b18d-d2039e2410a8\n\nSee also\n\nResizing volumes\n\nChanging the storage policy for volumes\n\nCloning volumes\n\nManaging volume snapshots",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server volume attach --server <server> <volume>\r\n\n\n--server <server>\n\nVirtual machine ID or name\n<volume>\n\nVolume ID or name\n\nFor example, to attach the available volume with the ID e4cb5363-1fb2-41f5-b24b-18f98a388cba to the virtual machine myvm, run:# vinfra service compute server volume attach e4cb5363-1fb2-41f5-b24b-18f98a388cba --server myvm\r\n+--------+--------------------------------------+\r\n| Field  | Value                                |\r\n+--------+--------------------------------------+\r\n| device | /dev/vdb                             |\r\n| id     | e4cb5363-1fb2-41f5-b24b-18f98a388cba |\r\n+--------+--------------------------------------+\r\n\nThe name of the new device will be shown in the command output. To see all of the VM volumes, run:# vinfra service compute server volume list --server myvm\r\n+--------------------------------------+----------+\r\n| id                                   | device   |\r\n+--------------------------------------+----------+\r\n| e4cb5363-1fb2-41f5-b24b-18f98a388cba | /dev/vdb |\r\n| b325cc6e-8de1-4b6c-9807-5a497e3da7e3 | /dev/vda |\r\n+--------------------------------------+----------+\r\n\n",
                "title": "To attach a volume to a virtual machine"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server volume detach [--force] --server <server> <volume>\r\n\n\n--server <server>\n\nVirtual machine ID or name\n--force\n\nDetach a volume without checking if either the volume or server exists. When specifying the volume and server, use their IDs. No name lookup is performed.\n<volume>\n\nVolume ID or name\n\nFor example, to detach the volume with the ID e4cb5363-1fb2-41f5-b24b-18f98a388cba from the virtual machine myvm, run:# vinfra service compute server volume detach e4cb5363-1fb2-41f5-b24b-18f98a388cba \\\r\n--server 871fef54-519b-4111-b18d-d2039e2410a8\n",
                "title": "To detach a volume from a virtual machine"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Volumes screen, click an unused volume.\nOn the volume right pane, click Attach.\n\nIn the Attach volume window, select the VM from the drop-down list, and then click Done.\n\n\n\n\n\n\n",
                "title": "To attach a volume to a virtual machine"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Volumes screen, click a volume that is in use.\nIf the VM is stopped, click Detach on the volume right pane.\n\nIf the VM is running, click Force detach on the volume right pane.\n\nThere is a risk of data loss.\n\n\n\n",
                "title": "To detach a volume from a virtual machine"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/attaching-and-detaching-volumes.html"
    },
    {
        "title": "Backing up and restoring management database",
        "content": "Backing up and restoring management database\nThe product database is stored on the management node (the one with the admin panel) and backed up automatically. It is also replicated to the nodes included in the HA configuration, if high availability is enabled for the management node.\nThis management database contains the cluster configuration, service management and monitoring parameters, and metadata of compute objects, such as projects, domains, users, virtual machines, images, flavors, volumes, networks, and other. The database does not contain user data located on storage disks (for example, volumes, volume snapshots, and compute images), as well as data stored on local root disks (for example, logs, metrics data, and service internal data).\nBackups of the management database are created automatically via a daily cron job that starts at 3:00 a.m. If the management node is not highly available, restoring such a backup recovers the node in case of failure or database corruption. Backup files are stored in the /mnt/vstorage/webcp/backup/ directory. The retention policy for management node backups is the following:\n\nAll backups created within the last day are kept.\nFrom backups created within the last 7 days, the newest for each day is kept.\nFrom backups created within the last 7-14 days, the oldest one is kept.\nFrom backups created within the last 14-45 days, the oldest one for each week is kept.\nBackups older than 45 days are deleted.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backing-up-and-restoring-management-database.html"
    },
    {
        "title": "Backing up management database",
        "content": "Backing up management database\nTo back up the database manually \n\nAdmin panel\nGo to Settings > System settings > Management database backup, and then click Back up now.\n\nOnce the backup is complete, the date and time in the Last backup time field will be refreshed.\n\nDo not rename the backup file! Otherwise you will not be able to restore the management database from it.\n\nCommand-line interface\nUse the following command:vinfra cluster backup create\r\n\nYou can view the details of the last cluster backup and the ID of the ongoing backup task, if any, in the vinfra cluster backup show output:# vinfra cluster backup show\r\n+----------------------+-----------------------------+\r\n| Field                | Value                       |\r\n+----------------------+-----------------------------+\r\n| last_backup_date     | 2019-08-21T15:41:24+00:00   |\r\n| last_backup_location | /mnt/vstorage/webcp/backup/ |\r\n| ready                | True                        |\r\n| tasks                | []                          |\r\n+----------------------+-----------------------------+\r\n\n\nWhat's next\n\nRestoring management database from backup\n\nRestoring management database with the compute cluster",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster backup create\r\n\nYou can view the details of the last cluster backup and the ID of the ongoing backup task, if any, in the vinfra cluster backup show output:# vinfra cluster backup show\r\n+----------------------+-----------------------------+\r\n| Field                | Value                       |\r\n+----------------------+-----------------------------+\r\n| last_backup_date     | 2019-08-21T15:41:24+00:00   |\r\n| last_backup_location | /mnt/vstorage/webcp/backup/ |\r\n| ready                | True                        |\r\n| tasks                | []                          |\r\n+----------------------+-----------------------------+\r\n\n",
                "title": "To back up the database manually "
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nGo to Settings > System settings > Management database backup, and then click Back up now.\n\n\n\n\nOnce the backup is complete, the date and time in the Last backup time field will be refreshed.\n\nDo not rename the backup file! Otherwise you will not be able to restore the management database from it.\n\n",
                "title": "To back up the database manually "
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backing-up-management-database.html"
    },
    {
        "title": "Backup storage requirements",
        "content": "Backup storage requirements\nBackup storage requirements depend on the destination storage you choose. Backup storage supports the following backup destinations:\n\nVirtuozzo Hybrid Infrastructure storage clusters\nNFS shares\nPublic clouds, including a number of S3 solutions, as well as Microsoft Azure, OpenStack Swift, and Google Cloud Platform\n\nWith the public cloud destination, you can also deploy backup storage inside virtual machines.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-requirements.html"
    },
    {
        "title": "Best practices for cluster security",
        "content": "Best practices for cluster security\nTo secure your Virtuozzo Hybrid Infrastructure cluster and prevent possible compromise of the system, follow these guidelines:\n\nUse a strong password for the default admin user. You can change the existing password after the product installation, as described in Managing admin panel users.\n\nUse a strong password for the root user on each cluster node. You can change the existing password for a node by using the passwd command. For example:# passwd\r\nChanging password for user root.\r\nNew password:\r\nRetype new password:\r\npasswd: all authentication tokens updated successfully.\n\nUse a separate and isolated network for the admin panel and SSH access. You can assign traffic types to infrastructure networks and assign networks to node network interfaces, as described in Managing traffic types and Changing network interface parameters.\nConfigure inbound firewall rules to limit the admin panel and SSH access, as described in Configuring inbound firewall rules.\n\nProhibit password authentication for SSH access. You can add an SSH key, as described in Securing root access to cluster nodes over SSH.\nOnce the key is added, you can access nodes via SSH and disable password authentication in the sshd configuration file:\n\nOpen the /etc/ssh/sshd_config file for editing and set PasswordAuthentication to no:\r\n# vi /etc/ssh/sshd_config\r\n\n\nCheck that your changes are successfully applied:# grep ^PasswordAuthentication /etc/ssh/sshd_config\r\nPasswordAuthentication no\r\n\n\nRestart the service:# systemctl restart sshd\n\nFor more information, refer to the sshd_config manual page.\n\n[For backup storage] Configure the latest TLS version and only desired ciphers to be used for connections to backup storage, as described in Changing TLS configuration for backup storage.\n[For object storage] Configure desired TLS version and ciphers to be used for connections to object storage, as described in Changing the TLS configuration for S3.\n\nUse only necessary traffic types in a public network, according to the deployed services. You can reassign traffic types, as described in Managing traffic types.\nFor more information about network ports used for different services and associated with traffic types, refer to Network ports.\n\nSee also\n\nAccessing the admin panel via SSL\n\nEnabling data encryption",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/best-practices-for-cluster-security.html"
    },
    {
        "title": "Backup storage in a public cloud",
        "content": "Backup storage in a public cloud\n\nGeneral requirements are listed in General requirements.\n\nNote the additional requirements for backup storage with the public cloud destination:\n\nWhen working with public clouds, Backup Gateway uses the local storage as the staging area as well as to keep service information. It means that the data to be uploaded to a public cloud is first stored locally and only then sent to the destination. For this reason, it is vital that the local storage is persistent and redundant so the data does not get lost. There are multiple ways to ensure the persistence and redundancy of local storage. You can deploy Backup Gateway on multiple cluster nodes and select a good redundancy mode. If  Virtuozzo Hybrid Infrastructure with the gateway is deployed on a single physical node, you can make the local storage redundant by replicating it among local disks. If Virtuozzo Hybrid Infrastructure with the gateway is deployed in a virtual machine, make sure it is made redundant by the virtualization solution it runs on.\n\r\n        Make sure the local storage cluster has plenty of logical space for staging. For example, if you perform backup daily, provide enough space for at least 1.5 days\u00e2\u0080\u0099 worth of backups. If the daily backup total is 2 TB, provide at least 3 TB of logical space. The required raw storage will vary depending on the encoding mode: 9 TB (3 TB per node) in the 1+2 mode, 5 TB (1 TB per node) in the 3+2 mode, etc.\r\n        \nA separate object container is required for each backup storage cluster.\r\n    \n\nTo better understand how to calculate the hardware configuration for backup storage with the public cloud destination, consider the following example with RAM and CPU reservations.\nIf you have 1 virtual machine (1 system+metadata disk and 1 storage disk) and want to use the 1+0 encoding mode with the disk failure domain, refer to the table below for the calculations. Note that in this configuration redundancy is provided by the virtualization solution.\n\nBackup storage in a public cloud                    \n\nService\nThe only node\n\nSystem\n4.5 GB,\t3.3 cores\n\nStorage services\n\n1 storage disk1 The storage disk is used for data staging. and 1 metadata on system disk (each takes 0.5 GB and 0.2 cores), that is 1 GB and 0.4 cores in total\n\nBackup Gateway\n1 GB, 0.5 cores\n\nService reservations\n6.5 GB of RAM and\r\n4.2 cores\n\nMinimum hardware configuration\n8 GB of RAM and 4 cores\n\nRecommended hardware configuration\n16 GB2 All extra RAM is used to cache disk reads. of RAM and 6 cores\n\nSee also\n\nBackup storage in a virtual machine\n\nBackup storage network requirements\n\nProvisioning backup storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-in-a-public-cloud.html"
    },
    {
        "title": "Backup storage alerts",
        "content": "Backup storage alerts\nThe following backup storage alerts are generated and displayed in the admin panel:\n\n Backup storage SSL certificate will expire in 21 days\n\nThe <type> certificate will expire in 21 days. Path: <path>. Registration name: <reg_name>.\n\nUpdate the certificate, as described in Updating registration certificates.\n\nBackup storage SSL certificate will expire in 14 days\n\nThe <type> certificate will expire in 14 days. Path: <path>. Registration name: <reg_name>.\n\nUpdate the certificate, as described in Updating registration certificates.\n\nBackup storage SSL certificate will expire in 7 days\n\nThe <type> certificate will expire in 7 days. Path: <path>. Registration name: <reg_name>.\n\nUpdate the certificate, as described in Updating registration certificates.\n\n Backup storage SSL certificate has expired\n\nThe <type> certificate has expired. Path: <path>. Registration name: <reg_name>.\n\nUpdate the certificate, as described in Updating registration certificates.\n\nBackup storage CRL is not up to date\n\nThe CRL has not been updated for more than 2 days. Path: <path>. Registration name: <reg_name>.\n\nThe URI to the CRL is not accessible, check the CRL distribution points as follows:# cd /mnt/vstorage/vols/acronis-backup/certs.<reg_name>\r\n# openssl x509 -in reg.crt -text -noout | grep URI\r\n# openssl x509 -in dc.crt -text -noout | grep URI\r\n# openssl x509 -in abgw.pem -text -noout | grep URI\nOne distribution point can be used for several or even for all certificates.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n Backup storage throttling is activated\n\nBackup storage started to throttle write operations due to the lack of free space. Visit https://kb.acronis.com/content/62823 to learn how to troubleshoot this issue.\n\nCheck the Storage services > Backup storage > Overview > Append throttle chart, as described in Monitoring backup storage.\nAdd more space to the backup storage.\n\nFor more details, refer to the Knowledge Base article \"Throttling on backup storage\".\n\n Different number of collaborating backup storage services\n\nSome backup storage services report a different number of collaborating services. Please contact the technical support.\n\n Attempt to use migrated accounts\n\nOne or more attempts to use migrated accounts detected for the last 24 hours. Please contact the technical support.\n\n Storage I/O error\n\nOne or more errors detected during storage I/O operations for the last 24 hours. Please contact the technical support.\n\n Backup storage has high replica open error rate\n\nBackup storage has the error rate when opening replica files \"<error label>\" higher than 5%.\n\nCheck the backup storage logs at /var/log/abgw/abgw.log.zst on the primary geo-replication cluster.\n\n Backup storage has high replica write error rate\n\nBackup storage has the error rate when writing replica files \"<error label>\" higher than 5%.\n\nCheck the backup storage logs at /var/log/abgw/abgw.log.zst on the primary geo-replication cluster.\n\n Backup storage has high replica removal error rate\n\nBackup storage has the error rate when removing secondary replica files \"<error label>\" higher than 5%.\n\nCheck the backup storage logs at /var/log/abgw/abgw.log.zst on the primary geo-replication cluster.\n\nWhat's next\n\nGetting technical support\n\nSee also\n\nInfrastructure alerts\n\nCore storage alerts\n\nObject storage alerts\n\nBlock storage alerts\n\nCompute alerts",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-alerts.html"
    },
    {
        "title": "Backup storage on the local cluster",
        "content": "Backup storage on the local cluster\n\nGeneral requirements are listed in General requirements.\n\nTo better understand how to calculate the hardware configuration for backup storage with the local cluster destination, consider the following examples with RAM and CPU reservations.\nExample 1. If you have 1 physical node (1 system+metadata disk and 5 storage disks) and want use the 3+2 encoding mode with the disk failure domain, refer to the table below for the calculations. Note that this configuration has redundancy only in case of a disk failure, the node failure will lead to data unavailability and potential data loss.\n\n1 node for backup storage on the local cluster                    \n\nService\nThe only node\n\nSystem\n4.5 GB,\t3.3 cores\n\nStorage services\n\n5 storage disks and 1 metadata on system disk (each takes 0.5 GB and 0.2 cores), that is 3 GB and 1.2 cores in total\n\nBackup Gateway\n1 GB, 0.5 cores\n\nService reservations\n7.5 GB of RAM and\r\n4.6 cores\n\nMinimum hardware configuration\n12 GB of RAM and 6 cores\n\nRecommended hardware configuration\n16 GB1 All extra RAM is used to cache disk reads. of RAM and 6 cores\n\nExample 2. If you have 5 nodes (1 system disk, 1 metadata disk, and 10 storage disks) and want to use them for backup storage, refer to the table below for the calculations. Note that three nodes are used for the management node high availability, and each of them meets the requirements for the management node.\n\n5 nodes for backup storage on the local cluster                   \n\nService\nManagement nodes (nodes 1-3)\nSecondary nodes (nodes 4-5)\n\nSystem\n4.5 GB,\t3.3 cores\n1.5 GB,\t1.1 cores\n\nStorage services\n10 storage disks and 1 metadata disk (each takes 0.5 GB and 0.2 cores), that is 5.5 GB and 2.2 cores in total\n10 storage disks and 1 metadata disk (each takes 0.5 GB and 0.2 cores), that is 5.5 GB and 2.2 cores in total\n\nBackup Gateway\n1 GB, 0.5 cores\n1 GB, 0.5 cores\n\nService reservations\n11 GB of RAM and\r\n6 cores\n8 GB of RAM and\r\n3.8 cores\n\nMinimum hardware configuration\n12 GB of RAM and 6 cores\n8 GB of RAM and 4 cores\n\nRecommended hardware configuration\n242 All extra RAM is used to cache disk reads. GB of RAM and 8 cores\n16 GB3 All extra RAM is used to cache disk reads. of RAM and 6 cores\n\nSee also\n\nBackup storage network requirements\n\nProvisioning backup storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-on-the-local-cluster.html"
    },
    {
        "title": "Backup storage on an NFS share",
        "content": "Backup storage on an NFS share\n\nGeneral requirements are listed in General requirements.\n\nTo better understand how to calculate the hardware configuration for backup storage with the NFS share destination, consider the following example with RAM and CPU reservations.   \nIf you have 1 node (1 system+metadata disk and 1 storage disk) and want to use it for backup storage, refer to the table below for the calculations.\n\nBackup storage on an NFS share                   \n\nService\nThe only node\n\nSystem\n4.5 GB,\t3.3 cores\n\nStorage services\n\n1 storage disk1 Though data is not stored locally, it is required to have one  100+ GB disk the with storage role as it will be used for internal needs., 1 metadata on system disk (each takes 0.5 GB and 0.2 cores), that is 1 GB and 0.4 cores in total\n\nBackup Gateway\n1 GB, 0.5 cores\n\nService reservations\n6.5 GB of RAM and\r\n4.2 cores\n\nMinimum hardware configuration\n8 GB of RAM and 4 cores\n\nRecommended hardware configuration\n8 GB of RAM and 6 cores\n\nSee also\n\nBackup storage network requirements\n\nProvisioning backup storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-on-an-nfs-share.html"
    },
    {
        "title": "Backup storage metrics",
        "content": "Backup storage metrics\nMetrics used for monitoring backup storage are configured in the Prometheus recording rules and can be found in the /var/lib/prometheus/rules/abgw.rules file on any node in the cluster. The most important of these metrics are described in the table:\n\nMetric\nDescription\n\nFES object counters\n\nabgw_accounts\n\nNumber of accounts backup storage is currently working with (that is, number of accounts with open backup archives)\n\nabgw_files\n\nNumber of currently open backup archives. Backup archives are open for reading and writing only during a backup operation. Other operations, such as restoring, browsing, and validation, open backup archives only for reading.\n\nabgw_conns[proto]\n\nNumber of current connections between backup storage and clients. The value is an array of counters. Details of the backup storage protocol (V1/V2) are available.\n\nConnection counters\n\nabgw_conns_total\n\nTotal number of connections between backup storage and clients since the service startup\n\nabgw_client_conns_cur[name]\n\nNumber of currently connected clients, divided by type\n\nabgw_client_conns_total[name]\n\nTotal number of clients since the service startup, divided by type\n\nCertificate errors and expiration times\n\nabgw_verify_certs_errors_total[err]\n\nTotal number of certificate verification errors since the service startup, divided by error type\n\nabgw_next_certificate_expiration[path]\n\nExpiration date of backup storage certificates\n\nabgw_cert_update_fail_total\n\nNumber of failed attempts to update the certificate revocation list. The list is required to correctly apply a new quota in Acronis Cyber Protect Cloud, when the current customer certificate is revoked and a new certificate is requested.\n\nabgw_crl_download_fail_total\n\nNumber of failed attempts to download the certificate revocation list. The list is required to correctly apply a new quota in Acronis Cyber Protect Cloud, when the current customer certificate is revoked and a new certificate is requested.\n\nBackup storage protocol V1 request histograms and counters\n\nabgw_read_reqs_total\n\nNumber of read requests since the service startup\n\nabgw_write_reqs_total\n\nNumber of write requests since the service startup\n\nabgw_req_errs_total[req][err]\n\nArray with request errors, divided by request type and error codes\n\nabgw_req_latency_ms[req]\n\nHistogram with request latency\n\nBackup storage protocol V2 request histograms and counters\n\nabgw_v2_ireq_errs_total[req][err]\n\nNumber of read requests since the service startup\n\nabgw_v2_ireq_latency_ms[req][lat]\n\nNumber of write requests since the service startup\n\nabgw_v2_ereq_errs_total[req][err]\n\nArray with request errors, divided by request type and error codes\n\nabgw_v2_ereq_latency_ms[req][err]\n\nHistogram with request latency\n\nByte counters\n\nabgw_read_bytes_total[proxied]\n\nNumber of bytes read from a disk since the service startup. The proxied parameter shows data read via a reverse proxy.\n\nabgw_write_bytes_total[proxied]\n\nNumber of bytes written to a disk since the service startup. The proxied parameter shows data written via a reverse proxy.\n\nabgw_write_rollback_bytes_total\n\n Size of data overwritten by backup storage per client's request when backup storage could not confirm to the client that data was already written. The metric is used only for the backup storage protocol V1 and legacy backup clients.\n\nFile operation and I/O operation metrics\n\nabgw_file_lookup_errs_total[err]\n\nNumber of failed attempts to open files or find already open files, divided by error codes\n\nabgw_fop_latency_ms_bucket[fop][proxied][err]\n\nHistogram with the sum of file operation latency, divided by operation type (read, write, sync, stat), proxied or not, by error number, and other file operations\n\nabgw_iop_latency_ms_bucket[iop][proxied][err]\n\nHistogram with I/O operation latency, divided by operation type, proxied or not, and by error number\n\nabgw_io_limiting_failures_total[type]\n\nNumber of failed I/O requests to backup storage since the service startup, due to poor performance of the underlying storage\n\nabgw_iop_wd_timeouts[iop]\n\nNumber of file operations that take more than two minutes, divided by operation type\n\nMigration metrics\n\nabgw_account_pull_errs_total[err]\n\nNumber of failed attempts to retrieve the account list by the destination backup storage from the source backup storage before the migration start\n\nabgw_nr_files_to_pull\n\nNumber of files to migrate from the source backup storage to the destination backup storage (includes all files for which migration is not completed)\n\nabgw_pull_backlog_bytes\n\nNumber of bytes on the source backup storage that are not yet migrated to the destination backup storage\n\nabgw_pull_progress_bytes_total\n\nNumber of bytes on the destination backup storage that are already migrated from the source backup storage since the service startup\n\nabgw_file_migration_source_open_errs_total[err]\n\nNumber of failed attempts to open files for migration on the source backup storage since the service startup\n\nabgw_file_migration_source_read_errs_total[err]\n\nNumber of failed attempts to read files  for migration on the source backup storage since the service startup\n\nabgw_nr_accounts_pull_pending\n\nNumber of accounts that are awaiting migration\n\nabgw_nr_accounts_pull_started\n\nNumber of accounts that are in the process of migration\n\nabgw_nr_accounts_pull_errors\n\nNumber of accounts that have migration errors in at least one file\n\nObject storage and geo-replication metrics\n\nabgw_push_backlog_bytes[ostor, replica]\n\nNumber of bytes to be written to the object destination storage, or to the secondary cluster in case of geo-replication\n\nabgw_push_progress_bytes_total[ostor, replica]\n\nNumber of bytes written to the object destination storage, or to the secondary cluster in case of geo-replication. This metric helps to understand the speed of data replication or copying.\n\nabgw_push_replica_errs_total[err]\n\nNumber of failed attempts to write files to the object destination storage, or to the secondary cluster in case of geo-replication, since the service startup, divided by error type\n\nabgw_replica_integrity_checks_fail_total\n\nNumber of corrupted replicas on the secondary cluster since the service startup\n\nabgw_file_replica_auto_errs_total[err]\n\nNumber of geo-replication errors for new files (created after configuring geo-replication) since the service startup, divided by error type\n\nabgw_file_replica_open_errs_total[err]\n\nNumber of failed attempts by the primary cluster to open files for writing on the secondary cluster since the service startup, divided by error code\n\nabgw_rm_file_push_errs_total[err]\n\nNumber of errors occurred when removing files from the secondary cluster since the service startup, divided by error type. A secondary replica is deleted after disabling replication on the primary cluster.\n\nabgw_push_replica_total_size_by_brand[brand]\n\nTotal disk space occupied by replication-enabled files on the primary cluster, grouped by brand ID.\n\nabgw_push_progress_by_brand[brand]\n\nCurrent number of bytes successfully replicated from the primary cluster to the secondary one, grouped by brand ID.\n\nObject destination storage metrics\n\nabgw_ostor_used_space_bytes\n\nSpace size used by all backup archives, including data and unused space, on the object destination storage\n\nabgw_nr_ostor_sequence_mismatch_total\n\nNumber of files failed to be opened by backup storage due to their version mismatch on the object destination storage\n\nabgw_ostor_garbage_bytes\n\nUnused space size inside all backup archives that is not yet physically cleaned up on the object destination storage\n\nContainer archive validation results\n\nabgw_containers_validate_segments_fail_total\n\nNumber of archives with failed validation (segments) on the NFS and object destination storage\n\nabgw_containers_validate_trees_fail_total\n\nNumber of archives with failed validation (trees) on the NFS and object destination storage\n\nOther metrics\n\nabgw_append_throttle_delay_ms_total\n\nTotal sum of delays injected since the service startup. The metric helps to understand if throttling is enabled for backup storage.\n\nabgw_iop_ebusy\n\nNumber of I/O errors for open file operations since the service startup\n\nHistogram metrics with the \"_bucket\" suffix have corresponding metrics ending with \"_sum\" and \"_counter\", for example: \n\nabgw_iop_latency_ms_bucket shows the current measurement for I/O operation latency per bucket\nabgw_iop_latency_ms_count shows the total sum of all measurements for I/O operation latency per bucket\nabgw_iop_latency_ms_sum shows the number of stored measurements for I/O operation latency per bucket\n\nSee also\n\nCore storage metrics\n\nObject storage metrics\n\nCompute metrics\n\nCluster update metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-metrics.html"
    },
    {
        "title": "Backup storage in a virtual machine",
        "content": "Backup storage in a virtual machine\nVMware vSphere is the only officially supported hypervisor for running Virtuozzo Hybrid Infrastructure. Do not use other virtualization platforms as it may result in data corruption.\nTo be able to run the Virtuozzo Hybrid Infrastructure on VMware vSphere, make sure the following requirements are met:\n\nVMware vSphere version: 6.7 and newer\nVM version: 14 and newer\nThe host should have enough memory. At least 8 GB of RAM is required for a node with one storage disk running Backup Gateway.\n\nThe vSphere datastore should have enough free storage space. Each virtual machine occupies at least 425 GB (two 200 GB storage disks and a 25 GB system disk). The Virtuozzo Hybrid Infrastructure template also takes up about 35 GB. The maximum recommended size for one virtual disk is 16 TB.\n\nSee also\n\nBackup storage in a public cloud\n\nBackup storage network requirements\n\nProvisioning backup storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/backup-storage-in-a-virtual-machine.html"
    },
    {
        "title": "Cache configuration",
        "content": "Cache configuration\nSupported device types\nCurrently supported drives include HDD, SSD, and NVMe devices. Their characteristics are described in the table below.\n\nType\nCost\nPerformance\nInterface and form-factor\n\nHard disk drives\r\n(HDD)\nLow\n\nUp to 200 MB/s\nTens/hundreds IOPS\n\nSAS or SATA\n\nSolid-state drives (SSD)\nAverage\n\nUp to 600 MB/s\nTens of thousands IOPS\n\nSAS or SATA\n\nNon-volatile memory express (NVMe)\nHigh\n\n\tFrom 1 to 10 GB/s\nHundreds of thousands IOPS\n\n2.5\" U.2, PCIe Add-In-Card (AIC), or M.2\n\nPMem or NVRAM devices are not officially supported.\n\nThe amount and type of cache devices supported in your cluster should be checked per cluster node. In order to be of any use, devices that provide acceleration must be faster than the underlying devices. For this reason, the possible combinations of cache and capacity devices are the following:\n\nCache devices configured in RAID1 mirroring are not officially supported.\n\nIt is recommended that all capacity devices in the same storage tier should be identical in terms of technology and size. Otherwise, there may be unpredictable performance and behavior in case of a hardware failure. Moreover, all cluster nodes should offer the same amount of storage. If this requirement is not met, the storage space in the cluster will be limited by the smallest node.\nA similar recommendation applies to cache devices. As the writing speed is constrained by the slowest device in the cluster, we strongly recommend using cache devices of the same technology and size.\nChoosing a cache device\nAs all the data ingested in the system goes through cache devices, the choice of a cache device should be based not only on speed, but also on device endurance. Device endurance is measured in two ways:\n\nDrive Writes per Day (DWPD) measures the number of times the device can be completely overwritten each day, to reach the expected device end-of-life (usually five years).\nTerabytes Written (TBW) measures the expected amount of data that can be written before the device fails.\n\nBoth parameters are equivalent and should be carefully evaluated. For example, you have a 1-TB flash drive with 1 DPWD, that means you can write 1 TB into it every day over its lifetime. If its warranty period is five years, that works out to 1 TB per day * 365 days/year * 5 years = 1825 TB of cumulative writes, after which the drive usually will have to be replaced. Thus, the drive\u00e2\u0080\u0099s TBW will be 1825.\nThe DWPD of a typical consumer-grade SSD drive can be as low as 0.1, while a high-end datacenter-grade flash drive can have up to 60 DWPD. For a cache device, the recommended minimum is 10 DWPD.\nAnother parameter to consider is power loss protection of the device. Some consumer-grade flash drives are known to silently ignore data flushing requests, which may lead to data loss in case of a power outage. Examples of such drives include OCZ Vertex 3, Intel 520, Intel X25-E, and Intel X-25-M G2. We recommend avoiding these drives (or test them with the vstorage-hwflush-check tool), and using enterprise-grade or datacenter-grade devices instead. For more information on checking power loss protection, refer to Checking disk data flushing capabilities.\nProvisioning cache devices\nThe minimum number of cache devices per node is one. However, note that in this case, if caching is used for all capacity devices, the cache device becomes a single point of failure, which may make the entire node unavailable. In order to avoid this, at least three cache devices per node are recommended.\nUsing multiple cache devices also provides the following improvements:\n\nMore capacity. This can be helpful if data is written in long bursts or if the cache fails in offloading to the underlying device.\nPerformance boost. If there is enough parallelism on the client side, the workload can be split among several cache devices, thus increasing the overall throughput.\nHigh availability. With fewer capacity devices per cache device or with RAID mirroring, you can lower the probability of a downtime or its impact.\n\nIt is generally recommended to provision one cache device to every 4-12 capacity devices. Keep in mind that the speed of a cache device should be at least twice as high as that of the underlying capacity devices combined. Otherwise, the cache device may be a performance bottleneck. In this case, however, using cache can still improve latency and even performance in systems with lower parallelism.\nTo calculate the optimal number of cache devices for your cluster, consider the following formula:N = 0.8 * (cache_speed / capacity_speed)\nWhere:\n\nN is the maximum number of capacity devices for each cache device.\ncache_speed is the sustained write speed of a cache device.\ncapacity_speed is the sustained write speed of a capacity device.\n\nFor more accurate results, the device speed should be determined experimentally with real workloads. For evaluation purposes, you may use the sustained speed of sequential or random workloads, depending on the type of a workload.\nTo avoid performance degradation, the resulting number of capacity devices for each cache device (N) should be considered as an upper bound. For optimal performance, use half of this value or less.\nJournal sizing\nRegardless of a cache device size, its journal size can be different, depending on the available space and number of chunk services that share the cache device. There are scenarios when using a journal smaller than the available capacity leads to performance improvements.\nOn one hand, if the size of all journals is less than the amount of available RAM, then the journal is only used to write temporary data and guarantee consistency. Its small size allows the system to keep the journal in RAM, avoiding all reads from the journal and resulting in fewer I/O operations. Ultimately, this reduces the load on the device and in some cases may improve the overall performance (for example, when the performance of a cache and capacity devices is the same or similar).\nOn the other hand, if the size of all journals is more than the amount of available RAM, then the journal also serves as a read and write cache. This boosts the performance of both read and write requests. However, in this case, the cache device should be at least twice as fast as all of the underlying capacity devices combined, to be beneficial to the overall performance. If this is not the case, it is preferable to have a smaller journal. As speed is also largely dependent on the workload, this might not be obvious.\nCache sizing\nTo decide on a cache device size, consider the endurance factor of a particular device and its journal size.\nIf you use cache for user data, then the cache device should be able to withstand sustained high throughput for as long as needed without filling up. The cache must offload its contents to the underlying device periodically, and this process depends on the speed of the underlying device. If the cache device becomes full, the system performance will degrade to the speed of the underlying devices, thus negating the caching benefits. Therefore, if the expected workload comes in bursts of a certain duration (for example, during office hours), the cache should be able to store at least the amount of data written during that period of time.\nRisks and possible failures\nThough cache devices may significantly improve the cluster performance, you need to consider their possible failures. Flash devices generally have a shorter lifespan and their use in this context exposes them to greater wear, when compared to capacity devices.\nAlso, keep in mind that as one cache device can be used to store multiple journals, all capacity devices associated with a cache device will become unavailable if this cache device fails.\nConsider the following possible issues when using cache devices:\n\nData loss. A cache device failure may lead to data loss if the data has no replicas or RAID mirroring is not configured.\nPerformance degradation. If a cache device fails, the system will use other devices for storing data, which may result in a performance bottleneck or trigger the data rebalancing process to restore the data redundancy. This, in turn, will lead to increased disk and network usage and reduce the cluster performance.\nLow availability. With a failed cache device, data redundancy may be degraded, which may result in a read-only or unreadable cluster in severe cases.\nLess capacity. If a cache device fails, several capacity devices may become unavailable, leading to a lack of disk space available for writing new data.\n\nTo prevent these issues, use optimal redundancy policies and multiple cache devices in your system. Additionally, you can consider the possibility of using local replication (for example, RAID1) on top of distributed replication, especially in systems with low replication factors (1 replica or 1+0 encoding).\nSee also\n\nStorage cache architecture\n\nQuantity of disks per node\n\nHDD/SSD configuration\n\nServer requirements\n\nNetwork requirements and recommendations",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/cache-configuration.html"
    },
    {
        "title": "Calculating disk health",
        "content": "Calculating disk health\nYou can monitor node disks by using the vstorage-disks-monitor service. This service runs on every  management node and queries chunk server (CS) metrics from the Prometheus service for further analysis.\r\n\r\n vstorage-disks-monitor detects CSes that are not responding and marks them as ill (unresponsive). To avoid degrading the cluster performance, such CSes are fenced from the cluster I/O.\r\n\nThe service also calculates disk health, in percent, based on each metric weight. Weights can be configured in the /etc/disks-monitor/analyzers.yml\r\n\r\nconfiguration file. The service logs are stored in /var/log/disks-monitor/disks-monitor.log.\nThe service can work in two modes:\n\nAs a daemon if you use the vstorage-disks-monitor sidecar command\nAs a tool for listing disk statuses and alerts if you run vstorage-disks-monitor health and vstorage-disks-monitor alerts\n\nYou can disable fencing ill CSes by running the vstorage-disks-monitor sidecar \u2011\u2011fencing.enable command.\nLimitations\n\nDetection of unresponsive disks is disabled in clusters deployed on virtual machines.\n\nWhat's next\n\nTroubleshooting node disks",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/calculating-disk-health.html"
    },
    {
        "title": "Changing Kubernetes node flavors",
        "content": "Changing Kubernetes node flavors\nYou can modify the default flavors that will be used for Kubernetes master and worker nodes. If required, self-service users are able to select other flavors, instead of the default ones, when creating Kubernetes clusters.\nTo change the flavor for Kubernetes master nodes\nUse the following command:vinfra service compute k8saas defaults set --master-flavor <flavor> <version>\n\n--master-flavor <flavor>\n\nThe flavor to be used for Kubernetes master nodes.\n<version>\n\nKubernetes version to apply new defaults for.\n\nFor example, to set the master node flavor to xlarge and apply this change for all of the supported Kubernetes versions, run:# vinfra service compute k8saas defaults set --master-flavor xlarge\nTo set this flavor only for version 1.24.3, append the version number to the command:# vinfra service compute k8saas defaults set --master-flavor xlarge v1.24.3\r\n\nTo change the flavor for Kubernetes worker nodes\nUse the following command:vinfra service compute k8saas defaults set --flavor <flavor> <version>\n\n--flavor <flavor>\n\nThe flavor to be used for Kubernetes worker nodes.\n<version>\n\nKubernetes version to apply new defaults for.\n\nFor example, to set the worker node flavor to medium and apply this change for all of the supported Kubernetes versions, run:# vinfra service compute k8saas defaults set --flavor medium\nTo set this flavor only for version 1.24.3, append the version number to the command:# vinfra service compute k8saas defaults set --flavor medium v1.24.3\nSee also\n\nConfiguring the Kubernetes system volume\n\nConfiguring Kubernetes load balancers\n\nConfiguring Kubernetes DNS and discovery parameters",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-kubernetes-node-flavors.html"
    },
    {
        "title": "Changing the default load balancer flavor",
        "content": "Changing the default load balancer flavor\nBy default, a load balancer is created by using the private amphora flavor that cannot be managed via vinfra. You can, however, change it by using the OpenStack command-line tool.\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo change the default load balancer flavor\n\nCheck if the default amphora flavor exists:# openstack --insecure flavor list --all\r\n+---------+---------+-------+------+-----------+-------+-----------+\r\n| ID      | Name    |   RAM | Disk | Ephemeral | VCPUs | Is Public |\r\n+---------+---------+-------+------+-----------+-------+-----------+\r\n| 100     | tiny    |   512 |    0 |         0 |     1 | True      |\r\n| 101     | small   |  2048 |    0 |         0 |     1 | True      |\r\n| 102     | medium  |  4096 |    0 |         0 |     2 | True      |\r\n| 103     | large   |  8192 |    0 |         0 |     4 | True      |\r\n| 104     | xlarge  | 16384 |    0 |         0 |     8 | True      |\r\n| amphora | amphora |  4096 |   30 |         0 |     2 | False     |\r\n+---------+---------+-------+------+-----------+-------+-----------+\r\n\n\nDelete this flavor:# openstack --insecure flavor delete amphora\r\n\n\nCreate a new amphora flavor with custom parameters. For example:# openstack --insecure flavor create amphora --id amphora --ram 8192 \\\r\n--vcpus 4 --disk 60 --private\r\n+----------------------------+---------+\r\n| Field                      | Value   |\r\n+----------------------------+---------+\r\n| OS-FLV-DISABLED:disabled   | False   |\r\n| OS-FLV-EXT-DATA:ephemeral  | 0       |\r\n| disk                       | 60      |\r\n| id                         | amphora |\r\n| name                       | amphora |\r\n| os-flavor-access:is_public | False   |\r\n| properties                 |         |\r\n| ram                        | 8192    |\r\n| rxtx_factor                | 1.0     |\r\n| swap                       |         |\r\n| vcpus                      | 4       |\r\n+----------------------------+---------+\r\n\n\nChange the load balancer flavor by performing its failover. For example:# openstack --insecure loadbalancer failover mylbaas\r\n\n\nThe load balancer mylbaas will be re-created with 4 vCPUs, 8 GB of RAM, and 30 GB of disk space.\nSee also\n\nManaging balancing pools",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-the-default-load-balancer-flavor.html"
    },
    {
        "title": "Changing retention period for metrics",
        "content": "Changing retention period for metrics\n\nExtending retention period increases the Gnocchi database size insignificantly compared to its default value.\n\nYou can change the retention period for metrics through an archive policy. By default, the ceilometer-low-rate and low policies are used to store metrics. Note that you cannot change the metering granularity for these policies.\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo show the details of a particular archive policy\nUse the gnocchi archive-policy show command. For example:# gnocchi --insecure archive-policy show ceilometer-low-rate\r\n+---------------------+---------------------------------------+\r\n| Field               | Value                                 |\r\n+---------------------+---------------------------------------+\r\n| aggregation_methods | rate:mean, mean                       |\r\n| back_window         | 0                                     |\r\n| definition          | - points: 8640, granularity: 0:05:00, |\r\n|                     |   timespan: 30 days, 0:00:00          |\r\n| name                | ceilometer-low-rate                   |\r\n+---------------------+---------------------------------------+\r\n\nIn the output:\n\n8640  points shows how many aggregates will be retained\nthe granularity of 5 minutes defines the time between two aggregates\nthe timespan of 30 days specifies the retention period for aggregates\n\nTo sum it up, metrics with the ceilometer-low-rate policy will keep 8640 computed aggregates for one month with 5-minute granularity.\nTo change the policy definition\nUse the gnocchi archive-policy update command. To calculate the correct number of points required for the desired timespan, refer to this formula:points = timespan \\ granularity\nFor example, to keep aggregates for 2 months with the 5-minute granularity, specify 17280 points:# gnocchi --insecure archive-policy update ceilometer-low-rate \\\r\n-d points:17280,granularity:0:05:00,timespan:60d\r\n+---------------------+----------------------------------------+\r\n| Field               | Value                                  |\r\n+---------------------+----------------------------------------+\r\n| aggregation_methods | rate:mean, mean                        |\r\n| back_window         | 0                                      |\r\n| definition          | - points: 17280, granularity: 0:05:00, |\r\n|                     |   timespan: 60 days, 0:00:00           |\r\n| name                | ceilometer-low-rate                    |\r\n+---------------------+----------------------------------------+\nSee also\n\nViewing resources, metrics, and measures\n\nViewing outgoing traffic usage\n\nViewing resource usage per project",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-retention-period-for-metrics.html"
    },
    {
        "title": "Changing Kubernetes service parameters",
        "content": "Changing Kubernetes service parameters\nFor Kubernetes clusters, you can change the following parameters:\n\nSize and storage policy of the system volume on master nodes\nFlavors for master and worker nodes\nFlavors for Kubernetes  load balancers\nDNS nameserver and discovery URL\n\nYou can modify the default Kubernetes parameters either for all Kubernetes versions or for each version separately.\nTo view the current default Kubernetes parameters\nUse the vinfra service compute k8saas defaults show command:# vinfra service compute k8saas defaults show\r\n+---------+-------------------------------------------------------------+\r\n| Field   | Value                                                       |\r\n+---------+-------------------------------------------------------------+\r\n| default | discovery_url: null                                         |\r\n|         | dns_nameserver: null                                        |\r\n|         | flavor: null                                                |\r\n|         | labels:                                                     |\r\n|         |   auto_scaling_enabled: true                                |\r\n|         |   availability_zone: nova                                   |\r\n|         |   boot_volume_size: '20'                                    |\r\n|         |   boot_volume_type: default                                 |\r\n|         |   cgroup_driver: systemd                                    |\r\n|         |   cinder_csi_enabled: 'true'                                |\r\n|         |   cinder_csi_plugin_tag: v1.22.2                            |\r\n|         |   cloud_provider_enabled: 'true'                            |\r\n|         |   cloud_provider_tag: v1.22.0                               |\r\n|         |   csi_attacher_tag: v3.1.0                                  |\r\n|         |   csi_snapshotter_tag: v3.0.3                               |\r\n|         |   docker_volume_type: default                               |\r\n|         |   etcd_lb_disabled: 'true'                                  |\r\n|         |   etcd_tag: v3.4.6                                          |\r\n|         |   flannel_tag: v0.11.0-amd64                                |\r\n|         |   heat_container_agent_tag: 5.3.11                          |\r\n|         |   octavia_api_lb_flavor: ACTIVE_STANDBY                     |\r\n|         |   octavia_default_flavor: SINGLE                            |\r\n|         |   use_podman: 'true'                                        |\r\n|         | master_flavor: null                                         |\r\n| v1.21.3 | labels:                                                     |\r\n|         |   autoscaler_tag: 1.21.0                                    |\r\n|         |   hyperkube_image: docker.io/virtuozzo/hci-binary-hyperkube |\r\n|         |   kube_tag: v1.21.3                                         |\r\n|         |   kube_version: v1.21.3                                     |\r\n| v1.22.2 | labels:                                                     |\r\n|         |   autoscaler_tag: 1.22.2                                    |\r\n|         |   hyperkube_image: docker.io/virtuozzo/hci-binary-hyperkube |\r\n|         |   kube_tag: v1.22.2                                         |\r\n|         |   kube_version: v1.22.2                                     |\r\n| v1.23.5 | labels:                                                     |\r\n|         |   autoscaler_tag: 1.23.0                                    |\r\n|         |   hyperkube_image: docker.io/virtuozzo/hci-binary-hyperkube |\r\n|         |   kube_tag: v1.23.5                                         |\r\n|         |   kube_version: v1.23.5                                     |\r\n| v1.24.3 | labels:                                                     |\r\n|         |   autoscaler_tag: 1.24.0                                    |\r\n|         |   container_runtime: containerd                             |\r\n|         |   hyperkube_image: docker.io/virtuozzo/hci-binary-hyperkube |\r\n|         |   kube_tag: v1.24.3                                         |\r\n|         |   kube_version: v1.24.3                                     |\r\n+---------+-------------------------------------------------------------+\n\nSee also\n\nUpdating Kubernetes clusters\n\nTroubleshooting Kubernetes clusters",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-kubernetes-service-parameters.html"
    },
    {
        "title": "Changing network configuration",
        "content": "Changing network configuration\nYou can change your network configuration and IP address assignment to cluster nodes by using network migration.\nLimitations\n\nDHCP can be enabled for the source network but must be disabled for the target network. After migration, IP addresses obtained via DHCP will become static.\n\nPrerequisites\n\nAll of the connected node interfaces are online.\nEach network interface has only one IP address.\nHigh availability is disabled, as described in Managing high availability configuration. You can enable high availability later, if required.\nIf a network is the default gateway network, all nodes connected to it must use the same default gateway.\nIf you have restricted outbound traffic in your cluster, you need to manually add a rule that will allow outbound traffic on TCP and UDP ports 60000\u00e2\u0080\u009360100, as described in Configuring outbound firewall rules.\n\nTo migrate a network from the source configuration to the target one\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Migrate.\n\nIn the Migrate network: <network name> window, review the current network configuration, and important information about potential risks, and edit the new network configuration, if required.\nIf you plan to move your cluster to another location, which implies cluster manual shutdown, select Cluster relocation with shutdown is planned.\nThen, click Next.\n\nOn the next step, specify new IP addresses for cluster nodes, and click Try new configuration. Then, confirm your action by clicking Continue in the Try new configuration window.\n\nIf you plan cluster relocation, you can shut down your cluster nodes and then turn them on in a new datacenter, as described in Shutting down and starting up the cluster. After cluster relocation, click Resume.\n\nWait until the new configuration is created and then click Apply.\n\nWhile network migration is in progress, users cannot perform other tasks in the admin panel. Moreover, the self-service users may not have access to the portal and will need to wait until the migration is complete.\n\nIf the connectivity checks fail, you need to fix the found issues and try again. If the specified new IP addresses are not available or valid, you can change them in the wizard and click Retry. With other network issues, revert to your old network configuration by clicking Revert, fix the issue, and try again.\nWait until the migration is complete on all the connected interfaces, and then click Done.\nIf you migrate a network with the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\nCommand-line interface\n\nStart the network migration by using the following command:vinfra cluster network migration start <network> [--subnet <subnet>]\r\n                                       [--netmask <netmask>]\r\n                                       [--gateway <gateway>] [--shutdown]\r\n                                       [--node <node> <address>]\r\n\n\n--network <network>\n\nNetwork ID or name\n--subnet <subnet>\n\nNew network subnet\n--netmask <netmask>\n\nNew network mask\n--gateway <gateway>\n\nNew network gateway\n--shutdown\n\nPrepare the cluster to be shut down manually for relocation\n--node <node> <address>\n\nNew node address in the format:\n\n<node>: node ID or hostname\n<address>: IPv4 address\n\nThis option can be used multiple times.\n\nFor example:# vinfra cluster network migration start --network \"Private\" \\\r\n--subnet 192.168.128.0 --netmask 255.255.255.0 --node node001 192.168.128.11 \\\r\n--node node002 192.168.128.12 --node node003 192.168.128.13\r\n+----------------------------+--------------------------------------------------+\r\n| Field                      | Value                                            |\r\n+----------------------------+--------------------------------------------------+\r\n| configuration              | network_id: 3e3619b7-2c93-4e90-a187-135c6f8b9060 |\r\n| link                       | href: /api/v2/network/migration/2d4ec3a9-<...>/  |\r\n|                            | method: GET                                      |\r\n|                            | rel: network-migration-details                   |\r\n| operation                  | network-migration                                |\r\n| progress                   | 0.0                                              |\r\n| single_interface_migration | False                                            |\r\n| state                      | preparing                                        |\r\n| task_id                    | 2d4ec3a9-7714-479d-a03c-1efbe6ffecf5             |\r\n| transitions                | 0                                                |\r\n+----------------------------+--------------------------------------------------+\r\n\n\nView the current network migration details. For example:# vinfra cluster network migration show\r\n+----------------------------+-------------------------------------------------+\r\n| Field                      | Value                                           |\r\n+----------------------------+-------------------------------------------------+\r\n| link                       | href: /api/v2/network/migration/2d4ec3a9-<...>/ |\r\n|                            | method: GET                                     |\r\n|                            | rel: network-migration-details                  |\r\n| operation                  | network-migration                               |\r\n| progress                   | 1.0                                             |\r\n| single_interface_migration | False                                           |\r\n| state                      | test-passed                                     |\r\n| task_id                    | 2d4ec3a9-7714-479d-a03c-1efbe6ffecf5            |\r\n| transitions                | 5                                               |\r\n+----------------------------+-------------------------------------------------+\r\n\nThe output shows that the new network configuration has been tested and can be applied.\n\nIf you plan cluster relocation, you can shut down your cluster nodes and then turn them on in a new datacenter, as described in Shutting down and starting up the cluster. After cluster relocation, run:# vinfra cluster network migration resume\n\nContinue the network migration and apply the new network configuration. For example:# vinfra cluster network migration apply\n\nIf you migrate a network with the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\nIf the connectivity checks fail, you need to fix the found issues and try again. If the specified new IP addresses are not available or valid, you can change them by using the following command:vinfra cluster network migration retry [--subnet <subnet>]\r\n                                       [--netmask <netmask>]\r\n                                       [--node <node> <address>]\r\n\n\n--subnet <subnet>\n\nNew network subnet\n--netmask <netmask>\n\nNew network mask\n--node <node> <address>\n\nNew node address in the format:\n\n<node>: node ID or hostname\n<address>: IPv4 address\n\nThis option can be used multiple times.\n\nFor example:# vinfra cluster network migration retry --subnet 192.168.128.0 \\\r\n--netmask 255.255.255.0 --node node001 192.168.128.12 --node node002 192.168.128.13 \\\r\n--node node003 192.168.128.14\r\n+----------------------------+-------------------------------------------------+\r\n| Field                      | Value                                           |\r\n+----------------------------+-------------------------------------------------+\r\n| link                       | href: /api/v2/network/migration/2d4ec3a9-<...>/ |\r\n|                            | method: GET                                     |\r\n|                            | rel: network-migration-details                  |\r\n| operation                  | network-migration                               |\r\n| progress                   | 0.9                                             |\r\n| single_interface_migration | False                                           |\r\n| state                      | failed-to-apply                                 |\r\n| task_id                    | 2ce42f0e-6401-47c1-a52f-33e7c68d0df4            |\r\n| transitions                | 5                                               |\r\n+----------------------------+-------------------------------------------------+\r\n\nWith other network issues, revert to your old network configuration with vinfra cluster network migration revert, fix the issue, and try again.\n\nTo troubleshoot a failed migration\n\nConnect to your cluster via SSH.\nInvestigate /var/log/vstorage-ui-backend/celery.log to find the root cause.\nFix the issue.\nGo back to the wizard screen and click Retry.\n\nSee also\n\nManaging networks\n\nManaging traffic types",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nStart the network migration by using the following command:vinfra cluster network migration start <network> [--subnet <subnet>]\r\n                                       [--netmask <netmask>]\r\n                                       [--gateway <gateway>] [--shutdown]\r\n                                       [--node <node> <address>]\r\n\n\n--network <network>\n\nNetwork ID or name\n--subnet <subnet>\n\nNew network subnet\n--netmask <netmask>\n\nNew network mask\n--gateway <gateway>\n\nNew network gateway\n--shutdown\n\nPrepare the cluster to be shut down manually for relocation\n--node <node> <address>\n\n\nNew node address in the format:\n\n<node>: node ID or hostname\n<address>: IPv4 address\n\nThis option can be used multiple times.\n\n\nFor example:# vinfra cluster network migration start --network \"Private\" \\\r\n--subnet 192.168.128.0 --netmask 255.255.255.0 --node node001 192.168.128.11 \\\r\n--node node002 192.168.128.12 --node node003 192.168.128.13\r\n+----------------------------+--------------------------------------------------+\r\n| Field                      | Value                                            |\r\n+----------------------------+--------------------------------------------------+\r\n| configuration              | network_id: 3e3619b7-2c93-4e90-a187-135c6f8b9060 |\r\n| link                       | href: /api/v2/network/migration/2d4ec3a9-<...>/  |\r\n|                            | method: GET                                      |\r\n|                            | rel: network-migration-details                   |\r\n| operation                  | network-migration                                |\r\n| progress                   | 0.0                                              |\r\n| single_interface_migration | False                                            |\r\n| state                      | preparing                                        |\r\n| task_id                    | 2d4ec3a9-7714-479d-a03c-1efbe6ffecf5             |\r\n| transitions                | 0                                                |\r\n+----------------------------+--------------------------------------------------+\r\n\n\n\nView the current network migration details. For example:# vinfra cluster network migration show\r\n+----------------------------+-------------------------------------------------+\r\n| Field                      | Value                                           |\r\n+----------------------------+-------------------------------------------------+\r\n| link                       | href: /api/v2/network/migration/2d4ec3a9-<...>/ |\r\n|                            | method: GET                                     |\r\n|                            | rel: network-migration-details                  |\r\n| operation                  | network-migration                               |\r\n| progress                   | 1.0                                             |\r\n| single_interface_migration | False                                           |\r\n| state                      | test-passed                                     |\r\n| task_id                    | 2d4ec3a9-7714-479d-a03c-1efbe6ffecf5            |\r\n| transitions                | 5                                               |\r\n+----------------------------+-------------------------------------------------+\r\n\nThe output shows that the new network configuration has been tested and can be applied.\n\n\nIf you plan cluster relocation, you can shut down your cluster nodes and then turn them on in a new datacenter, as described in Shutting down and starting up the cluster. After cluster relocation, run:# vinfra cluster network migration resume\n\n\nContinue the network migration and apply the new network configuration. For example:# vinfra cluster network migration apply\n\n\nIf you migrate a network with the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\n\nIf the connectivity checks fail, you need to fix the found issues and try again. If the specified new IP addresses are not available or valid, you can change them by using the following command:vinfra cluster network migration retry [--subnet <subnet>]\r\n                                       [--netmask <netmask>]\r\n                                       [--node <node> <address>]\r\n\n\n--subnet <subnet>\n\nNew network subnet\n--netmask <netmask>\n\nNew network mask\n--node <node> <address>\n\n\nNew node address in the format:\n\n<node>: node ID or hostname\n<address>: IPv4 address\n\nThis option can be used multiple times.\n\n\nFor example:# vinfra cluster network migration retry --subnet 192.168.128.0 \\\r\n--netmask 255.255.255.0 --node node001 192.168.128.12 --node node002 192.168.128.13 \\\r\n--node node003 192.168.128.14\r\n+----------------------------+-------------------------------------------------+\r\n| Field                      | Value                                           |\r\n+----------------------------+-------------------------------------------------+\r\n| link                       | href: /api/v2/network/migration/2d4ec3a9-<...>/ |\r\n|                            | method: GET                                     |\r\n|                            | rel: network-migration-details                  |\r\n| operation                  | network-migration                               |\r\n| progress                   | 0.9                                             |\r\n| single_interface_migration | False                                           |\r\n| state                      | failed-to-apply                                 |\r\n| task_id                    | 2ce42f0e-6401-47c1-a52f-33e7c68d0df4            |\r\n| transitions                | 5                                               |\r\n+----------------------------+-------------------------------------------------+\r\n\nWith other network issues, revert to your old network configuration with vinfra cluster network migration revert, fix the issue, and try again.\n",
                "title": "To migrate a network from the source configuration to the target one"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Migrate.\n\nIn the Migrate network: <network name> window, review the current network configuration, and important information about potential risks, and edit the new network configuration, if required.\nIf you plan to move your cluster to another location, which implies cluster manual shutdown, select Cluster relocation with shutdown is planned.\nThen, click Next.\n\n\n\n\nOn the next step, specify new IP addresses for cluster nodes, and click Try new configuration. Then, confirm your action by clicking Continue in the Try new configuration window.\n\n\n\nIf you plan cluster relocation, you can shut down your cluster nodes and then turn them on in a new datacenter, as described in Shutting down and starting up the cluster. After cluster relocation, click Resume.\n\nWait until the new configuration is created and then click Apply.\n\n\n\nWhile network migration is in progress, users cannot perform other tasks in the admin panel. Moreover, the self-service users may not have access to the portal and will need to wait until the migration is complete.\n\n\nIf the connectivity checks fail, you need to fix the found issues and try again. If the specified new IP addresses are not available or valid, you can change them in the wizard and click Retry. With other network issues, revert to your old network configuration by clicking Revert, fix the issue, and try again.\nWait until the migration is complete on all the connected interfaces, and then click Done.\nIf you migrate a network with the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\n",
                "title": "To migrate a network from the source configuration to the target one"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-network-configuration.html"
    },
    {
        "title": "Changing S3 protocol settings",
        "content": "Changing S3 protocol settings\nAfter creating the S3 storage, you can change the protocol settings of the S3 endpoint.\nPrerequisites\n\nS3 clusters are created, as described in Creating the S3 cluster.\n\nTo change S3 protocol settings\n\nAdmin panel\n\n Open the Storage services > S3 > Settings screen, and then click Protocol.\n\nSelect an S3 endpoint protocol: HTTP, HTTPS, or both.\n\nIt is recommended to use only HTTPS for production deployments.\n\nIf you selected the HTTPS protocol, do one of the following: \n\nSelect Upload a certificate, specify the prepared SSL certificate, and then specify the SSL key or passphrase (for PKCS#12 files).\nYou need to acquire a key and a trusted wildcard SSL certificate for endpoint\u00e2\u0080\u0099s bottom-level domain. For example, the endpoint s3storage.example.com would need a wildcard certificate for *.s3storage.example.com with the subject alternative name s3storage.example.com.\nIf you acquired an SSL certificate from an intermediate certificate authority (CA)\n\nYou should have an end-user certificate along with a CA bundle that contains the root and intermediate certificates. To be able to use these certificates, you need to merge them into a chain first. A certificate chain includes the end-user certificate, the certificates of intermediate CAs, and the certificate of a trusted root CA. In this case, an SSL certificate can only be trusted if every certificate in the chain is properly issued and valid.\nFor example, if you have an end-user certificate, two intermediate CA certificates, and a root CA certificate, create a new certificate file and add all certificates to it in the following order:# End-user certificate issued by the intermediate CA 1\r\n-----BEGIN CERTIFICATE-----\r\nMIICiDCCAg2gAwIBAgIQNfwmXNmET8k9Jj1X<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 1 certificate issued by the intermediate CA 2\r\n-----BEGIN CERTIFICATE-----\r\nMIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9ON9<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 2 certificate issued by the root CA\r\n-----BEGIN CERTIFICATE-----\r\nMIIC8jCCAdqgAwIBAgICZngwDQYJKoZIhvcN<...>\r\n-----END CERTIFICATE-----\r\n# Root CA certificate\r\n-----BEGIN CERTIFICATE-----\r\nMIIDODCCAiCgAwIBAgIGIAYFFnACMA0GCSqG<...>\r\n-----END CERTIFICATE-----\r\n\n\nSelect Generate a certificate, to get a self-signed certificate for HTTPS evaluation purposes.\n\nS3 geo-replication requires a certificate from a trusted authority. It does not work with self-signed certificates.\nTo access the data in the S3 cluster via a browser, add the self-signed certificate to browser\u00e2\u0080\u0099s exceptions.\n\nClick Save to apply your changes.\n\nCommand-line interface\nUse the following command:vinfra service s3 cluster change [--self-signed | --no-ssl | --cert-file <cert_file>]\r\n                                 [--insecure] [--key-file <key_file>] [--password]\n\n--self-signed\n\n        Generate a new self-signed certificate (default)\n--no-ssl\n\nDo not generate a self-signed certificate\n--cert-file <cert_file>\n\nPath to a file with the new certificate\n--insecure\n\nAllow insecure connections in addition to secure ones (only used with the --cert-file and --self-signed options)\n--key-file <key_file>\n\nPath to a file with the private key (only used with the --cert-file option)\n--password\n\nRead certificate password from stdin (only used with the --cert-file option)\n\nFor example,  to use a self-signed certificate for the S3 storage, run:# vinfra service s3 cluster change --self-signed\n\nSee also\n\nManaging S3 users\n\nManaging S3 buckets\n\nChanging the redundancy scheme for S3 data\n\nReplicating S3 data between datacenters\n\nChanging the TLS configuration for S3",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service s3 cluster change [--self-signed | --no-ssl | --cert-file <cert_file>]\r\n                                 [--insecure] [--key-file <key_file>] [--password]\n\n--self-signed\n\n        Generate a new self-signed certificate (default)\n--no-ssl\n\nDo not generate a self-signed certificate\n--cert-file <cert_file>\n\nPath to a file with the new certificate\n--insecure\n\nAllow insecure connections in addition to secure ones (only used with the --cert-file and --self-signed options)\n--key-file <key_file>\n\nPath to a file with the private key (only used with the --cert-file option)\n--password\n\nRead certificate password from stdin (only used with the --cert-file option)\n\nFor example,  to use a self-signed certificate for the S3 storage, run:# vinfra service s3 cluster change --self-signed\n",
                "title": "To change S3 protocol settings"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n Open the Storage services > S3 > Settings screen, and then click Protocol.\n\nSelect an S3 endpoint protocol: HTTP, HTTPS, or both.\n\nIt is recommended to use only HTTPS for production deployments.\n\n\n\n\n\nIf you selected the HTTPS protocol, do one of the following: \n\n\nSelect Upload a certificate, specify the prepared SSL certificate, and then specify the SSL key or passphrase (for PKCS#12 files).\nYou need to acquire a key and a trusted wildcard SSL certificate for endpoint\u00e2\u0080\u0099s bottom-level domain. For example, the endpoint s3storage.example.com would need a wildcard certificate for *.s3storage.example.com with the subject alternative name s3storage.example.com.\nIf you acquired an SSL certificate from an intermediate certificate authority (CA)\n\nYou should have an end-user certificate along with a CA bundle that contains the root and intermediate certificates. To be able to use these certificates, you need to merge them into a chain first. A certificate chain includes the end-user certificate, the certificates of intermediate CAs, and the certificate of a trusted root CA. In this case, an SSL certificate can only be trusted if every certificate in the chain is properly issued and valid.\nFor example, if you have an end-user certificate, two intermediate CA certificates, and a root CA certificate, create a new certificate file and add all certificates to it in the following order:# End-user certificate issued by the intermediate CA 1\r\n-----BEGIN CERTIFICATE-----\r\nMIICiDCCAg2gAwIBAgIQNfwmXNmET8k9Jj1X<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 1 certificate issued by the intermediate CA 2\r\n-----BEGIN CERTIFICATE-----\r\nMIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9ON9<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 2 certificate issued by the root CA\r\n-----BEGIN CERTIFICATE-----\r\nMIIC8jCCAdqgAwIBAgICZngwDQYJKoZIhvcN<...>\r\n-----END CERTIFICATE-----\r\n# Root CA certificate\r\n-----BEGIN CERTIFICATE-----\r\nMIIDODCCAiCgAwIBAgIGIAYFFnACMA0GCSqG<...>\r\n-----END CERTIFICATE-----\r\n\n\n\n\n\nSelect Generate a certificate, to get a self-signed certificate for HTTPS evaluation purposes.\n\n\nS3 geo-replication requires a certificate from a trusted authority. It does not work with self-signed certificates.\nTo access the data in the S3 cluster via a browser, add the self-signed certificate to browser\u00e2\u0080\u0099s exceptions.\n\n\n\n\n\nClick Save to apply your changes.\n\n",
                "title": "To change S3 protocol settings"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-s3-protocol-settings.html"
    },
    {
        "title": "Changing security group assignment",
        "content": "Changing security group assignment\nWhen you create a VM, you select security groups for the VM\u00a0network interfaces. You can also change assigned security groups later.\nLimitations\n\nYou cannot configure security groups if spoofing protection is disabled or IP address management is disabled for the selected network.\n\nTo view virtual machines assigned to a security group\n\nOn the Compute > Network > Security groups tab, click the required security group.\nOn the group right pane, navigate to the Assigned VMs tab. All the assigned virtual machines will be shown along with their status.\n\nYou can click the VM name to go to the VM Overview pane and change the security group assignment for its network interfaces.\nTo assign a security group to a virtual machine\n\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines screen, click the required virtual machine.\nOn the Overview tab, click the pencil icon in the Networks section. \nClick the ellipsis icon next to the network interface to assign a security group to, and then click Edit.\nIn the Edit network interface window, go to the Security groups tab.\nSelect one or more security groups from the drop-down list, and then click Save.\n\nThe rules from chosen security groups will be applied at runtime.\n\nCommand-line interface\n\nList the VM's network interfaces with assigned security groups. For example:# vinfra service compute server iface list --server myvm -c id -c security_groups --long\r\n+--------------------------------------+----------------------------------------+\r\n| id                                   | security_groups                        |\r\n+--------------------------------------+----------------------------------------+\r\n| 8c11c29b-9a73-4017-baff-1e872b18b54b | - d3a7d0c3-0f5c-4e77-8add-dafebae4a225 |\r\n+--------------------------------------+----------------------------------------+\r\n\n\nEdit the security group of the network interface. For example:# vinfra service compute server iface set --server myvm --security-group mygroup \\\r\n8c11c29b-9a73-4017-baff-1e872b18b54b\r\n+---------------------+--------------------------------------+\r\n| Field               | Value                                |\r\n+---------------------+--------------------------------------+\r\n| fixed_ips           | - 192.168.128.100                    |\r\n| id                  | 8c11c29b-9a73-4017-baff-1e872b18b54b |\r\n| mac_addr            | fa:16:3e:a6:d4:32                    |\r\n| network_id          | 8774a1a4-f7a0-4729-be9b-d282751434c5 |\r\n| security_groups     | 12e6b260-0b61-4551-8168-3e59602a2433 |\r\n| spoofing_protection | True                                 |\r\n+---------------------+--------------------------------------+\r\n\n\nSee also\n\nCreating virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nList the VM's network interfaces with assigned security groups. For example:# vinfra service compute server iface list --server myvm -c id -c security_groups --long\r\n+--------------------------------------+----------------------------------------+\r\n| id                                   | security_groups                        |\r\n+--------------------------------------+----------------------------------------+\r\n| 8c11c29b-9a73-4017-baff-1e872b18b54b | - d3a7d0c3-0f5c-4e77-8add-dafebae4a225 |\r\n+--------------------------------------+----------------------------------------+\r\n\n\n\nEdit the security group of the network interface. For example:# vinfra service compute server iface set --server myvm --security-group mygroup \\\r\n8c11c29b-9a73-4017-baff-1e872b18b54b\r\n+---------------------+--------------------------------------+\r\n| Field               | Value                                |\r\n+---------------------+--------------------------------------+\r\n| fixed_ips           | - 192.168.128.100                    |\r\n| id                  | 8c11c29b-9a73-4017-baff-1e872b18b54b |\r\n| mac_addr            | fa:16:3e:a6:d4:32                    |\r\n| network_id          | 8774a1a4-f7a0-4729-be9b-d282751434c5 |\r\n| security_groups     | 12e6b260-0b61-4551-8168-3e59602a2433 |\r\n| spoofing_protection | True                                 |\r\n+---------------------+--------------------------------------+\r\n\n\n\n",
                "title": "To assign a security group to a virtual machine"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines screen, click the required virtual machine.\nOn the Overview tab, click the pencil icon in the Networks section. \nClick the ellipsis icon next to the network interface to assign a security group to, and then click Edit.\nIn the Edit network interface window, go to the Security groups tab.\nSelect one or more security groups from the drop-down list, and then click Save.\n\nThe rules from chosen security groups will be applied at runtime.\n",
                "title": "To assign a security group to a virtual machine"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-security-group-assignment.html"
    },
    {
        "title": "Changing placement assignment",
        "content": "Changing placement assignment\nLimitations\n\nAfter adding a node to a placement, VMs already hosted on the node will not be automatically assigned this placement.\nA virtual machine that is assigned a placement can only be migrated between nodes in this placement. When adding nodes to placements, make sure to provide migration options for various scenarios, including high availability and maintenance. Avoid situations when VMs cannot migrate because of limitations imposed by placements. In this case, a VM placement can be edited, as described in Managing virtual machines in placements.\n\nPrerequisites\n\nPlacements for compute nodes are created, as described in Creating placements.\n\nTo assign a placement to a node\n\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, click the required placement.\nGo to the Nodes tab, and then click Add.\nSelect the nodes to assign the placement to, and then click Add.\n\nCommand-line interface\nUse the following command:vinfra service compute placement assign --nodes <nodes> <placement>\r\n\n\n--nodes <nodes>\n\nA comma-separated list of compute node IDs or hostnames to assign a compute placement to\n<placement>\n\nPlacement ID or name\n\nFor example, to assign the placement placement1 to the node node005.vstoragedomain, run:# vinfra service compute placement assign --nodes node005 placement1\n\nTo assign a placement to an image\n\nAdmin panel\n\nOpen the Compute > Nodes > Placements tab, and then click the required placement.\nOn the Properties tab, click Add in the Images section.\nSelect one or more images to assign the placement to, and then click Add.\n\nWhen you select this image while creating a VM, the corresponding placement will be selected automatically.\n\nCommand-line interface\nUse the following command:vinfra service compute placement assign --images <images> <placement>\r\n\n\n--images <images>\n\nA comma-separated list of image IDs or names to assign a compute placement to\n<placement>\n\nPlacement ID or name\n\nFor example, to assign the placement placement1 to the image cirros, run:# vinfra service compute placement assign --images cirros placement1\n\nTo assign a placement to a flavor\n\nAdmin panel\n\nOpen the Compute > Nodes > Placements tab, and then click the required placement.\nOn the Properties tab, click Add in the Flavors section.\nSelect one or more flavors to assign the placement to, and then click Add.\n\nWhen you select this flavor while creating a VM, the corresponding placement will be selected automatically.\n\nCommand-line interface\nUse the following command:vinfra service compute placement assign --flavors <images> <placement>\r\n\n\n--flavors <flavors>\n\nA comma-separated list of flavor IDs or names to assign a compute placement to\n<placement>\n\nPlacement ID or name\n\nFor example, to assign the placement placement1 to the flavor 102, run:# vinfra service compute placement assign --flavors 102 placement1\n\nTo remove placement assignments\n\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, click the required placement.\nOn the Properties tab, click the bin icon next to an image or a flavor to remove it.\nGo to the Nodes tab, and click the bin icon next to a node to remove it.\n\nCommand-line interface\n\nView the placement details, to check if the placement is assigned to any images, flavors, or nodes. For example:# vinfra service compute placement show placement1\r\n+-------------+--------------------------------------+\r\n| Field       | Value                                |\r\n+-------------+--------------------------------------+\r\n| description |                                      |\r\n| flavors     | 0                                    |\r\n| id          | e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| images      | 1                                    |\r\n| name        | placement1                           |\r\n| nodes       | 3                                    |\r\n| servers     | 0                                    |\r\n+-------------+--------------------------------------+\r\n\n\nList objects that the placement is assigned to, if any, to find out their names. For example:# vinfra service compute node list --long -c id -c placements\r\n+------------------------+----------------------------------------+\r\n| host                   | placements                             | \r\n+------------------------+----------------------------------------+\r\n| node001.vstoragedomain | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| node002.vstoragedomain | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| node003.vstoragedomain | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| node004.vstoragedomain | []                                     |\r\n| node005.vstoragedomain | []                                     |\r\n+------------------------+----------------------------------------+\r\n# vinfra service compute image list --long -c name -c placements\r\n+--------------------------+----------------------------------------+\r\n| name                     | placements                             |\r\n+--------------------------+----------------------------------------+\r\n| fedora-coreos-x64-k8saas | []                                     |\r\n| amphora-x64-haproxy      | []                                     |\r\n| cirros                   | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n+--------------------------+----------------------------------------+\r\n\n\nRemove all of the placement assignments. For example:# vinfra service compute placement delete-assign --node node001 placement1\r\n# vinfra service compute placement delete-assign --node node002 placement1\r\n# vinfra service compute placement delete-assign --node node003 placement1\r\n# vinfra service compute placement delete-assign --image cirros placement1\n\nSee also\n\nEditing and deleting placements",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute placement assign --nodes <nodes> <placement>\r\n\n\n--nodes <nodes>\n\nA comma-separated list of compute node IDs or hostnames to assign a compute placement to\n<placement>\n\nPlacement ID or name\n\nFor example, to assign the placement placement1 to the node node005.vstoragedomain, run:# vinfra service compute placement assign --nodes node005 placement1\n",
                "title": "To assign a placement to a node"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute placement assign --images <images> <placement>\r\n\n\n--images <images>\n\nA comma-separated list of image IDs or names to assign a compute placement to\n<placement>\n\nPlacement ID or name\n\nFor example, to assign the placement placement1 to the image cirros, run:# vinfra service compute placement assign --images cirros placement1\n",
                "title": "To assign a placement to an image"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute placement assign --flavors <images> <placement>\r\n\n\n--flavors <flavors>\n\nA comma-separated list of flavor IDs or names to assign a compute placement to\n<placement>\n\nPlacement ID or name\n\nFor example, to assign the placement placement1 to the flavor 102, run:# vinfra service compute placement assign --flavors 102 placement1\n",
                "title": "To assign a placement to a flavor"
            },
            {
                "example": "\nCommand-line interface\n\n\nView the placement details, to check if the placement is assigned to any images, flavors, or nodes. For example:# vinfra service compute placement show placement1\r\n+-------------+--------------------------------------+\r\n| Field       | Value                                |\r\n+-------------+--------------------------------------+\r\n| description |                                      |\r\n| flavors     | 0                                    |\r\n| id          | e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| images      | 1                                    |\r\n| name        | placement1                           |\r\n| nodes       | 3                                    |\r\n| servers     | 0                                    |\r\n+-------------+--------------------------------------+\r\n\n\n\nList objects that the placement is assigned to, if any, to find out their names. For example:# vinfra service compute node list --long -c id -c placements\r\n+------------------------+----------------------------------------+\r\n| host                   | placements                             | \r\n+------------------------+----------------------------------------+\r\n| node001.vstoragedomain | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| node002.vstoragedomain | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| node003.vstoragedomain | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| node004.vstoragedomain | []                                     |\r\n| node005.vstoragedomain | []                                     |\r\n+------------------------+----------------------------------------+\r\n# vinfra service compute image list --long -c name -c placements\r\n+--------------------------+----------------------------------------+\r\n| name                     | placements                             |\r\n+--------------------------+----------------------------------------+\r\n| fedora-coreos-x64-k8saas | []                                     |\r\n| amphora-x64-haproxy      | []                                     |\r\n| cirros                   | - e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n+--------------------------+----------------------------------------+\r\n\n\n\nRemove all of the placement assignments. For example:# vinfra service compute placement delete-assign --node node001 placement1\r\n# vinfra service compute placement delete-assign --node node002 placement1\r\n# vinfra service compute placement delete-assign --node node003 placement1\r\n# vinfra service compute placement delete-assign --image cirros placement1\n\n\n",
                "title": "To remove placement assignments"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, click the required placement.\nGo to the Nodes tab, and then click Add.\nSelect the nodes to assign the placement to, and then click Add.\n\n",
                "title": "To assign a placement to a node"
            },
            {
                "example": "\nAdmin panel\n\nOpen the Compute > Nodes > Placements tab, and then click the required placement.\nOn the Properties tab, click Add in the Images section.\nSelect one or more images to assign the placement to, and then click Add.\n\nWhen you select this image while creating a VM, the corresponding placement will be selected automatically.\n",
                "title": "To assign a placement to an image"
            },
            {
                "example": "\nAdmin panel\n\nOpen the Compute > Nodes > Placements tab, and then click the required placement.\nOn the Properties tab, click Add in the Flavors section.\nSelect one or more flavors to assign the placement to, and then click Add.\n\nWhen you select this flavor while creating a VM, the corresponding placement will be selected automatically.\n",
                "title": "To assign a placement to a flavor"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, click the required placement.\nOn the Properties tab, click the bin icon next to an image or a flavor to remove it.\nGo to the Nodes tab, and click the bin icon next to a node to remove it.\n\n",
                "title": "To remove placement assignments"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-placement-assignment.html"
    },
    {
        "title": "Changing parameters in OpenStack configuration files",
        "content": "Changing parameters in OpenStack configuration files\nYou can modify the following parameters in the OpenStack configuration files:\n\nParameter\nConfiguration file\nDescription\nValue\n\nscheduler_host_subset_size\n\n/etc/kolla/nova-scheduler/nova.conf\n\nDefines a number of compute nodes best suited for a new VM, one of which is randomly chosen by the scheduler.\nValid values are 1 or greater. Any value less than 1 is treated as 1. The higher the value, the less optimal the chosen node may be for a VM. The default value is 1.\n\nvxlan_udp_port\n\n/etc/kolla/neutron-openvswitch-agent/ml2_conf.ini\n\nSpecifies the UDP port used for VXLAN tunnels. When changing the port, iptables rules are automatically configured for both the old and new ports.\nThe default port is 4789.\n\ncpu_allocation_ratio\n\n/etc/kolla/nova-compute/nova.conf\n\nDefines the virtual CPU to physical CPU allocation ratio.\n\nChanging CPU allocation ratio will not affect virtual CPUs already provisioned for virtual machines.\n\nValid values are positive integer or float. The default value is 8.0.\n\nram_allocation_ratio\n\n/etc/kolla/nova-compute/nova.conf\n\nDefines the maximum reserved RAM to physical RAM allocation ratio.\n\nChanging RAM allocation ratio will not affect RAM already provisioned for virtual machines.\n\nValid values are positive integer or float. The default value is 1.0. The maximum recommended value is 1.5.\n\nTo change a compute parameter\nUse the following command:vinfra service compute set [--custom-param <service_name> <config_file> <section> <property> <value>]\r\n                           [--nova-compute-ram-allocation-ratio <value>]\r\n                           [--neutron-openvswitch-vxlan-port <value>]\r\n                           [--nova-scheduler-host-subset-size <value>]\r\n                           [--nova-compute-cpu-allocation-ratio <value>]\r\n\n\n--custom-param <service_name> <config_file> <section> <property> <value>\n\nSet custom parameters for OpenStack configuration files:\n\nservice_name is the service name: nova-compute or neutron-openvswitch-agent\nconfig_file specifies the service configuration file: nova.conf for nova-compute or ml2_conf.ini for neutron-openvswitch-agent\nsection specifies the section in the service configuration file where the parameter is defined: DEFAULT in nova.conf or agent in ml2_conf.ini\nproperty is the parameter to be changed: ram_allocation_ratio, scheduler_host_subset, and cpu_allocation_ratio in nova.conf; vxlan_udp_port in ml2_conf.ini\nvalue is a new parameter value\n\n--nova-compute-ram-allocation-ratio <value>\n\nShortcut for --custom-param nova-compute nova.conf DEFAULT ram_allocation_ratio <value>\n--neutron-openvswitch-vxlan-port <value>\n\nShortcut for --custom-param neutron-openvswitch-agent ml2_conf.ini agent vxlan_udp_port <value>\n--nova-scheduler-host-subset-size <value>\n\nShortcut for --custom-param nova-scheduler nova.conf DEFAULT scheduler_host_subset_size <value>\n--nova-compute-cpu-allocation-ratio <value>\n\nShortcut for --custom-param nova-scheduler nova.conf DEFAULT cpu_allocation_ratio <value>\n\nFor example, to change cpu_allocation_ratio and vxlan_udp_port, run:# vinfra service compute set --nova-compute-cpu-allocation-ratio 16.0 --neutron-openvswitch-vxlan-port 4787\r\n\nTo check that the custom parameters are successfully modified, run:# vinfra service compute show\r\n+--------------+-------------------------------------------+\r\n| Field        | Value                                     |\r\n+--------------+-------------------------------------------+\r\n| <...>        | <...>                                     |\r\n| options      | cpu_model: ''                             |\r\n|              | custom_params:                            |\r\n|              | - config_file: nova.conf                  |\r\n|              |   property: cpu_allocation_ratio          |\r\n|              |   section: DEFAULT                        |\r\n|              |   service_name: nova-compute              |\r\n|              |   value: 16.0                             |\r\n|              | - config_file: ml2_conf.ini               |\r\n|              |   property: vxlan_udp_port                |\r\n|              |   section: agent                          |\r\n|              |   service_name: neutron-openvswitch-agent |\r\n|              |   value: 4787                             |\r\n|              | notification_forwarding: disabled         |\r\n| status       | active                                    |\r\n+--------------+-------------------------------------------+\r\n\nThe applied changes are consistent on all compute nodes and not overwritten after product updates and upgrades.\nSee also\n\nConfiguring scheduling of virtual machines\n\nChanging the default project quotas\n\nConnecting to OpenStack command-line interface\n\nConfiguring memory for virtual machines\n\nConfiguring CPU features for virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-parameters-in-openstack-configuration-files.html"
    },
    {
        "title": "Changing network interface parameters",
        "content": "Changing network interface parameters\nChanging network interface configuration after deploying storage or compute services includes network migration of this interface.\nPrerequisites\n\nA network interface is configured, as described in Configuring node network interfaces.\n\nTo change network interface parameter\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface. \nOn the interface right pane, click Edit.\n\nIn the Edit network interface window, change the required network settings. For example, you can change the IP address or infrastructure network assignment.\n\nClick Save to apply your changes. \nIf you have already deployed storage or compute services, you will see the migration wizard. Wait until the new configuration is created, and then click Apply.\n\nCommand-line interface\nUse the following command:vinfra node iface set [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                      [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                      [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                      [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                      [--network <network> | --no-network] [--connected-mode | --datagram-mode]\r\n                      [--ifaces <ifaces>] [--bond-type <bond-type>] [--node <node>] <iface>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--no-network\n\nRemove a network from the interface\n--connected-mode\n\nEnable connected mode (InfiniBand interfaces only)\n--datagram-mode\n\nEnable datagram mode (InfiniBand interfaces only)\n--ifaces <ifaces>\n\nA comma-separated list of network interface names, for example, iface1,iface2,...,ifaceN\n--bond-type <bond-type>\n\nBond type (balance-rr, balance-xor, broadcast, 802.3ad, balance-tlb, balance-alb)\nBond type for an OVS interface (balance-tcp, active-backup)\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nFor example, to change the IP address of the eth1 network interface located on the node node002 to 192.168.128.91/24, run:# vinfra node iface set eth1 --node node002 --ipv4 192.168.128.91/24\r\n+----------------------------+-----------------------------------------------------------------------+\r\n| Field                      | Value                                                                 |\r\n+----------------------------+-----------------------------------------------------------------------+\r\n| configuration              | network_id: f50605a3-64f4-4f0c-b50e-9481ec221c72                      |\r\n| link                       | href: /api/v2/network/migration/ba7854ed-167e-4d6b-ab19-7244371a1b27/ |\r\n|                            | method: GET                                                           |\r\n|                            | rel: network-migration-details                                        |\r\n| operation                  | network-migration                                                     |\r\n| progress                   | 0.0                                                                   |\r\n| single_interface_migration | True                                                                  |\r\n| state                      | preparing                                                             |\r\n| task_id                    | ba7854ed-167e-4d6b-ab19-7244371a1b27                                  |\r\n| transitions                | 0                                                                     |\r\n+----------------------------+-----------------------------------------------------------------------+\nIf you have already deployed storage or compute services, you will see the output above. Wait until the new network configuration is tested, and then apply it:# vinfra cluster network migration show | state\r\n| state                      | test-passed                                  |\r\n# vinfra cluster network migration apply\n\nSee also\n\nChanging network configuration\n\nManaging network interfaces",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface set [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                      [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                      [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                      [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                      [--network <network> | --no-network] [--connected-mode | --datagram-mode]\r\n                      [--ifaces <ifaces>] [--bond-type <bond-type>] [--node <node>] <iface>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--no-network\n\nRemove a network from the interface\n--connected-mode\n\nEnable connected mode (InfiniBand interfaces only)\n--datagram-mode\n\nEnable datagram mode (InfiniBand interfaces only)\n--ifaces <ifaces>\n\nA comma-separated list of network interface names, for example, iface1,iface2,...,ifaceN\n--bond-type <bond-type>\n\n\nBond type (balance-rr, balance-xor, broadcast, 802.3ad, balance-tlb, balance-alb)\nBond type for an OVS interface (balance-tcp, active-backup)\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nFor example, to change the IP address of the eth1 network interface located on the node node002 to 192.168.128.91/24, run:# vinfra node iface set eth1 --node node002 --ipv4 192.168.128.91/24\r\n+----------------------------+-----------------------------------------------------------------------+\r\n| Field                      | Value                                                                 |\r\n+----------------------------+-----------------------------------------------------------------------+\r\n| configuration              | network_id: f50605a3-64f4-4f0c-b50e-9481ec221c72                      |\r\n| link                       | href: /api/v2/network/migration/ba7854ed-167e-4d6b-ab19-7244371a1b27/ |\r\n|                            | method: GET                                                           |\r\n|                            | rel: network-migration-details                                        |\r\n| operation                  | network-migration                                                     |\r\n| progress                   | 0.0                                                                   |\r\n| single_interface_migration | True                                                                  |\r\n| state                      | preparing                                                             |\r\n| task_id                    | ba7854ed-167e-4d6b-ab19-7244371a1b27                                  |\r\n| transitions                | 0                                                                     |\r\n+----------------------------+-----------------------------------------------------------------------+\nIf you have already deployed storage or compute services, you will see the output above. Wait until the new network configuration is tested, and then apply it:# vinfra cluster network migration show | state\r\n| state                      | test-passed                                  |\r\n# vinfra cluster network migration apply\n",
                "title": "To change network interface parameter"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface. \nOn the interface right pane, click Edit.\n\nIn the Edit network interface window, change the required network settings. For example, you can change the IP address or infrastructure network assignment.\n\n\n\n\n\nClick Save to apply your changes. \nIf you have already deployed storage or compute services, you will see the migration wizard. Wait until the new configuration is created, and then click Apply.\n\n\n\n\n\n",
                "title": "To change network interface parameter"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-network-interface-parameters.html"
    },
    {
        "title": "Changing the default project quotas",
        "content": "Changing the default project quotas\nThe default quotas are applied when creating a project via the OpenStack API. You can modify the following default quotas:\n\nParameter\nConfiguration file\nDescription\nValue\n\ndefault_trait_quota\n\n/etc/kolla/placement-api/placement.conf\n\nSets the maximum number of placements per project.\nValid values are integer. The default value is set to 0. To make the parameter unlimited, set the value to -1.\n\ndefault_load_balancer_quota\n\n/etc/kolla/octavia-api/octavia.conf\n\nSets the maximum number of load balancers per project.\n\nmax_clusters_per_project\n\n/etc/kolla/magnum-api/magnum.conf\n\nSets the maximum number of Kubernetes clusters per project.\n\nIf required, you can also change default quotas for other compute resources. To do this, refer to the official OpenStack documentation.\nTo change the default project quota\nUse the following command:vinfra service compute set [--custom-param <service_name> <config_file> <section> <property> <value>]\r\n                           [--load-balancer-default-quota <value>] [--k8s-default-quota <value>]\r\n                           [--placement-default-quota <value>]\n\n--custom-param <service_name> <config_file> <section> <property> <value>\n\nSet custom parameters for OpenStack configuration files:\n\nservice_name is the service name: octavia-api, magnum-api, or placement-api\nconfig_file specifies the service configuration file: octavia.conf for octavia-api, magnum.conf for magnum-api, or placement.conf for placement-api\nsection specifies the section in the service configuration file where the parameter is defined: quotas in octavia.conf and magnum.conf, or quota in placement.conf\nproperty is the parameter to be changed: default_load_balancer_quota in octavia.conf, max_clusters_per_project in magnum.conf, or default_trait_quota in placement.conf\nvalue is a new parameter value\n\n--load-balancer-default-quota <value>\n\nShortcut for --custom-param octavia-api octavia.conf quotas default_load_balancer_quota <value>\n--k8s-default-quota <value>\n\nShortcut for --custom-param magnum-api magnum.conf quotas max_clusters_per_project <value>\n--placement-default-quota <value>\n\nShortcut for --custom-param placement-api placement.conf quota default_trait_quota <value>\n\nFor example, to make the number of placements per project unlimited, but limit the maximum number of load balancers to 100 and Kubernetes clusters to 10 per project, run:# vinfra service compute set --placement-default-quota -1 --load-balancer-default-quota 100 --k8s-default-quota 10\r\n\nTo check that the default project quotas are successfully modified, run:# vinfra service compute show\r\n+--------------+-----------------------------------------+\r\n| Field        | Value                                   |\r\n+--------------+-----------------------------------------+\r\n| <...>        | <...>                                   |\r\n| options      | cpu_model: ''                           |\r\n|              | custom_params:                          |\r\n|              | - config_file: octavia.conf             |\r\n|              |   property: default_load_balancer_quota |\r\n|              |   section: quotas                       |\r\n|              |   service_name: octavia-api             |\r\n|              |   value: 100                            |\r\n|              | - config_file: magnum.conf              |\r\n|              |   property: max_clusters_per_project    |\r\n|              |   section: quotas                       |\r\n|              |   service_name: magnum-api              |\r\n|              |   value: 10                             |\r\n|              | - config_file: placement.conf           |\r\n|              |   property: default_trait_quota         |\r\n|              |   section: quota                        |\r\n|              |   service_name: placement-api           |\r\n|              |   value: -1                             |\r\n|              | notification_forwarding: disabled       |\r\n| status       | active                                  |\r\n+--------------+-----------------------------------------+\nSee also\n\nChanging parameters in OpenStack configuration files\n\nConfiguring scheduling of virtual machines\n\nConnecting to OpenStack command-line interface\n\nConfiguring memory for virtual machines\n\nConfiguring CPU features for virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-the-default-project-quotas.html"
    },
    {
        "title": "Checking the RDMA network",
        "content": "Checking the RDMA network\nYou can check the RDMA network state via vendor-provided command line tools.\nTo check the network hardware\nLists the RDMA devices available for use:# ibv_devices\r\n    device          \t   node GUID\r\n    ------          \t----------------\r\n    rdmao1          \t5e6f69fffe27b644\r\n    rdmao2          \t5e6f69fffe27b645\r\n\nTo check the network connectivity\nStart the RDMA-ping server on any node:# rping -s -C 10 -v\nThen, start the RDMA-ping client on any other node:# rping -c -a <server_IP> -C 10 -v\nWhere <server_IP> is the IP address of the RDMA-ping server.\nThe tool will have an output similar to this:ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqr\r\nping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrs\r\nping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrst\r\nping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstu\r\nping data: rdma-ping-4: EFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuv\r\nping data: rdma-ping-5: FGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvw\r\nping data: rdma-ping-6: GHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwx\r\nping data: rdma-ping-7: HIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxy\r\nping data: rdma-ping-8: IJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz\r\nping data: rdma-ping-9: JKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyzA\r\nclient DISCONNECT EVENT...\r\n\nTo check the network bandwidth\nStart the benchmark server on any node:# ib_send_bw -d mlx4_0 -i 1 -F --report_gbits\nThen, start the benchmark client on any other node:# ib_send_bw -d mlx4_0 -i 1 -F --report_gbits <server_IP>\nWhere <server_IP> is the IP address of the benchmark server.\nThe tool will have an output similar to this:Send BW Test\r\nDual-port : OFF Device : mlx4_0\r\nNumber of qps : 1 Transport type : IB\r\nConnection type : RC\r\nRX depth : 512\r\nCQ Moderation : 100\r\nMtu : 1024[B]\r\nLink type : Ethernet\r\nGid index : 0\r\nMax inline data : 0[B]\r\nrdma_cm QPs : OFF\r\nData ex. method : Ethernet\r\n--------------------------------------------------------------------------\r\nlocal address: LID 0000 QPN 0x0065 PSN 0xc8f367\r\nGID: 254:128:00:00:00:00:00:00:246:82:20:255:254:23:27:129\r\nremote address: LID 0000 QPN 0x005d PSN 0x884d7d\r\nGID: 254:128:00:00:00:00:00:00:246:82:20:255:254:23:31:225\r\n--------------------------------------------------------------------------\r\n#bytes #iterations BW peak[Gb/sec] BW average[Gb/sec] MsgRate[Mpps]\r\n65536  1000        0.00            36.40              0.069428\r\n\nWhat's next\n\nConfiguring RDMA automatically",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/checking-the-rdma-network.html"
    },
    {
        "title": "Changing the vGPU type for physical GPUs",
        "content": "Changing the vGPU type for physical GPUs\nAfter enabling vGPU for the compute cluster, you can change the vGPU type specified for a physical GPU, if necessary.\nPrerequisites\n\nThe compute cluster is reconfigured for vGPU support, as described in Enabling PCI passthrough and vGPU support.\nEnsure that the compute cluster has no virtual machines with the current vGPU type.\n\nTo change the vGPU type for a physical GPU\n\nModify the configuration file. For example, replace nvidia-224 with nvidia-228 in the vgpu_type field:- device_type: pgpu\r\n  device: \"0000:01:00.0\"\r\n  vgpu_type: nvidia-228\n\nPass the configuration file to the vinfra service compute set command. For example:# vinfra service compute set --pci-passthrough-config config.yaml\n\nReboot the node with the physical GPU to apply changes:# reboot\n\nWhat's next\n\nCreating virtual machines with virtual GPUs\n\nCreating virtual machines with different vGPU types",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-the-vgpu-type-for-physical-gpus.html"
    },
    {
        "title": "Checking disk data flushing capabilities",
        "content": "Checking disk data flushing capabilities\nIt is highly recommended to ensure that all storage devices you plan to include in your cluster can flush data from cache to disk if the power goes out unexpectedly. Thus you will find devices that may lose data in a power failure.\nVirtuozzo Hybrid Infrastructure ships with the vstorage-hwflush-check tool that checks how a storage device flushes data to disk in emergencies. The tool is implemented as a client/server utility:\n\nThe client continuously writes blocks of data to the storage device. When a data block is written, the client increases a special counter and sends it to the server that keeps it.\nThe server keeps track of counters incoming from the client and always knows the next counter number. If the server receives a counter smaller than the one it has (for example, because the power has failed and the storage device has not flushed the cached data to disk), the server reports an error.\n\nTo check that a storage device can successfully flush data to disk when power fails, follow the procedure below:\n\nOn one node, run the server: # vstorage-hwflush-check -l\n\nOn a different node that hosts the storage device you want to test, run the client. For example:# vstorage-hwflush-check -s vstorage1.example.com -d /vstorage/stor1-ssd/test -t 50\nwhere\n\nvstorage1.example.com is the host name of the server.\n/vstorage/stor1-ssd/test is the directory to use for data flushing tests. During execution, the client creates a file in this directory and writes data blocks to it.\n50 is the number of threads for the client to write data to disk. Each thread has its own file and counter. You can increase the number of threads (max. 200) to test your system in more stressful conditions. You can also specify other options when running the client. For more information on available options, refer to the vstorage-hwflush-check manual page.\n\nWait for at least 10-15 seconds, cut power from the client node (either press the Power button or pull the power cord out), and then power it on again.\n\nRestart the client:\n# vstorage-hwflush-check -s vstorage1.example.com -d /vstorlage/stor1-ssd/test -t 50\nOnce launched, the client will read all previously written data, determine the version of data on the disk, and restart the test from the last valid counter. It then will send this valid counter to the server and the server will compare it to the latest counter it has. You may see output like:\nid<N>:<counter_on_disk> -> <counter_on_server>\n\nwhich means one of the following:\n\nIf the counter on the disk is lower than the counter on the server, the storage device has failed to flush the data to the disk. Avoid using this storage device in production, especially for CS or journals, as you risk losing data.\nIf the counter on the disk is higher than the counter on the server, the storage device has flushed the data to the disk but the client has failed to report it to the server. The network may be too slow or the storage device may be too fast for the set number of load threads, so consider increasing it. This storage device can be used in production.\nIf both counters are equal, the storage device has flushed the data to the disk and the client has reported it to the server. This storage device can be used in production.\n\nTo be on the safe side, repeat the procedure several times. Once you have checked your first storage device, continue with all of the remaining devices you plan to use in the cluster. You need to test all devices you plan to use in the cluster: SSD disks used for CS journaling, disks used for MDS journals and chunk servers.\nSee also\n\nQuantity of disks per node\n\nHDD/SSD configuration\n\nProtecting data during a power outage\n\nServer requirements\n\nNetwork requirements and recommendations",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/checking-disk-data-flushing-capabilities.html"
    },
    {
        "title": "Changing TLS configuration for backup storage",
        "content": "Changing TLS configuration for backup storage\nTo filter connections to backup storage, an administrator can configure allowed TLS protocol versions and ciphers. By default, only TLS protocol version 1.2 is accepted and recommended to use for connections to backup storage.\nTo change the default TLS protocol version\nSpecify the appropriate value in the advanced.min_tls_version parameter in the /etc/vstorage/abgw.config file. The following values are available:\r\n\n\n0: Allows 1.0, 1.1, and 1.2 TLS protocol versions\n1: Allows 1.1 and 1.2 TLS protocol versions\n2: Allows only 1.2 TLS protocol version\n\nFor example, to allow using the 1.1 and 1.2 TLS protocol versions, do the following:\n\nIn /etc/vstorage/abgw.config, set the advanced.min_tls_version parameter to 1:advanced.min_tls_version=1\n\nRestart the service:# systemctl restart vstorage-abgw\n\nThis operation should be performed on all backup storage nodes.\nTo accept connections to backup storage only with particular TLS ciphers\nSpecify them in the advanced.tls_ciphers parameter in the /etc/vstorage/abgw.config file. For the cipher format and full set, refer to the cipher list section in the ciphers manual page.\n\nAfter changing the allowed TLS ciphers, you may need to regenerate certificates.\n\nIf a client has none of the specified ciphers, the connection will fail and the client will not be able to reach the service. \n\nBy default, the following ciphers are used:\n\nECDHE-ECDSA-CHACHA20-POLY1305\nECDHE-RSA-CHACHA20-POLY1305\nECDHE-ECDSA-AES256-GCM-SHA384\nECDHE-RSA-AES256-GCM-SHA384\nECDHE-ECDSA-AES128-GCM-SHA256\nECDHE-RSA-AES128-GCM-SHA256\nECDHE-ECDSA-AES128-SHA256\nECDHE-RSA-AES128-SHA256\nDHE-RSA-AES128-GCM-SHA256\nDHE-RSA-AES128-SHA256\nECDHE-RSA-AES256-SHA\nECDHE-ECDSA-AES128-SHA\nECDHE-RSA-AES128-SHA\nDHE-RSA-AES128-SHA\nAES128-GCM-SHA256\nAES128-SHA256\nAES128-SHA\n\nNote the following:\n\nIf you specify one cipher (for example, RSA-AES128) and it is not supported, the connection will fail. \nIf you specify two ciphers (for example, CAMELIA and RSA-AES128) and only one of them is supported (for example, CAMELIA), the connection will be established based on the supported cipher (in this case, CAMELIA).\nIf you specify an empty value, all connections will fail.\n\nFor example, to limit the allowed TLS ciphers only to ECDHE-ECDSA-CHACHA20-POLY1305 and ECDHE-RSA-CHACHA20-POLY1305, do the following:\n\nIn /etc/vstorage/abgw.config, specify the required ciphers, separated by colons, in the advanced.tls_ciphers parameter:\r\nadvanced.tls_ciphers=ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305\n\nRestart the service:# systemctl restart vstorage-abgw\n\nThis operation should be performed on all backup storage nodes.\nSee also\n\nChanging the redundancy scheme for backup storage\n\nManaging registrations for backup storage\n\nManaging geo-replication for backup storage\n\nReleasing nodes from backup storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/changing-tls-configuration-for-backup-storage.html"
    },
    {
        "title": "Chunks chart",
        "content": "Chunks chart\n\nAdmin panel\nYou can monitor the state of all chunks in the cluster on the Chunks chart. Chunks can be in the following states:\n\nHealthy\n\nNumber and percentage of chunks that have enough active replicas. The normal state of chunks.\nOffline\n\nNumber and percentage of chunks all replicas of which are offline. Such chunks are completely inaccessible for the cluster and cannot be replicated, read from or written to. All requests to an offline chunk are frozen until a CS that stores that chunk\u00e2\u0080\u0099s replica goes online.\nGet offline chunk servers back online as fast as possible, to avoid losing data.\n\nBlocked\n\nNumber and percentage of chunks that have fewer active replicas than the set minimum amount. Write requests to a blocked chunk are frozen until it has at least the set minimum amount of replicas. Read requests to blocked chunks are allowed, however, as they still have some active replicas left. Blocked chunks have a higher replication priority than degraded chunks.\nHaving blocked chunks in the cluster increases the risk of losing data, so postpone any maintenance on working cluster nodes and get offline chunk servers back online as fast as possible.\n\nDegraded\n\nNumber and percentage of chunks whose active replicas are few, but not below the set minimum. Such chunks can be read from and written to. However, in the latter case a degraded chunk becomes urgent.\n\nHealthy chunks are highlighted on the scale in green, offline in red, blocked in yellow, and degraded in grey. For example:\n\nThe Replication section shows the information about replication activity in the cluster.\n\nCommand-line interface\nUse the following command:vinfra cluster overview\nFor example, to view the information about chunks in the cluster cluster1, take a look at these lines from the command output:\r\n+-------------------+-------------------------+\r\n| Field             | Value                   |\r\n+-------------------+-------------------------+\r\n| ...               | ...                     |\r\n| chunks            | blocked: 0              |\r\n|                   | degraded: 0             |\r\n|                   | deleting: 0             |\r\n|                   | healthy: 153            |\r\n|                   | offline: 0              |\r\n|                   | overcommitted: 0        |\r\n|                   | pending: null           |\r\n|                   | replicating: 0          |\r\n|                   | standby: null           |\r\n|                   | total: 153              |\r\n|                   | unique: 0               |\r\n|                   | urgent: 0               |\r\n|                   | void: 0                 |\r\n| ...               | ...                     |\r\n+-------------------+-------------------------+\r\n\n\nblocked\n\nNumber of chunks that have fewer active replicas than the set minimum amount. Write requests to a blocked chunk are frozen until it has at least the set minimum amount of replicas. Read requests to blocked chunks are allowed, however, as they still have some active replicas left. Blocked chunks have a higher replication priority than degraded chunks.\nHaving blocked chunks in the cluster increases the risk of losing data, so postpone any maintenance on working cluster nodes and get offline chunk servers back online as fast as possible.\n\ndegraded\n\nNumber of chunks whose active replicas are few, but not below the set minimum. Such chunks can be read from and written to. However, in the latter case, a degraded chunk becomes urgent.\ndeleting\n\nNumber of chunks queued for deletion.\nhealthy\n\nNumber of chunks that have enough active replicas. The normal state of chunks.\noffline\n\nNumber of chunks all replicas of which are offline. Such chunks are completely inaccessible for the cluster and cannot be replicated, read from or written to. All requests to an offline chunk are frozen until a CS that stores that chunk\u00e2\u0080\u0099s replica goes online.\nGet offline chunk servers back online as fast as possible, to avoid losing data.\n\novercommitted\n\nNumber of chunks that have more replicas than normal. Usually these chunks appear after the normal number of replicas has been lowered or a lot of data has been deleted. Extra replicas are eventually dropped, however, this process may slow down during replication.\npending\n\nNumber of chunks that must be replicated immediately. For a write request from client to a chunk to complete, the chunk must have at least the set minimum amount of replicas. If it does not, the chunk is blocked and the write request cannot be completed. As blocked chunks must be replicated as soon as possible, the cluster places them in a special high-priority replication queue and reports them as pending.\nreplicating\n\nNumber and percentage of chunks which are being replicated. Write operations on such chunks are frozen until replication ends.\nstandby\n\nNumber of chunks that have one or more replicas in the standby state. A replica is marked standby if it has been inactive for no more than 5 minutes.\ntotal\n\nTotal number of all chunks in the storage cluster.\nunique\n\nNumber of chunks that do not have replicas.\nurgent\n\nNumber and percentage of chunks which are degraded and have non-identical replicas. Replicas of a degraded chunk may become non-identical if some of them are not accessible during a write operation. As a result, some replicas happen to have the new data while some still have the old data. The latter are dropped by the cluster as fast as possible. Urgent chunks do not affect information integrity as the actual data is stored in at least the set minimum amount of replicas.\nvoid\n\nNumber and percentage of chunks that have been allocated but never used yet. Such chunks contain no data. It is normal to have some void chunks in the cluster.\n\nSee also\n\nI/O activity charts\n\nServices chart\n\nPhysical space chart\n\nLogical space chart",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/chunks-chart.html"
    },
    {
        "title": "Cluster update metrics",
        "content": "Cluster update metrics\nMetrics that are used to generate cluster update alerts are added to the alerting rules in /var/lib/prometheus/alerts/backend.rules. These metrics are described in the table:\n\nMetric\nDescription\n\nLabels\n\nsoftwareupdates_cluster_info\n\nInformation about software updates for the cluster\n\navailable_version\n\nAvailable version for the cluster\nimpact\n\nUpdate impact:\n\nreboot: reboot is required during the update\nmaintenance: maintenance is required during the update\nno_impact:  reboot and maintenance are not required during the update\n\nstate\n\nUpdate state for the cluster\nversion\n\nCurrently installed version for the cluster\n\nsoftwareupdates_cluster_available\n\nUpdate availability for the cluster\n\navailable_version\n\nAvailable version for the cluster\nimpact\n\nUpdate impact:\n\nreboot: reboot is required during the update\nmaintenance: maintenance is required during the update\nno_impact:  reboot and maintenance are not required during the update\n\nversion\n\nCurrently installed version for the cluster\n\nsoftwareupdates_node_uptodate\n\nShows if a specific node is updated to the latest version\n\navailable_version\n\nAvailable version for a node\nimpact\n\nUpdate impact:\n\nreboot: reboot is required during the update\nmaintenance: maintenance is required during the update\nno_impact:  reboot and maintenance are not required during the update\n\nnode\n\nNode ID\nversion\n\nCurrently installed version for a node\n\nsoftwareupdates_node_available\n\nUpdate availability for a specific node\n\navailable_version\n\nAvailable version for a node\nimpact\n\nUpdate impact:\n\nreboot: reboot is required during the update\nmaintenance: maintenance is required during the update\nno_impact:  reboot and maintenance are not required during the update\n\nnode\n\nNode ID\nversion\n\nCurrently installed version for a node\n\nsoftwareupdates_node_info\n\nInformation about software updates for a specific node\n\navailable_version\n\nAvailable version for a node\nimpact\n\nUpdate impact:\n\nreboot: reboot is required during the update\nmaintenance: maintenance is required during the update\nno_impact:  reboot and maintenance are not required during the update\n\nnode\n\nNode ID\nstate\n\nUpdate state for a node\nversion\n\nCurrently installed version for a node\n\nsoftwareupdates_node_state\n\nUpdate state of a specific node\n\nnode\n\nNode ID\nstate\n\nUpdate state of a node\n\nSee also\n\nInfrastructure alerts\n\nCore storage metrics\n\nObject storage metrics\n\nBackup storage metrics\n\nCompute metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/cluster-update-metrics.html"
    },
    {
        "title": "Cloning volumes",
        "content": "Cloning volumes\nLimitations\n\nYou can clone volumes that are not attached to VMs or attached to stopped VMs.\n\nPrerequisites\n\nA volume is created, as described in Creating and deleting volumes.\n\nTo clone a volume\n\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click a volume.\nOn the volume right pane, click Clone.\n\nIn the Clone volume window, specify a volume name, size, and storage policy. Click Clone.\n\nCommand-line interface\nUse the following command:vinfra service compute volume clone --name <name> [--size <size-gb>]\r\n                                    [--storage-policy <storage_policy>] <volume>\n\n--name <name>\n\n        New volume name\n--size <size-gb>\n\nVolume size, in gigabytes\n--storage-policy <storage_policy>\n\nStorage policy ID or name\n<volume>\n\nVolume ID or name\n\nFor example, to clone the volume myvolume to myvolume2, run:# vinfra service compute volume clone myvolume --name myvolume2\r\n+--------------------------------+------------------------------------------+\r\n| Field                          | Value                                    |\r\n+--------------------------------+------------------------------------------+\r\n| attachments                    | []                                       |\r\n| availability_zone              | nova                                     |\r\n| bootable                       | True                                     |\r\n| consistencygroup_id            |                                          |\r\n| created_at                     | 2021-10-18T16:36:39.937068               |\r\n| description                    |                                          |\r\n| encrypted                      | False                                    |\r\n| id                             | 22eb7529-0a2c-44ce-a73c-24f3152bdb54     |\r\n| imageRef                       |                                          |\r\n| migration_status               |                                          |\r\n| multiattach                    | False                                    |\r\n| name                           | myvolume2                                |\r\n| network_install                | False                                    |\r\n| os-vol-host-attr:host          | node003.vstoragedomain@vstorage#vstorage |\r\n| os-vol-mig-status-attr:migstat |                                          |\r\n| os-vol-mig-status-attr:name_id |                                          |\r\n| project_id                     | b906404c55bb44729da99987536ac5bc         |\r\n| replication_status             |                                          |\r\n| size                           | 64                                       |\r\n| snapshot_id                    |                                          |\r\n| source_volid                   | c80f58c9-c52e-41c4-ad5f-d5b5ed072d4a     |\r\n| status                         | creating                                 |\r\n| storage_policy_name            | default                                  |\r\n| traits                         | []                                       |\r\n| updated_at                     | 2021-10-18T16:36:40.133516               |\r\n| user_id                        | c727a901a6444ee1a8ad31e3d5b53b3a         |\r\n| volume_image_metadata          |                                          |\r\n+--------------------------------+------------------------------------------+\r\n\n\nSee also\n\nAttaching and detaching volumes\n\nResizing volumes\n\nChanging the storage policy for volumes\n\nTransferring volumes between projects",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute volume clone --name <name> [--size <size-gb>]\r\n                                    [--storage-policy <storage_policy>] <volume>\n\n--name <name>\n\n        New volume name\n--size <size-gb>\n\nVolume size, in gigabytes\n--storage-policy <storage_policy>\n\nStorage policy ID or name\n<volume>\n\nVolume ID or name\n\nFor example, to clone the volume myvolume to myvolume2, run:# vinfra service compute volume clone myvolume --name myvolume2\r\n+--------------------------------+------------------------------------------+\r\n| Field                          | Value                                    |\r\n+--------------------------------+------------------------------------------+\r\n| attachments                    | []                                       |\r\n| availability_zone              | nova                                     |\r\n| bootable                       | True                                     |\r\n| consistencygroup_id            |                                          |\r\n| created_at                     | 2021-10-18T16:36:39.937068               |\r\n| description                    |                                          |\r\n| encrypted                      | False                                    |\r\n| id                             | 22eb7529-0a2c-44ce-a73c-24f3152bdb54     |\r\n| imageRef                       |                                          |\r\n| migration_status               |                                          |\r\n| multiattach                    | False                                    |\r\n| name                           | myvolume2                                |\r\n| network_install                | False                                    |\r\n| os-vol-host-attr:host          | node003.vstoragedomain@vstorage#vstorage |\r\n| os-vol-mig-status-attr:migstat |                                          |\r\n| os-vol-mig-status-attr:name_id |                                          |\r\n| project_id                     | b906404c55bb44729da99987536ac5bc         |\r\n| replication_status             |                                          |\r\n| size                           | 64                                       |\r\n| snapshot_id                    |                                          |\r\n| source_volid                   | c80f58c9-c52e-41c4-ad5f-d5b5ed072d4a     |\r\n| status                         | creating                                 |\r\n| storage_policy_name            | default                                  |\r\n| traits                         | []                                       |\r\n| updated_at                     | 2021-10-18T16:36:40.133516               |\r\n| user_id                        | c727a901a6444ee1a8ad31e3d5b53b3a         |\r\n| volume_image_metadata          |                                          |\r\n+--------------------------------+------------------------------------------+\r\n\n",
                "title": "To clone a volume"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click a volume.\nOn the volume right pane, click Clone.\n\nIn the Clone volume window, specify a volume name, size, and storage policy. Click Clone.\n\n\n\n\n\n\n",
                "title": "To clone a volume"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/cloning-volumes.html"
    },
    {
        "title": "Compute cluster architecture",
        "content": "Compute cluster architecture\nThe following diagram shows the major compute components of Virtuozzo Hybrid Infrastructure.\n\nThe identity service provides authentication and authorization capabilities for Virtuozzo Hybrid Infrastructure.\nThe compute service enables users to create, run, and manage virtual machines. This service relies on the custom QEMU/KVM hypervisor.\nThe networking service provides physical and virtual network capabilities for virtual machines.\nThe image service enables users to upload, store, and use images of supported guest operating systems and virtual disks. This service relies on the base storage cluster for data redundancy.\nThe storage service provides virtual disks to virtual machines. This service relies on the base storage cluster for data redundancy.\nThe Kubernetes service allows users to deploy Kubernetes clusters in virtual machines.\nThe load balancer service distributes incoming network traffic across virtual machines from a balancing pool.\n\nSee also\n\nCompute network architecture",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/compute-cluster-architecture.html"
    },
    {
        "title": "Cluster objects and traps",
        "content": "Cluster objects and traps\nCluster-related objects\n\nVSTORAGE-MIB:cluster\n\nGeneral cluster information.\nVSTORAGE-MIB:csStatTable\n\nChunk server statistics table.\nVSTORAGE-MIB:mdsStatTable\n\nMetadata server statistics table.\nVSTORAGE-MIB::clusterName\n\nCluster name.\nVSTORAGE-MIB::healthStatus\n\nCluster health status.\nVSTORAGE-MIB::usedLogicalSpace\n\nThe space occupied by all data chunks and their replicas, plus the space occupied by any other data stored on the cluster nodes\u00e2\u0080\u0099 disks.\nVSTORAGE-MIB::totalLogicalSpace\n\nThe total space on all cluster nodes\u00e2\u0080\u0099 disks.\nVSTORAGE-MIB::freeLogicalSpace\n\nThe unused space on all cluster nodes\u00e2\u0080\u0099 disks.\nVSTORAGE-MIB::licenseStatus\n\nLicense status.\nVSTORAGE-MIB::licenseCapacity\n\nThe maximum disk space available as defined by the license.\nVSTORAGE-MIB::licenseExpirationStatus\n\nLicense expiration status.\nVSTORAGE-MIB::ioReadOpS\n\nCurrent read speed, in operations per second.\nVSTORAGE-MIB::ioWriteOpS\n\nCurrent write speed, in operations per second.\nVSTORAGE-MIB::ioReads\n\nCurrent read speed, in bytes per second.\nVSTORAGE-MIB::ioWrites\n\nCurrent write speed, in bytes per second.\nVSTORAGE-MIB::csActive\n\nThe number of active chunk servers.\nVSTORAGE-MIB::csTotal\n\nThe total number of chunk servers.\nVSTORAGE-MIB::mdsAvail\n\nThe number of running metadata servers.\nVSTORAGE-MIB::mdsTotal\n\nThe total number of metadata servers.\nVSTORAGE-MIB::s3OsAvail\n\nThe number of running S3 object servers.\nVSTORAGE-MIB::s3OsTotal\n\nThe total number of S3 object servers.\nVSTORAGE-MIB::s3NsAvail\n\nThe number of running S3 name servers.\nVSTORAGE-MIB::s3NsTotal\n\nThe total number of S3 name servers.\nVSTORAGE-MIB::s3GwAvail\n\nThe number of running S3 gateways.\nVSTORAGE-MIB::s3GwTotal\n\nThe total number of S3 gateways.\n\nCS-related objects\n\nVSTORAGE-MIB::csId\n\nChunk server identifier.\nVSTORAGE-MIB::csStatus\n\nCurrent chunk server status.\nVSTORAGE-MIB::csIoReadOpS\n\nCurrent read speed of a chunk server, in operations per second.\nVSTORAGE-MIB::csIoWriteOpS\n\nCurrent write speed of a chunk server, in operations per second.\nVSTORAGE-MIB::csIoWait\n\nThe percentage of time spent waiting for I/O operations. Includes time spent waiting for synchronization.\nVSTORAGE-MIB::csIoReadS\n\nCurrent read speed of a chunk server, in bytes per second.\nVSTORAGE-MIB::csIoWriteS\n\nCurrent write speed of a chunk server, in bytes per second.\n\nMDS-related objects\n\nVSTORAGE-MIB::mdsId\n\nMetadata server identifier.\nVSTORAGE-MIB::mdsStatus\n\nCurrent metadata server status.\nVSTORAGE-MIB::mdsMemUsage\n\nThe amount of memory used by a metadata server.\nVSTORAGE-MIB::mdsCpuUsage\n\nThe percentage of the CPU\u00e2\u0080\u0099s capacity used by a metadata server.\nVSTORAGE-MIB::mdsUpTime\n\nTime since the startup of a metadata server.\n\nSNMP traps triggered by the specified alerts\n\nlicense expired\n\nThe license has expired.\nlicense_isnot_loaded\n\nThe license is not loaded.\ntoo few free space\n\nThe cluster is running out of logical space.\ntoo_few_free_phys_space\n\nThe cluster is running out of physical space.\noffline node\n\nA cluster node is offline.\ntoo few nodes\n\nToo few cluster nodes are left.\ntoo few mdses\n\nToo few MDSes are left.\ntoo_much_mdses\n\nMore than one MDS is on a node.\ntoo few cses\n\nToo few CSes are left.\nfailed mds\n\nThe MDS service has failed.\nfailed cs\n\nThe CS service has failed.\ncses_on_single_tier_have_different_journalling_settings\n\nA CS has incorrect journalling settings.\ncses_on_single_tier_have_different_encryption_settings\n\nA CS has incorrect encryption settings.\nsmart_failed\n\nA disk has failed a S.M.A.R.T. check.\ndisk_failed\n\nA disk has failed.\ntoo_few_root_space\n\nThe root partition on a node is out of space.\ntoo_few_space_on_metadata_disk\n\nAn MDS disk is out of space.\nlow_level_network_settings\n\nA network interface is missing important features.\nhalf_duplex\n\nA network interface is not in the full duplex mode.\nlow_speed\n\nA network interface has a speed lower than 1 Gbps.\nundefined_speed\n\nA network interface has an undefined speed.\nnetwork link\n\nA network interface is misconfigured.\nabgw_cert_expired\n\nThe Backup Gateway certificate has expired or will expire soon.\niscsi_redundancy_disk\n\nThe failure domain set for an iSCSI LUN does not make it highly available.\ns3_redundancy_disk\n\nThe failure domain set for an S3 cluster does not make it highly available.\nsoftware_updates\n\nSoftware updates exist for a node.\nno_internet_connection\n\nNo internet connection on a node.\ndisk_write_cache_enabled\n\nDisk write cache is enabled.\ndisk_write_cache_status_unknown\n\nDisk write cache has an unknown status.\ncompute_unavailable\n\nThe compute cluster has failed.\noom_happened\n\nThe OOM killer has been triggered.\nkernel_not_current\n\nThe kernel is outdated on a node.\nno_ha\n\nHigh availability for the admin panel is not configured.\ntime_not_synced\n\nTime is not synced on a node.\niscsi_upgrade_failed\n\niSCSI major upgrade has failed.\nbackend_backup_is_too_old\n\nThe last management node backup has failed, does not exist, or is too old.\n\nother\n\nOther alerts.\n\nSee also\n\nInfrastructure alerts\n\nCore storage alerts\n\nObject storage alerts",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/cluster-objects-and-traps.html"
    },
    {
        "title": "Compute network architecture",
        "content": "Compute network architecture\nVirtuozzo Hybrid Infrastructure supports distributed virtual switching on the basis of Open vSwitch. The latter runs on every compute node and forwards network traffic between virtual machines on the same node, and between virtual machines and infrastructure networks. Distributed virtual switching provides centralized management and monitoring of virtual network configuration across all nodes in the compute cluster.\nDistributed virtual routing used for virtual network connectivity enables placing virtual routers on compute nodes and routing VM traffic directly from hosting nodes. In the DNAT scenario, a floating IP is assigned directly to the VM\u00e2\u0080\u0099s network interface. If SNAT is used, then traffic is routed via management nodes.\nPhysical network connectivity\nPhysical networks are connected to infrastructure networks on Layer 2.\nThe physical representation of physical network connectivity can be shown as follows:\n\nOn the figure above:\n\nFive virtual machines are distributed across the compute cluster and connected to two untagged physical networks via two physical switches: VM1 and VM2 belong to one physical network, while VM3, VM4, and VM5 belong to the other one.\nFor each compute network, the DHCP server runs on the management node.\nThe compute nodes are connected to one physical switch via the eth0 network interfaces, and to the other physical switch via eth1, and reside in two separate L2 segments.\nThe eth0 and eth1 network interfaces are connected to the infrastructure networks with the VM public traffic type.\nThe physical router interconnects two physical networks created on top of the infrastructure ones and provides access to public networks, such as the Internet.\n\nLogically, the physical networking scheme can be represented as follows:\n\nVirtual network connectivity\nVXLAN technology used for virtual networks allows creating logical L2 networks in L3 networks by encapsulating (tunneling) Ethernet frames over UDP packets.\nThe physical representation of virtual network connectivity can be shown as follows:\n\nOn the figure above:\n\nThree virtual machines are distributed across the compute cluster and connected to two virtual networks via two virtual switches: VM1 and VM2 belong to one virtual network, VM3 belongs to the other one.\nFor each compute network, the DHCP server runs on the management node.\nThe distributed virtual router connects the virtual networks and the untagged physical network created on top of the infrastructure one.\nThe compute nodes are connected to the physical switch via the eth0 network interfaces and reside in one L2 segment.\nThe eth0 network interfaces are connected to the infrastructure network with the VM private and VM public traffic types.\nThe physical router provides access to public networks, such as the Internet.\n\nLogically, the virtual networking scheme can be represented as follows:\n\nSee also\n\nCompute cluster architecture",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/compute-network-architecture.html"
    },
    {
        "title": "Concepts and features",
        "content": "Concepts and features\nThis section explains key concepts and features of Virtuozzo Hybrid Infrastructure. This information could be useful for understanding how the product works.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/concepts-and-features.html"
    },
    {
        "title": "Compute metrics",
        "content": "Compute metrics\nMetrics that are used to generate compute alerts are added to the alerting rules and can be found in these files on any node in the cluster:\n\n/var/lib/prometheus/alerts/docker.rules\n\n/var/lib/prometheus/alerts/openstack_cluster.rules\n\n/var/lib/prometheus/alerts/openstack_node.rules\n\n/var/lib/prometheus/alerts/openstack_projects.rules\n\n/var/lib/prometheus/alerts/openstack_services.rules\n\n/var/lib/prometheus/alerts/rabbitmq.rules\n\nThe most important of these metrics are described in the table:\n\nMetric\nDescription\n\nService state metrics\n\nopenstack_cinder_up\n\nShows if the OpenStack Block Storage (Cinder) service is up and running\n\nopenstack_cinder_agent_state\n\nState of the OpenStack Block Storage (Cinder) agent\n\nopenstack_glance_up\n\nShows if the OpenStack Image (Glance) service is up and running\n\nopenstack_heat_up\n\nShows if the OpenStack Orchestration  (Heat) service is up and running\n\nopenstack_container_infra_up\n\nShows if the OpenStack Container (Magnum) service is up and running\n\nopenstack_neutron_up\n\nShows if the OpenStack Networking (Neutron) service is up and running\n\nopenstack_neutron_agent_state\n\nState of the OpenStack Networking (Neutron) agent\n\nopenstack_nova_up\n\nShows if the OpenStack Compute (Nova) service is up and running\n\nopenstack_nova_agent_state\n\nState of the OpenStack Compute (Nova) agent\n\nopenstack_loadbalancer_up\n\nShows if the OpenStack Load Balancer (Octavia) service is up and running\n\nopenstack_placement_up\n\nShows if the OpenStack Placement service is up and running\n\nResource metrics\n\nopenstack_nova_limits_memory_max\n\nProject memory quota, in megabytes\n\nopenstack_nova_limits_memory_used\n\nMemory used by a project, in megabytes\n\nopenstack_nova_limits_vcpus_max\n\nProject vCPU quota\n\nopenstack_nova_limits_vcpus_used\n\nvCPUs used by a project\n\nopenstack_neutron_network_ip_availabilities_total\n\nProject IP address quota\n\nopenstack_neutron_network_ip_availabilities_used\n\nIP addresses used by a project\n\nopenstack_placement_resource_allocation_ratio\n\nVirtual CPU/RAM to physical CPU/RAM allocation ratio\n\nopenstack_placement_resource_reserved\n\nNumber of vCPUs or amount of RAM reserved for the system or storage services\n\nopenstack_placement_resource_total\n\nTotal number of vCPUs or total amount of RAM\n\nopenstack_placement_resource_usage\n\nNumber of vCPUs or amount of RAM provisioned fo virtual machines\n\nRabbitMQ metrics\n\nsoftwareupdates_node_info\n\nCurrently installed version and available version for a specific node\n\nbackend_ha_reconfigure\n\nShows if the HA reconfiguration task is in progress\n\nbackend_node_management\n\nNumber of management nodes. Shows whether high availability is enabled or not.\n\nrabbitmq_build_info\n\nNumber of nodes in the RabbitMQ cluster\n\nrabbitmq_queues\n\nNumber of RabbitMQ queues\n\nOther metrics\n\nopenstack_identity_project_info\n\nProject information\n\nopenstack_nova_server_status\n\nVirtual machine status\n\nnode_systemd_unit_state\n\nState of the systemd services on a node\n\nSee also\n\nCompute alerts\n\nCore storage metrics\n\nObject storage metrics\n\nBackup storage metrics\n\nCluster update metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/compute-metrics.html"
    },
    {
        "title": "Configuring compute parameters",
        "content": "Configuring compute parameters\nAfter deploying the compute cluster, you may want to configure the OpenStack command-line client, custom parameters in the OpenStack configuration files, default quotas for projects, as well as RAM reservations and CPU features for virtual machines.\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-compute-parameters.html"
    },
    {
        "title": "Configuring bucket notifications",
        "content": "Configuring bucket notifications\nYou can use event notifications to receive notifications of certain bucket events. To enable event notifications for a bucket, you need to add a notification configuration that specifies which actions will trigger events and where these notifications will be delivered.\nThe following event types are supported:\n\nEvent\nDescription\n\ns3:ObjectCreated:*\nObject created event (all object create events)\n\ns3:ObjectCreated:Put\nObject created event (PUT request)\n\ns3:ObjectCreated:Post\nObject created event (POST request)\n\ns3:ObjectCreated:Copy\nObject created event (COPY request)\n\ns3:ObjectCreated:CompleteMultipartUpload\nObject created event (Multipart upload completed)\n\ns3:ObjectRemoved:*\nObject removal event (all object removal events)\n\ns3:ObjectRemoved:Delete\nObject removal event (DELETE request)\n\ns3:ObjectRemoved:DeleteMarkerCreated\nObject removal event (DELETE marker created)\n\ns3:ObjectLifecycle:Expiration:Current\t\nObject lifecycle event (current object expired)\n\ns3:ObjectLifecycle:Expiration:NonCurrent\nObject lifecycle event (noncurrent object expired)\n\ns3:ObjectLifecycle:Expiration:DeleteMarker\nObject lifecycle event (delete marker expired)\n\ns3:ObjectLifecycle:Expiration:AbortMultipartUpload\nObject lifecycle event (multipart upload aborted due to expiration)\n\ns3:LifecycleExpiration:*\nLifecycle expiration event (all lifecycle expiration events)\n\ns3:LifecycleExpiration:Delete\nLifecycle expiration event (DELETE request)\n\ns3:LifecycleExpiration:DeleteMarkerCreated\nLifecycle expiration event (DELETE marker created)\n\ns3:ObjectAcl:Put\nObject ACL event (PUT request)\n\ns3:Replication:OperationCompletedReplication\nCross-region replication event (replication operation completed)\n\ns3:Replication:OperationFailedReplication\r\n                    \n\nCross-region replication event (replication operation failed)\n\nAmazon S3 event notifications supports two actions:\n\nPUT Bucket notification configuration enables notifications of specified events for a bucket. Use only the TopicConfiguration element in the request body. To disable notifications, specify an empty NotificationConfiguration element.\n\nGET Bucket notification configuration returns the current notification configuration of a bucket. If notifications are disabled for a bucket, the request returns an empty NotificationConfiguration element.\n\nThe Virtuozzo Hybrid Infrastructure implementation of the Amazon S3 protocol supports only the Simple Notification Service (SNS) topics as the destination type for event notifications. An SNS topic contains details about the target endpoint where to deliver notification messages. You can manage SNS topics by using the ostor-topic-cmd tool. The supported endpoint types include:\n\nHTTP/HTTPS\nKafka\nAdvanced Message Queuing Protocol (AMQP)\n\nEach topic has a unique Amazon Resource Name (ARN), which needs to be specified in the TopicConfiguration element when creating the bucket notification configuration.\n\nUndelivered messages are lost upon restart of the NDS service.\n\nTo create an SNS topic\nUse the following command:ostor-topic-cmd create --user <user_id> --name <topic_name> --endpoint <url> [--verify-ssl <true|false>]\r\n                       [--kafka-ack-level <none|broker>] [--use-ssl <true|false>] [--ca-location <file>]\r\n                       [--opaque-data <data>] [--persistent <true|false>] [--cloudevents <true|false>]\r\n                       [--amqp-exchange <exchange>] [--amqp-ack-level <none|broker|routable>]\r\n                       [--mechanism <scram-sha-512|scram-sha-256|plain>] [--retry <count>] [--delay <count>]\n\n--user <user_id>\n\nUser ID of the topic owner\n--name <topic_name>\n\nName of the topic\n--endpoint <url>\n\nThe URI of an endpoint to send push notifications to\n--verify-ssl <true|false>\n\nIndicates whether the server certificate is validated by the client (default: true).\n--kafka-ack-level <none|broker>\n\nMessages may persist in the broker before being delivered to their final destinations (default: broker).\n--use-ssl <true|false>\n\nUse a secure connection to connect to the broker (default: false).\n--ca-location <file>\n\nCA will be used instead of the default CA to authenticate the broker.\n--opaque-data <data>\n\nOpaque data is set in the topic configuration and added to all notifications that are triggered by the topic.\n--persistent <true|false>\n\nIndicates whether notifications to this endpoint are persistent (asynchronous) or not persistent (default: false).\n--cloudevents <true|false>\n\nIndicates whether the HTTP header should contain attributes according to the S3 CloudEvents Specification (default: false).\n--amqp-exchange <exchange>\n\nThe exchanges must exist and must be able to route messages based on topics.\n--amqp-ack-level <none|broker|routable>\n\nMessages may persist in the broker before being delivered to their final destinations (default: broker)\n--mechanism <scram-sha-512|scram-sha-256|plain>\n\nSASL mechanism\n--retry <count>\n\nRetry count in the range 1-65535 (default: 1)\n--delay <count>\n\nDelay between retries, in seconds, in the range 1-86400 (default: 5)\n\nFor example, to create a topic with the name mytopic from the user with the ID b3b1223261a29452, run:# ostor-topic-cmd create --user b3b1223261a29452 --name mytopic --endpoint http://example.com\r\narn:aws:sns::b3b1223261a29452:mytopic\nThe command output shows the ARN of the created topic.\nTo get more details about the topic, use the ostor-topic-cmd info command specifying the user ID and the obtained topic ARN:# ostor-topic-cmd info --user b3b1223261a29452 --arn arn:aws:sns::b3b1223261a29452:mytopic\r\n<InformationResponse>\r\n  <User>b3b1223261a29452</User>\r\n  <Name>mytopic</Name>\r\n  <EndPoint>\r\n    <EndpointAddress>http://example.com</EndpointAddress>\r\n    <EndpointArgs></EndpointArgs>\r\n    <EndpointTopic>mytopic</EndpointTopic>\r\n    <HasStoredSecret>false</HasStoredSecret>\r\n    <Persistent>true</Persistent>\r\n  </EndPoint>\r\n  <TopicArn>arn:aws:sns::b3b1223261a29452:mytopic</TopicArn>\r\n  <OpaqueData></OpaqueData>\nTo delete an SNS topic\nUse the following command:ostor-topic-cmd delete --user <user_id> --arn <arn>\n\n--user <user_id>\n\nUser ID of the topic owner\n--arn <arn>\n\n Topic ARN\n\nFor example, to delete the topic with the ARN, run:# ostor-topic-cmd delete --user b3b1223261a29452 --arn arn:aws:sns::b3b1223261a29452:mytopic\nSee also\n\nSupported Amazon S3 features\n\nManaging S3 users\n\nManaging S3 buckets",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-bucket-notifications.html"
    },
    {
        "title": "Configuring Kubernetes DNS and discovery parameters",
        "content": "Configuring Kubernetes DNS and discovery parameters\nYou can set a custom DNS nameserver and discovery URL for Kubernetes clusters. The default DNS nameserver is 8.8.8.8. The discovery URL is used by Kubernetes to discover members of the etcd cluster. By default, Kubernetes uses the public discovery service at https://discovery.etcd.io and generates a unique URL for each cluster.\nTo change the DNS nameserver\nUse the following command:vinfra service compute k8saas defaults set --dns-nameserver <dns-nameserver> <version>\n\n--dns-nameserver <dns-nameserver>\n\nThe DNS nameserver to be used for Kubernetes clusters.\n<version>\n\nKubernetes version to apply new defaults for.\n\nFor example, to set the DNS nameserver to 1.1.1.1 and apply this change for all of the supported Kubernetes versions, run:# vinfra service compute k8saas defaults set --dns-nameserver 1.1.1.1\nTo set this DNS nameserver only for version 1.24.3, append the version number to the command:# vinfra service compute k8saas defaults set --dns-nameserver 1.1.1.1 v1.24.3\r\n\nTo change the discovery URL\nUse the following command:vinfra service compute k8saas defaults set --discovery-url <discovery-url> <version>\n\n--discovery-url <discovery-url>\n\nSpecifies a custom delivery URL for node discovery.\n<version>\n\nKubernetes version to apply new defaults for.\n\nFor example, to set the discovery URL to medium and apply this change for all of the supported Kubernetes versions, run:# vinfra service compute k8saas defaults set --discovery-url my.discovery.url\nTo set this flavor only for version 1.24.3, append the version number to the command:# vinfra service compute k8saas defaults set --discovery-url my.discovery.url v1.24.3\nSee also\n\nConfiguring the Kubernetes system volume\n\nChanging Kubernetes node flavors\n\nConfiguring Kubernetes load balancers",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-kubernetes-dns-and-discovery-parameters.html"
    },
    {
        "title": "Configuring Kubernetes load balancers",
        "content": "Configuring Kubernetes load balancers\nKubernetes clusters have two types of load balancers:\n\nThe API load balancer is created when the Kubernetes master node has high availability enabled and is used by the Kubernetes API.\nThe external load balancer is created by Kubernetes users and used by Kubernetes applications.\n\nBy default, these two load balancers use the preconfigured flavor that makes the load balancing service highly available.\nAfter updating the load balancer flavors, the changes will be applied only in new Kubernetes clusters. Existing Kubernetes clusters will retain their previous parameters.\n\nTo improve the stability of Kubernetes clusters, it is highly recommended to select a load balancer flavor that provides high availability.\n\nPrerequisites\n\nA custom load balancer flavor is created, as described in Creating custom load balancer flavors.\n\nTo change the load balancer flavor\nUse the following command:vinfra service compute k8saas defaults set --labels octavia_api_lb_flavor=<mode>,octavia_default_flavor=<mode> <version>\n\n--labels <key1=value1,key2=value2,key3=value3...>\n\nArbitrary labels in the form of key=value pairs to associate with a cluster:\n\noctavia_api_lb_flavor=<mode>: set the load balancer flavor for Kubernetes master nodes:\n\nSINGLE: create only one load balancer for Kubernetes master nodes\nACTIVE_STANDBY: create two load balancers that will work in the Active/Standby mode\n\noctavia_default_flavor=<mode>: set the load balancer flavor for Kubernetes applications:\n\nSINGLE: create only one load balancer for Kubernetes applications\nACTIVE_STANDBY: create two load balancers that will work in the Active/Standby mode\n\n<version>\n\nKubernetes version to apply new defaults for.\n\nFor example, to create only one load balancer for highly available Kubernetes master nodes and apply this change for all of the supported Kubernetes versions, run:# vinfra service compute k8saas defaults set --labels octavia_default_flavor=SINGLE\nTo set this flavor only for version 1.24.3, append the version number to the command:# vinfra service compute k8saas defaults set --labels octavia_default_flavor=SINGLE v1.24.3\nSee also\n\nConfiguring the Kubernetes system volume\n\nChanging Kubernetes node flavors\n\nConfiguring Kubernetes DNS and discovery parameters",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-kubernetes-load-balancers.html"
    },
    {
        "title": "Configuring memory for the storage services",
        "content": "Configuring memory for the storage services\nYou can configure memory limits and guarantees for the storage services at runtime by using the vinfra tool. You can do this for the entire cluster or a specific node.\nThe following memory parameters can be configured manually:\n\nMemory guarantee\nSwap size\nPage cache (which, in turn, is set using cache ratio, minimum, and maximum)\n\nPage cache is calculated according to the following formula:$PAGE_CACHE = minimum <= ratio * $TOTAL_MEMORY <= maximum\r\n\nThe minimum and maximum values are hard limits that are applied if the ratio * $TOTAL_MEMORY value is outside these limits.\nTo better understand how page cache size is calculated, consider the following examples:\n\n\u00a0\n\nExample 1\n(cache size is\r\nwithin limits)\n\nExample 2\n(cache size\r\nequals minimum)\n\nExample 3\n(cache size\r\nequals maximum)\n\nTotal memory\n4 GiB\n4 GiB\n4 GiB\n\nCache ratio\n0.5\n0.1\n0.9\n\nCache minimum\n1 GiB\n2 GiB\n1 GiB\n\nCache maximum\n3 GiB\n3 GiB\n3 GiB\n\nCache size\n2 GiB\n2 GiB\n3 GiB\n\nIf memory parameters are set both per node and per cluster, the per-node ones are applied. If no memory parameters are configured manually, the memory management is performed automatically by the vcmmd daemon as follows:\n\nEach CS (for example, storage disk) requires 512 MiB of RAM for page cache.\nThe page cache minimum is 1 GiB.\nIf the total memory is less than 48 GiB, the page cache maximum is calculated as two-thirds of it.\nIf the total memory is greater than 48 GiB, the page cache maximum is 32 GiB.\n\nTo check the current memory parameters for the storage services set by vcmmd, run:# vcmmdctl list\r\nname                                 type active guarantee    limit swap   cache\r\n<...>\r\nvstorage.slice/vstorage-services.sl\u00e2\u0080\u00a6 SRVC    yes   1310720 24522132    0 1048576\r\n",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-memory-for-the-storage-services.html"
    },
    {
        "title": "Compute alerts",
        "content": "Compute alerts\nBased on the metrics described in Compute metrics, the compute alerts are generated and displayed in the admin panel.\nCompute service alerts\n\n OpenStack Cinder API is down\n\nOpenStack Block Storage (Cinder) API service is down.\n\nEnsure that the cinder_api container is up on the management node by running:# docker ps --all | grep cinder_api\n\nIf the container is down, start it by running:# docker start cinder_api\n\nCheck the service log at /var/log/hci/cinder/cinder-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Cinder Scheduler is down\n\nOpenStack Block Storage (Cinder) Scheduler agent is down on host <hostname>.\n\nEnsure that the cinder_scheduler container is up on the specified node by running:# docker ps --all | grep cinder_scheduler\n\nIf the container is down, start it by running:# docker start cinder_scheduler\n\nCheck the service log at /var/log/hci/cinder/cinder-scheduler.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Cinder Volume agent is down\n\nOpenStack Block Storage (Cinder) Volume agent is down on host <hostname>.\n\nEnsure that the cinder_volume container is up on the specified node by running:# docker ps --all | grep cinder_volume\n\nIf the container is down, start it by running:# docker start cinder_volume\n\nCheck the service log at /var/log/hci/cinder/cinder-volume.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Glance API is down\n\nOpenStack Image (Glance) API service is down.\n\nEnsure that the glance_api container is up on the management node by running:# docker ps --all | grep glance_api\n\nIf the container is down, start it by running:# docker start glance_api\n\nCheck the service log at /var/log/hci/glance/glance-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Heat API is down\n\nOpenStack Orchestration API service (Heat) is down.\n\nEnsure that the heat_api container is up on the management node by running:# docker ps --all | grep heat_api\n\nIf the container is down, start it by running:# docker start heat_api\n\nCheck the service log at /var/log/hci/heat/heat-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Magnum API is down\n\nOpenStack Container API service (Magnum) is down.\n\nEnsure that the magnum_api container is up on the management node by running:# docker ps --all | grep magnum_api\n\nIf the container is down, start it by running:# docker start magnum_api\n\nCheck the service log at /var/log/hci/magnum/magnum-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Neutron API is down\n\nOpenStack Networking API service (Neutron) is down.\n\nEnsure that the neutron_server container is up on the management node by running:# docker ps --all | grep neutron_server\n\nIf the container is down, start it by running:# docker start neutron_server\n\nCheck the service log at /var/log/hci/neutron/neutron-server.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Neutron L3 agent is down\n\nOpenStack Networking (Neutron) L3 agent is down on host <hostname>.\n\nEnsure that the neutron_l3_agent container is up on the specified node by running:# docker ps --all | grep neutron_l3_agent\n\nIf the container is down, start it by running:# docker start neutron_l3_agent\n\nCheck the service log at /var/log/hci/neutron/neutron-l3-agent.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Neutron OpenvSwitch agent is down\n\nOpenStack Networking (Neutron) OpenvSwitch agent is down on host <hostname>.\n\nEnsure that the neutron_openvswitch_agent container is up on the specified node by running:# docker ps --all | grep neutron_openvswitch_agent\n\nIf the container is down, start it by running:# docker start neutron_openvswitch_agent\n\nCheck the service log at /var/log/hci/neutron/neutron-openvswitch-agent.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Neutron Metadata agent is down\n\nOpenStack Networking (Neutron) Metadata agent is down on host <hostname>.\n\nEnsure that the neutron_metadata_agent container is up on the specified node by running:# docker ps --all | grep neutron_metadata_agent\n\nIf the container is down, start it by running:# docker start neutron_metadata_agent\n\nCheck the service log at /var/log/hci/neutron/neutron-metadata-agent.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Neutron DHCP agent is down\n\nOpenStack Networking (Neutron) DHCP agent is down on host <hostname>.\n\nEnsure that the neutron_dhcp_agent container is up on the specified node by running:# docker ps --all | grep neutron_dhcp_agent\n\nIf the container is down, start it by running:# docker start neutron_dhcp_agent\n\nCheck the service log at /var/log/hci/neutron/neutron-dhcp-agent.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Nova API is down\n\nOpenStack Compute (Nova) API service is down.\n\nEnsure that the nova_api container is up on the management node by running:# docker ps --all | grep nova_api\n\nIf the container is down, start it by running:# docker start nova_api\n\nCheck the service log at /var/log/hci/nova/nova-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Nova Compute is down\n\nOpenStack Compute (Nova) agent is down on host <hostname>.\n\nEnsure that the nova_compute container is up on the specified node by running:# docker ps --all | grep nova_compute\n\nIf the container is down, start it by running:# docker start nova_compute\n\nCheck the service log at /var/log/hci/nova/nova-compute.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Nova Conductor is down\n\nOpenStack Compute (Nova) Conductor agent is down on host <hostname>.\n\nEnsure that the nova_conductor container is up on the specified node by running:# docker ps --all | grep nova_conductor\n\nIf the container is down, start it by running:# docker start nova_conductor\n\nCheck the service log at /var/log/hci/nova/nova-conductor.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Nova Scheduler is down\n\nOpenStack Compute (Nova) Scheduler agent is down on host <hostname>.\n\nEnsure that the nova_scheduler container is up on the specified node by running:# docker ps --all | grep nova_scheduler\n\nIf the container is down, start it by running:# docker start nova_scheduler\n\nCheck the service log at /var/log/hci/nova/nova-scheduler.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Octavia API is down\n\nOpenStack Load Balancer API service (Octavia) is down.\n\nEnsure that the octavia_api container is up on the management node by running:# docker ps --all | grep octavia_api\n\nIf the container is down, start it by running:# docker start octavia_api\n\nCheck the service log at /var/log/hci/octavia/octavia-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Octavia Provisioning Worker v1 is down\n\nOpenStack Loadbalancing (Octavia) provisioning worker version 1 is down on host <hostname>.\n\nEnsure that the octavia_worker container is up on the management node by running:# docker ps --all | grep octavia_worker\n\nIf the container is down, start it by running:# docker start octavia_worker\n\nCheck the service log at /var/log/hci/octavia/octavia-worker.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Octavia Provisioning Worker v2 is down\n\nOpenStack Loadbalancing (Octavia) provisioning worker version 2 is down on host <hostname>.\n\nEnsure that the octavia_worker container is up on the management node by running:# docker ps --all | grep octavia_worker\n\nIf the container is down, start it by running:# docker start octavia_worker\n\nCheck the service log at /var/log/hci/octavia/octavia-worker.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Octavia Housekeeping service is down\n\nOpenStack Loadbalancing (Octavia) housekeeping service is down on host <hostname>.\n\nEnsure that the octavia_housekeeping container is up on the management node by running:# docker ps --all | grep octavia_housekeeping\n\nIf the container is down, start it by running:# docker start octavia_housekeeping\n\nCheck the service log at /var/log/hci/octavia/octavia-housekeeping.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Octavia HealthManager service is down\n\nOpenStack Loadbalancing (Octavia) health manager service is down on host <hostname>.\n\nEnsure that the octavia_health_manager container is up on the management node by running:# docker ps --all | grep octavia_health_manager\n\nIf the container is down, start it by running:# docker start octavia_health_manager\n\nCheck the service log at /var/log/hci/octavia/octavia-health-manager.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n OpenStack Placement API is down\n\nOpenStack Placement API service is down.\n\nEnsure that the placement_api container is up on the management node by running:# docker ps --all | grep placement_api\n\nIf the container is down, start it by running:# docker start placement_api\n\nCheck the service log at /var/log/hci/placement/placement-api.log.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n High request error rate for OpenStack API requests detected\n\nRequest error rate more than 5% detected for <object_id> for the last 1 hour. Check <object_id> resource usage.\n\nCheck the status of the affected compute services.\nIf some services are down, bring them up.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\nCompute cluster alerts\n\n Compute cluster has failed\n\nCompute cluster has failed. Unable to manage virtual machines.\n\nGo to the Monitoring > Dashboard screen, and then click Grafana dashboard.\nOpen the Compute service status dashboard and find out the failed service.\nDepending on the service, follow the instructions from the Compute service alerts section.\n\n Cluster is running out of vCPU resources\n\nCluster has reached 80% of the vCPU allocation limit.\n\nThe compute cluster may soon experience the lack of vCPU resources that will lead to inability to accommodate new virtual machines. To avoid this, you can add more compute nodes or return to operation fenced nodes, if any.\n\n Cluster is out of vCPU resources\n\nCluster has reached 95% of the vCPU allocation limit.\n\nThe compute cluster will soon experience the lack of vCPU resources that will lead to inability to accommodate new virtual machines. To avoid this, you can add more compute nodes or return to operation fenced nodes, if any.\n\n Cluster is running out of memory\n\nCluster has reached 80% of the memory allocation limit.\n\nThe compute cluster may soon experience the lack of RAM resources that will lead to inability to accommodate new virtual machines. To avoid this, you can add more compute nodes or return to operation fenced nodes, if any.\n\n Cluster is out of memory\n\nCluster has reached 95% of the memory allocation limit.\n\nThe compute cluster will soon experience the lack of RAM resources that will lead to inability to accommodate new virtual machines. To avoid this, you can add more compute nodes or return to operation fenced nodes, if any.\n\n Virtual machine error\n\nVirtual machine <name> with ID <id> is in the 'Error' state.\n\nExamine the VM history in the History tab on the VM right pane and reset the VM state, as described in Troubleshooting virtual machines.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n Virtual machine state mismatch\n\nState of virtual machine <name> with ID <id> differs in the Nova databases and libvirt configuration.\n\nDo not try to migrate the VM or reset its state. Contact the technical support team.\n\n Volume attachment details mismatch\n\nAttachment details for volume with ID <id> differ in the Nova and libvirt databases.\n\nDo not try to migrate the VM or reset its state. Contact the technical support team.\n\n Virtual network port check failed\n\nNeutron port with ID <port_id> failed <check_type> check. The port type is <device_owner> with owner ID <device_id>.\n\nRun the openstack --insecure port check command specifying the port ID.\nCheck connectivity of the device owner.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\nCompute node alerts\n\n Node is running out of vCPU resources\n\nNode <node> with ID <id> has reached 80% of the vCPU allocation limit.\n\nThe compute node may soon experience the lack of vCPU resources that will lead to inability to accommodate new virtual machines. To avoid this, check the distribution of VMs in the compute cluster, and then migrate the VMs from the specified node to less loaded compute nodes.\n\n Node is out of vCPU resources\n\nNode <node> with ID <id> has reached 95% of the vCPU allocation limit.\n\nThe compute node will soon experience the lack of vCPU resources that will lead to inability to accommodate new virtual machines. To avoid this, check the distribution of VMs in the compute cluster, and then migrate the VMs from the specified node to less loaded compute nodes.\n\n Node is running out of memory\n\nNode <node> with ID <id> has reached 80% of the memory allocation limit.\n\nThe compute node may soon experience the lack of RAM resources that will lead to inability to accommodate new virtual machines. To avoid this, check the distribution of VMs in the compute cluster, and then migrate the VMs from the specified node to less loaded compute nodes.\n\n Node is out of memory\n\nNode <node> with ID <id> has reached 95% of the memory allocation limit.\n\nThe compute node will soon experience the lack of RAM resources that will lead to inability to accommodate new virtual machines. To avoid this, check the distribution of VMs in the compute cluster, and then migrate the VMs from the specified node to less loaded compute nodes.\n\nDomain quota alerts\n\n Domain is out of vCPU resources\n\nDomain <name> has reached <number>% of the vCPU allocation limit.\n\nThe domain will soon experience the lack of vCPU resources that will lead to inability to create new virtual machines. To avoid this, add more vCPUs to the domain quota.\n\n Domain is out of vCPU resources\n\nDomain <name> has reached 95% of the vCPU allocation limit.\n\nThe domain will soon experience the lack of vCPU resources that will lead to inability to create new virtual machines. To avoid this, add more vCPUs to the domain quota.\n\n Domain is out of memory\n\nDomain <name> has reached <number>% of the memory allocation limit.\n\nThe domain will soon experience the lack of RAM resources that will lead to inability to create new virtual machines. To avoid this, add more RAM to the domain quota.\n\n Domain is out of memory\n\nDomain <name> has reached 95% of the memory allocation limit.\n\nThe domain will soon experience the lack of RAM resources that will lead to inability to create new virtual machines. To avoid this, add more RAM to the domain quota.\n\n Domain is out of storage policy space\n\nDomain <name> has reached <number>% of the <policy_name> storage policy allocation limit.\n\nThe domain will soon experience the lack of storage policy space that will lead to inability to create new compute volumes with this storage policy. To avoid this, add more storage space to the domain quota.\n\n Domain is out of storage policy space\n\nDomain <name> has reached 95% of the <policy_name> storage policy allocation limit.\n\nThe domain will soon experience the lack of storage policy space that will lead to inability to create new compute volumes with this storage policy. To avoid this, add more storage space to the domain quota.\n\nProject quota alerts\n\n Project is out of vCPU resources\n\nProject <name> has reached 95% of the vCPU allocation limit.\n\nThe project will soon experience the lack of vCPU resources that will lead to inability to create new virtual machines. To avoid this, add more vCPUs to the project quota.\n\n Project is out of memory\n\nProject <name> has reached 95% of the memory allocation limit.\n\nThe project will soon experience the lack of RAM resources that will lead to inability to create new virtual machines. To avoid this, add more RAM to the project quota.\n\n Project is out of floating IP addresses\n\nProject <name> has reached 95% of the floating IP address allocation limit.\n\nThe project will soon experience the lack of floating IP addresses that will lead to inability to assign them to virtual machines. To avoid this, add more floating IPs to the project quota.\n\n Network is out of IP addresses\n\nNetwork <name> with ID <id> in project <name> has reached 95% of the IP address allocation limit.\n\nThe network will soon experience the lack of IP addresses that will lead to inability to connect new virtual machines to this network. To avoid this, add more allocation pools to the network.\n\n Project is out of storage policy space\n\nProject <name> has reached 95% of the <policy_name> storage policy allocation limit.\n\nThe project will soon experience the lack of storage policy space that will lead to inability to create new compute volumes with this storage policy. To avoid this, add more storage space to the project quota.\n\nOther alerts\n\n Libvirt service is down\n\nLibvirt service is down on node <node> with ID <id>. Check the service state and start it. If the service cannot start, contact the technical support.\n\nStart the libvirtd service on the specified node by running:# systemctl start libvirtd.service\n\n Docker service is down\n\nDocker service is down on host <hostname>.\n\nStart the Docker service on the specified node by running:# systemctl start docker.service\n\n RabbitMQ node is down\n\nOne or more nodes in the Rabbitmq cluster is down.\n\nContact the technical support team.\n\n RabbitMQ split brain detected\n\nRabbitMQ cluster has experienced a split brain due to a network partition.\n\nContact the technical support team.\n\n PostgreSQL database size is greater than 30 GB\n\nPostgreSQL database \"<name>\" on node \"<hostname>\" is greater than 30 GB in size. Verify that deleted entries are archived or contact the technical support.\n\n PostgreSQL database uses more than 50% of node root partition\n\nPostgreSQL databases on node \"<hostname>\" with ID \"<id>\" use more than 50% of node root partition. Verify that deleted entries are archived or contact the technical support.\n\nWhat's next\n\nGetting technical support\n\nSee also\n\nInfrastructure alerts\n\nCore storage alerts\n\nBackup storage alerts\n\nObject storage alerts\n\nBlock storage alerts",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/compute-alerts.html"
    },
    {
        "title": "Configuring inbound firewall rules",
        "content": "Configuring inbound firewall rules\nTo prevent access from untrusted sources to the cluster, you can configure inbound firewall rules on your nodes. To enable traffic filtering, you need to configure allow and deny lists for a network or a traffic type. By default, the lists are empty and all incoming traffic is allowed. You can create access rules in them to configure access for incoming traffic. Access rules in the allow list have higher priority than those in the deny list. If you have access rules for both networks and traffic types, access lists configured for traffic types will have higher priority than those of networks.\nLimitations\n\nIf you create allow rules but leave the deny list empty, all incoming traffic will still be allowed.\n\nTo filter incoming traffic for a network\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Edit.\n\nIn the Edit network window, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\nClick Save to apply your changes.\n\nThe updated access rules will be applied on all nodes connected to this network.\n\nCommand-line interface\nUse the following command:vinfra cluster network set [--inbound-allow-list <addresses> | --add-inbound-allow-list <addresses> |\r\n                           --del-inbound-allow-list <addresses> | --clear-inbound-allow-list]\r\n                           [--inbound-deny-list <addresses> | --add-inbound-deny-list <addresses> |\r\n                           --del-inbound-deny-list <addresses> | --clear-inbound-deny-list] <network>\r\n\n--inbound-allow-list <addresses>\nA comma-separated list of IP addresses (overwrites the current inbound allow rules)\n--add-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound allow rules)\n--del-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound allow rules)\n--clear-inbound-allow-list\n\nClear all inbound allow rules\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (overwrites the current inbound deny rules)\n--add-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound deny rules)\n--del-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound deny rules)\n--clear-inbound-deny-list <addresses>\n\nClear all inbound deny rules\n<network>\n\nNetwork ID or name\n\nFor example, to allow traffic from the subnet 10.136.100.0/24 in the MyNet network, run:# vinfra cluster network set MyNet --add-inbound-allow-list 10.136.100.0/24 --add-inbound-deny-list 0.0.0.0/0\n\nTo filter incoming traffic for a regular or custom traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the pencil icon next to the traffic type name.\n\nIn the Edit regular traffic type window, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\nClick Save to apply your changes.\n\nAfter you edit the allow and deny lists, the updated access rules are applied on all nodes connected to the network with this traffic type.\n\nCommand-line interface\nUse the following command:vinfra cluster traffic-type set [--inbound-allow-list <addresses> | --add-inbound-allow-list <addresses> |\r\n                                --del-inbound-allow-list <addresses> | --clear-inbound-allow-list]\r\n                                [--inbound-deny-list <addresses> | --add-inbound-deny-list <addresses> |\r\n                                --del-inbound-deny-list <addresses> | --clear-inbound-deny-list] <traffic-type>\r\n\n--inbound-allow-list <addresses>\nA comma-separated list of IP addresses (overwrites the current inbound allow rules)\n--add-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound allow rules)\n--del-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound allow rules)\n--clear-inbound-allow-list\n\nClear all inbound allow rules\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (overwrites the current inbound deny rules)\n--add-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound deny rules)\n--del-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound deny rules)\n--clear-inbound-deny-list <addresses>\n\nClear all inbound deny rules\n<traffic-type>\n\nTraffic type name\n\nFor example, to allow traffic from the subnet 10.136.100.0/24 for the MyTrafficType traffic type, run:# vinfra cluster traffic-type set MyTrafficType --add-inbound-allow-list 10.136.100.0/24 --add-inbound-deny-list 0.0.0.0/0\n\nTo view access rules for a network or traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, find a network or a traffic type with the shield icon next to its name.\nHover over the icon to see what access rules are configured.\n\nCommand-line interface\n\nFor a network, use vinfra cluster network show. For example:# vinfra cluster network show MyNet\r\n+---------------------+------------------------------------------+\r\n| Field               | Value                                    |\r\n+---------------------+------------------------------------------+\r\n| id                  | db43aed5-82ec-4c60-8c5a-d60767203d89     |\r\n| inbound_allow_list  | - 10.136.100.0/24                        |\r\n| inbound_deny_list   | - 0.0.0.0/0                              |\r\n| name                | MyNet                                    |\r\n| outbound_allow_list | - 0.0.0.0:tcp:8888:Admin panel           |\r\n|                     | - 0.0.0.0:tcp:80:HTTP                    |\r\n|                     | - 0.0.0.0:tcp:443:HTTPS                  |\r\n|                     | - 0.0.0.0:udp:53:DNS                     |\r\n|                     | - 0.0.0.0:tcp:53:DNS                     |\r\n|                     | - 0.0.0.0:udp:123:NTP                    |\r\n|                     | - 0.0.0.0:tcp:8443:ABGW registration     |\r\n|                     | - 0.0.0.0:tcp:44445:ABGW Geo-replication |\r\n|                     | - 0.0.0.0:tcp:9877:Acronis Cyber Protect |\r\n|                     | - 0.0.0.0:tcp:5900-6079:VM VNC Legacy    |\r\n|                     | - 0.0.0.0:udp:4789:VXLAN                 |\r\n|                     | - 0.0.0.0:tcp:15900-16900:VM VNC         |\r\n|                     | - 0.0.0.0:udp:2049:NFS                   |\r\n|                     | - 0.0.0.0:tcp:2049:NFS                   |\r\n|                     | - 0.0.0.0:tcp:111:NFS Rpcbind            |\r\n|                     | - 0.0.0.0:any:0:Allow all                |\r\n| traffic_types       |                                          |\r\n| vlan                |                                          |\r\n+---------------------+------------------------------------------+\n\nFor a traffic type, use vinfra cluster traffic-type show. For example:# vinfra cluster traffic-type show MyTrafficType\r\n+--------------------+-------------------+\r\n| Field              | Value             |\r\n+--------------------+-------------------+\r\n| exclusive          | False             |\r\n| hidden             | False             |\r\n| inbound_allow_list | - 10.136.100.0/24 |\r\n| inbound_deny_list  | - 0.0.0.0/0       |\r\n| name               | MyTrafficType     |\r\n| port               | 6900              |\r\n| type               | custom            |\r\n+--------------------+-------------------+\r\n\n\nSee also\n\nConfiguring outbound firewall rules\n\nManaging networks\n\nConfiguring data-in-transit encryption\n\nManaging traffic types",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network set [--inbound-allow-list <addresses> | --add-inbound-allow-list <addresses> |\r\n                           --del-inbound-allow-list <addresses> | --clear-inbound-allow-list]\r\n                           [--inbound-deny-list <addresses> | --add-inbound-deny-list <addresses> |\r\n                           --del-inbound-deny-list <addresses> | --clear-inbound-deny-list] <network>\r\n\n--inbound-allow-list <addresses>\nA comma-separated list of IP addresses (overwrites the current inbound allow rules)\n--add-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound allow rules)\n--del-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound allow rules)\n--clear-inbound-allow-list\n\nClear all inbound allow rules\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (overwrites the current inbound deny rules)\n--add-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound deny rules)\n--del-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound deny rules)\n--clear-inbound-deny-list <addresses>\n\nClear all inbound deny rules\n<network>\n\nNetwork ID or name\n\nFor example, to allow traffic from the subnet 10.136.100.0/24 in the MyNet network, run:# vinfra cluster network set MyNet --add-inbound-allow-list 10.136.100.0/24 --add-inbound-deny-list 0.0.0.0/0\n",
                "title": "To filter incoming traffic for a network"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster traffic-type set [--inbound-allow-list <addresses> | --add-inbound-allow-list <addresses> |\r\n                                --del-inbound-allow-list <addresses> | --clear-inbound-allow-list]\r\n                                [--inbound-deny-list <addresses> | --add-inbound-deny-list <addresses> |\r\n                                --del-inbound-deny-list <addresses> | --clear-inbound-deny-list] <traffic-type>\r\n\n--inbound-allow-list <addresses>\nA comma-separated list of IP addresses (overwrites the current inbound allow rules)\n--add-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound allow rules)\n--del-inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound allow rules)\n--clear-inbound-allow-list\n\nClear all inbound allow rules\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (overwrites the current inbound deny rules)\n--add-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (adds the specified inbound deny rules)\n--del-inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses (removes the specified inbound deny rules)\n--clear-inbound-deny-list <addresses>\n\nClear all inbound deny rules\n<traffic-type>\n\nTraffic type name\n\nFor example, to allow traffic from the subnet 10.136.100.0/24 for the MyTrafficType traffic type, run:# vinfra cluster traffic-type set MyTrafficType --add-inbound-allow-list 10.136.100.0/24 --add-inbound-deny-list 0.0.0.0/0\n",
                "title": "To filter incoming traffic for a regular or custom traffic type"
            },
            {
                "example": "\nCommand-line interface\n\n\nFor a network, use vinfra cluster network show. For example:# vinfra cluster network show MyNet\r\n+---------------------+------------------------------------------+\r\n| Field               | Value                                    |\r\n+---------------------+------------------------------------------+\r\n| id                  | db43aed5-82ec-4c60-8c5a-d60767203d89     |\r\n| inbound_allow_list  | - 10.136.100.0/24                        |\r\n| inbound_deny_list   | - 0.0.0.0/0                              |\r\n| name                | MyNet                                    |\r\n| outbound_allow_list | - 0.0.0.0:tcp:8888:Admin panel           |\r\n|                     | - 0.0.0.0:tcp:80:HTTP                    |\r\n|                     | - 0.0.0.0:tcp:443:HTTPS                  |\r\n|                     | - 0.0.0.0:udp:53:DNS                     |\r\n|                     | - 0.0.0.0:tcp:53:DNS                     |\r\n|                     | - 0.0.0.0:udp:123:NTP                    |\r\n|                     | - 0.0.0.0:tcp:8443:ABGW registration     |\r\n|                     | - 0.0.0.0:tcp:44445:ABGW Geo-replication |\r\n|                     | - 0.0.0.0:tcp:9877:Acronis Cyber Protect |\r\n|                     | - 0.0.0.0:tcp:5900-6079:VM VNC Legacy    |\r\n|                     | - 0.0.0.0:udp:4789:VXLAN                 |\r\n|                     | - 0.0.0.0:tcp:15900-16900:VM VNC         |\r\n|                     | - 0.0.0.0:udp:2049:NFS                   |\r\n|                     | - 0.0.0.0:tcp:2049:NFS                   |\r\n|                     | - 0.0.0.0:tcp:111:NFS Rpcbind            |\r\n|                     | - 0.0.0.0:any:0:Allow all                |\r\n| traffic_types       |                                          |\r\n| vlan                |                                          |\r\n+---------------------+------------------------------------------+\n\n\nFor a traffic type, use vinfra cluster traffic-type show. For example:# vinfra cluster traffic-type show MyTrafficType\r\n+--------------------+-------------------+\r\n| Field              | Value             |\r\n+--------------------+-------------------+\r\n| exclusive          | False             |\r\n| hidden             | False             |\r\n| inbound_allow_list | - 10.136.100.0/24 |\r\n| inbound_deny_list  | - 0.0.0.0/0       |\r\n| name               | MyTrafficType     |\r\n| port               | 6900              |\r\n| type               | custom            |\r\n+--------------------+-------------------+\r\n\n\n\n",
                "title": "To view access rules for a network or traffic type"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Edit.\n\nIn the Edit network window, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\n\n\n\n\n\nClick Save to apply your changes.\n\nThe updated access rules will be applied on all nodes connected to this network.\n",
                "title": "To filter incoming traffic for a network"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the pencil icon next to the traffic type name.\n\nIn the Edit regular traffic type window, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\n\n\n\n\n\nClick Save to apply your changes.\n\nAfter you edit the allow and deny lists, the updated access rules are applied on all nodes connected to the network with this traffic type.\n",
                "title": "To filter incoming traffic for a regular or custom traffic type"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, find a network or a traffic type with the shield icon next to its name.\nHover over the icon to see what access rules are configured.\n\n",
                "title": "To view access rules for a network or traffic type"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-inbound-firewall-rules.html"
    },
    {
        "title": "Configuring data-in-transit encryption",
        "content": "Configuring data-in-transit encryption\nTo protect networks against eavesdropping attacks and traffic hijacking, Virtuozzo Hybrid Infrastructure supports data-in-transit encryption between cluster nodes. Data transmitted over a network is encrypted by using the AES-128 standard. Data-in-transit encryption is implemented via the IP Security (IPsec) protocol in transport mode. Authentication is based on X.509 certificates, which are installed on nodes during registration when installing the product, or during an upgrade to version 5.2 and later. Node certificates are rotated automatically once per year.\nBy default, data-in-transit encryption is disabled. You can enable it for an infrastructure network to encrypt all traffic that moves between cluster nodes in this subnet.\n\nIf you have services that operate in the same subnet and exchange data externally, you need to add exceptions for them. In this case, a particular IP address, prefix, or port added to the exceptions will bypass the encryption.\n\nWhen encryption is enabled for a network, the following traffic types bypass data-in-transit encryption if they are assigned to this network:\n\nBackup (ABGW) private (this traffic is encrypted by default with the TLS protocol)\nCompute API\n\nSSH\n\niSCSI\n\nS3 public\n\nBackup (ABGW) public\n\nAdmin panel\n\nNFS\n\nVM public\n\nSelf-service panel\n\nSNMP\n\nCustom traffic types\n\nTherefore, data-in-transit encryption only applies to the following exclusive traffic : Internal management, Storage, OSTOR private, VM private, and VM backups. When encryption is enabled for a network with the Storage traffic type, internal IP addresses of the storage services are automatically reconfigured to the IPv6 mode.\nLimitations\n\nYou cannot reassign the Storage traffic type from an encrypted network to an unencrypted one. You can either disable encryption for the source network or enable it for the target network, and then proceed to the traffic type reassignment.\nWith data-in-transit encryption enabled, the iSCSI service has degraded write I/O performance, which may decrease by half, compared to clusters with encryption disabled.\n\nSee also\n\nManaging networks\n\nConfiguring inbound firewall rules\n\nConfiguring outbound firewall rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-data-in-transit-encryption.html"
    },
    {
        "title": "Configuring memory for virtual machines",
        "content": "Configuring memory for virtual machines\nTo optimize memory usage by virtual machines, Virtuozzo Hybrid Infrastructure uses a Linux feature called kernel same-page merging (KSM). The KSM daemon periodically scans memory for pages with identical content and merges those into a single page. That page is marked as copy-on-write (COW), so when a virtual machine needs to change its contents, the kernel creates a copy of the page for that VM and changes it. KSM allows using RAM overcommitment and avoiding swapping when many similar workloads run on the same server. However, we strongly recommend configuring swap space if you enable memory overcommitment.\nAdditionally, virtual machines running on a server are bound to its NUMA nodes, to keep VM processes as close to the memory they access as possible. When the load difference between NUMA nodes exceeds 50%, hosted VMs are rebalanced based on their memory and CPU usage.\nYou can configure the amount of memory that can be provisioned for virtual machines by setting the RAM overcommitment ratio. This is the ratio of the amount of maximum reserved RAM to physical. The default ratio is 1, which means that you cannot provision more than the amount of physical RAM available on all of the compute nodes. By increasing the ratio, you can increase the number of virtual machines running in the compute cluster but potentially reduce their performance. By default, memory guarantees for virtual machines are set to 50%, which allows increasing the RAM overcommitment ratio up to 2. However, the maximum recommended RAM overcommitment ratio is 1.5.\nMemory overcommitment for virtual machines is only available if all of the compute nodes have enough swap space. Before enabling RAM overcommitment, you need to calculate the required swap space, and then configure it on each compute node.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-memory-for-virtual-machines.html"
    },
    {
        "title": "Configuring InfiniBand devices",
        "content": "Configuring InfiniBand devices\nLimitations\n\nAs the admin panel only shows IP states and does not show InfiniBand (IB) connection states, it may report plugged in but yet unconfigured IB devices as Disconnected. The status will change to Connected once you assign an IP address to such a device.\n\nPrerequisites\n\nThe Storage traffic type is moved to a dedicated network.\n\nTo configure InfiniBand devices\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, assign the traffic type Storage to an empty network (without any other traffic types). If needed, create a new network by clicking Create network.\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface.\nOn the interface right pane, click Edit.\nIn the Edit network interface window, specify the network with the Storage traffic type, select Manually, and then specify the IP address in CIDR notation by clicking Add.\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\nSelect Connected mode.\n\nEnter the 65520 value in the MTU field.\n\nSetting a custom MTU in the admin panel prior to configuring it on the network hardware will result in network failure on the node and require manual resetting. Setting an MTU that differs from the one configured on the network hardware may result in a network outage or poor performance.\n\nClick Save to apply your changes.\nRepeat the steps for each IB device on your infrastructure nodes.\n\nCommand-line interface\nUse the following command:vinfra node iface set [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                      [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                      [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                      [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                      [--network <network> | --no-network] [--connected-mode | --datagram-mode]\r\n                      [--node <node>] <iface>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--no-network\n\nRemove a network from the interface\n--connected-mode\n\nEnable connected mode (InfiniBand interfaces only)\n--datagram-mode\n\nEnable datagram mode (InfiniBand interfaces only)\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nFor example, to assign the network Storage to the network interface ib2 located on the node node002, enable the connected mode, set its IP address to 192.168.30.20/24, and MTU to 65520,  run:# vinfra node iface set ib2 --network Storage --node node002 --ipv4 192.168.30.20/24 \\\r\n--mtu 65520 --connected-mode\n\nSee also\n\nChanging network interface parameters\n\nManaging network interfaces\n\nWhat's next\n\nEnabling RDMA",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface set [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                      [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                      [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                      [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                      [--network <network> | --no-network] [--connected-mode | --datagram-mode]\r\n                      [--node <node>] <iface>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--no-network\n\nRemove a network from the interface\n--connected-mode\n\nEnable connected mode (InfiniBand interfaces only)\n--datagram-mode\n\nEnable datagram mode (InfiniBand interfaces only)\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nFor example, to assign the network Storage to the network interface ib2 located on the node node002, enable the connected mode, set its IP address to 192.168.30.20/24, and MTU to 65520,  run:# vinfra node iface set ib2 --network Storage --node node002 --ipv4 192.168.30.20/24 \\\r\n--mtu 65520 --connected-mode\n",
                "title": "To configure InfiniBand devices"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, assign the traffic type Storage to an empty network (without any other traffic types). If needed, create a new network by clicking Create network.\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface.\nOn the interface right pane, click Edit.\nIn the Edit network interface window, specify the network with the Storage traffic type, select Manually, and then specify the IP address in CIDR notation by clicking Add.\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\nSelect Connected mode.\n\nEnter the 65520 value in the MTU field.\n\nSetting a custom MTU in the admin panel prior to configuring it on the network hardware will result in network failure on the node and require manual resetting. Setting an MTU that differs from the one configured on the network hardware may result in a network outage or poor performance.\n\n\nClick Save to apply your changes.\nRepeat the steps for each IB device on your infrastructure nodes.\n\n",
                "title": "To configure InfiniBand devices"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-infiniband-devices.html"
    },
    {
        "title": "Configuring node network interfaces",
        "content": "Configuring node network interfaces\nBefore creating the storage cluster, you need to configure the network interfaces on each node. You can specify network parameters and assign networks to network interfaces by editing them.\nAccording to your network requirements, you might also need to create bonded and VLAN connections. In addition, if your node network adapters support RDMA, configure InfiniBand network infrastructure.\nPrerequisites\n\nYour network hardware meets the requirements listed in Network requirements and recommendations.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-node-network-interfaces.html"
    },
    {
        "title": "Configuring new disks manually",
        "content": "Configuring new disks manually\nLimitations\n\nYou can assign a role to a disk only if its size is greater than 1 GiB.\nYou can assign an additional role to a system disk only if its size is at least 100 GiB.\nIt is recommended to assign the System and Metadata roles to either an SSD disk or different HDDs. Assigning both of these roles to the same HDD disk will result in mediocre performance suitable only for cold data (for example, archiving).\nThe System role cannot be combined with the Cache and Metadata+Cache roles. The reason is that the I/O generated by the operating system and applications would contend with the I/O generated by journaling, thus negating its performance benefits.\nYou can use shingled magnetic recording (SMR) HDDs only with the Storage role and only if the node has an SSD disk with the Cache role.\nYou cannot use SMR and standard disks in the same tier.\nYou cannot assign roles to system and non-system disks at a time.\n\nPrerequisites\n\nA clear understanding of the storage cluster architecture and disk roles, which are explained in About the storage cluster.\nThe failed disk is released, as described in Releasing node disks, and the new disk for replacement is connected to the node.\n\nTo manually assign roles to a new disk\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node.\nOn the Disks tab, click the new disk without a role.\nOn the disk right pane, click Assign role.\n\nIn the Assign role window, select a disk role, that is how you want to use the disk:\n\n[Only for SSD drives] To store write cache\n\nSelect the Cache role.\nSelect a storage tier that you want to cache.\n\nFor storage disks to use cache, the Cache role must be assigned before the Storage role. You can also assign both of these roles to disks at the same time, and the system will configure the cache disk first.\n\nTo store data\n\nSelect the Storage role.\nSelect a storage tier where to store your data. To make better use of data redundancy, do not assign all of the disks on a node to the same tier. Instead, make sure that each tier is evenly distributed across the cluster.\n\nEnable data caching and checksumming:\n\nEnable SSD caching and checksumming. Available and recommended only for nodes with SSDs.\nEnable checksumming (default). Recommended for nodes with HDDs as it provides better reliability.\nDisable checksumming. Not recommended for production. For an evaluation or testing environment, you can disable checksumming for nodes with HDDs, to provide better performance.\n\nTo store cluster metadata\n\nSelect the Metadata role.\n\nIt is recommended to have only one disk with the Metadata role per node and maximum five such disks in a cluster.\n\n[Only for SSD drives] To store both metadata and write cache\n\nSelect the Metadata+Cache role.\nSelect a storage tier that you want to cache.\n\nClick Assign.\n\nCommand-line interface\nUse the following command:vinfra node disk assign --disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]\r\n                        [--node <node>]\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n\nFor example, to assign the role cs to the disk sdc on the node node003, run:# vinfra node disk assign --disk sdc:cs --node node003\nYou can view the node's disk configuration in the vinfra node disk list output:# vinfra node disk list --node node003\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| id                                   | device | type | role       | disk_status | used    | size     | physical_size | service_id | service_status |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| 2A006CA5-732F-4E17-8FB0-B82CE0F28DB2 | sdc    | hdd  | cs         | ok          | 11.2GiB | 125.8GiB | 128.0GiB      | 1026       | active         |\r\n| 642A7162-B66C-4550-9FB2-F06866FB7EA1 | sdb    | hdd  | cs         | ok          | 8.7GiB  | 125.8GiB | 128.0GiB      | 1025       | active         |\r\n| 45D38CD2-3B94-4F0F-8864-9D51F716D3B1 | sda    | hdd  | mds-system | ok          | 21.0GiB | 125.9GiB | 128.0GiB      | 1          | avail          |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n\n\nSee also\n\nMonitoring node disks",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node disk assign --disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]\r\n                        [--node <node>]\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n\nFor example, to assign the role cs to the disk sdc on the node node003, run:# vinfra node disk assign --disk sdc:cs --node node003\nYou can view the node's disk configuration in the vinfra node disk list output:# vinfra node disk list --node node003\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| id                                   | device | type | role       | disk_status | used    | size     | physical_size | service_id | service_status |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| 2A006CA5-732F-4E17-8FB0-B82CE0F28DB2 | sdc    | hdd  | cs         | ok          | 11.2GiB | 125.8GiB | 128.0GiB      | 1026       | active         |\r\n| 642A7162-B66C-4550-9FB2-F06866FB7EA1 | sdb    | hdd  | cs         | ok          | 8.7GiB  | 125.8GiB | 128.0GiB      | 1025       | active         |\r\n| 45D38CD2-3B94-4F0F-8864-9D51F716D3B1 | sda    | hdd  | mds-system | ok          | 21.0GiB | 125.9GiB | 128.0GiB      | 1          | avail          |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n\n",
                "title": "To manually assign roles to a new disk"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node.\nOn the Disks tab, click the new disk without a role.\nOn the disk right pane, click Assign role.\n\nIn the Assign role window, select a disk role, that is how you want to use the disk:\n\n\n[Only for SSD drives] To store write cache\n\n\nSelect the Cache role.\nSelect a storage tier that you want to cache.\n\n\nFor storage disks to use cache, the Cache role must be assigned before the Storage role. You can also assign both of these roles to disks at the same time, and the system will configure the cache disk first.\n\n\n\n\n\nTo store data\n\n\nSelect the Storage role.\nSelect a storage tier where to store your data. To make better use of data redundancy, do not assign all of the disks on a node to the same tier. Instead, make sure that each tier is evenly distributed across the cluster.\n\nEnable data caching and checksumming:\n\nEnable SSD caching and checksumming. Available and recommended only for nodes with SSDs.\nEnable checksumming (default). Recommended for nodes with HDDs as it provides better reliability.\nDisable checksumming. Not recommended for production. For an evaluation or testing environment, you can disable checksumming for nodes with HDDs, to provide better performance.\n\n\n\n\n\n\n\nTo store cluster metadata\n\nSelect the Metadata role.\n\nIt is recommended to have only one disk with the Metadata role per node and maximum five such disks in a cluster.\n\n\n\n\n\n[Only for SSD drives] To store both metadata and write cache\n\n\nSelect the Metadata+Cache role.\nSelect a storage tier that you want to cache.\n\n\n\n\n\n\n\n\nClick Assign.\n\n",
                "title": "To manually assign roles to a new disk"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-new-disks-manually.html"
    },
    {
        "title": "Configuring multitenancy",
        "content": "Configuring multitenancy\nTo configure multitenancy for the compute cluster, you need to create domains and projects, assign users to them, and define project quotas.\nLimitations\n\nYou can set project quotas only after deploying the compute cluster.\n\nPrerequisites\n\nA clear understanding of the concept Multitenancy.\nAs quotas can exceed the existing virtual resources and virtual resources are not reserved for each project, the compute cluster must have enough virtual resources for all projects in all domains.\n\nTo create a domain\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click Create domain.\n\nIn the Create domain window, specify the domain name and, optionally, description.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra domain create [--description <description>] [--enable | --disable] <name>\r\n\n\n--description <description>\n\nDomain description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--enable\n\nEnable domain\n--disable\n\nDisable domain\n<name>\n\nDomain name\n\nFor example, to create the domain mydomain, run:# vinfra domain create mydomain\nThe created domain will appear in the vinfra domain list output:# vinfra domain list\r\n+--------------+----------+---------+--------------------+\r\n| id           | name     | enabled | description        |\r\n+--------------+----------+---------+--------------------+\r\n| default      | Default  | True    | The default domain |\r\n| 24986479e<\u00e2\u0080\u00a6> | mydomain | True    |                    |\r\n+--------------+----------+---------+--------------------+\r\n\n\nTo create a project\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click a domain within which the project will be created.\nOn the Projects tab, click Create project.\n\nIn the Create project window, specify the project name and, optionally, description. The project name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nClear the Enabled check box to disable the created project.\n\nDefine quotas for virtual resources that will be available inside the project. To specify a certain value for a resource, clear the Unlimited check box next to it first.\n\nThe default storage policy must be shared with projects that will use the Kubernetes-as-a-service feature.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra domain project create [--description <description>] [--enable | --disable]\r\n                             --domain <domain> <name>\r\n\n\n--description <description>\n\nProject description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--enable\n\nEnable project\n--disable\n\nDisable project\n--domain <domain>\n\nDomain name or ID\n<name>\n\nProject name\n\nFor example, to create the project myproject within the domain mydomain and add a description to it, run:# vinfra domain project create myproject --domain mydomain --description \"A custom project\"\nThe created project will appear in the vinfra domain project list output:# vinfra domain project list --domain mydomain\r\n+-------------+-----------+---------+------------------+--------------+\r\n| id          | name      | enabled | description      | domain_id    |\r\n+-------------+-----------+---------+------------------+--------------+\r\n| 79830e3c<\u00e2\u0080\u00a6> | myproject | True    | A custom project | 24986479e<\u00e2\u0080\u00a6> |\r\n+-------------+-----------+---------+------------------+--------------+\r\n\n\nTo create a self-service user\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click a domain within which the user will be created.\nGo to the Domain users tab, and then click Create user.\n\nIn the Create user window, specify the user name, password, and, if required, a user email address and description. The user name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nSelect the user role:\n\nTo create a domain administrator\n\nSelect the Domain administrator role.\n\nEnable Image uploading to allow the user to upload images and configure this permission for other domain users.\n\nEnable Project and quota management to allow the user to manage projects and quotas, as well as configure this permission for other domain administrators.\n\nTo create a project administrator\n\nSelect the Project member role.\n\nEnable Image uploading to allow the user to upload images.\n\nClick Manage in the Projects section and select a project to assign the user to. Then, click Save.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra domain user create [--email <email>] [--description <description>]\r\n                          [--assign <project> <role>] [--assign-domain <domain> <roles>]\r\n                          [--domain-permissions <domain_permissions>]\r\n                          [--enable | --disable] --domain <domain> <name>\r\n\n\n--email <email>\n\nUser email\n--description <description>\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--assign <project> <role>\n\nAssign a user to a project with one or more permission sets. Specify this option multiple times to assign the user to multiple projects.\n\n<project>: project ID or name\n<role>: user role in the project (project_admin)\n\n--assign-domain <domain> <roles>\n\nAssign a user to a domain with one or more permission sets. Specify this option multiple times to assign the user to multiple domains. This option is only valid for service accounts.\n\n<domain>: domain ID or name\n<roles>: a comma-separated list of service account roles (compute)\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--enable\n\nEnable user\n--disable\n\nDisable user\n--domain <domain>\n\nDomain name or ID\n<name>\n\nUser name\n\nExample 1. To create a domain administrator account called myadmin within the domain mydomain and grant this user the permission to manage projects and their quotas, run:# vinfra domain user create myadmin --domain mydomain --domain-permissions domain_admin,quota_manager\nSpecify the user password when prompted.\nExample 2. To create the project member myuser for the project myproject within the domain mydomain and grant this user the permission to upload images, run:# vinfra domain user create myuser --domain mydomain --assign myproject project_admin --domain-permissions image_upload\nSpecify the user password when prompted.\nThe created users will appear in the vinfra domain user list output:# vinfra domain user list --domain mydomain\r\n+-------------+---------+-------+---------+-------------+--------------------+---------------------------+\r\n| id          | name    | email | enabled | description | domain_permissions | assigned_projects         |\r\n+-------------+---------+-------+---------+-------------+--------------------+---------------------------+\r\n| 28aa0207<\u00e2\u0080\u00a6> | myadmin |       | True    |             | - domain_admin     | []                        |\r\n|             |         |       |         |             | - quota_manager    |                           |\r\n| fb9fa0b2<\u00e2\u0080\u00a6> | myuser  |       | True    |             | - image_upload     | - project_id: 79830e3c<\u00e2\u0080\u00a6> |\r\n|             |         |       |         |             |                    |   role: project_admin     |\r\n+-------------+---------+-------+---------+-------------+--------------------+---------------------------+\r\n\n\nSee also\n\nManaging domains, users, and projects\n\nWhat's next\n\nProviding access to the self-service portal",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain create [--description <description>] [--enable | --disable] <name>\r\n\n\n--description <description>\n\n\nDomain description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--enable\n\nEnable domain\n--disable\n\nDisable domain\n<name>\n\nDomain name\n\nFor example, to create the domain mydomain, run:# vinfra domain create mydomain\nThe created domain will appear in the vinfra domain list output:# vinfra domain list\r\n+--------------+----------+---------+--------------------+\r\n| id           | name     | enabled | description        |\r\n+--------------+----------+---------+--------------------+\r\n| default      | Default  | True    | The default domain |\r\n| 24986479e<\u00e2\u0080\u00a6> | mydomain | True    |                    |\r\n+--------------+----------+---------+--------------------+\r\n\n",
                "title": "To create a domain"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain project create [--description <description>] [--enable | --disable]\r\n                             --domain <domain> <name>\r\n\n\n--description <description>\n\n\nProject description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--enable\n\nEnable project\n--disable\n\nDisable project\n--domain <domain>\n\nDomain name or ID\n<name>\n\nProject name\n\nFor example, to create the project myproject within the domain mydomain and add a description to it, run:# vinfra domain project create myproject --domain mydomain --description \"A custom project\"\nThe created project will appear in the vinfra domain project list output:# vinfra domain project list --domain mydomain\r\n+-------------+-----------+---------+------------------+--------------+\r\n| id          | name      | enabled | description      | domain_id    |\r\n+-------------+-----------+---------+------------------+--------------+\r\n| 79830e3c<\u00e2\u0080\u00a6> | myproject | True    | A custom project | 24986479e<\u00e2\u0080\u00a6> |\r\n+-------------+-----------+---------+------------------+--------------+\r\n\n",
                "title": "To create a project"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user create [--email <email>] [--description <description>]\r\n                          [--assign <project> <role>] [--assign-domain <domain> <roles>]\r\n                          [--domain-permissions <domain_permissions>]\r\n                          [--enable | --disable] --domain <domain> <name>\r\n\n\n--email <email>\n\nUser email\n--description <description>\n\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--assign <project> <role>\n\n\nAssign a user to a project with one or more permission sets. Specify this option multiple times to assign the user to multiple projects.\n\n<project>: project ID or name\n<role>: user role in the project (project_admin)\n\n\n--assign-domain <domain> <roles>\n\n\nAssign a user to a domain with one or more permission sets. Specify this option multiple times to assign the user to multiple domains. This option is only valid for service accounts.\n\n<domain>: domain ID or name\n<roles>: a comma-separated list of service account roles (compute)\n\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--enable\n\nEnable user\n--disable\n\nDisable user\n--domain <domain>\n\nDomain name or ID\n<name>\n\nUser name\n\nExample 1. To create a domain administrator account called myadmin within the domain mydomain and grant this user the permission to manage projects and their quotas, run:# vinfra domain user create myadmin --domain mydomain --domain-permissions domain_admin,quota_manager\nSpecify the user password when prompted.\nExample 2. To create the project member myuser for the project myproject within the domain mydomain and grant this user the permission to upload images, run:# vinfra domain user create myuser --domain mydomain --assign myproject project_admin --domain-permissions image_upload\nSpecify the user password when prompted.\nThe created users will appear in the vinfra domain user list output:# vinfra domain user list --domain mydomain\r\n+-------------+---------+-------+---------+-------------+--------------------+---------------------------+\r\n| id          | name    | email | enabled | description | domain_permissions | assigned_projects         |\r\n+-------------+---------+-------+---------+-------------+--------------------+---------------------------+\r\n| 28aa0207<\u00e2\u0080\u00a6> | myadmin |       | True    |             | - domain_admin     | []                        |\r\n|             |         |       |         |             | - quota_manager    |                           |\r\n| fb9fa0b2<\u00e2\u0080\u00a6> | myuser  |       | True    |             | - image_upload     | - project_id: 79830e3c<\u00e2\u0080\u00a6> |\r\n|             |         |       |         |             |                    |   role: project_admin     |\r\n+-------------+---------+-------+---------+-------------+--------------------+---------------------------+\r\n\n",
                "title": "To create a self-service user"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click Create domain.\n\nIn the Create domain window, specify the domain name and, optionally, description.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n\n\n\nClick Create.\n\n",
                "title": "To create a domain"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click a domain within which the project will be created.\nOn the Projects tab, click Create project.\n\nIn the Create project window, specify the project name and, optionally, description. The project name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\nClear the Enabled check box to disable the created project.\n\n\nDefine quotas for virtual resources that will be available inside the project. To specify a certain value for a resource, clear the Unlimited check box next to it first.\n\nThe default storage policy must be shared with projects that will use the Kubernetes-as-a-service feature.\n\n\n\n\n\n\nClick Create.\n\n",
                "title": "To create a project"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click a domain within which the user will be created.\nGo to the Domain users tab, and then click Create user.\n\nIn the Create user window, specify the user name, password, and, if required, a user email address and description. The user name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\nSelect the user role:\n\n\nTo create a domain administrator\n\n\nSelect the Domain administrator role.\n\nEnable Image uploading to allow the user to upload images and configure this permission for other domain users.\n\n\nEnable Project and quota management to allow the user to manage projects and quotas, as well as configure this permission for other domain administrators.\n\n\n\n\n\n\n\n\n\n\nTo create a project administrator\n\n\nSelect the Project member role.\n\nEnable Image uploading to allow the user to upload images.\n\nClick Manage in the Projects section and select a project to assign the user to. Then, click Save.\n\n\n\n\n\n\n\n\n\n\nClick Create.\n\n",
                "title": "To create a self-service user"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-multitenancy.html"
    },
    {
        "title": "Configuring NVMe performance",
        "content": "Configuring NVMe performance\nIn the storage cluster, the performance of any disk is limited by the performance of a chunk service (CS), which is single threaded. To increase the parallelism and throughput of fast disks such as non-volatile memory express (NVMe) devices, you can enable NVMe performance for specific storage tiers. You can benefit from using this feature when running workloads with random writes, such as virtual machines. With NVMe performance enabled, one physical device will be able to run multiple chunk services. Note, however, that the amount of RAM and CPU cores reserved on a node will increase linearly with the number of CSes.\nA storage tier with the disk failure domain cannot have NVMe performance enabled. If you want to use such tiers in your cluster, ensure that they have NVMe performance disabled before assigning storage disks to them.\nThough it is convenient to configure NVMe performance before creating the storage cluster, you can also do this at any time afterwards. However, configuring NVMe performance in the existing cluster requires releasing and reattaching each device in the tier, for the change to take effect.\nLimitations\n\nIt is recommended to enable NVMe performance only for NVMe or SSD devices. Using this feature with other devices will lead to higher resource consumption without actual performance benefits.\nWith NVMe performance enabled, it is recommended to configure redundancy by replication. Using erasure coding in this case will not improve the storage cluster performance.\nYou cannot configure NVMe performance for a storage tier with the failure domain set to \"Disk\".\nUsing multiple chunk services per physical drive is only recommended for NVMe devices above 512 GB in size.\nIn a single-node deployment, using multiple chunk services per physical drive is not supported.\n\nPrerequisites\n\nThe infrastructure nodes are equipped with NVMe or SSD disks.\nWhen increasing the number of chunk services per physical device, ensure that each CS has sufficient space allocated to it, that is, at least 100 GB.\nA clear understanding of the CPU and RAM reservations, which are described in General requirements.\n\nTo enable NVMe performance for a tier\n\nAdmin panel\n\nGo to the Settings > System settings > Storage performance screen.\n\nIn the Custom NVMe performance section, select the desired number of chunk services per physical device for the required storage tier. The maximum number is 4. The recommended number is 2 for NVMe devices and 4 for PMem devices.\n\nClick Save to apply the changes.\nIf you have enabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n\nCommand-line interface\nUse the following command:vinfra cluster settings number-of-cses-per-disk set [--tier0 <number>] [--tier1 <number>] \r\n                                                    [--tier2 <number>] [--tier3 <number>]\n\n--tier0 <number>\n\nSet the number of CSes per disk for tier 0.\n--tier1 <number>\n\nSet the number of CSes per disk for tier 1.\n--tier2 <number>\n\nSet the number of CSes per disk for tier 2.\n--tier3 <number>\n\nSet the number of CSes per disk for tier 3.\n\nFor example, to set 4 CSes per disk for the tier 0 and 4 CSes per disk for the tier 1, run:# vinfra cluster settings number-of-cses-per-disk set --tier0 4 --tier1 4\nYou can check that the setting has been applied by running vinfra cluster settings number-of-cses-per-disk show:# vinfra cluster settings number-of-cses-per-disk show\r\n+---------+-------+\r\n| Field   | Value |\r\n+---------+-------+\r\n| maximal | 10    |\r\n| minimal | 1     |\r\n| tier0   | 4     |\r\n| tier1   | 4     |\r\n| tier2   | 1     |\r\n| tier3   | 1     |\r\n+---------+-------+\nIf you have enabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n\nTo disable NVMe performance for a tier\n\nAdmin panel\n\nGo to the Settings > System settings > Storage performance screen.\nSelect 1 as the number of chunk services per physical device for the required storage tier.\nClick Save to apply the changes.\nIf you have disabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n\nCommand-line interface\nUse the following command:vinfra cluster settings number-of-cses-per-disk set [--tier0 <number>] [--tier1 <number>] \r\n                                                    [--tier2 <number>] [--tier3 <number>]\n\n--tier0 <number>\n\nSet the number of CSes per disk for tier 0.\n--tier1 <number>\n\nSet the number of CSes per disk for tier 1.\n--tier2 <number>\n\nSet the number of CSes per disk for tier 2.\n--tier3 <number>\n\nSet the number of CSes per disk for tier 3.\n\nFor example, to disable NVMe performance for the tier 1, run:# vinfra cluster settings number-of-cses-per-disk set --tier1 1\nYou can check that the setting has been applied by running vinfra cluster settings number-of-cses-per-disk show:# vinfra cluster settings number-of-cses-per-disk show\r\n+---------+-------+\r\n| Field   | Value |\r\n+---------+-------+\r\n| maximal | 10    |\r\n| minimal | 1     |\r\n| tier0   | 4     |\r\n| tier1   | 1     |\r\n| tier2   | 1     |\r\n| tier3   | 1     |\r\n+---------+-------+\nIf you have disabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n\nSee also\n\nReleasing node disks\n\nWhat's next\n\nConfiguring node locations",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster settings number-of-cses-per-disk set [--tier0 <number>] [--tier1 <number>] \r\n                                                    [--tier2 <number>] [--tier3 <number>]\n\n--tier0 <number>\n\nSet the number of CSes per disk for tier 0.\n--tier1 <number>\n\nSet the number of CSes per disk for tier 1.\n--tier2 <number>\n\nSet the number of CSes per disk for tier 2.\n--tier3 <number>\n\nSet the number of CSes per disk for tier 3.\n\nFor example, to set 4 CSes per disk for the tier 0 and 4 CSes per disk for the tier 1, run:# vinfra cluster settings number-of-cses-per-disk set --tier0 4 --tier1 4\nYou can check that the setting has been applied by running vinfra cluster settings number-of-cses-per-disk show:# vinfra cluster settings number-of-cses-per-disk show\r\n+---------+-------+\r\n| Field   | Value |\r\n+---------+-------+\r\n| maximal | 10    |\r\n| minimal | 1     |\r\n| tier0   | 4     |\r\n| tier1   | 4     |\r\n| tier2   | 1     |\r\n| tier3   | 1     |\r\n+---------+-------+\nIf you have enabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n",
                "title": "To enable NVMe performance for a tier"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster settings number-of-cses-per-disk set [--tier0 <number>] [--tier1 <number>] \r\n                                                    [--tier2 <number>] [--tier3 <number>]\n\n--tier0 <number>\n\nSet the number of CSes per disk for tier 0.\n--tier1 <number>\n\nSet the number of CSes per disk for tier 1.\n--tier2 <number>\n\nSet the number of CSes per disk for tier 2.\n--tier3 <number>\n\nSet the number of CSes per disk for tier 3.\n\nFor example, to disable NVMe performance for the tier 1, run:# vinfra cluster settings number-of-cses-per-disk set --tier1 1\nYou can check that the setting has been applied by running vinfra cluster settings number-of-cses-per-disk show:# vinfra cluster settings number-of-cses-per-disk show\r\n+---------+-------+\r\n| Field   | Value |\r\n+---------+-------+\r\n| maximal | 10    |\r\n| minimal | 1     |\r\n| tier0   | 4     |\r\n| tier1   | 1     |\r\n| tier2   | 1     |\r\n| tier3   | 1     |\r\n+---------+-------+\nIf you have disabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n",
                "title": "To disable NVMe performance for a tier"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Settings > System settings > Storage performance screen.\n\nIn the Custom NVMe performance section, select the desired number of chunk services per physical device for the required storage tier. The maximum number is 4. The recommended number is 2 for NVMe devices and 4 for PMem devices.\n\n\n\n\n\nClick Save to apply the changes.\nIf you have enabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n\n",
                "title": "To enable NVMe performance for a tier"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Settings > System settings > Storage performance screen.\nSelect 1 as the number of chunk services per physical device for the required storage tier.\nClick Save to apply the changes.\nIf you have disabled NVMe performance after creating the storage cluster, release and reattach each NVMe disk in the affected storage tiers.\n\n",
                "title": "To disable NVMe performance for a tier"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-nvme-performance.html"
    },
    {
        "title": "Configuring node locations",
        "content": "Configuring node locations\nThe location for a node can be selected before it is joined to the storage cluster. By default, the nodes are added to the Default rack in the Default row in the Default room.\nLocations are designed to be used as failure domains. You can create locations, move nodes between them, rename and delete locations.\nLimitations\n\nThe location can be changed for unassigned nodes only. If the node is added to the storage cluster, release it first. After moving the node, you will be able to join it back to the cluster.\nYou can only delete empty locations. If a location contains some nodes, move them first. The default location cannot be deleted as well. However, you can rename it to match your infrastructure.\n\nPrerequisites\n\nA clear understanding of the concept Failure domains.\n\nTo create locations\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the icon  to show the node filters and location (if hidden).\n\nIn the location sidebar, click the top of the tree with the cluster name.\n\nClick the Create room toolbar button and enter the room name.\nTo add a new row, click the created room. Click the Create row button and enter a name for it.\nTo add a new rack, click the created row. Click the Create rack button and enter a name for it. You can now move nodes to this rack.\n\nCommand-line interface\nUse the following command:vinfra location create --fd <fd> --name <location-name> [--parent-id <parent-id>]\r\n\n\n--fd <fd>\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n--name <location-name>\n\nName of the location to be created\n--parent-id <parent-id>\n\nID of the parent location where the child location should be created in\n\nFor example, to create the location row2, run:# vinfra location create --fd 3 --name row2 --parent-id 0\r\n+----------+-------+\r\n| Field    | Value |\r\n+----------+-------+\r\n| children | []    |\r\n| id       | 1     |\r\n| name     | row2  |\r\n| parent   | 0     |\r\n+----------+-------+\r\n\n\nTo create a location of level 4 (room), do not use the --parent-id argument.\n\nThe created location will appear in the vinfra location list output:# vinfra location list --fd 3\r\n+----+-------------+--------+----------+\r\n| id | name        | parent | children |\r\n+----+-------------+--------+----------+\r\n| 0  | Default row | 0      | - 0      |\r\n| 1  | row2        | 0      | []       |\r\n+----+-------------+--------+----------+\r\n\n\nTo rename locations\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the icon  to show the node filters and location (if hidden).\nIn the location sidebar, click the parent location for the item you want to rename. For example, click the row if you want to rename a rack in it.\nIn the list, click the required location. On the right pane, click Rename and enter a new name.\n\nThe location will be renamed.\n\nYou can rename the room, row, and rack locations by using the command-line interface. For example, you can rename rack to chassis to match your actual server location.\n\nCommand-line interface\nUse the following command:vinfra location rename --fd <fd> --id <location-id> --name <location-name>\r\n\n\n--fd <fd>\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n--id <location-id>\n\nID of the location to rename\n--name <location-name>\n\nA new location name\n\nFor example, to rename the location with the ID 1 to row_renamed, run:# vinfra location rename --fd 3 --id 1 --name row_renamed\nYou can also change names for failure domain levels 2, 3 and 4 by using the following command:vinfra failure domain rename {2,3,4} <singular-name> <plural-name>\r\n\n\n{2,3,4}\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n<singular-name>\n\nSingular name of the specified failure domain\n<plural-name>\n\nPlural name of the specified failure domain\n\nFor example, to rename the failure domain 2 to chassis, run:# vinfra failure domain rename 2 chassis chassis\n\nIf you use a name other than zone, enclosure, chassis, blade server, it will be replaced with location in the admin panel.\n\nTo move nodes to a new location\n\nOn the Infrastructure > Nodes screen, there are two ways to move a node to a new location. Either from the node location tree, you can select the rack to move the node to, and click Move nodes. Or you can click the line with a node to move, and click Move node on the right pane.\nIn the Move nodes window, select the required node/location and click Move.\nYou can now join this node to the cluster. To do this, click the line with the node, and on the right pane click Join to cluster. Click Join in the open window.\n\nThe node will be moved to the specified location.\nTo delete locations\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the icon  to show the node filters and location (if hidden).\nIn the location sidebar, click the parent location for the item you want to delete. For example, click the row if you want to delete a rack from it.\nIn the list, click the required location. On the right pane, click Delete.\n\nThe location will be deleted.\n\nCommand-line interface\nUse the following command:vinfra location delete --fd <fd> --id <location-id>\r\n\n\n--fd <fd>\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n--id <location-id>\n\nID of the location to delete\n\nFor example, to delete the location with the ID 1, run:# vinfra location delete --fd 3 --id 1\n\nWhat's next\n\nDeploying the storage cluster",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra location create --fd <fd> --name <location-name> [--parent-id <parent-id>]\r\n\n\n--fd <fd>\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n--name <location-name>\n\nName of the location to be created\n--parent-id <parent-id>\n\nID of the parent location where the child location should be created in\n\nFor example, to create the location row2, run:# vinfra location create --fd 3 --name row2 --parent-id 0\r\n+----------+-------+\r\n| Field    | Value |\r\n+----------+-------+\r\n| children | []    |\r\n| id       | 1     |\r\n| name     | row2  |\r\n| parent   | 0     |\r\n+----------+-------+\r\n\n\nTo create a location of level 4 (room), do not use the --parent-id argument.\n\nThe created location will appear in the vinfra location list output:# vinfra location list --fd 3\r\n+----+-------------+--------+----------+\r\n| id | name        | parent | children |\r\n+----+-------------+--------+----------+\r\n| 0  | Default row | 0      | - 0      |\r\n| 1  | row2        | 0      | []       |\r\n+----+-------------+--------+----------+\r\n\n",
                "title": "To create locations"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra location rename --fd <fd> --id <location-id> --name <location-name>\r\n\n\n--fd <fd>\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n--id <location-id>\n\nID of the location to rename\n--name <location-name>\n\nA new location name\n\nFor example, to rename the location with the ID 1 to row_renamed, run:# vinfra location rename --fd 3 --id 1 --name row_renamed\nYou can also change names for failure domain levels 2, 3 and 4 by using the following command:vinfra failure domain rename {2,3,4} <singular-name> <plural-name>\r\n\n\n{2,3,4}\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n<singular-name>\n\nSingular name of the specified failure domain\n<plural-name>\n\nPlural name of the specified failure domain\n\nFor example, to rename the failure domain 2 to chassis, run:# vinfra failure domain rename 2 chassis chassis\n\nIf you use a name other than zone, enclosure, chassis, blade server, it will be replaced with location in the admin panel.\n\n",
                "title": "To rename locations"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra location delete --fd <fd> --id <location-id>\r\n\n\n--fd <fd>\n\nFailure domain ID: 0=disk, 1=host, 2=rack, 3=row, 4=room. You can view the list of failure domains by using vinfra failure domain list.\n--id <location-id>\n\nID of the location to delete\n\nFor example, to delete the location with the ID 1, run:# vinfra location delete --fd 3 --id 1\n",
                "title": "To delete locations"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the icon  to show the node filters and location (if hidden).\n\nIn the location sidebar, click the top of the tree with the cluster name.\n\n\n\nClick the Create room toolbar button and enter the room name.\nTo add a new row, click the created room. Click the Create row button and enter a name for it.\nTo add a new rack, click the created row. Click the Create rack button and enter a name for it. You can now move nodes to this rack.\n\n",
                "title": "To create locations"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the icon  to show the node filters and location (if hidden).\nIn the location sidebar, click the parent location for the item you want to rename. For example, click the row if you want to rename a rack in it.\nIn the list, click the required location. On the right pane, click Rename and enter a new name.\n\nThe location will be renamed.\n\nYou can rename the room, row, and rack locations by using the command-line interface. For example, you can rename rack to chassis to match your actual server location.\n\n",
                "title": "To rename locations"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the icon  to show the node filters and location (if hidden).\nIn the location sidebar, click the parent location for the item you want to delete. For example, click the row if you want to delete a rack from it.\nIn the list, click the required location. On the right pane, click Delete.\n\nThe location will be deleted.\n",
                "title": "To delete locations"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-node-locations.html"
    },
    {
        "title": "Configuring outbound firewall rules",
        "content": "Configuring outbound firewall rules\nTo control outbound traffic from your cluster nodes, you can configure outbound firewall rules for public networks by using the vinfra tool. By default, ports used by system services are opened, to ensure non-disruptive cluster operation. Additionally, outbound traffic is always allowed in the subnet dedicated to internal communication between cluster nodes. As a private network is not publicly exposed and does not communicate with any external endpoints, you do not need to restrict outbound traffic for it. A network is recognized as private if it is assigned any of these traffic types:\n\nOSTOR private\nBackup (ABGW) private\nInternal management\nStorage\n\nA private network always has the rule <private_subnet_cidr>:any:0, which allows all outbound traffic in the current subnet. This rule is not visible via the vinfra commands and exists only in iptables.\nTo block all outbound traffic except that which is necessary for cluster operation, perform the following steps:\n\nCreate additional firewall rules, to allow outbound traffic for particular services.\nRemove the rule that allows all outbound traffic.\nCheck your network settings.\n\nSee also\n\nConfiguring inbound firewall rules\n\nManaging networks\n\nConfiguring data-in-transit encryption\n\nManaging traffic types",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-outbound-firewall-rules.html"
    },
    {
        "title": "Configuring retention policy for Prometheus metrics",
        "content": "Configuring retention policy for Prometheus metrics\nThe Prometheus service used for monitoring the cluster runs and stores its data on the management node. By default, Prometheus metrics are stored for seven days. This retention period can be insufficient for troubleshooting purposes. You can increase it manually by modifying the Prometheus configuration file.\nHowever, with a long retention period, the root partition where the data is stored may run out of free space. To prevent this, you can define the maximum size for the Prometheus metrics. The oldest data will be removed first.\nTo increase the retention period\n\nOn the management node, open the file /etc/sysconfig/prometheus to edit, set the needed retention period for the STORAGE_RETENTION option, and then save your changes. For example:STORAGE_RETENTION=\"--storage.tsdb.retention.time=30d\"\r\n\n\nRestart the Prometheus service:systemctl restart prometheus.service\r\n\n\nIf high availability is enabled in the storage cluster, repeat these steps for the other two management nodes.\nTo change the time retention policy to the size retention policy\n\nOn the management node, open the file /etc/sysconfig/prometheus to edit, change the flag for the STORAGE_RETENTION option, and then save your changes. For example:STORAGE_RETENTION=\"--storage.tsdb.retention.size=10GB\"\r\n\n\nRestart the Prometheus service:systemctl restart prometheus.service\r\n\n\nIf high availability is enabled in the storage cluster, repeat these steps for the other two management nodes.\nSee also\n\nUsing built-in Prometheus for monitoring\n\nUsing external Prometheus for monitoring\n\nPrometheus metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-retention-policy-for-prometheus-metrics.html"
    },
    {
        "title": "Configuring RDMA automatically",
        "content": "Configuring RDMA automatically\nLimitations\n\nYou can only configure RDMA automatically before creating the storage cluster.\n\nPrerequisites\n\nYou have checked the RDMA network by following the instructions in Checking the RDMA network.\n\nTo configure RDMA automatically\n\nAdmin panel\nUse the Enable RDMA toggle switch on the Settings > System settings > Storage performance screen. \n\nCommand-line interface\nUse the following command:vinfra cses-config change (--enable | --disable)\n\n--enable\n\nEnable RDMA\n--disable\n\nDisable RDMA\n\nFor example, to enable RDMA for the storage cluster, run:# vinfra cses-config change --enable\nYou can check that the setting has been applied by running vinfra cses-config show:# vinfra cses-config show\r\n+-------+-------+\r\n| Field | Value |\r\n+-------+-------+\r\n| rdma  | True  |\r\n+-------+-------+\r\n\n\nWhat's next\n\nAdding external DNS servers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cses-config change (--enable | --disable)\n\n--enable\n\nEnable RDMA\n--disable\n\nDisable RDMA\n\nFor example, to enable RDMA for the storage cluster, run:# vinfra cses-config change --enable\nYou can check that the setting has been applied by running vinfra cses-config show:# vinfra cses-config show\r\n+-------+-------+\r\n| Field | Value |\r\n+-------+-------+\r\n| rdma  | True  |\r\n+-------+-------+\r\n\n",
                "title": "To configure RDMA automatically"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nUse the Enable RDMA toggle switch on the Settings > System settings > Storage performance screen. \n",
                "title": "To configure RDMA automatically"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-rdma-automatically.html"
    },
    {
        "title": "Configuring the self-service panel",
        "content": "Configuring the self-service panel\nAfter you provide access to the self-service panel, you can configure  its virtual IP address and branding theme.\nPrerequisites\n\nThe port for accessing the self-service panel is opened by following the instructions in Providing access to the self-service portal.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-the-self-service-panel.html"
    },
    {
        "title": "Configuring throttling for backup storage",
        "content": "Configuring throttling for backup storage\nWhen the percentage of used space on underlying storage reaches a certain threshold, backup storage starts to throttle write operations to slow down the data flow. With the public cloud destination, however, this threshold applies to the local storage usage as backup storage uses the local storage as the staging area.\nBy default, the throttling starting value is 85%. Throttling intensity increases with space consumption, until the percentage of free storage space is half the difference between the total storage space and the throttling starting value. After that, throttling works with maximum intensity until more storage space is added. For example, if the starting value is 85%, throttling maximum intensity will be reached when the storage has (100%-85%)/2 of free space left, that is, 7.5%. The throttling starting value can be increased up to 95%.\nWhen working with public clouds, backup storage throttling may also start for certain backup archives even if there is still enough free space left on the local storage. This happens when the size occupied by a single backup archive in the staging area is greater than or equal to the per-archive threshold. By default, this threshold is set to 300 MiB, which is a recommended value for small clusters below 500 GiB in size. If your cluster has more storage space, you can increase the throttling starting value per archive:\n\nFor medium clusters with 500 GiB to 1 TiB of available space, it is recommended to set a value of 1 to 3 GiB, depending on the number of files on the storage.\nFor large clusters above 1 TiB in size, the throttling starting value can be changed up to 100 GiB.\n\nTo monitor backup storage throttling, go to Storage services > Backup storage > Overview and check the Append throttling chart.\nPrerequisites\n\nThe backup storage cluster is created and registered in the Cloud Management Panel, as described in Provisioning backup storage space.\n\nTo change the throttling starting value\n\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Settings tab, and then click Throttling configuration.\nSelect the desired throttling starting value for local storage.\n[For backup storage with the public cloud destination] Select the desired throttling starting value per archive.\nClick Save.\n\nCommand-line interface\nUse the following command:vinfra service backup throttling set [--soft-threshold <soft_threshold>]\r\n                                     [--s3-threshold <s3_threshold>]\r\n\n\n--soft-threshold <soft_threshold>\n\nSoft threshold limit, in percent. The valid range is 85-95%.\n--s3-threshold <s3_threshold>\n\nObject storage threshold limit, in MiB. Must be greater than the default limit of 300 MiB. This parameter can only be used for backup storage with the public cloud destination.\n\nFor example, to change the throttling starting value to 90%, run:# vinfra service backup throttling set --soft-threshold 90\nThe updated parameters will be shown in the vinfra service backup throttling show output:# vinfra service backup throttling show\r\n+----------------+-------+\r\n| Field          | Value |\r\n+----------------+-------+\r\n| soft_threshold | 90    |\r\n+----------------+-------+\n\nTo reset the throttling starting value to default\n\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Settings tab, and then click Throttling configuration.\nClick Reset to default.\nIn the confirmation window, click Reset.\n\nCommand-line interface\nUse the following command:vinfra service backup throttling reset\nFor example, to reset the throttling starting value to 85%, run:# vinfra service backup throttling reset\nThe updated parameters will be shown in the vinfra service backup throttling show output:# vinfra service backup throttling show\r\n+----------------+-------+\r\n| Field          | Value |\r\n+----------------+-------+\r\n| soft_threshold | 85    |\r\n+----------------+-------+\n\nSee also\n\nMonitoring backup storage\n\nScaling the storage cluster",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup throttling set [--soft-threshold <soft_threshold>]\r\n                                     [--s3-threshold <s3_threshold>]\r\n\n\n--soft-threshold <soft_threshold>\n\nSoft threshold limit, in percent. The valid range is 85-95%.\n--s3-threshold <s3_threshold>\n\nObject storage threshold limit, in MiB. Must be greater than the default limit of 300 MiB. This parameter can only be used for backup storage with the public cloud destination.\n\nFor example, to change the throttling starting value to 90%, run:# vinfra service backup throttling set --soft-threshold 90\nThe updated parameters will be shown in the vinfra service backup throttling show output:# vinfra service backup throttling show\r\n+----------------+-------+\r\n| Field          | Value |\r\n+----------------+-------+\r\n| soft_threshold | 90    |\r\n+----------------+-------+\n",
                "title": "To change the throttling starting value"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup throttling reset\nFor example, to reset the throttling starting value to 85%, run:# vinfra service backup throttling reset\nThe updated parameters will be shown in the vinfra service backup throttling show output:# vinfra service backup throttling show\r\n+----------------+-------+\r\n| Field          | Value |\r\n+----------------+-------+\r\n| soft_threshold | 85    |\r\n+----------------+-------+\n",
                "title": "To reset the throttling starting value to default"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Settings tab, and then click Throttling configuration.\nSelect the desired throttling starting value for local storage.\n[For backup storage with the public cloud destination] Select the desired throttling starting value per archive.\nClick Save.\n\n",
                "title": "To change the throttling starting value"
            },
            {
                "example": "\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Settings tab, and then click Throttling configuration.\nClick Reset to default.\nIn the confirmation window, click Reset.\n\n",
                "title": "To reset the throttling starting value to default"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-throttling-for-backup-storage.html"
    },
    {
        "title": "Connecting to OpenStack command-line interface",
        "content": "Connecting to OpenStack command-line interface\nFor managing the compute cluster, you can also use the OpenStack command-line client, which is automatically installed along with the Virtuozzo Hybrid Infrastructure.\nTo connect to and be able to use the OpenStack CLI\n\nLocate the node with the management role in the admin panel. Open the Infrastructure > Nodes screen. The management node runs the  Admin panel service.\n\nAccess the management node via SSH and log in as the service user. For example:# ssh node001.vstoragedomain\r\n# su - vstoradmin\r\n\n\nGenerate the admin OpenRC script that sets environment variables:# kolla-ansible post-deploy\r\n\nThe command will create the /etc/kolla/admin-openrc.sh bash script:export OS_PROJECT_DOMAIN_NAME=Default\r\nexport OS_USER_DOMAIN_NAME=Default\r\nexport OS_PROJECT_NAME=admin\r\nexport OS_USERNAME=vstorage-service-user\r\nexport OS_PASSWORD=<password>\r\nexport OS_AUTH_URL=https://<MN_IP_address>:5000/v3\r\nexport OS_IDENTITY_API_VERSION=3\r\nexport OS_AUTH_TYPE=password\r\nexport OS_INSECURE=true\r\nexport PYTHONWARNINGS=\"ignore:Unverified HTTPS request is being made\"\r\nexport NOVACLIENT_INSECURE=true\r\nexport NEUTRONCLIENT_INSECURE=true\r\nexport CINDERCLIENT_INSECURE=true\r\nexport OS_PLACEMENT_API_VERSION=1.22\r\n\nBy default, the script is created to authorize OpenStack commands in the admin project under the vstorage-service-user user for managing the compute cluster with administrative privileges.\n\nTo perform administrative actions, run this script:\n\nYou need to run the script each session.\n# source /etc/kolla/admin-openrc.sh\r\n\n\nIf you want to work in another project under another user, you need to make changes to the admin-openrc.sh script. For example, to authorize OpenStack commands in the myproject project under the myuser user within the mydomain domain, do the following:\n\nCopy the script to the chosen directory with a new name. For example:# cp /etc/kolla/admin-openrc.sh /root/myscript.sh\r\n\n\nOpen the copied script for editing and change the first five variables as follows:export OS_PROJECT_DOMAIN_NAME=mydomain\r\nexport OS_USER_DOMAIN_NAME=mydomain\r\nexport OS_PROJECT_NAME=myproject\r\nexport OS_USERNAME=myuser\r\nexport OS_PASSWORD=<myuser_password>\r\n\nLeave other variables as is and save your changes.\n\nRun the modified script:\n\nYou need to run the script each session.\n# source /root/myscript.sh\r\n\n\nNow you can work in the project you have authorized in by executing OpenStack commands with the --insecure option. For example:# openstack --insecure server list\r\n+---------------------+------+--------+------------------------+-------+--------+\r\n| ID                  | Name | Status | Networks               | Image | Flavor |\r\n+---------------------+------+--------+------------------------+-------+--------+\r\n| 32b0f95d-477f-<...> | vm1  | ACTIVE | private=192.168.128.87 |       | tiny   |\r\n+---------------------+------+--------+------------------------+-------+--------+\r\n\nSee also\n\nChanging parameters in OpenStack configuration files\n\nConfiguring scheduling of virtual machines\n\nChanging the default project quotas\n\nConfiguring memory for virtual machines\n\nConfiguring CPU features for virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/connecting-to-openstack-command-line-interface.html"
    },
    {
        "title": "Configuring Windows boot volumes",
        "content": "Configuring Windows boot volumes\nWindows guests have neither Cloudbase-Init nor OpenSSH Server preinstalled by default. You need to install and configure them manually.\nTo install Cloudbase-Init and OpenSSH Server inside a Windows virtual machine\n\nLog in to a Windows VM.\nCreate a new administrator account that will be used for SSH connections and log in with it.\n\nTo install and configure OpenSSH Server:\n\nRun Windows PowerShell with administrator privileges and set the execution policy to unrestricted to be able to run scripts:> Set-ExecutionPolicy Unrestricted\r\n\n\nDownload OpenSSH Server (for example, from the GitHub repository), extract the archive into the C:\\Program Files directory, and then install it by running:> & 'C:\\Program Files\\OpenSSH-Win64\\install-sshd.ps1'\r\n\n\nStart the sshd service and set its startup type to \u00e2\u0080\u009cAutomatic\u00e2\u0080\u009d:> net start sshd> Set-Service sshd -StartupType Automatic\r\n\n\nOpen TCP port 22 for the OpenSSH service in the Windows Firewall:\n\nOn Windows 8.1, Windows Server 2012, and newer versions, run:> New-NetFirewallRule -Protocol TCP -LocalPort 22 -Direction Inbound -Action Allow -DisplayName OpenSSH\r\n\n\nOn Windows Server 2008/2008 R2, run:> netsh advfirewall firewall add rule name=sshd dir=in action=allow protocol=TCP localport=22\r\n\n\nOpen the C:\\ProgramData\\ssh\\sshd_config file:> notepad 'C:\\ProgramData\\ssh\\sshd_config'\r\n\nComment out the following lines at the end of the file:#Match Group administrators#AuthorizedKeysFile __PROGRAMDATA__/ssh/administrators_authorized_keys\r\n\nSave the changes.\n\nCreate the .ssh directory in C:\\Users\\<current_user> and an empty authorized_keys file inside it:> cd C:\\Users\\<current_user>> mkdir .ssh> notepad .\\.ssh\\authorized_keys\r\n\nRemove the .txt extension from the created file:> move .\\.ssh\\authorized_keys.txt .\\.ssh\\authorized_keys\r\n\n\nModify the permissions for the created file to disable inheritance:> icacls .\\.ssh\\authorized_keys /inheritance:r\r\n\n\nDownload Cloudbase-Init from https://cloudbase.it/cloudbase-init/#download, and then install it by following the procedure from the Installation section at https://cloudbase.it/cloudbase-init/.\n\nThe password for the user specified during the Cloudbase-Init installation will be reset on the next VM startup. If this user does not exist, a new user account will be created. You will be able to log in with this account by using the key authentication method or you can set a new password with a customization script. If there are multiple Windows users at the image preparation time, the passwords for other users will not be changed.\nWhen the Cloudbase-Init installation is complete, do not select the option to run Sysprep before clicking Finish. Otherwise, you will not be able to modify cloudbase-init.conf.\n\nRun Windows PowerShell with administrator privileges and open the file C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\conf\\cloudbase-init.conf:> notepad 'C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\conf\\cloudbase-init.conf'\r\n\nAdd metadata_services and plugins on two lines:metadata_services=\\cloudbaseinit.metadata.services.configdrive.ConfigDriveService,\\cloudbaseinit.metadata.services.httpservice.HttpServiceplugins=cloudbaseinit.plugins.common.mtu.MTUPlugin,\\cloudbaseinit.plugins.windows.ntpclient.NTPClientPlugin,\\cloudbaseinit.plugins.common.sethostname.SetHostNamePlugin,\\cloudbaseinit.plugins.windows.createuser.CreateUserPlugin,\\cloudbaseinit.plugins.common.networkconfig.NetworkConfigPlugin,\\cloudbaseinit.plugins.windows.licensing.WindowsLicensingPlugin,\\cloudbaseinit.plugins.common.sshpublickeys.SetUserSSHPublicKeysPlugin,\\cloudbaseinit.plugins.windows.extendvolumes.ExtendVolumesPlugin,\\cloudbaseinit.plugins.common.setuserpassword.SetUserPasswordPlugin,\\cloudbaseinit.plugins.common.userdata.UserDataPlugin,\\cloudbaseinit.plugins.windows.winrmlistener.ConfigWinRMListenerPlugin,\\cloudbaseinit.plugins.windows.winrmcertificateauth.\\ConfigWinRMCertificateAuthPlugin,\\cloudbaseinit.plugins.common.localscripts.LocalScriptsPlugin\r\n\n\nMake sure to remove all backslashes in the lines above.\n\nSave the changes.\n\nWhat's next\n\nEnabling logging for virtual machines\n\nCreating templates",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/configuring-windows-boot-volumes.html"
    },
    {
        "title": "Connecting to virtual machines",
        "content": "Connecting to virtual machines\nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.\nTo be able to connect via SSH, the virtual machine must have cloud-init and OpenSSH installed. \n\nTo connect to a virtual machine via the VNC console\nSelect a VM, and then click Console on its right pane. The console will open in a separate browser window. In the console, you can send a key combination to a VM, take a screenshot of the console window, and download the console log (refer to Troubleshooting virtual machines).\nTo connect to a virtual machine via SSH\nSpecify the username and VM IP address in the SSH terminal:# ssh <username>@<VM_IP_address>\r\n\nLinux cloud images have the default login, depending on the operating system, for example, centos or ubuntu. To connect to a Windows VM, enter the username that you specified during Cloudbase-Init installation.\nIf you have deployed a VM without specifying an SSH key, you also need to enter a password to log in to the VM.\nSee also\n\nSetting a password inside virtual machines\n\nManaging guest tools\n\nManaging virtual machine power state\n\nReconfiguring virtual machines\n\nMonitoring virtual machines\n\nMigrating virtual machines\n\nWhat's next\n\nManaging guest tools",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/connecting-to-virtual-machines.html"
    },
    {
        "title": "Creating and deleting security groups",
        "content": "Creating and deleting security groups\nLimitations\n\nYou cannot delete the default security group.\nYou cannot delete a security group if it is assigned to a VM.\n\nTo create a security group\n\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click Add security group.\n\nIn the Add security group window, specify a name and description for the group, and then click Add.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nBy default, the new security group will deny all incoming traffic and allow only outgoing traffic to assigned virtual machines.\n\nCommand-line interface\nUse the following command:vinfra service compute security-group create [--description <description>]\r\n                                             <name>\n\n--description <description>\n\nSecurity group description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n<name>\n\nSecurity group name\n\nFor example, to create a security group mygroup, run:# vinfra service compute security-group create mygroup\r\n+----------------------+---------------------------------------------------+\r\n| Field                | Value                                             |\r\n+----------------------+---------------------------------------------------+\r\n| description          |                                                   |\r\n| id                   | 12e6b260-0b61-4551-8168-3e59602a2433              |\r\n| name                 | mygroup                                           |\r\n| project_id           | e215189c0472482f93e71d10e1245253                  |\r\n| security_group_rules | - description: null                               |\r\n|                      |   direction: egress                               |\r\n|                      |   ethertype: IPv4                                 |\r\n|                      |   id: ce854e2b-537f-4618-bea9-e9ec3d8616ac        |\r\n|                      |   port_range_max: null                            |\r\n|                      |   port_range_min: null                            |\r\n|                      |   project_id: e215189c0472482f93e71d10e1245253    |\r\n|                      |   protocol: null                                  |\r\n|                      |   remote_group_id: null                           |\r\n|                      |   remote_ip_prefix: null                          |\r\n|                      |   security_group_id: 12e6b260-0b61-4551-8168<...> |\r\n|                      | - description: null                               |\r\n|                      |   direction: egress                               |\r\n|                      |   ethertype: IPv6                                 |\r\n|                      |   id: a7c65861-df3d-47f2-bec3-089747141936        |\r\n|                      |   port_range_max: null                            |\r\n|                      |   port_range_min: null                            |\r\n|                      |   project_id: e215189c0472482f93e71d10e1245253    |\r\n|                      |   protocol: null                                  |\r\n|                      |   remote_group_id: null                           |\r\n|                      |   remote_ip_prefix: null                          |\r\n|                      |   security_group_id: 12e6b260-0b61-4551-8168<...> |\r\n| tags                 | []                                                |\r\n+----------------------+---------------------------------------------------+\nThe created security group will appear in the vinfra service compute security-group list output:# vinfra service compute security-group list -c id -c name\r\n+--------------------------------------+---------+\r\n| id                                   | name    |\r\n+--------------------------------------+---------+\r\n| 062f75cf-abc0-419d-bb1a-92989ad9383f | default |\r\n| 12e6b260-0b61-4551-8168-3e59602a2433 | mygroup |\r\n+--------------------------------------+---------+\r\n\n\nTo delete a security group\n\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click the required security group.\nOn the group right pane, click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service compute security-group delete <security-group>\n\n<security-group>\n\nSecurity group name or ID\n\nFor example, to delete the security group mygroup, run:# vinfra service compute security-group delete mygroup\n\nSee also\n\nChanging security group assignment\n\nWhat's next\n\nManaging security group rules",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute security-group create [--description <description>]\r\n                                             <name>\n\n--description <description>\n\n\nSecurity group description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n<name>\n\nSecurity group name\n\nFor example, to create a security group mygroup, run:# vinfra service compute security-group create mygroup\r\n+----------------------+---------------------------------------------------+\r\n| Field                | Value                                             |\r\n+----------------------+---------------------------------------------------+\r\n| description          |                                                   |\r\n| id                   | 12e6b260-0b61-4551-8168-3e59602a2433              |\r\n| name                 | mygroup                                           |\r\n| project_id           | e215189c0472482f93e71d10e1245253                  |\r\n| security_group_rules | - description: null                               |\r\n|                      |   direction: egress                               |\r\n|                      |   ethertype: IPv4                                 |\r\n|                      |   id: ce854e2b-537f-4618-bea9-e9ec3d8616ac        |\r\n|                      |   port_range_max: null                            |\r\n|                      |   port_range_min: null                            |\r\n|                      |   project_id: e215189c0472482f93e71d10e1245253    |\r\n|                      |   protocol: null                                  |\r\n|                      |   remote_group_id: null                           |\r\n|                      |   remote_ip_prefix: null                          |\r\n|                      |   security_group_id: 12e6b260-0b61-4551-8168<...> |\r\n|                      | - description: null                               |\r\n|                      |   direction: egress                               |\r\n|                      |   ethertype: IPv6                                 |\r\n|                      |   id: a7c65861-df3d-47f2-bec3-089747141936        |\r\n|                      |   port_range_max: null                            |\r\n|                      |   port_range_min: null                            |\r\n|                      |   project_id: e215189c0472482f93e71d10e1245253    |\r\n|                      |   protocol: null                                  |\r\n|                      |   remote_group_id: null                           |\r\n|                      |   remote_ip_prefix: null                          |\r\n|                      |   security_group_id: 12e6b260-0b61-4551-8168<...> |\r\n| tags                 | []                                                |\r\n+----------------------+---------------------------------------------------+\nThe created security group will appear in the vinfra service compute security-group list output:# vinfra service compute security-group list -c id -c name\r\n+--------------------------------------+---------+\r\n| id                                   | name    |\r\n+--------------------------------------+---------+\r\n| 062f75cf-abc0-419d-bb1a-92989ad9383f | default |\r\n| 12e6b260-0b61-4551-8168-3e59602a2433 | mygroup |\r\n+--------------------------------------+---------+\r\n\n",
                "title": "To create a security group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute security-group delete <security-group>\n\n<security-group>\n\nSecurity group name or ID\n\nFor example, to delete the security group mygroup, run:# vinfra service compute security-group delete mygroup\n",
                "title": "To delete a security group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click Add security group.\n\nIn the Add security group window, specify a name and description for the group, and then click Add.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n\n\n\n\nBy default, the new security group will deny all incoming traffic and allow only outgoing traffic to assigned virtual machines.\n",
                "title": "To create a security group"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click the required security group.\nOn the group right pane, click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a security group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-and-deleting-security-groups.html"
    },
    {
        "title": "Creating a kickstart file",
        "content": "Creating a kickstart file\nVirtuozzo Hybrid Infrastructure uses the same kickstart file syntax as Red Hat Enterprise Linux.\nThe following sections describe the options and scripts you will need to include in your kickstart file, and provide an example you can start from.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-a-kickstart-file.html"
    },
    {
        "title": "Creating custom load balancer flavors",
        "content": "Creating custom load balancer flavors\nLoad balancer flavors are predefined sets of provider configuration options. They are defined per provider driver and expose the unique capabilities of each provider.\nBy default, there are two load balancer flavors:\n\nACTIVE_STANDBY is used to build highly available load balancers with two instances\nSINGLE is used to build load balancers with single instances\n\nIf needed, administrators can add more flavors and set them as default for Kubernetes clusters. Additionally, self-service users can specify the external load balancer flavor for their Kubernetes applications (refer to \"Creating external load balancers in Kubernetes\" in the Self-Service Guide).\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo create a load balancer flavor\n\nList the load balancer provider capabilities and choose those that will be configured in the flavor. As we use the amphora provider, run:# openstack --insecure loadbalancer provider capability list amphora\r\n+-------------------+-----------------------+---------------------------------------------------+\r\n|type               | name                  | description                                       |\r\n+-------------------+-----------------------+---------------------------------------------------+\r\n| flavor            | loadbalancer_topology | The load balancer topology. One of:               |\r\n|                   |                       | SINGLE - One amphora per load balancer.           |\r\n|                   |                       | ACTIVE_STANDBY - Two amphora per load balancer.   |\r\n| flavor            | compute_flavor        | The compute driver flavor ID.                     |\r\n| availability_zone | compute_zone          | The compute availability zone.                    |\r\n| availability_zone | management_network    | The management network ID for the amphora.        |\r\n| availability_zone | valid_vip_networks    | List of network IDs that are allowed for VIP use. |\r\n|                   |                       | This overrides/replaces the list of allowed       |\r\n|                   |                       | networks configured in `octavia.conf`.            |\r\n+-------------------+-----------------------+---------------------------------------------------+\n\nList the current compute flavors to learn their IDs:# openstack --insecure flavor list\r\n+-----+--------+-------+------+-----------+-------+-----------+\r\n| ID  | Name   |   RAM | Disk | Ephemeral | VCPUs | Is Public |\r\n+-----+--------+-------+------+-----------+-------+-----------+\r\n| 100 | tiny   |   512 |    0 |         0 |     1 | True      |\r\n| 101 | small  |  2048 |    0 |         0 |     1 | True      |\r\n| 102 | medium |  4096 |    0 |         0 |     2 | True      |\r\n| 103 | large  |  8192 |    0 |         0 |     4 | True      |\r\n| 104 | xlarge | 16384 |    0 |         0 |     8 | True      |\r\n+-----+--------+-------+------+-----------+-------+-----------+\r\n\n\nCreate a flavor profile that is responsible for the load balancer topology and associated with a compute flavor. Load balancers use the same compute flavor as virtual machines. For example, to create a flavor profile that will build highly available load balancers with the small flavor, run:# openstack --insecure loadbalancer flavorprofile create --name amphora-ha-small --provider amphora  \\\r\n--flavor-data '{\"loadbalancer_topology\": \"ACTIVE_STANDBY\", \"compute_flavor\": \"101\"}'\r\n+---------------+-----------------------------------------------------------------------+\r\n| Field         | Value                                                                 |\r\n+---------------+-----------------------------------------------------------------------+\r\n| id            | a1c7d342-df30-490d-b3ec-29d36ff1b0ba                                  |\r\n| name          | amphora-ha-small                                                      |\r\n| provider_name | amphora                                                               |\r\n| flavor_data   | {\"loadbalancer_topology\": \"ACTIVE_STANDBY\"}, \"compute_flavor\": \"101\"} |\r\n+---------------+-----------------------------------------------------------------------+\r\n\nThe created flavor profile will appear in the flavor profile list:# openstack --insecure loadbalancer flavorprofile list\r\n+--------------------------------------+--------------------+---------------+\r\n| id                                   | name               | provider_name |\r\n+--------------------------------------+--------------------+---------------+\r\n| a1c7d342-df30-490d-b3ec-29d36ff1b0ba | amphora-ha-small   | amphora       |\r\n| c8fccae7-25da-4493-985e-c5dc8def85ba | amphora-single     | amphora       |\r\n| d20a9a33-60f9-40c4-85ff-8434bb393ef7 | amphora-act-stndby | amphora       |\r\n+--------------------------------------+--------------------+---------------+\r\n\n\nCreate a load balancer flavor using the new flavor profile. For example:# openstack --insecure loadbalancer flavor create --name HA-SMALL --enable --flavorprofile amphora-ha-small\r\n+-------------------+--------------------------------------+\r\n| Field             | Value                                |\r\n+-------------------+--------------------------------------+\r\n| id                | b8b84c9e-7a15-4693-8a06-20ddb1faa75e |\r\n| name              | HA-SMALL                             |\r\n| flavor_profile_id | a1c7d342-df30-490d-b3ec-29d36ff1b0ba |\r\n| enabled           | True                                 |\r\n| description       |                                      |\r\n+-------------------+--------------------------------------+\r\n\n\nThe new load balancer flavor will appear in the openstack loadbalancer flavor list output:# openstack --insecure loadbalancer flavor list\r\n+--------------------------------------+----------------+--------------------------------------+---------+\r\n| id                                   | name           | flavor_profile_id                    | enabled |\r\n+--------------------------------------+----------------+--------------------------------------+---------+\r\n| aab01389-4496-4641-b84a-815f6eedfe37 | ACTIVE_STANDBY | d20a9a33-60f9-40c4-85ff-8434bb393ef7 | True    |\r\n| b8b84c9e-7a15-4693-8a06-20ddb1faa75e | HA-SMALL       | 3dd5a148-b8c5-4105-9489-4d8d1b0c5fc0 | True    |\r\n| d42cae3e-ff87-4155-b6a2-17bfa38111a1 | SINGLE         | c8fccae7-25da-4493-985e-c5dc8def85ba | True    |\r\n+--------------------------------------+----------------+--------------------------------------+---------+\r\n\nNow, you can use the new flavor for creating load balancers through OpenStack command-line interface or via the Kuberenetes deployment file. For more details, refer to the official OpenStack documentation.\nSee also\n\nUpdating Kubernetes clusters\n\nWhat's next\n\nChanging Kubernetes service parameters",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-custom-load-balancer-flavors.html"
    },
    {
        "title": "Creating and deleting volumes",
        "content": "Creating and deleting volumes\nLimitations\n\nA volume is removed along with all of its snapshots.\n\nTo create a volume\n\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click Create volume.\n\nIn the Create volume window, specify a volume name and size in gigabytes, select a storage policy, and then click Create.\n\nCommand-line interface\nUse the following command:vinfra service compute volume create [--description <description>]\r\n                                     [--network-install <network_install>]\r\n                                     [--image <image>]\r\n                                     [--snapshot <snapshot>]\r\n                                     --storage-policy <storage_policy>\r\n                                     --size <size-gb> <volume-name>\r\n\n\n--description <description>\n\nVolume description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--network-install <network_install>\n\nPerform network installation (true or false).\n--image <image>\n\nSource compute image ID or name\n--snapshot <snapshot>\n\nSource compute volume snapshot ID or name\n--storage-policy <storage_policy>\n\nStorage policy ID or name\n--size <size-gb>\n\nVolume size, in gigabytes\n<volume-name>\n\nVolume name\n\nFor example, to create a volume myvolume sized 8 GB and with the default storage policy, run:# vinfra service compute volume create myvolume --storage-policy default --size 8\r\n+--------------------------------+--------------------------------------+\r\n| Field                          | Value                                |\r\n+--------------------------------+--------------------------------------+\r\n| attachments                    | []                                   |\r\n| availability_zone              | nova                                 |\r\n| bootable                       | False                                |\r\n| consistencygroup_id            |                                      |\r\n| created_at                     | 2018-09-12T12:30:12.665916           |\r\n| description                    |                                      |\r\n| encrypted                      | False                                |\r\n| id                             | c9c0e9e7-ce7a-4566-99d5-d7e40f2987ab |\r\n| imageRef                       |                                      |\r\n| migration_status               |                                      |\r\n| multiattach                    | False                                |\r\n| name                           | myvolume                             |\r\n| network_install                | False                                |\r\n| os-vol-host-attr:host          |                                      |\r\n| os-vol-mig-status-attr:migstat |                                      |\r\n| os-vol-mig-status-attr:name_id |                                      |\r\n| project_id                     | 72a5db3a033c403a86756021e601ef34     |\r\n| replication_status             |                                      |\r\n| size                           | 8                                    |\r\n| snapshot_id                    |                                      |\r\n| source_volid                   |                                      |\r\n| status                         | creating                             |\r\n| storage_policy_name            | default                              |\r\n| updated_at                     |                                      |\r\n| user_id                        | 98bf389983c24c07af9677b931783143     |\r\n| volume_image_metadata          |                                      |\r\n+--------------------------------+--------------------------------------+\r\n\nThe new volume will appear in the vinfra service compute volume list output:# vinfra service compute volume list -c id -c name -c size -c status\r\n+--------------------------------------+----------+------+-----------+\r\n| id                                   | name     | size | status    |\r\n+--------------------------------------+----------+------+-----------+\r\n| c9c0e9e7-ce7a-4566-99d5-d7e40f2987ab | myvolume |    8 | available |\r\n+--------------------------------------+----------+------+-----------+\r\n\n\nTo remove a volume\n\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, check the status of the volume you want to remove.\n If the status is \"In use\", click the volume, and then click Force detach.\nIf the status is \"Available\", click the volume, and then click Delete. \n\nCommand-line interface\nUse the following command:vinfra service compute volume delete <volume>\r\n\n\n<volume>\n\nVolume ID or name\n\nFor example, to delete the volume myvolume2, run:# vinfra service compute volume delete myvolume2\r\nOperation successful\r\n\n\nSee also\n\nResizing volumes\n\nChanging the storage policy for volumes\n\nCloning volumes\n\nManaging volume snapshots\n\nWhat's next\n\nAttaching and detaching volumes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute volume create [--description <description>]\r\n                                     [--network-install <network_install>]\r\n                                     [--image <image>]\r\n                                     [--snapshot <snapshot>]\r\n                                     --storage-policy <storage_policy>\r\n                                     --size <size-gb> <volume-name>\r\n\n\n--description <description>\n\n\nVolume description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--network-install <network_install>\n\nPerform network installation (true or false).\n--image <image>\n\nSource compute image ID or name\n--snapshot <snapshot>\n\nSource compute volume snapshot ID or name\n--storage-policy <storage_policy>\n\nStorage policy ID or name\n--size <size-gb>\n\nVolume size, in gigabytes\n<volume-name>\n\nVolume name\n\nFor example, to create a volume myvolume sized 8 GB and with the default storage policy, run:# vinfra service compute volume create myvolume --storage-policy default --size 8\r\n+--------------------------------+--------------------------------------+\r\n| Field                          | Value                                |\r\n+--------------------------------+--------------------------------------+\r\n| attachments                    | []                                   |\r\n| availability_zone              | nova                                 |\r\n| bootable                       | False                                |\r\n| consistencygroup_id            |                                      |\r\n| created_at                     | 2018-09-12T12:30:12.665916           |\r\n| description                    |                                      |\r\n| encrypted                      | False                                |\r\n| id                             | c9c0e9e7-ce7a-4566-99d5-d7e40f2987ab |\r\n| imageRef                       |                                      |\r\n| migration_status               |                                      |\r\n| multiattach                    | False                                |\r\n| name                           | myvolume                             |\r\n| network_install                | False                                |\r\n| os-vol-host-attr:host          |                                      |\r\n| os-vol-mig-status-attr:migstat |                                      |\r\n| os-vol-mig-status-attr:name_id |                                      |\r\n| project_id                     | 72a5db3a033c403a86756021e601ef34     |\r\n| replication_status             |                                      |\r\n| size                           | 8                                    |\r\n| snapshot_id                    |                                      |\r\n| source_volid                   |                                      |\r\n| status                         | creating                             |\r\n| storage_policy_name            | default                              |\r\n| updated_at                     |                                      |\r\n| user_id                        | 98bf389983c24c07af9677b931783143     |\r\n| volume_image_metadata          |                                      |\r\n+--------------------------------+--------------------------------------+\r\n\nThe new volume will appear in the vinfra service compute volume list output:# vinfra service compute volume list -c id -c name -c size -c status\r\n+--------------------------------------+----------+------+-----------+\r\n| id                                   | name     | size | status    |\r\n+--------------------------------------+----------+------+-----------+\r\n| c9c0e9e7-ce7a-4566-99d5-d7e40f2987ab | myvolume |    8 | available |\r\n+--------------------------------------+----------+------+-----------+\r\n\n",
                "title": "To create a volume"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute volume delete <volume>\r\n\n\n<volume>\n\nVolume ID or name\n\nFor example, to delete the volume myvolume2, run:# vinfra service compute volume delete myvolume2\r\nOperation successful\r\n\n",
                "title": "To remove a volume"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOn the Compute > Storage > Volumes tab, click Create volume.\n\n\n\n\n\nIn the Create volume window, specify a volume name and size in gigabytes, select a storage policy, and then click Create.\n\n",
                "title": "To create a volume"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, check the status of the volume you want to remove.\n If the status is \"In use\", click the volume, and then click Force detach.\nIf the status is \"Available\", click the volume, and then click Delete. \n\n",
                "title": "To remove a volume"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-and-deleting-volumes.html"
    },
    {
        "title": "Creating backup storage on the local cluster",
        "content": "Creating backup storage on the local cluster\nLimitations\n\nRedundancy by replication is not supported for backup storage.\n\nPrerequisites\n\nA clear understanding of the concept Storage policies.\nThe storage cluster has at least one disk with the Storage role.\nThe destination storage has enough space for both existing and new backups.\nEnsure that each node to join the backup storage cluster has the TCP port 44445 open for outgoing Internet connections, as well as for incoming connections from Acronis backup software.\n\nTo select the local cluster as the backup destination\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the Backup (ABGW) private and Backup (ABGW) public traffic types are added to the networks you intend to use.\nOpen the Storage services > Backup storage screen, and then click Create backup storage.\nOn the Backup destination step, select Virtuozzo Hybrid Infrastructure cluster.\nOn the Nodes step, select nodes to add to the backup storage cluster, and then click Next.\n\nOn the Storage policy step, select the desired tier, failure domain, and data redundancy mode. Then, click Next.\n\nOn the DNS step, do one of the following:\n\nSelect Register now, and then specify an external DNS name for backup storage (for example, backupstorage.example.com). Backup agents will use this DNS name and the TCP port 44445 to upload backup data.\n\nConfigure your DNS server according to the example suggested in the admin panel.\nEach time you change the network configuration of nodes in the backup storage cluster, adjust the DNS records accordingly.\n\nSelect Register later to add registrations for your backup storage later or configure it as the secondary cluster for geo-replication.\n\nFor complex environments, HAProxy might be used to build a scalable and redundant load balancing platform, which can be easily moved or migrated and is independent from Virtuozzo Hybrid Infrastructure. For more information, refer to https://kb.acronis.com/content/64787.\n\nIf you selected Register now, specify the following information for your Acronis product on the Acronis account step:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\nOn the Summary step, review the configuration, and then click Create.\n\nAfter creating the backup storage, you can increase its storage capacity at any time by adding space to the local storage cluster, as described in Scaling the storage cluster.\n\nCommand-line interface\nUse the following command:vinfra service backup cluster deploy-standalone --nodes <nodes> --name <name> --address <address>\r\n                                                [--location <location>] --username <username>\r\n                                                --account-server <account-server>\r\n                                                --tier {0,1,2,3} --encoding <M>+<N> \r\n                                                --failure-domain {0,1,2,3,4}\r\n                                                --storage-type local [--stdin]\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n--name <name>\n\nBackup registration name.\n--address <address>\n\nBackup registration address.\n--location <location>\n\nBackup registration location.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--account-server <account-server>\n\nURL of the cloud management portal or the hostname/IP address and port of the local management server.\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--storage-type local\n\nSet the storage type to local\n--stdin\n\nUse for setting registration password from stdin.\n\nFor example, to create the backup cluster from three nodes on the local storage, run:# vinfra service backup cluster create --nodes node001,node002,node003 --name registration1 \\\r\n--address backupstorage.example.com --storage-type local --tier 0 --encoding 1+2 \\\r\n--failure-domain 1 --username account@example.com --account-server https://cloud.acronis.com/ --stdin\nThis command also specifies the registration name and address, tier, failure domain, registration account and server.\nYou can view the backup storage details in the vinfra service backup cluster show output:# vinfra service backup cluster show\r\n+-----------------+---------------------------------------------+\r\n| Field           | Value                                       |\r\n+-----------------+---------------------------------------------+\r\n| dc_uid          | 966ac53e-a92c-11ec-be79-fa163ea9f01a        |\r\n| deployment_mode | - standalone                                |\r\n| geo_replication |                                             |\r\n| hosts           | - hostname: node001.vstoragedomain          |\r\n|                 |   id: 24a953ce-b50e-40c2-bf44-0668aafb421d  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node002.vstoragedomain          |\r\n|                 |   id: c1de8940-c38a-d7ae-41b5-bdd35581a906  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node003.vstoragedomain          |\r\n|                 |   id: 2307dc2c-a954-70a2-3673-8a8f832bd46a  |\r\n|                 |   systemd: active                           |\r\n| registrations   | - account_server: https://cloud.acronis.com |\r\n|                 |   address: backupstorage.example.com        |\r\n|                 |   expires: '2025-03-20T15:20:59+00:00'      |\r\n|                 |   id: be526718-d9f8-4f2c-9bd3-04a987f7e4c4  |\r\n|                 |   name: registration1                       |\r\n|                 |   type: ABC                                 |\r\n|                 |   username: account@example.com             |\r\n| status          | deployed                                    |\r\n| storage_params  |                                             |\r\n| storage_type    | local                                       |\r\n| upstreams       | []                                          |\r\n+-----------------+---------------------------------------------+\r\n\n\nWhat's next\n\nAdding backup locations to Acronis Cyber Protect and Acronis Cyber Protect Cloud",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup cluster deploy-standalone --nodes <nodes> --name <name> --address <address>\r\n                                                [--location <location>] --username <username>\r\n                                                --account-server <account-server>\r\n                                                --tier {0,1,2,3} --encoding <M>+<N> \r\n                                                --failure-domain {0,1,2,3,4}\r\n                                                --storage-type local [--stdin]\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n--name <name>\n\nBackup registration name.\n--address <address>\n\nBackup registration address.\n--location <location>\n\nBackup registration location.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--account-server <account-server>\n\nURL of the cloud management portal or the hostname/IP address and port of the local management server.\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--storage-type local\n\nSet the storage type to local\n--stdin\n\nUse for setting registration password from stdin.\n\nFor example, to create the backup cluster from three nodes on the local storage, run:# vinfra service backup cluster create --nodes node001,node002,node003 --name registration1 \\\r\n--address backupstorage.example.com --storage-type local --tier 0 --encoding 1+2 \\\r\n--failure-domain 1 --username account@example.com --account-server https://cloud.acronis.com/ --stdin\nThis command also specifies the registration name and address, tier, failure domain, registration account and server.\nYou can view the backup storage details in the vinfra service backup cluster show output:# vinfra service backup cluster show\r\n+-----------------+---------------------------------------------+\r\n| Field           | Value                                       |\r\n+-----------------+---------------------------------------------+\r\n| dc_uid          | 966ac53e-a92c-11ec-be79-fa163ea9f01a        |\r\n| deployment_mode | - standalone                                |\r\n| geo_replication |                                             |\r\n| hosts           | - hostname: node001.vstoragedomain          |\r\n|                 |   id: 24a953ce-b50e-40c2-bf44-0668aafb421d  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node002.vstoragedomain          |\r\n|                 |   id: c1de8940-c38a-d7ae-41b5-bdd35581a906  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node003.vstoragedomain          |\r\n|                 |   id: 2307dc2c-a954-70a2-3673-8a8f832bd46a  |\r\n|                 |   systemd: active                           |\r\n| registrations   | - account_server: https://cloud.acronis.com |\r\n|                 |   address: backupstorage.example.com        |\r\n|                 |   expires: '2025-03-20T15:20:59+00:00'      |\r\n|                 |   id: be526718-d9f8-4f2c-9bd3-04a987f7e4c4  |\r\n|                 |   name: registration1                       |\r\n|                 |   type: ABC                                 |\r\n|                 |   username: account@example.com             |\r\n| status          | deployed                                    |\r\n| storage_params  |                                             |\r\n| storage_type    | local                                       |\r\n| upstreams       | []                                          |\r\n+-----------------+---------------------------------------------+\r\n\n",
                "title": "To select the local cluster as the backup destination"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the Backup (ABGW) private and Backup (ABGW) public traffic types are added to the networks you intend to use.\nOpen the Storage services > Backup storage screen, and then click Create backup storage.\nOn the Backup destination step, select Virtuozzo Hybrid Infrastructure cluster.\nOn the Nodes step, select nodes to add to the backup storage cluster, and then click Next.\n\nOn the Storage policy step, select the desired tier, failure domain, and data redundancy mode. Then, click Next.\n\n\n\n\n\n\nOn the DNS step, do one of the following:\n\n\nSelect Register now, and then specify an external DNS name for backup storage (for example, backupstorage.example.com). Backup agents will use this DNS name and the TCP port 44445 to upload backup data.\n\n\nConfigure your DNS server according to the example suggested in the admin panel.\nEach time you change the network configuration of nodes in the backup storage cluster, adjust the DNS records accordingly.\n\n\n\n\n\n\n\n\nSelect Register later to add registrations for your backup storage later or configure it as the secondary cluster for geo-replication.\n\n\n\nFor complex environments, HAProxy might be used to build a scalable and redundant load balancing platform, which can be easily moved or migrated and is independent from Virtuozzo Hybrid Infrastructure. For more information, refer to https://kb.acronis.com/content/64787.\n\n\n\nIf you selected Register now, specify the following information for your Acronis product on the Acronis account step:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\n\n\n\n\n\nOn the Summary step, review the configuration, and then click Create.\n\nAfter creating the backup storage, you can increase its storage capacity at any time by adding space to the local storage cluster, as described in Scaling the storage cluster.\n",
                "title": "To select the local cluster as the backup destination"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-backup-storage-on-the-local-cluster.html"
    },
    {
        "title": "Creating domain groups",
        "content": "Creating domain groups\nLimitations\n\nYou can only create domain groups with the role System administrator within the Default domain. For details, refer to Managing admin panel users.\n\nPrerequisites\n\nA clear understanding of user roles described in Multitenancy.\n\nTo create a domain group\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click a domain within which a domain group will be created.\nGo to the Domain groups tab, and then click Create domain group.\n\nIn the Create domain group window, specify the group name and optionally description. The group name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nSelect the user role:\n\nTo create a group of domain administrators\n\nSelect the Domain administrator role.\n\nEnable Image uploading to allow the user to upload images and configure this permission for other domain users.\n\nEnable Project and quota management to allow the user to manage projects and quotas, as well as configure this permission for other domain administrators.\n\nTo create a group of system administrators\n\nSelect the System administrator role.\n\nSelect the permissions to be granted to the user account from the System permission set section:\n\nFull (System administrator): has all permissions and can perform all management operations, including creating projects and managing other users.\nCompute: can create and manage the compute cluster.\nISCSI: can create and manage iSCSI targets, LUNs, and CHAP users.\nS3: can create and manage the S3 cluster.\nABGW: can create and manage the Backup Gateway cluster.\nNFS: can create and manage NFS shares and exports.\nCluster: can create the storage cluster, join nodes to it, and manage (assign and release) disks.\nNetwork: can modify networks and traffic types.\nUpdate: can install updates.\nSSH: can add and remove SSH keys for cluster nodes access.\n\nThe view permission is always enabled.\n\nEnable the full Domain permissions set to allow the user to manage virtual objects in all projects within the Default domain and other users in the self-service panel.\n\nEnable Image uploading to allow the user to upload images.\n\nTo create a group of project administrators\n\nSelect the Project member role.\n\nEnable Image uploading to allow the user to upload images.\n\nClick Manage in the Projects section and select a project to assign the user to. Then, click Save.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra domain group create [--description <description>] [--assign <project> <role>]\r\n                           [--domain-permissions <domain_permissions>]\r\n                           [--system-permissions <system_permissions>]\r\n                           --domain <domain> <name>\n\n--description <description>\n\nGroup description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--assign <project> <role>\n\nAssign a group to a project with one or more permission sets. Specify this option multiple times to assign the group to multiple projects.\n\n<project>: project ID or name\n<role>: group role in the project (project_admin)\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--system-permissions <system_permissions>\n\nA comma-separated list of system permissions. View the list of available system permissions using vinfra domain user list-available-roles | grep system.\n--domain <domain>\n\nDomain name or ID\n<name>\n\nGroup name\n\nExample 1. To create a group of domain administrators called domain_admins within the domain mydomain, run:# vinfra domain group create domain_admins --domain mydomain --domain-permissions domain_admin\nExample 2. To create a group of system administrators called sys_admins within the domain Default, to manage the compute cluster, run:# vinfra domain group create mysysadmin --domain Default --system-permissions compute\nExample 3. To create a group of project members called users for the project myproject within the domain mydomain and grant this user group the permission to upload images, run:# vinfra domain group create myusers --domain mydomain --assign myproject project_admin --domain-permissions image_upload\nThe created groups will appear in the vinfra domain group list output:# vinfra domain group list --domain mydomain\r\n+-------------+---------------+-------------+--------------------+---------------------------+\r\n| id          | name          | description | domain_permissions | assigned_projects         |\r\n+-------------+---------------+-------------+--------------------+---------------------------+\r\n| 1670fbc6<\u00e2\u0080\u00a6> | domain_admins |             | - domain_admin     | []                        |\r\n| d2fb8a2d<\u00e2\u0080\u00a6> | myusers       |             | - image_upload     | - project_id: db49fd71<\u00e2\u0080\u00a6> |\r\n|             |               |             |                    |   role: project_admin     |\r\n+-------------+---------------+-------------+--------------------+---------------------------+\n\nWhat's next\n\nManaging user assignment to domain groups\n\nEditing and deleting domain groups\n\nSee also\n\nAdding  identity providers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain group create [--description <description>] [--assign <project> <role>]\r\n                           [--domain-permissions <domain_permissions>]\r\n                           [--system-permissions <system_permissions>]\r\n                           --domain <domain> <name>\n\n--description <description>\n\n\nGroup description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--assign <project> <role>\n\n\nAssign a group to a project with one or more permission sets. Specify this option multiple times to assign the group to multiple projects.\n\n<project>: project ID or name\n<role>: group role in the project (project_admin)\n\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--system-permissions <system_permissions>\n\nA comma-separated list of system permissions. View the list of available system permissions using vinfra domain user list-available-roles | grep system.\n--domain <domain>\n\nDomain name or ID\n<name>\n\nGroup name\n\nExample 1. To create a group of domain administrators called domain_admins within the domain mydomain, run:# vinfra domain group create domain_admins --domain mydomain --domain-permissions domain_admin\nExample 2. To create a group of system administrators called sys_admins within the domain Default, to manage the compute cluster, run:# vinfra domain group create mysysadmin --domain Default --system-permissions compute\nExample 3. To create a group of project members called users for the project myproject within the domain mydomain and grant this user group the permission to upload images, run:# vinfra domain group create myusers --domain mydomain --assign myproject project_admin --domain-permissions image_upload\nThe created groups will appear in the vinfra domain group list output:# vinfra domain group list --domain mydomain\r\n+-------------+---------------+-------------+--------------------+---------------------------+\r\n| id          | name          | description | domain_permissions | assigned_projects         |\r\n+-------------+---------------+-------------+--------------------+---------------------------+\r\n| 1670fbc6<\u00e2\u0080\u00a6> | domain_admins |             | - domain_admin     | []                        |\r\n| d2fb8a2d<\u00e2\u0080\u00a6> | myusers       |             | - image_upload     | - project_id: db49fd71<\u00e2\u0080\u00a6> |\r\n|             |               |             |                    |   role: project_admin     |\r\n+-------------+---------------+-------------+--------------------+---------------------------+\n",
                "title": "To create a domain group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click a domain within which a domain group will be created.\nGo to the Domain groups tab, and then click Create domain group.\n\nIn the Create domain group window, specify the group name and optionally description. The group name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\nSelect the user role:\n\n\nTo create a group of domain administrators\n\n\nSelect the Domain administrator role.\n\nEnable Image uploading to allow the user to upload images and configure this permission for other domain users.\n\n\nEnable Project and quota management to allow the user to manage projects and quotas, as well as configure this permission for other domain administrators.\n\n\n\n\n\n\n\n\n\n\nTo create a group of system administrators\n\n\nSelect the System administrator role.\n\nSelect the permissions to be granted to the user account from the System permission set section:\n\nFull (System administrator): has all permissions and can perform all management operations, including creating projects and managing other users.\nCompute: can create and manage the compute cluster.\nISCSI: can create and manage iSCSI targets, LUNs, and CHAP users.\nS3: can create and manage the S3 cluster.\nABGW: can create and manage the Backup Gateway cluster.\nNFS: can create and manage NFS shares and exports.\nCluster: can create the storage cluster, join nodes to it, and manage (assign and release) disks.\nNetwork: can modify networks and traffic types.\nUpdate: can install updates.\nSSH: can add and remove SSH keys for cluster nodes access.\n\n\nThe view permission is always enabled.\n\n\n\nEnable the full Domain permissions set to allow the user to manage virtual objects in all projects within the Default domain and other users in the self-service panel.\n\n\nEnable Image uploading to allow the user to upload images.\n\n\n\n\n\n\n\n\n\n\nTo create a group of project administrators\n\n\nSelect the Project member role.\n\nEnable Image uploading to allow the user to upload images.\n\nClick Manage in the Projects section and select a project to assign the user to. Then, click Save.\n\n\n\n\n\n\n\n\n\n\nClick Create.\n\n",
                "title": "To create a domain group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-domain-groups.html"
    },
    {
        "title": "Creating outbound firewall rules",
        "content": "Creating outbound firewall rules\nTo create custom outbound firewall rules\nUse the following command:vinfra cluster network set --add-outbound-allow-list <rules> <network>\n\n--add-outbound-allow-list <rules>\n\nA comma-separated list of allow rules in the format: <address>:<protocol>:<port>:<description>, where:\n\n<address> is a single IP address (10.10.10.10), address range (10.10.10.0-10.10.10.10), or subnet CIDR (10.10.10.0/32)\n<protocol> can be udp, tcp, or any\n<port> is an integer value (22) or a range (20-22)\n<description> usually contains the name of the service that uses the specified port\n\n<network>\n\nNetwork ID or name\n\nThe cases when you need to create an additional rule are the following:\n\nIf you connect a remote iSCSI device to your cluster node, manually add a rule specifying the port number used for connecting this iSCSI device. For example:# vinfra cluster network set Public --add-outbound-allow-list \"0.0.0.0:tcp:3260:Remote iSCSI\"\n\nIf you plan to change the network configuration and IP address assignment of your cluster nodes by using network migration, manually add a rule specifying TCP and UDP ports 60000\u00e2\u0080\u009360100. For example:# vinfra cluster network set Public --add-outbound-allow-list \\\r\n\"0.0.0.0:tcp:60000-60100:Network migration\",\"0.0.0.0:udp:60000-60100:Network migration\"\n\nIf you plan to reassign an exclusive traffic type from one network to another, manually add rules specifying TCP and UDP ports 60000\u00e2\u0080\u009360100 for both networks. For example:# vinfra cluster network set Public --add-outbound-allow-list \\\r\n\"0.0.0.0:tcp:60000-60100:Network migration\",\"0.0.0.0:udp:60000-60100:Network migration\"\r\n# vinfra cluster network set MyNet --add-outbound-allow-list \\\r\n\"0.0.0.0:tcp:60000-60100:Network migration\",\"0.0.0.0:udp:60000-60100:Network migration\"\n\nIf you enable user authentication in an NFS share with Kerberos V5, manually add rules specifying TCP ports 88 and 749, UDP port 88, and the Kerberos server IP address. For example, if the IP address of the Kerberos server is 10.128.168.20, run:# vinfra cluster network set Public --add-outbound-allow-list \\\r\n\"10.128.168.20:tcp:88:Kerberos\",\"10.128.168.20:tcp:749:Kerberos\",\\\r\n\"10.128.168.20:udp:88:Kerberos\"\n\nIf you configure a custom port for a particular service, manually add a rule specifying the used port number. For example: # vinfra cluster network set Public --add-outbound-allow-list \"0.0.0.0:udp:161:Zabbix\"\n\nWhat's next\n\nRemoving outbound firewall rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-outbound-firewall-rules.html"
    },
    {
        "title": "Creating external storage policies",
        "content": "Creating external storage policies\nOnce you connect an external storage to the compute cluster, you can start using it by creating external storage policies and applying them to new volumes.\nLimitations\n\nExternal storage policies cannot be applied to existing compute volumes.\nExternal storage policies can only be edited via the command-line interface.\n\nPrerequisites\n\nAn external storage is attached to the compute cluster, as described in Attaching external iSCSI storage or Attaching external NFS storage.\n\nTo create an external storage policy\nUse the vinfra service compute storage-policy create command specifying the external storage and desired custom parameters in the key-value format.# vinfra service compute storage-policy create <policy_name> --storage <storage_name> --params <key=value>[,<key2=value2>,...]\nFor example, to create the storage policy pure-policy for the external storage pure-storage, run:# vinfra service compute storage-policy create pure-policy --storage pure-storage\nIn the admin panel, the created storage policy will appear with the External type.\nOnce you create as many storage policies for your external storage as required, you can start applying them to new compute volumes.\nSee also\n\nManaging compute volumes\n\nManaging storage policies\n\nUsing volume QoS policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-external-storage-policies.html"
    },
    {
        "title": "Creating network bonds",
        "content": "Creating network bonds\nBonding multiple network interfaces is optional but provides the following benefits:\n\nHigh network availability. If one of the interfaces fails, the traffic will be automatically routed through the working interface(s).\nHigher network performance. For example, two bonded Gigabit interfaces will deliver the throughput of about 1.7 Gbit/s or up to 200 MB/s. For a storage node, the required number of network interfaces to bond may depend on the number of disks. For example, an HDD can deliver data at speeds of up to 1 Gbps.\n\nLimitations\n\nYou cannot bond Ethernet and Infiniband interfaces.\nYou cannot bond network bridges and Infiniband interfaces.\n\nIf you are bonding an Open vSwitch-based bridge used in the compute cluster with another network interface, choose between these two bonding modes:\n\nbalance-tcp, where load balancing is done based on L2-L4 data like the destination MAC address, IP address, and TCP port. This mode requires that Link Aggregation Control Protocol (LACP) be enabled on the physical switch the node is connected to.\nactive-backup, where one network interface is active and all other interfaces are passive. If the active interface fails, a passive one becomes active instead.\n\nTo create a bond\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click Create.\nIn the Create network interface window, select the Bond type and network interfaces to bond, and then click Next.\n\nSelect the bonding mode. For both fault tolerance and good performance, it is recommended to set the balance-xor mode.\n\nSelect a network to assign the bond to, and then specify the network parameters:\n\nSelect Automatically (DHCP) to obtain the IP address, DNS, and routing settings from the DHCP server.\nSelect Automatically (DHCP address only) to obtain only the IP address from the DHCP server.\nSelect Manually, and then specify the IP address in CIDR notation by clicking Add.\n\nDynamic IP address allocation will cause network issues as soon as the IP addresses of cluster nodes change. Configure static IP addresses from the start or as soon as possible.\n\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\n\nEnter the desired MTU value in the MTU field. If you leave Auto, the subordinate interface MTU will be used.\n\nSelect the MAC address in the MAC field. Select Auto to automatically choose between the MAC addresses of the subordinate interfaces or select one of them manually.\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra node iface create-bond [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                              [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                              [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                              [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                              [--bonding-opts <bonding_opts>] [--network <network>]\r\n                              [--node <node>] --bond-type <bond-type> --ifaces <ifaces>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--bonding-opts <bonding_opts>\n\nAdditional bonding options\n--bond-type <bond-type>\n\nBond type (balance-rr, active-backup, balance-xor, broadcast, 802.3ad, balance-tlb, balance-alb)\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n--ifaces <ifaces>\n\nA comma-separated list of network interface names, for example, iface1,iface2,\u00e2\u0080\u00a6,iface<N>\n\nFor example, to bond network interfaces eth2 and eth3 into bond0 of the type balance-xor on the node node002, run:# vinfra node iface create-bond --ifaces eth2,eth3 --bond-type balance-xor --dhcp4 --node node002\n\nSee also\n\nChanging network interface parameters\n\nManaging network interfaces\n\nWhat's next\n\nCreating VLAN interfaces",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface create-bond [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                              [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                              [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                              [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                              [--bonding-opts <bonding_opts>] [--network <network>]\r\n                              [--node <node>] --bond-type <bond-type> --ifaces <ifaces>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--bonding-opts <bonding_opts>\n\nAdditional bonding options\n--bond-type <bond-type>\n\nBond type (balance-rr, active-backup, balance-xor, broadcast, 802.3ad, balance-tlb, balance-alb)\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n--ifaces <ifaces>\n\nA comma-separated list of network interface names, for example, iface1,iface2,\u00e2\u0080\u00a6,iface<N>\n\nFor example, to bond network interfaces eth2 and eth3 into bond0 of the type balance-xor on the node node002, run:# vinfra node iface create-bond --ifaces eth2,eth3 --bond-type balance-xor --dhcp4 --node node002\n",
                "title": "To create a bond"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click Create.\nIn the Create network interface window, select the Bond type and network interfaces to bond, and then click Next.\n\nSelect the bonding mode. For both fault tolerance and good performance, it is recommended to set the balance-xor mode.\n\n\nSelect a network to assign the bond to, and then specify the network parameters:\n\nSelect Automatically (DHCP) to obtain the IP address, DNS, and routing settings from the DHCP server.\nSelect Automatically (DHCP address only) to obtain only the IP address from the DHCP server.\nSelect Manually, and then specify the IP address in CIDR notation by clicking Add.\n\n\nDynamic IP address allocation will cause network issues as soon as the IP addresses of cluster nodes change. Configure static IP addresses from the start or as soon as possible.\n\n\n\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\n\n\nEnter the desired MTU value in the MTU field. If you leave Auto, the subordinate interface MTU will be used.\n\nSelect the MAC address in the MAC field. Select Auto to automatically choose between the MAC addresses of the subordinate interfaces or select one of them manually.\nClick Create.\n\n\n\n\n\n",
                "title": "To create a bond"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-network-bonds.html"
    },
    {
        "title": "Creating backup storage in a public cloud",
        "content": "Creating backup storage in a public cloud\nWith Backup Gateway, you can have Acronis Cyber Protect Cloud or Acronis Cyber Protect store backups in a number of public clouds and on-premises object storage solutions:\n\nAmazon S3\nIBM Cloud\nAlibaba Cloud\nIIJ\nCleversafe\nCloudian\nMicrosoft Azure\nSwift object storage\nSoftlayer (Swift)\nGoogle Cloud Platform\nWasabi\nOther solutions using S3\n\nHowever, compared to the local storage cluster, storing backup data in a public cloud increases the latency of all I/O requests to backups and reduces performance. For this reason, it is recommended to use the local storage cluster as the storage backend.\nBackups are cold data with a specific access pattern: the data is not accessed frequently but is expected to be available immediately when accessed. For this use case, it is cost-efficient to choose storage classes intended for long-term storage with infrequently accessed data. The recommended storage classes include the following:\n\nInfrequent Access for Amazon S3\nCool Blob Storage for Microsoft Azure\nNearline and Coldline storage for Google Cloud Platform\n\nArchive storage classes like Amazon S3 Glacier, Azure Archive Blob, or Google Archive cannot be used for backup because they do not provide instant access to data. High access latency (several hours) makes it technically impossible to browse archives, restore data fast, and create incremental backups. Even though the archive storage is usually very cost-efficient, keep in mind that there are a number of different cost factors. In fact, the total cost of public cloud storage consists of payments for storing data, operations, traffic, data retrieval, early deletion, and so on. For example, an archive storage service can charge six months\u00e2\u0080\u0099 storage payment for just one data recall operation. If the storage data is expected to be accessed more frequently, the added costs increase significantly the total cost of data storage. In order to avoid the low data retrieval rate and to cut expenses, we recommend using Acronis Cyber Protect Cloud for storing backup data.\nLimitations\n\nRedundancy by replication is not supported for backup storage.\nWith external backup destinations, redundancy has to be provided by the external storage. Backup storage does not provide data redundancy or perform data deduplication itself.\n\nPrerequisites\n\nA clear understanding of the concept Storage policies.\nThe storage cluster has at least one disk with the Storage role.\nThe destination storage has enough space for both existing and new backups.\nEnsure that each node to join the backup storage cluster has the TCP port 44445 open for outgoing Internet connections, as well as for incoming connections from Acronis backup software.\n\nTo select a public cloud as the backup destination\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the Backup (ABGW) private and Backup (ABGW) public traffic types are added to the networks you intend to use.\nOpen the Storage services > Backup storage screen, and then click Create backup storage.\nOn the Backup destination step, select Public cloud.\nOn the Nodes step, select nodes to add to the backup storage cluster, and then click Next.\n\nOn the Public cloud step, specify information relevant for your public cloud provider:\n\nSelect a public cloud provider. If your provider is S3 compatible but not in the list, try AuthV2 compatible (S3) or AuthV4 compatible (S3).\nDepending on the provider, specify Region, Authentication (keystone) URL, or Endpoint URL.\nIn the case of Swift object storage, specify the authentication protocol version and attributes required by it.\nSpecify user credentials. In the case of Google Cloud, select a JSON file with keys to upload.\n\nSpecify the folder (bucket, container) to store backups in. The folder must be writeable.\n\nClick Next.\n\nOn the Storage policy step, select the desired tier, failure domain, and data redundancy mode for the local storage. Then, click Next.\n\nOn the DNS step, do one of the following:\n\nSelect Register now, and then specify an external DNS name for backup storage (for example, backupstorage.example.com). Backup agents will use this DNS name and the TCP port 44445 to upload backup data.\n\nConfigure your DNS server according to the example suggested in the admin panel.\nEach time you change the network configuration of nodes in the backup storage cluster, adjust the DNS records accordingly.\n\nSelect Register later to add registrations for your backup storage later or configure it as the secondary cluster for geo-replication.\n\nFor complex environments, HAProxy might be used to build a scalable and redundant load balancing platform, which can be easily moved or migrated and is independent from Virtuozzo Hybrid Infrastructure. For more information, refer to https://kb.acronis.com/content/64787.\n\nIf you selected Register now, specify the following information for your Acronis product on the Acronis account step:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\nOn the Summary step, review the configuration, and then click Create.\n\nAfter creating the backup storage, you can increase its storage capacity at any time by adding space to the public cloud storage.\n\nCommand-line interface\nUse the following command:vinfra service backup cluster deploy-standalone --nodes <nodes> --name <name> --address <address>\r\n                                                [--location <location>] --username <username>\r\n                                                --account-server <account-server>\r\n                                                --tier {0,1,2,3} --encoding <M>+<N> \r\n                                                --failure-domain {0,1,2,3,4}\r\n                                                --storage-type {s3,swift,azure,google}\r\n                                                [--s3-flavor <flavor>]\r\n                                                [--s3-region <region>]\r\n                                                [--s3-bucket <bucket>]\r\n                                                [--s3-endpoint <endpoint>]\r\n                                                [--s3-access-key-id <access-key-id>]\r\n                                                [--s3-secret-key-id <secret-key-id>]\r\n                                                [--s3-cert-verify <cert-verify>]\r\n                                                [--swift-auth-url <auth-url>]\r\n                                                [--swift-auth-version <auth-version>]\r\n                                                [--swift-user-name <user-name>]\r\n                                                [--swift-api-key <api-key>]\r\n                                                [--swift-domain <domain>]\r\n                                                [--swift-domain-id <domain-id>]\r\n                                                [--swift-tenant <tenant>]\r\n                                                [--swift-tenant-id <tenant-id>]\r\n                                                [--swift-tenant-domain <tenant-domain>]\r\n                                                [--swift-tenant-domain-id <tenant-domain-id>]\r\n                                                [--swift-trust-id <trust-id>]\r\n                                                [--swift-region <region>]\r\n                                                [--swift-internal <internal>]\r\n                                                [--swift-container <container>]\r\n                                                [--swift-cert-verify <cert-verify>]\r\n                                                [--azure-endpoint <endpoint>]\r\n                                                [--azure-container <container>]\r\n                                                [--azure-account-name <account-name>]\r\n                                                [--azure-account-key <account-key>]\r\n                                                [--google-bucket <bucket>]\r\n                                                [--google-credentials <credentials>] [--stdin]\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n--name <name>\n\nBackup registration name.\n--address <address>\n\nBackup registration address.\n--location <location>\n\nBackup registration location.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--account-server <account-server>\n\nURL of the cloud management portal or the hostname/IP address and port of the local management server.\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--storage-type {local,nfs,s3,swift,azure,google}\n\nStorage type\n--stdin\n\nUse for setting registration password from stdin.\n\nStorage parameters for the s3 storage type:\n\n--s3-flavor <flavor> (optional)\nFlavor name\n--s3-region <region> (optional)\nSet region for Amazon S3.\n--s3-bucket <bucket>\n\nBucket name\n--s3-endpoint <endpoint>\n\nEndpoint URL\n--s3-access-key-id <access-key-id>\n\nAccess key ID\n--s3-secret-key-id <secret-key-id>\n\nSecret key ID\n--s3-cert-verify <cert-verify> (optional)\nAllow self-signed certificate of the S3 endpoint\n\nStorage parameters for the swift storage type:\n\n--swift-auth-url <auth-url>\n\nAuthentication (keystone) URL\n--swift-auth-version <auth-version> (optional)\nAuthentication protocol version\n--swift-user-name <user-name>\n\nUser name\n--swift-api-key <api-key>\n\nAPI key (password)\n--swift-domain <domain> (optional)\nDomain name\n--swift-domain-id <domain-id> (optional)\nDomain ID\n--swift-tenant <tenant> (optional)\nTenant name\n--swift-tenant-id <tenant-id> (optional)\nTenant ID\n--swift-tenant-domain <tenant-domain> (optional)\nTenant domain name\n--swift-tenant-domain-id <tenant-domain-id> (optional)\nTenant domain ID\n--swift-trust-id <trust-id> (optional)\nTrust ID\n--swift-region <region> (optional)\nRegion name\n--swift-container <container> (optional)\nContainer name\n--swift-cert-verify <cert-verify> (optional)\nAllow self-signed certificate of the Swift endpoint (true or false)\n\nStorage parameters for the azure storage type:\n\n--azure-endpoint <endpoint>\n\nEndpoint URL\n--azure-container <container>\n\nContainer name\n--azure-account-name <account-name>\n\nAccount name\n--azure-account-key <account-key>\n\nAccount key\n\nStorage parameters for the google storage type:\n\n--google-bucket <bucket>\n\nGoogle bucket name\n--google-credentials <credentials>\n\nPath to the file with Google credentials\n\nFor example, to create the backup cluster from three nodes on the S3 storage, run:# vinfra service backup cluster create --nodes node001,node002,node003 --name registration1 \\\r\n--address backupstorage.example.com --storage-type s3 --tier 0 --encoding 1+2 --failure-domain host --s3-bucket mybucket \\\r\n--s3-endpoint s3.amazonaws.com --s3-access-key-id e302a06df8adbe9fAIF1 --s3-secret-key-id x1gXquRH<\u00e2\u0080\u00a6> \\\r\n--s3-cert-verify true --username account@example.com --account-server https://cloud.acronis.com/ --stdin\nThis command also specifies the registration name and address, tier, failure domain, registration account and server, as well as the required S3 parameters.\nYou can view the backup storage details in the vinfra service backup cluster show output:# vinfra service backup cluster show\r\n+-----------------+---------------------------------------------+\r\n| Field           | Value                                       |\r\n+-----------------+---------------------------------------------+\r\n| dc_uid          | 966ac53e-a92c-11ec-be79-fa163ea9f01a        |\r\n| deployment_mode | - standalone                                |\r\n| geo_replication |                                             |\r\n| hosts           | - hostname: node001.vstoragedomain          |\r\n|                 |   id: 24a953ce-b50e-40c2-bf44-0668aafb421d  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node002.vstoragedomain          |\r\n|                 |   id: c1de8940-c38a-d7ae-41b5-bdd35581a906  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node003.vstoragedomain          |\r\n|                 |   id: 2307dc2c-a954-70a2-3673-8a8f832bd46a  |\r\n|                 |   systemd: active                           |\r\n| registrations   | - account_server: https://cloud.acronis.com |\r\n|                 |   address: backupstorage.example.com        |\r\n|                 |   expires: '2025-03-20T15:20:59+00:00'      |\r\n|                 |   id: be526718-d9f8-4f2c-9bd3-04a987f7e4c4  |\r\n|                 |   name: registration1                       |\r\n|                 |   type: ABC                                 |\r\n|                 |   username: account@example.com             |\r\n| status          | deployed                                    |\r\n| storage_params  |                                             |\r\n| storage_type    | local                                       |\r\n| upstreams       | []                                          |\r\n+-----------------+---------------------------------------------+\r\n\n\nWhat's next\n\nAdding backup locations to Acronis Cyber Protect and Acronis Cyber Protect Cloud",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup cluster deploy-standalone --nodes <nodes> --name <name> --address <address>\r\n                                                [--location <location>] --username <username>\r\n                                                --account-server <account-server>\r\n                                                --tier {0,1,2,3} --encoding <M>+<N> \r\n                                                --failure-domain {0,1,2,3,4}\r\n                                                --storage-type {s3,swift,azure,google}\r\n                                                [--s3-flavor <flavor>]\r\n                                                [--s3-region <region>]\r\n                                                [--s3-bucket <bucket>]\r\n                                                [--s3-endpoint <endpoint>]\r\n                                                [--s3-access-key-id <access-key-id>]\r\n                                                [--s3-secret-key-id <secret-key-id>]\r\n                                                [--s3-cert-verify <cert-verify>]\r\n                                                [--swift-auth-url <auth-url>]\r\n                                                [--swift-auth-version <auth-version>]\r\n                                                [--swift-user-name <user-name>]\r\n                                                [--swift-api-key <api-key>]\r\n                                                [--swift-domain <domain>]\r\n                                                [--swift-domain-id <domain-id>]\r\n                                                [--swift-tenant <tenant>]\r\n                                                [--swift-tenant-id <tenant-id>]\r\n                                                [--swift-tenant-domain <tenant-domain>]\r\n                                                [--swift-tenant-domain-id <tenant-domain-id>]\r\n                                                [--swift-trust-id <trust-id>]\r\n                                                [--swift-region <region>]\r\n                                                [--swift-internal <internal>]\r\n                                                [--swift-container <container>]\r\n                                                [--swift-cert-verify <cert-verify>]\r\n                                                [--azure-endpoint <endpoint>]\r\n                                                [--azure-container <container>]\r\n                                                [--azure-account-name <account-name>]\r\n                                                [--azure-account-key <account-key>]\r\n                                                [--google-bucket <bucket>]\r\n                                                [--google-credentials <credentials>] [--stdin]\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n--name <name>\n\nBackup registration name.\n--address <address>\n\nBackup registration address.\n--location <location>\n\nBackup registration location.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--account-server <account-server>\n\nURL of the cloud management portal or the hostname/IP address and port of the local management server.\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--storage-type {local,nfs,s3,swift,azure,google}\n\nStorage type\n--stdin\n\nUse for setting registration password from stdin.\n\nStorage parameters for the s3 storage type:\n\n--s3-flavor <flavor> (optional)\nFlavor name\n--s3-region <region> (optional)\nSet region for Amazon S3.\n--s3-bucket <bucket>\n\nBucket name\n--s3-endpoint <endpoint>\n\nEndpoint URL\n--s3-access-key-id <access-key-id>\n\nAccess key ID\n--s3-secret-key-id <secret-key-id>\n\nSecret key ID\n--s3-cert-verify <cert-verify> (optional)\nAllow self-signed certificate of the S3 endpoint\n\nStorage parameters for the swift storage type:\n\n--swift-auth-url <auth-url>\n\nAuthentication (keystone) URL\n--swift-auth-version <auth-version> (optional)\nAuthentication protocol version\n--swift-user-name <user-name>\n\nUser name\n--swift-api-key <api-key>\n\nAPI key (password)\n--swift-domain <domain> (optional)\nDomain name\n--swift-domain-id <domain-id> (optional)\nDomain ID\n--swift-tenant <tenant> (optional)\nTenant name\n--swift-tenant-id <tenant-id> (optional)\nTenant ID\n--swift-tenant-domain <tenant-domain> (optional)\nTenant domain name\n--swift-tenant-domain-id <tenant-domain-id> (optional)\nTenant domain ID\n--swift-trust-id <trust-id> (optional)\nTrust ID\n--swift-region <region> (optional)\nRegion name\n--swift-container <container> (optional)\nContainer name\n--swift-cert-verify <cert-verify> (optional)\nAllow self-signed certificate of the Swift endpoint (true or false)\n\nStorage parameters for the azure storage type:\n\n--azure-endpoint <endpoint>\n\nEndpoint URL\n--azure-container <container>\n\nContainer name\n--azure-account-name <account-name>\n\nAccount name\n--azure-account-key <account-key>\n\nAccount key\n\nStorage parameters for the google storage type:\n\n--google-bucket <bucket>\n\nGoogle bucket name\n--google-credentials <credentials>\n\nPath to the file with Google credentials\n\nFor example, to create the backup cluster from three nodes on the S3 storage, run:# vinfra service backup cluster create --nodes node001,node002,node003 --name registration1 \\\r\n--address backupstorage.example.com --storage-type s3 --tier 0 --encoding 1+2 --failure-domain host --s3-bucket mybucket \\\r\n--s3-endpoint s3.amazonaws.com --s3-access-key-id e302a06df8adbe9fAIF1 --s3-secret-key-id x1gXquRH<\u00e2\u0080\u00a6> \\\r\n--s3-cert-verify true --username account@example.com --account-server https://cloud.acronis.com/ --stdin\nThis command also specifies the registration name and address, tier, failure domain, registration account and server, as well as the required S3 parameters.\nYou can view the backup storage details in the vinfra service backup cluster show output:# vinfra service backup cluster show\r\n+-----------------+---------------------------------------------+\r\n| Field           | Value                                       |\r\n+-----------------+---------------------------------------------+\r\n| dc_uid          | 966ac53e-a92c-11ec-be79-fa163ea9f01a        |\r\n| deployment_mode | - standalone                                |\r\n| geo_replication |                                             |\r\n| hosts           | - hostname: node001.vstoragedomain          |\r\n|                 |   id: 24a953ce-b50e-40c2-bf44-0668aafb421d  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node002.vstoragedomain          |\r\n|                 |   id: c1de8940-c38a-d7ae-41b5-bdd35581a906  |\r\n|                 |   systemd: active                           |\r\n|                 | - hostname: node003.vstoragedomain          |\r\n|                 |   id: 2307dc2c-a954-70a2-3673-8a8f832bd46a  |\r\n|                 |   systemd: active                           |\r\n| registrations   | - account_server: https://cloud.acronis.com |\r\n|                 |   address: backupstorage.example.com        |\r\n|                 |   expires: '2025-03-20T15:20:59+00:00'      |\r\n|                 |   id: be526718-d9f8-4f2c-9bd3-04a987f7e4c4  |\r\n|                 |   name: registration1                       |\r\n|                 |   type: ABC                                 |\r\n|                 |   username: account@example.com             |\r\n| status          | deployed                                    |\r\n| storage_params  |                                             |\r\n| storage_type    | local                                       |\r\n| upstreams       | []                                          |\r\n+-----------------+---------------------------------------------+\r\n\n",
                "title": "To select a public cloud as the backup destination"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the Backup (ABGW) private and Backup (ABGW) public traffic types are added to the networks you intend to use.\nOpen the Storage services > Backup storage screen, and then click Create backup storage.\nOn the Backup destination step, select Public cloud.\nOn the Nodes step, select nodes to add to the backup storage cluster, and then click Next.\n\nOn the Public cloud step, specify information relevant for your public cloud provider:\n\nSelect a public cloud provider. If your provider is S3 compatible but not in the list, try AuthV2 compatible (S3) or AuthV4 compatible (S3).\nDepending on the provider, specify Region, Authentication (keystone) URL, or Endpoint URL.\nIn the case of Swift object storage, specify the authentication protocol version and attributes required by it.\nSpecify user credentials. In the case of Google Cloud, select a JSON file with keys to upload.\n\nSpecify the folder (bucket, container) to store backups in. The folder must be writeable.\n\nClick Next.\n\n\n\n\n\n\n\nOn the Storage policy step, select the desired tier, failure domain, and data redundancy mode for the local storage. Then, click Next.\n\n\n\n\n\n\nOn the DNS step, do one of the following:\n\n\nSelect Register now, and then specify an external DNS name for backup storage (for example, backupstorage.example.com). Backup agents will use this DNS name and the TCP port 44445 to upload backup data.\n\n\nConfigure your DNS server according to the example suggested in the admin panel.\nEach time you change the network configuration of nodes in the backup storage cluster, adjust the DNS records accordingly.\n\n\n\n\n\n\n\n\nSelect Register later to add registrations for your backup storage later or configure it as the secondary cluster for geo-replication.\n\n\n\nFor complex environments, HAProxy might be used to build a scalable and redundant load balancing platform, which can be easily moved or migrated and is independent from Virtuozzo Hybrid Infrastructure. For more information, refer to https://kb.acronis.com/content/64787.\n\n\n\nIf you selected Register now, specify the following information for your Acronis product on the Acronis account step:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\n\n\n\n\n\nOn the Summary step, review the configuration, and then click Create.\n\nAfter creating the backup storage, you can increase its storage capacity at any time by adding space to the public cloud storage.\n",
                "title": "To select a public cloud as the backup destination"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-backup-storage-in-a-public-cloud.html"
    },
    {
        "title": "Creating NFS exports",
        "content": "Creating NFS exports\nTo create NFS exports, follow these steps:\n\nCreate the root export.\n\nMount the root export, as described in the Storage User Guide.\n\nDo not mount NFS shares on cluster nodes. It may lead to node freeze.\n\nCreate user exports in the mounted root export.\nMount these user exports the same way you have mounted the root export.\n\nPrerequisites\n\nNFS shares are created, as described in Creating NFS shares.\n\nTo create the root export\n\nAdmin panel\n\nOn the Storage services > NFS > Shares screen, click the name of the desired share. This will open the share screen.\nOn the share screen, click Add export.\n\n In the Add export window, specify root as the export name and / as its path, and then select the Read and write access mode. Then, click Add.\n\nDo not use other names or paths for the root export.\n\nThis will create a directory with a default path that designates the export location inside the share. This path is automatically generated based on the share name and used (alongside the share\u00e2\u0080\u0099s IP address) to mount the export.\n\nDo not give the users access to the root export.\n\nThe root export will be shown in the export list.\n\nCommand-line interface\nUse the following command:vinfra service nfs export create --path <path> --access-type <access-type> --security-types <security-types>\r\n                                 <share-name> <export-name>\n\n--path <path>\n\nPath to the NFS export\n--access-type <access-type>\n\nType of access to the NFS export (none, rw, or ro)\n--security-types <security-types>\n\nTypes of NFS export security (none, sys, krb5, krb5i, or krb5p)\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to create the root export for the share share1, run:# vinfra service nfs export create share1 root --path / --access-type rw --security-types none\nThe created root export will appear in the vinfra service nfs export list output:# vinfra service nfs export list --share-name share1\r\n+---------+-----------------+-------------+\r\n| name    | path            | access_type |\r\n+---------+-----------------+-------------+\r\n| root    | /share1         | rw          |\r\n+---------+-----------------+-------------+\n\nTo create user exports\n\nAdmin panel\n\nIn the mounted root export, create a subdirectory for a user export, for example, export1.\nOn the share screen, click Add export.\n\nIn the Add export window, specify the following:\n\nEnter a user export name and specify /export1 as a path.\nSelect the access mode between Read and write and Read.\n\nIn the Advanced settings section, select the desired root squashing option:\n\nIf you select Disallow access with root UID, you map root users to the anonymous user and group.\nIf you select Disallow access with root UID but allow access to user groups with non-zero GIDs, you map root users to the anonymous user, but allow non-zero groups of root users.\nIf you select Squash all users to anonymous, you map all users to the anonymous user.\nIf you select Allow access with root UID, you allow root users keep administrative access privileges to remote files.\n\nChange the anonymous user and group identifiers. Their default value is -2, which maps to the \"nobody\" user.\n\nClick Add.\nThe user export will appear in the export list.\n\nCommand-line interface\nUse the following command:vinfra service nfs export create --path <path> --access-type <access-type> --security-types <security-types>\r\n                                 [--client <address=ip_addresses:access=access_type:security=security_types>]\r\n                                 [--squash <squash>] [--anonymous-gid <anonymous-gid>] [--anonymous-uid <anonymous-uid>]\r\n                                 <share-name> <export-name>\n\n--path <path>\n\nPath to the NFS export\n--access-type <access-type>\n\nType of access to the NFS export (none, rw, or ro)\n--security-types <security-types>\n\nTypes of NFS export security (none, sys, krb5, krb5i, or krb5p)\n--client <address=ip_addresses:access=access_type:security=security_types>\n\nClient access list of the NFS export\n--squash <squash>\n\nNFS export squash (root_squash, root_id_squash, all_squash, or none)\n--anonymous-gid <anonymous-gid>\n\nAnonymous GID of the NFS export\n--anonymous-uid <anonymous-uid>\n\nAnonymous UID of the NFS export\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to create the user export export1 at /export1, run:# vinfra service nfs export create share1 export1 --path /export1 --access-type rw --security-types none\nThe created NFS exports will appear in the vinfra service nfs export list output:# vinfra service nfs export list --share-name share1\r\n+---------+-----------------+-------------+\r\n| name    | path            | access_type |\r\n+---------+-----------------+-------------+\r\n| export1 | /share1/export1 | rw          |\r\n| root    | /share1         | rw          |\r\n+---------+-----------------+-------------+\n\nWhat's next\n\nManaging file storage\n\nMonitoring file storage",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs export create --path <path> --access-type <access-type> --security-types <security-types>\r\n                                 <share-name> <export-name>\n\n--path <path>\n\nPath to the NFS export\n--access-type <access-type>\n\nType of access to the NFS export (none, rw, or ro)\n--security-types <security-types>\n\nTypes of NFS export security (none, sys, krb5, krb5i, or krb5p)\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to create the root export for the share share1, run:# vinfra service nfs export create share1 root --path / --access-type rw --security-types none\nThe created root export will appear in the vinfra service nfs export list output:# vinfra service nfs export list --share-name share1\r\n+---------+-----------------+-------------+\r\n| name    | path            | access_type |\r\n+---------+-----------------+-------------+\r\n| root    | /share1         | rw          |\r\n+---------+-----------------+-------------+\n",
                "title": "To create the root export"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs export create --path <path> --access-type <access-type> --security-types <security-types>\r\n                                 [--client <address=ip_addresses:access=access_type:security=security_types>]\r\n                                 [--squash <squash>] [--anonymous-gid <anonymous-gid>] [--anonymous-uid <anonymous-uid>]\r\n                                 <share-name> <export-name>\n\n--path <path>\n\nPath to the NFS export\n--access-type <access-type>\n\nType of access to the NFS export (none, rw, or ro)\n--security-types <security-types>\n\nTypes of NFS export security (none, sys, krb5, krb5i, or krb5p)\n--client <address=ip_addresses:access=access_type:security=security_types>\n\nClient access list of the NFS export\n--squash <squash>\n\nNFS export squash (root_squash, root_id_squash, all_squash, or none)\n--anonymous-gid <anonymous-gid>\n\nAnonymous GID of the NFS export\n--anonymous-uid <anonymous-uid>\n\nAnonymous UID of the NFS export\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to create the user export export1 at /export1, run:# vinfra service nfs export create share1 export1 --path /export1 --access-type rw --security-types none\nThe created NFS exports will appear in the vinfra service nfs export list output:# vinfra service nfs export list --share-name share1\r\n+---------+-----------------+-------------+\r\n| name    | path            | access_type |\r\n+---------+-----------------+-------------+\r\n| export1 | /share1/export1 | rw          |\r\n| root    | /share1         | rw          |\r\n+---------+-----------------+-------------+\n",
                "title": "To create user exports"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Storage services > NFS > Shares screen, click the name of the desired share. This will open the share screen.\nOn the share screen, click Add export.\n\n In the Add export window, specify root as the export name and / as its path, and then select the Read and write access mode. Then, click Add.\n\nDo not use other names or paths for the root export.\n\n\n\n\n\nThis will create a directory with a default path that designates the export location inside the share. This path is automatically generated based on the share name and used (alongside the share\u00e2\u0080\u0099s IP address) to mount the export.\n\nDo not give the users access to the root export.\n\nThe root export will be shown in the export list.\n\n\n",
                "title": "To create the root export"
            },
            {
                "example": "\nAdmin panel\n\nIn the mounted root export, create a subdirectory for a user export, for example, export1.\nOn the share screen, click Add export.\n\nIn the Add export window, specify the following:\n\nEnter a user export name and specify /export1 as a path.\nSelect the access mode between Read and write and Read.\n\nIn the Advanced settings section, select the desired root squashing option:\n\nIf you select Disallow access with root UID, you map root users to the anonymous user and group.\nIf you select Disallow access with root UID but allow access to user groups with non-zero GIDs, you map root users to the anonymous user, but allow non-zero groups of root users.\nIf you select Squash all users to anonymous, you map all users to the anonymous user.\nIf you select Allow access with root UID, you allow root users keep administrative access privileges to remote files.\n\n\n\nChange the anonymous user and group identifiers. Their default value is -2, which maps to the \"nobody\" user.\n\n\n\n\n\n\n\nClick Add.\nThe user export will appear in the export list.\n\n",
                "title": "To create user exports"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-nfs-exports.html"
    },
    {
        "title": "Creating QoS policies",
        "content": "Creating QoS policies\nPrerequisites\n\nA clear understanding of QoS policy rules, which are explained in QoS policy rules.\nTo create a QoS policy as a system or domain administrator, ensure that the environment file for this user is created, as described in Connecting to OpenStack command-line interface.\n\nTo create a QoS policy with rules\n\nCreate a QoS policy:\n\n If you are creating a policy as a system administrator\n\nUse the environment file for the system administrator:# source /etc/kolla/admin-openrc.sh\r\n\n\nCreate a QoS policy within a project it will be applied to. For example:# openstack --insecure network qos policy create --project 3823a2d908ea4dd6909a8f93a6f66018 policy1\n\nIf you are creating a policy as a domain administrator\n\nUse the environment file for the domain administrator. For example:# source domain-admin.sh\n\nUse the project name variable for the project where you want to create a QoS policy. For example:# export OS_PROJECT_NAME=testproject\n\nCreate a QoS policy. For example:# openstack --insecure network qos policy create policy1\n\nCreate a rule for the QoS policy:\n\n To create a bandwidth limit, specify bandwidth-limit for the --type option and specify rule parameters. For example, to limit the egress traffic to 3 Mbps, run:# openstack --insecure network qos rule create --type bandwidth-limit \\\r\n--max-kbps 3000 --max-burst-kbits 2400 --egress policy1\r\n+----------------+--------------------------------------------------+\r\n| Field          | Value                                            |\r\n+----------------+--------------------------------------------------+\r\n| direction      | egress                                           |\r\n| id             | 6f036f09-d952-420d-986b-27c7eb14b2da             |\r\n| location       | Munch({'project': Munch({'domain_name': Default, |\r\n|                | 'domain_id': None, 'name': admin,                |\r\n|                | 'id': u'e215189c0472482f93e71d10e1245253'}),     |\r\n|                | 'cloud': '', 'region_name': '', 'zone': None})   |\r\n| max_burst_kbps | 2400                                             |\r\n| max_kbps       | 3000                                             |\r\n| name           | None                                             |\r\n| project_id     |                                                  |\r\n+----------------+--------------------------------------------------+\n\nTo create a minimum bandwidth guarantee, specify minimum-bandwidth for the --type option and specify rule parameters. For example, to guarantee the minimum of 100 Kbps to the ingress traffic, run:# openstack --insecure network qos rule create --type minimum-bandwidth \\\r\n--min-kbps 1000 --ingress policy1\r\n+------------+--------------------------------------------------+\r\n| Field      | Value                                            |\r\n+------------+--------------------------------------------------+\r\n| direction  | ingress                                          |\r\n| id         | 4eb79c67-e2b7-4ee7-845c-4cbe39f095cd             |\r\n| location   | Munch({'project': Munch({'domain_name': Default, |\r\n|            | 'domain_id': None, 'name': admin,                |\r\n|            | 'id': u'e215189c0472482f93e71d10e1245253'}),     |\r\n|            | 'cloud': '', 'region_name': '', 'zone': None})   |\r\n| min_kbps   | 1000                                             |\r\n| name       | None                                             |\r\n| project_id |                                                  |\r\n+------------+--------------------------------------------------+\n\nSee also\n\nModifying QoS policy rules\n\nWhat's next\n\nSetting the default QoS policy\n\nAssigning QoS policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-qos-policies.html"
    },
    {
        "title": "Creating target groups",
        "content": "Creating target groups\nPrerequisites\n\nThe storage cluster has at least one disk with the Storage role.\n\nTo create a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, and then click Create target group. The Create target group wizard will open.\n\nOn Name, enter a name for the iSCSI target group.\n\nOn Nodes, select nodes to add to the target group. On these nodes, iSCSI targets will run. You can only choose nodes with network interfaces that are assigned the iSCSI traffic type. It is recommended to have at least two nodes in the target group to achieve high availability. If you plan to use multiple iSCSI initiators, you should have as many nodes in the target group. The optimal way is to create a single target per node.\nIf the node network interfaces are not configured, click the cogwheel icon, select the networks as required, and then click Apply.\n\nOn Targets, select iSCSI interfaces to add to the target group. You can choose from a list of network interfaces that are assigned the iSCSI traffic type. If you plan to use multiple iSCSI initiators, you should select as many interfaces per node. One interface can be added to multiple target groups, although it may reduce performance.\n\nOn Volumes, select volumes to attach to target group LUNs. You can select from a list of volumes that are not attached to any target groups. If no volumes are available, you can create them on this step so they are attached to the target group automatically, or attach them manually later.\n\nOn Access control, configure access to the target group. It is recommended to use CHAP or ACL in untrusted public networks. Without access control, any connections to the target group are allowed.\n\nOn Summary, review the target group details. You can go back to change them if necessary. Click Create.\n\nThe created target group will appear on the Target groups tab. Its targets will start automatically.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group create --type <type> --target <name:node:ip1,ip2...> <name>\n\n--type <type>\n\nType of targets in the target group: iscsi or fc.\n--target <name:node:ip1,ip2...>\n\nTarget name, node ID or hostname, and the IP addresses of the node network interfaces assigned the iSCSI traffic type\n<name>\n\nTarget group name\n\nFor example, to create the target group tg1 with two iSCSI targets target1 and target2 that will run on the node node001 with the IP address 10.10.10.11 and on the node node002 with the IP address 10.10.10.12, run:# vinfra service block-storage target-group create tg1 --type iscsi --target target1:node001:10.10.10.11 \\\r\n--target target2:node002:10.10.10.12\nThe created target group will appear in the vinfra service block-storage target-group list output:# vinfra service block-storage target-group list\r\n+--------------------------------------+------+-------+-------+---------+\r\n| id                                   | name | type  | state | running |\r\n+--------------------------------------+------+-------+-------+---------+\r\n| 1b661da7-cb4d-4b81-9f09-f8ad54fa631e | tg1  | iscsi | ok    | False   |\r\n+--------------------------------------+------+-------+-------+---------+\r\n\n\nSee also\n\nManaging block storage\n\nMonitoring block storage\n\nWhat's next\n\nCreating volumes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group create --type <type> --target <name:node:ip1,ip2...> <name>\n\n--type <type>\n\nType of targets in the target group: iscsi or fc.\n--target <name:node:ip1,ip2...>\n\nTarget name, node ID or hostname, and the IP addresses of the node network interfaces assigned the iSCSI traffic type\n<name>\n\nTarget group name\n\nFor example, to create the target group tg1 with two iSCSI targets target1 and target2 that will run on the node node001 with the IP address 10.10.10.11 and on the node node002 with the IP address 10.10.10.12, run:# vinfra service block-storage target-group create tg1 --type iscsi --target target1:node001:10.10.10.11 \\\r\n--target target2:node002:10.10.10.12\nThe created target group will appear in the vinfra service block-storage target-group list output:# vinfra service block-storage target-group list\r\n+--------------------------------------+------+-------+-------+---------+\r\n| id                                   | name | type  | state | running |\r\n+--------------------------------------+------+-------+-------+---------+\r\n| 1b661da7-cb4d-4b81-9f09-f8ad54fa631e | tg1  | iscsi | ok    | False   |\r\n+--------------------------------------+------+-------+-------+---------+\r\n\n",
                "title": "To create a target group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, and then click Create target group. The Create target group wizard will open.\n\nOn Name, enter a name for the iSCSI target group.\n\n\n\n\n\n\nOn Nodes, select nodes to add to the target group. On these nodes, iSCSI targets will run. You can only choose nodes with network interfaces that are assigned the iSCSI traffic type. It is recommended to have at least two nodes in the target group to achieve high availability. If you plan to use multiple iSCSI initiators, you should have as many nodes in the target group. The optimal way is to create a single target per node.\nIf the node network interfaces are not configured, click the cogwheel icon, select the networks as required, and then click Apply.\n\n\n\n\n\n\nOn Targets, select iSCSI interfaces to add to the target group. You can choose from a list of network interfaces that are assigned the iSCSI traffic type. If you plan to use multiple iSCSI initiators, you should select as many interfaces per node. One interface can be added to multiple target groups, although it may reduce performance.\n\n\n\n\n\n\nOn Volumes, select volumes to attach to target group LUNs. You can select from a list of volumes that are not attached to any target groups. If no volumes are available, you can create them on this step so they are attached to the target group automatically, or attach them manually later.\n\n\n\n\n\n\nOn Access control, configure access to the target group. It is recommended to use CHAP or ACL in untrusted public networks. Without access control, any connections to the target group are allowed.\n\n\n\n\n\n\nOn Summary, review the target group details. You can go back to change them if necessary. Click Create.\n\n\nThe created target group will appear on the Target groups tab. Its targets will start automatically.\n",
                "title": "To create a target group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-target-groups.html"
    },
    {
        "title": "Creating placements",
        "content": "Creating placements\nThough you can create and configure placements only in the admin panel, they are also applied in the  self-service panel. Self-service users can use placements by creating VMs from images and flavors with assigned placements. After uploading an image in the self-service panel, a user cannot assign any placements to it. A VM created from such an image can only be placed on nodes that have placements in the soft mode or on nodes without any placements. When creating placements, ensure that you have either placements in the soft mode or some unassigned nodes. Otherwise self-service users will not be able to create VMs from their custom images.\nLimitations\n\nAfter adding a node to a placement, VMs already hosted on the node will not be automatically assigned this placement.\nA virtual machine that is assigned a placement can only be migrated between nodes in this placement. When adding nodes to placements, make sure to provide migration options for various scenarios, including high availability and maintenance. Avoid situations when VMs cannot migrate because of limitations imposed by placements. In this case, a VM placement can be edited, as described in Managing virtual machines in placements.\nIf you create a placement after creating a project, the placement is not automatically enabled in the project quotas.\n\nPrerequisites\n\nA clear understanding of the placement modes, which are explained in Placement modes.\n\nTo create a placement\n\nAdmin panel\n\nOpen the Compute > Nodes > Placements tab, and then click Create placement.\n\nSelect the placement mode:\n\nIn the Soft mode, a VM can be placed on a node that is assigned at least the same placements as the VM. This mode allows placing a VM without assigned placements on any node.\nIn the Hard mode, a VM can be placed on a node that is assigned exactly the same placements as the VM.\n\nSpecify a name for the new placement and, optionally, a description. The name should clearly state the distinctive feature of nodes in the placement. For example, Microsoft Windows Server license.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nIn the Nodes section, click Add, and then select the nodes to assign the placement to. The same node can be added to several placements.\n\nIn the Images and Flavors sections, click Add, and then select images and flavors to assign the placement to. VMs created from such images or flavors will automatically be assigned this placement.\n\nClick Create.\n\nThe new placement will appear in the list. To allow self-service users to create virtual machines from the images that are assigned this placement, include the placement into your project quotas.\n\nCommand-line interface\nUse the following command:vinfra service compute placement create [--isolated | --non-isolated] [--description <description>]\r\n                                        [--nodes <nodes>] [--images <images>] [--flavors <flavors>]\r\n                                        <placement-name>\r\n\n\n--isolated\n\nCreate an isolated placement (hard policy, default)\n--non-isolated\n\nCreate a non-isolated placement (soft policy)\n--description <description>\n\nPlacement description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--nodes <nodes>\n\nA comma-separated list of compute node IDs or hostnames to assign to a compute placement\n--images <images>\n\nA comma-separated list of image IDs or names to assign to a compute placement\n--flavors <flavors>\n\nA comma-separated list of flavor IDs or names to assign to a compute placement\n<placement-name>\n\nPlacement name\n\nFor example, to create a placement called placement1 with the hard policy and assign it to the nodes node001, node002, node003, as well as to the flavor with the ID 101, run:# vinfra service compute placement create placement1 --nodes node001,node002,node003 --flavors 101\r\n+-------------+--------------------------------------+\r\n| Field       | Value                                |\r\n+-------------+--------------------------------------+\r\n| description |                                      |\r\n| flavors     | 1                                    |\r\n| id          | e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| images      | 0                                    |\r\n| name        | placement1                           |\r\n| nodes       | 3                                    |\r\n| servers     | 0                                    |\r\n+-------------+--------------------------------------+\r\n\nThe new placement will appear in the vinfra service compute placement list output:# vinfra service compute placement list -c id -c name -c nodes -c images -c flavors -c isolated\r\n+---------------------+------------+-------------+-------+--------+---------+---------+----------+\r\n| id                  | name       | description | nodes | images | servers | flavors | isolated |\r\n+---------------------+------------+-------------+-------+--------+---------+---------+----------+\r\n| e4230b75-a858-<...> | placement1 |             | 3     | 0      | 0       | 1       | True     |\r\n+---------------------+------------+-------------+-------+--------+---------+---------+----------+\r\n\n\nSee also\n\nManaging virtual machines in placements\n\nChanging placement assignment\n\nEditing and deleting placements",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute placement create [--isolated | --non-isolated] [--description <description>]\r\n                                        [--nodes <nodes>] [--images <images>] [--flavors <flavors>]\r\n                                        <placement-name>\r\n\n\n--isolated\n\nCreate an isolated placement (hard policy, default)\n--non-isolated\n\nCreate a non-isolated placement (soft policy)\n--description <description>\n\n\nPlacement description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--nodes <nodes>\n\nA comma-separated list of compute node IDs or hostnames to assign to a compute placement\n--images <images>\n\nA comma-separated list of image IDs or names to assign to a compute placement\n--flavors <flavors>\n\nA comma-separated list of flavor IDs or names to assign to a compute placement\n<placement-name>\n\nPlacement name\n\nFor example, to create a placement called placement1 with the hard policy and assign it to the nodes node001, node002, node003, as well as to the flavor with the ID 101, run:# vinfra service compute placement create placement1 --nodes node001,node002,node003 --flavors 101\r\n+-------------+--------------------------------------+\r\n| Field       | Value                                |\r\n+-------------+--------------------------------------+\r\n| description |                                      |\r\n| flavors     | 1                                    |\r\n| id          | e4230b75-a858-404c-be3b-4b3f2dedb057 |\r\n| images      | 0                                    |\r\n| name        | placement1                           |\r\n| nodes       | 3                                    |\r\n| servers     | 0                                    |\r\n+-------------+--------------------------------------+\r\n\nThe new placement will appear in the vinfra service compute placement list output:# vinfra service compute placement list -c id -c name -c nodes -c images -c flavors -c isolated\r\n+---------------------+------------+-------------+-------+--------+---------+---------+----------+\r\n| id                  | name       | description | nodes | images | servers | flavors | isolated |\r\n+---------------------+------------+-------------+-------+--------+---------+---------+----------+\r\n| e4230b75-a858-<...> | placement1 |             | 3     | 0      | 0       | 1       | True     |\r\n+---------------------+------------+-------------+-------+--------+---------+---------+----------+\r\n\n",
                "title": "To create a placement"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen the Compute > Nodes > Placements tab, and then click Create placement.\n\nSelect the placement mode:\n\nIn the Soft mode, a VM can be placed on a node that is assigned at least the same placements as the VM. This mode allows placing a VM without assigned placements on any node.\nIn the Hard mode, a VM can be placed on a node that is assigned exactly the same placements as the VM.\n\n\n\nSpecify a name for the new placement and, optionally, a description. The name should clearly state the distinctive feature of nodes in the placement. For example, Microsoft Windows Server license.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\nIn the Nodes section, click Add, and then select the nodes to assign the placement to. The same node can be added to several placements.\n\nIn the Images and Flavors sections, click Add, and then select images and flavors to assign the placement to. VMs created from such images or flavors will automatically be assigned this placement.\n\nClick Create.\n\nThe new placement will appear in the list. To allow self-service users to create virtual machines from the images that are assigned this placement, include the placement into your project quotas.\n",
                "title": "To create a placement"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-placements.html"
    },
    {
        "title": "Creating templates",
        "content": "Creating templates\nPrerequisites\n\nLinux virtual machines have cloud-init installed, as described in Obtaining Linux templates.\nWindows virtual machines have Cloudbase-Init and OpenSSH Server installed, as described in Configuring Windows boot volumes.\n\nLogging is enabled inside a virtual machine, as instructed in Enabling logging for virtual machines.\n\nTo create a template from a boot volume\n\nAdmin panel\n\nPower off the VM that the original volume is attached to.\nSwitch to the Compute > Storage > Volumes tab, click volume\u00e2\u0080\u0099s ellipsis button and select Create image.\n\nIn the Create image window, enter an image name, and then click Create.\n\nThe new image will appear on the Images screen.\n\nCommand-line interface\n\nShut down the VM which boot volume you want to use. For example:# vinfra service compute server stop myvm\n\nFind out the boot volume's ID. For example:# vinfra service compute server show myvm\r\n+---------------+--------------------------------------------+\r\n| Field         | Value                                      |\r\n+---------------+--------------------------------------------+\r\n| config_drive  |                                            |\r\n| created       | 2021-06-10T08:55:53Z                       |\r\n| description   |                                            |\r\n| fault         |                                            |\r\n| flavor        | disk: 0                                    |\r\n|               | ephemeral: 0                               |\r\n|               | extra_specs: {}                            |\r\n|               | original_name: tiny                        |\r\n|               | ram: 512                                   |\r\n|               | swap: 0                                    |\r\n|               | vcpus: 1                                   |\r\n| ha_enabled    | True                                       |\r\n| host          | amigai-ac-ve0.vstoragedomain               |\r\n| host_status   | UP                                         |\r\n| id            | 6d0fc132-7ea7-41f0-81ca-a4a2b2a2c893       |\r\n| key_name      |                                            |\r\n| metadata      | {}                                         |\r\n| name          | myvm                                       |\r\n| networks      | - id: bd17c207-5291-4096-be6a-0a8a4bf67792 |\r\n|               |   ipam_enabled: true                       |\r\n|               |   ips:                                     |\r\n|               |   - 192.168.128.100                        |\r\n|               |   mac_addr: fa:16:3e:6b:6c:83              |\r\n|               |   name: private                            |\r\n|               |   spoofing_protection: true                |\r\n| orig_hostname | amigai-ac-ve0                              |\r\n| placements    | []                                         |\r\n| power_state   | SHUTDOWN                                   |\r\n| project_id    | dfd99654b8c94b939b638f94abb2ad73           |\r\n| status        | SHUTOFF                                    |\r\n| task_state    |                                            |\r\n| updated       | 2021-06-15T11:24:05Z                       |\r\n| user_data     |                                            |\r\n| vm_state      | stopped                                    |\r\n| volumes       | - delete_on_termination: false             |\r\n|               |   id: 49be1057-c026-494f-b85d-e013728d41bd |\r\n|               | - delete_on_termination: false             |\r\n|               |   id: eca9f679-7e35-4768-ad20-9bcb6af6fd59 |\r\n+---------------+--------------------------------------------+\r\n\nThe first volume in the output is the boot one.\n\nUpload the boot volume to an image specifying the image name. For example:# vinfra service compute volume upload-to-image 49be1057-c026-494f-b85d-e013728d41bd \\\r\n--name image_from_volume\n\nThe new image will appear in the vinfra service compute image list output:# vinfra service compute image list\r\n+--------------------------------------+-------------------+-----------+--------+-------------+\r\n| id                                   | name              | size      | status | disk_format |\r\n+--------------------------------------+-------------------+-----------+--------+-------------+\r\n| d51ad587-6524-4685-b54c-56b7f3e0591d | image_from_volume | 171966464 | active | qcow2       |\r\n| cd964608-edef-479e-b10e-9851dbc0b431 | cirros            | 12716032  | active | qcow2       |\r\n+--------------------------------------+-------------------+-----------+--------+-------------+\n\nWhat's next\n\nCreating virtual machines\n\nRescuing virtual machines\n\nManaging images",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nShut down the VM which boot volume you want to use. For example:# vinfra service compute server stop myvm\n\n\nFind out the boot volume's ID. For example:# vinfra service compute server show myvm\r\n+---------------+--------------------------------------------+\r\n| Field         | Value                                      |\r\n+---------------+--------------------------------------------+\r\n| config_drive  |                                            |\r\n| created       | 2021-06-10T08:55:53Z                       |\r\n| description   |                                            |\r\n| fault         |                                            |\r\n| flavor        | disk: 0                                    |\r\n|               | ephemeral: 0                               |\r\n|               | extra_specs: {}                            |\r\n|               | original_name: tiny                        |\r\n|               | ram: 512                                   |\r\n|               | swap: 0                                    |\r\n|               | vcpus: 1                                   |\r\n| ha_enabled    | True                                       |\r\n| host          | amigai-ac-ve0.vstoragedomain               |\r\n| host_status   | UP                                         |\r\n| id            | 6d0fc132-7ea7-41f0-81ca-a4a2b2a2c893       |\r\n| key_name      |                                            |\r\n| metadata      | {}                                         |\r\n| name          | myvm                                       |\r\n| networks      | - id: bd17c207-5291-4096-be6a-0a8a4bf67792 |\r\n|               |   ipam_enabled: true                       |\r\n|               |   ips:                                     |\r\n|               |   - 192.168.128.100                        |\r\n|               |   mac_addr: fa:16:3e:6b:6c:83              |\r\n|               |   name: private                            |\r\n|               |   spoofing_protection: true                |\r\n| orig_hostname | amigai-ac-ve0                              |\r\n| placements    | []                                         |\r\n| power_state   | SHUTDOWN                                   |\r\n| project_id    | dfd99654b8c94b939b638f94abb2ad73           |\r\n| status        | SHUTOFF                                    |\r\n| task_state    |                                            |\r\n| updated       | 2021-06-15T11:24:05Z                       |\r\n| user_data     |                                            |\r\n| vm_state      | stopped                                    |\r\n| volumes       | - delete_on_termination: false             |\r\n|               |   id: 49be1057-c026-494f-b85d-e013728d41bd |\r\n|               | - delete_on_termination: false             |\r\n|               |   id: eca9f679-7e35-4768-ad20-9bcb6af6fd59 |\r\n+---------------+--------------------------------------------+\r\n\nThe first volume in the output is the boot one.\n\n\nUpload the boot volume to an image specifying the image name. For example:# vinfra service compute volume upload-to-image 49be1057-c026-494f-b85d-e013728d41bd \\\r\n--name image_from_volume\n\n\nThe new image will appear in the vinfra service compute image list output:# vinfra service compute image list\r\n+--------------------------------------+-------------------+-----------+--------+-------------+\r\n| id                                   | name              | size      | status | disk_format |\r\n+--------------------------------------+-------------------+-----------+--------+-------------+\r\n| d51ad587-6524-4685-b54c-56b7f3e0591d | image_from_volume | 171966464 | active | qcow2       |\r\n| cd964608-edef-479e-b10e-9851dbc0b431 | cirros            | 12716032  | active | qcow2       |\r\n+--------------------------------------+-------------------+-----------+--------+-------------+\n",
                "title": "To create a template from a boot volume"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nPower off the VM that the original volume is attached to.\nSwitch to the Compute > Storage > Volumes tab, click volume\u00e2\u0080\u0099s ellipsis button and select Create image.\n\nIn the Create image window, enter an image name, and then click Create.\n\n\n\n\n\n\nThe new image will appear on the Images screen.\n",
                "title": "To create a template from a boot volume"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-templates.html"
    },
    {
        "title": "Creating physical compute networks",
        "content": "Creating physical compute networks\nPhysical networks can host multiple IPv4, IPv6, and dual-stack subnets. IPv6 subnets support three IP address assignment modes: Stateless Address Autoconfiguration (SLAAC), DHCPv6 stateless, and DHCPv6 stateful. The modes are explained in the following table:\n\nIPv6 address mode\nVM address assignment\nExternal router configuration\nDHCP server configuration\n\nSLAAC\nA VM obtains an IPv6 address, the default gateway, and the subnet prefix via Router Advertisements (RA) from an external router. DNS servers and a hostname are not automatically configured.\nAn external router should send RA messages without the M (Managed address configuration) and O (Other configuration) flags. \nThe built-in DHCPv6 server is automatically disabled.\n\nDHCPv6 stateless\nA VM obtains an IPv6 address and the default gateway via RA messages from an external router and other information (the subnet prefix, DNS servers, a hostname) from the built-in DHCPv6 server.\nAn external router should send RA messages with the O flag. \nThe built-in DHCPv6 server is automatically enabled.\n\nDHCPv6 stateful\nA VM obtains an IPv6 address and other information (the subnet prefix, DNS servers, a hostname) from the built-in DHCPv6 server. The default gateway is received via RA messages from an external router.\nAn external router should send RA messages with the M flag. \nThe built-in DHCPv6 server is automatically enabled.\n\nIPv6 address assignment inside a virtual machine also depends on the network settings of a guest operating system.\n\nLimitations\n\nYou can create only one untagged physical network over an infrastructure network.\nWhen providing network access to an entire domain, it is configured only for the existing projects within this domain. Newly created projects will not have access to the network.\nYou cannot connect IPv6 subnets to routers. Therefore, floating IPv6 addresses are not supported.\nIPv6 addresses are not supported for Kubernetes clusters.\nA VM that is connected to a dual-stack network always receives an IPv6 address, if the IPv6 subnet is in the SLAAC or DHCPv6 stateless mode.\nTo be able to work in a SLAAC-enabled IPv6 subnet by using cloud-init, a VM guest operating system must have cloud-init version 19.4 or newer.\nA physical network MTU cannot exceed that of the underlying network interface.\n\nPrerequisites\n\nA clear understanding of the compute architecture, which is explained in Compute network architecture.\nFor VLAN-based networks, a virtual switch is connected to the trunk network interface, as described in Connecting virtual switches to trunk interfaces.\n\nTo add a physical compute network\n\nAdmin panel\n\nOn the Compute > Network > Networks tab, click Create network.\n\nOn the Network configuration step:\n\nEnable or disable IP address management:\n\nWith IP address management enabled, VMs connected to the network will automatically be assigned IP addresses from allocation pools by the built-in DHCP server and use custom DNS servers. Additionally, spoofing protection will be enabled for all VM network ports by default. Each VM network interface will be able to accept and send IP packets only if it has IP and MAC addresses assigned. You can disable spoofing protection manually for a VM interface, if required.\nWith IP address management disabled, VMs connected to the network will obtain IP addresses from the DHCP servers in that network, if any. Also, spoofing protection will be disabled for all VM network ports, and you cannot enable it manually. This means that each VM network interface, with or without assigned IP and MAC addresses, will be able to accept and send IP packets.\n\nIn any case, you will be able to manually assign static IP addresses from inside the VMs.\n\nSelect the Physical network type.\n\nSpecify a network name, and then select an infrastructure network with the VM public traffic type.\n\nTo create a VLAN-based network, select VLAN and specify a VLAN ID. To create a flat physical network, select Untagged.\nThe network MTU is set to 1500 by default. If required, you can adjust this value according to the MTU of the underlying network interface.\nClick Next.\n\nIf you enabled IP address management, you will move on to the IP address management step, where you can add IPv4 and IPv6 subnets:\n\nTo add an IPv4 subnet\n\nIn the Subnets section, click Add and select IPv4 subnet.\nIn the Add IPv4 subnet window, specify the network\u00e2\u0080\u0099s IPv4 address range and, optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\n\nEnable or disable the built-in DHCP server:\n\nWith the DHCP server enabled, VM network interfaces will automatically be assigned IP addresses: either from allocation pools or, if there are no pools, from the network\u00e2\u0080\u0099s entire IP range. The DHCP server will receive the first two IP addresses from the IP pool. For example:\n\n In a subnet with CIDR 192.168.128.0/24 and without a gateway, the DHCP server will be assigned the IP addresses 192.168.128.1 and 192.168.128.2.\n In a subnet with CIDR 192.168.128.0/24 and the gateway IP address set to 192.168.128.1, the DHCP server will be assigned the IP addresses 192.168.128.2 and 192.168.128.3.\n\nWith the DHCP server disabled, VM network interfaces will still get IP addresses, but you will have to manually assign them inside VMs.\n\nThe virtual DHCP service will work only within the current network and will not be exposed to other networks.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nSpecify DNS servers that will be used by virtual machines. These servers can be delivered to VMs via the built-in DHCP server or by using the cloud-init network configuration (if cloud-init is installed in the VM).\nClick Add.\n\nTo add an IPv6 subnet\n\nIn the Subnets section, click Add and select IPv6 subnet.\nIn the Add IPv6 subnet window, specify the network\u00e2\u0080\u0099s IPv6 address range and, optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\nSelect the desired IPv6 address mode, referring to the table above.\n\nIf you have selected the IPv6 address mode None, enable or disable the built-in DHCP server:\n\nWith the DHCP server enabled, a VM will automatically obtain an IPv6 address.\nWith the DHCP server disabled, you will need to assign an IPv6 address for a VM manually.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nIf you have selected the IPv6 address mode DHCPv6 stateless or DHCPv6 stateful, specify DNS servers that will be send to virtual machines via the built-in DHCP server.\nClick Add.\n\nOn the Network access step, you can configure the network access:\n\nSelect projects to provide network access to:\n\nIf you want the network to be accessed from all existing and new projects, select All projects.\nIf you want the network to be accessed from all existing projects within a domain, select Select projects, and then select the check box next to the required domain.\nIf you want the network to be accessed from a particular project within a domain, select Select projects, click the domain name, and then select the required project.\nIf you do not want to share the network, skip this step by clicking Next.\n\nSelect the access type:\n\nBy providing full access, you allow virtual machines in the selected projects to communicate with this network either directly or via virtual routers.\nBy providing routed access, you allow virtual machines in the selected projects to communicate with this network only via virtual routers.\nBy providing direct access, you only allow a direct connection of virtual machines in the selected projects to this network.\n\nClick Next.\n\nOn the Summary step, review the configuration, and then click Add network.\n\nCommand-line interface\nUse the following command:vinfra service compute network create [--dhcp | --no-dhcp] [--dns-nameserver <dns-nameserver>]\r\n                                      [--allocation-pool <allocation-pool>] [--gateway <gateway> | --no-gateway]\r\n                                      [--rbac-policies <rbac-policies>] [--physical-network <physical-network>]\r\n                                      [--vlan-network <vlan-network>] [--vlan <vlan>] [--mtu <mtu>]\r\n                                      [--cidr <cidr>] [--ipv6-address-mode <ipv6-address-mode>] <network-name>\r\n\n\n--dhcp\n\nEnable DHCP.\n--no-dhcp\n\nDisable DHCP.\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used  multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: ip_addr_start-ip_addr_end. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n--rbac-policies <rbac-policies>\n\nComma-separated list of RBAC policies in the format: <target>:<target_id>:<action> | none. Valid targets: project, domain. Valid actions: direct, full, routed. \u00e2\u0080\u0098*\u00e2\u0080\u0099 is valid target_id for all targets. Pass none to clear out all existing policies.\nExample: domain:default:routed,project:uuid1:full\n\n--physical-network <physical-network>\n\nAn infrastructure network to link to a physical network\n--vlan-network <vlan-network>\n\nA VLAN network to link\n--vlan <vlan>\n\nVirtual network VLAN ID\n--mtu <mtu>\n\nCustom MTU value\n--cidr <cidr>\n\nSubnet range in CIDR notation\n--ipv6-address-mode <ipv6-address-mode>\n\nIPv6 address mode: dhcpv6-stateful, dhcpv6-stateless, slaac\n<network-name>\n\nNetwork name\n\nExample 1. To create an untagged physical network over the Public infrastructure network, with enabled IP management, the specified network parameters, and full network access between all the projects within the specified domain, run:# vinfra service compute network create mypubnet --physical-network Public \\\r\n--cidr 10.136.16.0/22 --gateway 10.136.16.1 --dns-nameserver 10.35.11.7 \\\r\n--allocation-pool 10.136.18.141-10.136.18.148 \\\r\n--rbac-policies domain:cd421db9f3e84e3e8cd2c932c1f7a698:full\nExample 2. To create a VLAN-based physical network over the Public infrastructure network, with the VLAN ID 10, enabled IP management, the specified network parameters, and direct network access between all the projects in the infrastructure, run:# vinfra service compute network create mypubnet_vlan --vlan 10 \\\r\n--physical-network Public --cidr 10.136.16.0/22 --gateway 10.136.16.1 \\\r\n--dns-nameserver 10.35.11.7 --allocation-pool 10.136.18.131-10.136.18.138 \\--rbac-policies project:*:direct\nThe new compute network will appear in the vinfra service compute network list output:# vinfra service compute network list -c id -c name -c cidr -c allocation_pools\r\n+----------------+---------------+------------------+-------------------------------+\r\n| id             | name          | cidr             | allocation_pools              |\r\n+----------------+---------------+------------------+-------------------------------+\r\n| 22674f9d-<...> | mypubnet      | 10.136.16.0/22   | - 10.136.18.141-10.136.18.148   |\r\n| 8f0dc747-<...> | mypubnet_vlan | 10.136.16.0/22   | - 10.136.18.131-10.136.18.138   |\r\n| a0019b43-<...> | myprivnet     | 192.168.128.0/24 | - 192.168.128.2-192.168.128.254 |\r\n+----------------+---------------+------------------+-------------------------------+\n\nSee also\n\nCreating virtual compute networks\n\nEditing and deleting compute networks\n\nCreating virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute network create [--dhcp | --no-dhcp] [--dns-nameserver <dns-nameserver>]\r\n                                      [--allocation-pool <allocation-pool>] [--gateway <gateway> | --no-gateway]\r\n                                      [--rbac-policies <rbac-policies>] [--physical-network <physical-network>]\r\n                                      [--vlan-network <vlan-network>] [--vlan <vlan>] [--mtu <mtu>]\r\n                                      [--cidr <cidr>] [--ipv6-address-mode <ipv6-address-mode>] <network-name>\r\n\n\n--dhcp\n\nEnable DHCP.\n--no-dhcp\n\nDisable DHCP.\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used  multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: ip_addr_start-ip_addr_end. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n--rbac-policies <rbac-policies>\n\n\nComma-separated list of RBAC policies in the format: <target>:<target_id>:<action> | none. Valid targets: project, domain. Valid actions: direct, full, routed. \u00e2\u0080\u0098*\u00e2\u0080\u0099 is valid target_id for all targets. Pass none to clear out all existing policies.\nExample: domain:default:routed,project:uuid1:full\n\n--physical-network <physical-network>\n\nAn infrastructure network to link to a physical network\n--vlan-network <vlan-network>\n\nA VLAN network to link\n--vlan <vlan>\n\nVirtual network VLAN ID\n--mtu <mtu>\n\nCustom MTU value\n--cidr <cidr>\n\nSubnet range in CIDR notation\n--ipv6-address-mode <ipv6-address-mode>\n\nIPv6 address mode: dhcpv6-stateful, dhcpv6-stateless, slaac\n<network-name>\n\nNetwork name\n\nExample 1. To create an untagged physical network over the Public infrastructure network, with enabled IP management, the specified network parameters, and full network access between all the projects within the specified domain, run:# vinfra service compute network create mypubnet --physical-network Public \\\r\n--cidr 10.136.16.0/22 --gateway 10.136.16.1 --dns-nameserver 10.35.11.7 \\\r\n--allocation-pool 10.136.18.141-10.136.18.148 \\\r\n--rbac-policies domain:cd421db9f3e84e3e8cd2c932c1f7a698:full\nExample 2. To create a VLAN-based physical network over the Public infrastructure network, with the VLAN ID 10, enabled IP management, the specified network parameters, and direct network access between all the projects in the infrastructure, run:# vinfra service compute network create mypubnet_vlan --vlan 10 \\\r\n--physical-network Public --cidr 10.136.16.0/22 --gateway 10.136.16.1 \\\r\n--dns-nameserver 10.35.11.7 --allocation-pool 10.136.18.131-10.136.18.138 \\--rbac-policies project:*:direct\nThe new compute network will appear in the vinfra service compute network list output:# vinfra service compute network list -c id -c name -c cidr -c allocation_pools\r\n+----------------+---------------+------------------+-------------------------------+\r\n| id             | name          | cidr             | allocation_pools              |\r\n+----------------+---------------+------------------+-------------------------------+\r\n| 22674f9d-<...> | mypubnet      | 10.136.16.0/22   | - 10.136.18.141-10.136.18.148   |\r\n| 8f0dc747-<...> | mypubnet_vlan | 10.136.16.0/22   | - 10.136.18.131-10.136.18.138   |\r\n| a0019b43-<...> | myprivnet     | 192.168.128.0/24 | - 192.168.128.2-192.168.128.254 |\r\n+----------------+---------------+------------------+-------------------------------+\n",
                "title": "To add a physical compute network"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Networks tab, click Create network.\n\nOn the Network configuration step:\n\n\nEnable or disable IP address management:\n\nWith IP address management enabled, VMs connected to the network will automatically be assigned IP addresses from allocation pools by the built-in DHCP server and use custom DNS servers. Additionally, spoofing protection will be enabled for all VM network ports by default. Each VM network interface will be able to accept and send IP packets only if it has IP and MAC addresses assigned. You can disable spoofing protection manually for a VM interface, if required.\nWith IP address management disabled, VMs connected to the network will obtain IP addresses from the DHCP servers in that network, if any. Also, spoofing protection will be disabled for all VM network ports, and you cannot enable it manually. This means that each VM network interface, with or without assigned IP and MAC addresses, will be able to accept and send IP packets.\n\nIn any case, you will be able to manually assign static IP addresses from inside the VMs.\n\nSelect the Physical network type.\n\nSpecify a network name, and then select an infrastructure network with the VM public traffic type.\n\nTo create a VLAN-based network, select VLAN and specify a VLAN ID. To create a flat physical network, select Untagged.\nThe network MTU is set to 1500 by default. If required, you can adjust this value according to the MTU of the underlying network interface.\nClick Next.\n\n\n\n\n\n\n\nIf you enabled IP address management, you will move on to the IP address management step, where you can add IPv4 and IPv6 subnets:\n\n\nTo add an IPv4 subnet\n\n\nIn the Subnets section, click Add and select IPv4 subnet.\nIn the Add IPv4 subnet window, specify the network\u00e2\u0080\u0099s IPv4 address range and, optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\n\nEnable or disable the built-in DHCP server:\n\n\nWith the DHCP server enabled, VM network interfaces will automatically be assigned IP addresses: either from allocation pools or, if there are no pools, from the network\u00e2\u0080\u0099s entire IP range. The DHCP server will receive the first two IP addresses from the IP pool. For example:\n\n In a subnet with CIDR 192.168.128.0/24 and without a gateway, the DHCP server will be assigned the IP addresses 192.168.128.1 and 192.168.128.2.\n In a subnet with CIDR 192.168.128.0/24 and the gateway IP address set to 192.168.128.1, the DHCP server will be assigned the IP addresses 192.168.128.2 and 192.168.128.3.\n\n\nWith the DHCP server disabled, VM network interfaces will still get IP addresses, but you will have to manually assign them inside VMs.\n\nThe virtual DHCP service will work only within the current network and will not be exposed to other networks.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nSpecify DNS servers that will be used by virtual machines. These servers can be delivered to VMs via the built-in DHCP server or by using the cloud-init network configuration (if cloud-init is installed in the VM).\nClick Add.\n\n\n\n\n\n\n\n\n\nTo add an IPv6 subnet\n\n\nIn the Subnets section, click Add and select IPv6 subnet.\nIn the Add IPv6 subnet window, specify the network\u00e2\u0080\u0099s IPv6 address range and, optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\nSelect the desired IPv6 address mode, referring to the table above.\n\nIf you have selected the IPv6 address mode None, enable or disable the built-in DHCP server:\n\nWith the DHCP server enabled, a VM will automatically obtain an IPv6 address.\nWith the DHCP server disabled, you will need to assign an IPv6 address for a VM manually.\n\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nIf you have selected the IPv6 address mode DHCPv6 stateless or DHCPv6 stateful, specify DNS servers that will be send to virtual machines via the built-in DHCP server.\nClick Add.\n\n\n\n\n\n\n\n\n\n\n\nOn the Network access step, you can configure the network access:\n\n\nSelect projects to provide network access to:\n\nIf you want the network to be accessed from all existing and new projects, select All projects.\nIf you want the network to be accessed from all existing projects within a domain, select Select projects, and then select the check box next to the required domain.\nIf you want the network to be accessed from a particular project within a domain, select Select projects, click the domain name, and then select the required project.\nIf you do not want to share the network, skip this step by clicking Next.\n\n\n\nSelect the access type:\n\nBy providing full access, you allow virtual machines in the selected projects to communicate with this network either directly or via virtual routers.\nBy providing routed access, you allow virtual machines in the selected projects to communicate with this network only via virtual routers.\nBy providing direct access, you only allow a direct connection of virtual machines in the selected projects to this network.\n\n\nClick Next.\n\n\n\n\n\n\nOn the Summary step, review the configuration, and then click Add network.\n\n",
                "title": "To add a physical compute network"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-physical-compute-networks.html"
    },
    {
        "title": "Creating NFS shares",
        "content": "Creating NFS shares\nLimitations\n\nThe redundancy mode of an NFS share cannot be changed after its creation.\n\nPrerequisites\n\nA clear understanding of the concept Storage policies.\nThe NFS cluster is created by following the instructions in Creating the NFS cluster.\n\nTo create an NFS share\n\nAdmin panel\n\nOpen the Storage services > NFS > Shares screen, and then click Add share.\n\nIn the Add share window, specify a unique name and IP address, which must be unused and, if authentication is enabled, domain resolvable. In addition, this IP address should be within the network subnet of the node\u00e2\u0080\u0099s interface.\n\nTo authenticate clients in an NFS share, do the following:\n\nCreate a principal with its key table for the share in Kerberos KDC (Key Distribution Center), as described in Authenticating NFS share users via Kerberos.\n\nUpload the corresponding keytab file.\n\nClick Next.\n\nOn the next step, specify the share size, in gibibytes, and redundancy parameters, such as a storage tier, failure domain, and redundancy mode. For users accessing exports, the share size will be the size of the file system.\n\nClick Add.\n\nAfter the share is created, you can proceed to create NFS exports.\n\nCommand-line interface\nUse the following command:vinfra service nfs share create --node <node> --ip-address <ip_address> --size <size> --tier {0,1,2,3}\r\n                                (--replicas <norm> | --encoding <M>+<N>) --failure-domain {0,1,2,3,4} \r\n                                [--krb-keytab <krb-keytab>] <name>\n\n--node <node>\n\nNode ID\n--ip-address <ip_address>\n\nIP address of the NFS share\n--size <size>\n\nNFS share size, in bytes. You can also specify the following units: KiB for kibibytes, MiB for mebibytes, GiB for gibibytes, TiB for tebibytes, and PiB for pebibytes.\n--tier {0,1,2,3}\n\nStorage tier (default: 0)\n--replicas <norm>\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain (default: 1)\n\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: the number of data blocks\nN: the number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain (default: 0)\n--krb-keytab <krb-keytab>\n\nKerberos keytab file\n<name>\n\nNFS share name\n\nFor example, to create the NFS share share1 with the IP address 10.136.18.149 on the node with the ID 923926da-a879-5f56-1b24-1462917ed335, run:# vinfra service nfs share create share1 --node 923926da-a879-5f56-1b24-1462917ed335 \\\r\n--ip-address 10.136.18.149 --size 107374182400 --tier 0 --encoding 1+2 --failure-domain 1\nThe created NFS share will appear in the vinfra service nfs share list output:# vinfra service nfs share list\r\n+--------+---------------+------------------------------------------+\r\n| name   | ip_address    | node                                     |\r\n+--------+---------------+------------------------------------------+\r\n| share1 | 10.136.18.149 | cfgd_id: 1                               |\r\n|        |               | has_configd: true                        |\r\n|        |               | id: 923926da-a879-5f56-1b24-1462917ed335 |\r\n|        |               | ip_address: node001.vstoragedomain       |\r\n+--------+---------------+------------------------------------------+\n\nWhat's next\n\nCreating NFS exports",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs share create --node <node> --ip-address <ip_address> --size <size> --tier {0,1,2,3}\r\n                                (--replicas <norm> | --encoding <M>+<N>) --failure-domain {0,1,2,3,4} \r\n                                [--krb-keytab <krb-keytab>] <name>\n\n--node <node>\n\nNode ID\n--ip-address <ip_address>\n\nIP address of the NFS share\n--size <size>\n\nNFS share size, in bytes. You can also specify the following units: KiB for kibibytes, MiB for mebibytes, GiB for gibibytes, TiB for tebibytes, and PiB for pebibytes.\n--tier {0,1,2,3}\n\nStorage tier (default: 0)\n--replicas <norm>\n\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain (default: 1)\n\n\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: the number of data blocks\nN: the number of parity blocks\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain (default: 0)\n--krb-keytab <krb-keytab>\n\nKerberos keytab file\n<name>\n\nNFS share name\n\nFor example, to create the NFS share share1 with the IP address 10.136.18.149 on the node with the ID 923926da-a879-5f56-1b24-1462917ed335, run:# vinfra service nfs share create share1 --node 923926da-a879-5f56-1b24-1462917ed335 \\\r\n--ip-address 10.136.18.149 --size 107374182400 --tier 0 --encoding 1+2 --failure-domain 1\nThe created NFS share will appear in the vinfra service nfs share list output:# vinfra service nfs share list\r\n+--------+---------------+------------------------------------------+\r\n| name   | ip_address    | node                                     |\r\n+--------+---------------+------------------------------------------+\r\n| share1 | 10.136.18.149 | cfgd_id: 1                               |\r\n|        |               | has_configd: true                        |\r\n|        |               | id: 923926da-a879-5f56-1b24-1462917ed335 |\r\n|        |               | ip_address: node001.vstoragedomain       |\r\n+--------+---------------+------------------------------------------+\n",
                "title": "To create an NFS share"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen the Storage services > NFS > Shares screen, and then click Add share.\n\nIn the Add share window, specify a unique name and IP address, which must be unused and, if authentication is enabled, domain resolvable. In addition, this IP address should be within the network subnet of the node\u00e2\u0080\u0099s interface.\n\n\nTo authenticate clients in an NFS share, do the following:\n\nCreate a principal with its key table for the share in Kerberos KDC (Key Distribution Center), as described in Authenticating NFS share users via Kerberos.\n\nUpload the corresponding keytab file.\n\n\n\n\n\n\n\nClick Next.\n\nOn the next step, specify the share size, in gibibytes, and redundancy parameters, such as a storage tier, failure domain, and redundancy mode. For users accessing exports, the share size will be the size of the file system.\n\n\n\n\n\nClick Add.\n\nAfter the share is created, you can proceed to create NFS exports.\n",
                "title": "To create an NFS share"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-nfs-shares.html"
    },
    {
        "title": "Creating the NFS cluster",
        "content": "Creating the NFS cluster\nPrerequisites\n\nThe storage cluster has at least one disk with the Storage role.\n\nTo create the NFS storage cluster\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the OSTOR private and NFS traffic types are added to the networks you intend to use.\nOpen the Storage services > NFS screen, and then click Create NFS storage.\nSelect one or more nodes to add to the NFS storage cluster, and then click Create.\n\nAfter the NFS storage is created, you can proceed to create NFS shares.\n\nCommand-line interface\nUse the following command:vinfra service nfs cluster create --nodes <node>[:<ip_address>]\n\n--nodes <node>[:<ip_address>]\n\nA comma-separated list of node hostnames or IDs, and optionally their IP addresses\n\nFor example, to create the NFS cluster from node node001, run:# vinfra service nfs cluster create --nodes node001\n\nWhat's next\n\nCreating NFS shares",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs cluster create --nodes <node>[:<ip_address>]\n\n--nodes <node>[:<ip_address>]\n\nA comma-separated list of node hostnames or IDs, and optionally their IP addresses\n\nFor example, to create the NFS cluster from node node001, run:# vinfra service nfs cluster create --nodes node001\n",
                "title": "To create the NFS storage cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the OSTOR private and NFS traffic types are added to the networks you intend to use.\nOpen the Storage services > NFS screen, and then click Create NFS storage.\nSelect one or more nodes to add to the NFS storage cluster, and then click Create.\n\nAfter the NFS storage is created, you can proceed to create NFS shares.\n",
                "title": "To create the NFS storage cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-the-nfs-cluster.html"
    },
    {
        "title": "Creating virtual machines with physical GPUs",
        "content": "Creating virtual machines with physical GPUs\nLimitations\n\nVirtual machines with attached physical GPUs cannot be live migrated.\n\nPrerequisites\n\nThe compute cluster is reconfigured for GPU passthrough, as described in Enabling PCI passthrough and vGPU support.\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo create a virtual machine with an attached physical GPU\n\nCreate a flavor with the pci_passthrough property specifying the GPU alias from the pci-passthrough.yaml file and the number of GPUs to use. For example, to create the gpu-flavor flavor with 8 vCPUs and 16 GiB of RAM, run:# openstack --insecure flavor create --ram 16384 --vcpus 8 --property \"pci_passthrough:alias\"=\"gpu:1\" \\\r\n--public gpu-flavor\n\nSome drivers may require to hide the hypervisor signature. To do this, add the hide_hypervisor_id property to the flavor:# openstack --insecure flavor set gpu-flavor --property hide_hypervisor_id=true\n\nCreate a virtual machine specifying the gpu-flavor flavor. For example, to create the gpu-vm from the vol2 volume, run:# openstack --insecure server create --volume vol2 --flavor gpu-flavor gpu-vm\n\nSee also\n\nCreating virtual machines with virtual GPUs\n\nCreating virtual machines with different vGPU types\n\nCreating virtual machines with SR-IOV network ports\n\nCreating virtual machines\n\nDisabling PCI passthrough and vGPU support",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-machines-with-physical-gpus.html"
    },
    {
        "title": "Creating volumes",
        "content": "Creating volumes\nWhile it is convenient to create desired volumes while creating a target group, you can also do this at any time afterwards.\nPrerequisites\n\nA clear understanding of the concept Storage policies.\nA target group is created, as described in Creating target groups.\n\nTo create a volume\n\nAdmin panel\n\nOpen Storage services > Block storage > Volumes, and then click Create volume. A wizard will open.\n\nOn Name and size, enter a volume name and specify a size in gigabytes. Note that volumes can be extended later but not shrunk.\n\nOn Storage policy, select a redundancy mode, a storage tier, and a failure domain. To benefit from high availability, select a mode other than No redundancy and failure domain other than Disk.\n\nOn Summary, review the volume details. You can go back to change them if necessary. Click Create.\n\nCommand-line interface\nUse the following command:vinfra service block-storage volume create --size <size> --tier {0,1,2,3} (--replicas <norm> | --encoding <M>+<N>)\r\n                                           --failure-domain {0,1,2,3,4} <name>\n\n--size <size>\n\nVolume size, in bytes\n--tier {0,1,2,3}\n\nStorage tier (default: 0)\n--replicas <norm>\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain (default: 1)\n\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: the number of data blocks\nN: the number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n<name>\n\nVolume name\n\nFor example, to create the volume vol1 with the size of 100 GiB, the 3 replicas redundancy mode on tier 0 and the failure domain host, run:# vinfra service block-storage volume create vol1 --size 107374182400 --tier 0 --replicas 3 --failure-domain 1\nThe created volume will appear in the vinfra service block-storage volume list output:# vinfra service block-storage volume list\r\n+------------------+--------------+------+--------------+-----------+----------+--------+-----+\r\n| id               | serial       | name | size         | used_size | grp_name | grp_id | lun |\r\n+------------------+--------------+------+--------------+-----------+----------+--------+-----+\r\n| 9841d72f-5d68<\u00e2\u0080\u00a6> | cd96cf1031b6 | vol1 | 107374182400 | 1048576   |          |        |     |\r\n+------------------+--------------+------+--------------+-----------+----------+--------+-----+\n\nWhat's next\n\nAttaching volumes to target groups",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage volume create --size <size> --tier {0,1,2,3} (--replicas <norm> | --encoding <M>+<N>)\r\n                                           --failure-domain {0,1,2,3,4} <name>\n\n--size <size>\n\nVolume size, in bytes\n--tier {0,1,2,3}\n\nStorage tier (default: 0)\n--replicas <norm>\n\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain (default: 1)\n\n\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: the number of data blocks\nN: the number of parity blocks\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n<name>\n\nVolume name\n\nFor example, to create the volume vol1 with the size of 100 GiB, the 3 replicas redundancy mode on tier 0 and the failure domain host, run:# vinfra service block-storage volume create vol1 --size 107374182400 --tier 0 --replicas 3 --failure-domain 1\nThe created volume will appear in the vinfra service block-storage volume list output:# vinfra service block-storage volume list\r\n+------------------+--------------+------+--------------+-----------+----------+--------+-----+\r\n| id               | serial       | name | size         | used_size | grp_name | grp_id | lun |\r\n+------------------+--------------+------+--------------+-----------+----------+--------+-----+\r\n| 9841d72f-5d68<\u00e2\u0080\u00a6> | cd96cf1031b6 | vol1 | 107374182400 | 1048576   |          |        |     |\r\n+------------------+--------------+------+--------------+-----------+----------+--------+-----+\n",
                "title": "To create a volume"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > Volumes, and then click Create volume. A wizard will open.\n\nOn Name and size, enter a volume name and specify a size in gigabytes. Note that volumes can be extended later but not shrunk.\n\n\n\n\n\n\nOn Storage policy, select a redundancy mode, a storage tier, and a failure domain. To benefit from high availability, select a mode other than No redundancy and failure domain other than Disk.\n\n\n\n\n\nOn Summary, review the volume details. You can go back to change them if necessary. Click Create.\n\n",
                "title": "To create a volume"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-volumes.html"
    },
    {
        "title": "Creating virtual machines with virtual GPUs",
        "content": "Creating virtual machines with virtual GPUs\nIf you use only one vGPU type in the compute cluster, you need to create a flavor that requests one virtual GPU, and then create virtual machines with this flavor.\nLimitations\n\nVirtual machines with attached vGPUs cannot be suspended and live migrated.\n\nThe default QLX driver for the VNC console and the NVIDIA GPU driver are incompatible\n\nAfter installing the NVIDIA GPU driver inside a virtual machine with an attached vGPU, the VNC console stops working. You can use RDP for a remote connection. Alternatively, for templates that already have the NVIDIA GPU driver installed, you can set the hw_use_vgpu_display property, to disable the integrated QLX driver. For example:# openstack --insecure image set --property hw_use_vgpu_display 007db63f-9b41-4918-b572-2c5eef4c8f4b\n\nPrerequisites\n\nThe compute cluster is reconfigured for vGPU support, as described in Enabling PCI passthrough and vGPU support.\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo create a virtual machine with a vGPU\n\nCreate a flavor with the resources property specifying the number of vGPUs to use. For example, to create the vgpu-flavor flavor with 2 vCPUs and 4 GiB of RAM, run:# openstack --insecure flavor create --ram 4096 --vcpus 2 --property resources:VGPU=1 --public vgpu-flavor\n\nCreate a virtual machine specifying the vgpu-flavor flavor. For example, to create the vgpu-vm from the vol2 volume, run:# openstack --insecure server create --volume vol2 --flavor vgpu-flavor vgpu-vm\n\nThe created virtual machine will have a virtual GPU of the type that is configured in the compute cluster.\nSee also\n\nCreating virtual machines with different vGPU types\n\nCreating virtual machines with physical GPUs\n\nCreating virtual machines with SR-IOV network ports\n\nCreating virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-machines-with-virtual-gpus.html"
    },
    {
        "title": "Creating virtIO disks for virtual machines",
        "content": "Creating virtIO disks for virtual machines\nTo improve I/O performance of virtual machines, you can use virtIO disks with them. By default, virtual machines are created with disks attached to the SCSI bus, which cannot be changed later.\nYou can create a volume for a VM and attach this volume to the virtIO bus during the VM deployment via the vinfra tool. This method works for creating boot volumes from both ISO images and templates (QCOW2). You can also use it to attach non-boot volumes. Note that you need to run the vinfra command each time you create a virtual machine.\nAlternatively, it is possible to apply the virtIO bus property to an image via the OpenStack command-line tool. This property allows you to create multiple VMs from configured images in the command-line interface, as well as in the admin and self-service panels. However, it only works for templates. \nPrerequisites\n\nYou have an image uploaded to the compute cluster, as described in Preparing boot media for virtual machines.\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo create a virtual machine with virtIO volume\nUse the --volume bus=virtio option with the vinfra service compute server create command.\nExample 1. To create a VM from the mytemplate QCOW2 image, run:# vinfra service compute server create myvm1 --network id=private,fixed-ip=192.168.128.100 --flavor medium\\\r\n--volume source=image,id=mytemplate,bus=virtio,size=100\nExample 2. To create a VM from the myiso ISO image, run:# vinfra service compute server create myvm2 --network id=private,fixed-ip=192.168.128.100 --flavor medium\\\r\n--volume source=blank,size=100,bus=virtio,boot-index=0,type=disk \\\r\n--volume source=image,id=myiso,size=5,boot-index=1,type=cdrom\nAfter the VM is created, all volumes that you add to it will be attached to the virtIO bus.\nTo create a virtual machine with virtIO template\n\nApply the virtIO bus property to a template. For example:# openstack --insecure image set mytemplate --property hw_disk_bus=virtio\n\nCreate a volume from the virtIO template. For example:# openstack --insecure volume create --image=mytemplate --size=10 myvolume\n\nCreate a VM from the new volume. For example:# openstack --insecure server create myvm --volume=myvolume --flavor small --network public\n\nAfter the VM is created, all volumes that you add to it will be attached to the virtIO bus.\nSee also\n\nCreating virtual machines\n\nConfiguring virtual machine volumes",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtio-disks-for-virtual-machines.html"
    },
    {
        "title": "Creating virtual machines with SR-IOV network ports",
        "content": "Creating virtual machines with SR-IOV network ports\nLimitations\n\nVirtual machines with attached PCI devices cannot be live migrated.\n\nPrerequisites\n\nThe compute cluster is reconfigured for PCI passthrough, as described in Enabling PCI passthrough and vGPU support.\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo create a virtual machine with an SR-IOV network port\n\nCreate a physical compute network specifying the network adapter alias from the pci-passthrough.yaml file and the default vNIC type direct. You also need to disable the built-in DHCP server and specify the desired IP address range. For example, to create the sriov-network network with the 10.10.10.0/24 CIDR, run:# vinfra service compute network create sriov-network --physical-network sriovnet --default-vnic-type direct \\\r\n--no-dhcp --cidr 10.10.10.0/24\n\nCreate a virtual machine specifying the new network. For example, to create the VM sriov-vm from the template centos7 and with the large flavor, run:# vinfra service compute server create sriov-vm --network id=sriov-network --volume source=image,size=11,id=centos7 --flavor large\n\nIf the VM creation fails\nCheck whether the following error appears in /var/log/hci/nova/nova-compute.log:2021-08-27 17:56:21.349 6 ERROR nova.compute.manager [instance: 9fb738bf-afe5-40ef-943c-\r\n22e43696bfd9] libvirtError: internal error: qemu unexpectedly closed the monitor: \r\n2021-08-27T14:56:20.294985Z qemu-kvm: -device vfio-pci,host=01:00.3,id=hostdev0,\r\nbus=pci.0,addr=0x6: vfio error: 0000:01:00.3: group 1 is not viable\r\n2021-08-27 17:56:21.349 6 ERROR nova.compute.manager [instance: 9fb738bf-afe5-40ef-943c-\r\n22e43696bfd9] Please ensure all devices within the iommu_group are bound to their vfio \r\nbus driver.\nIn this case, the physical and virtual functions of the network adapter might belong to the same IOMMU group. You can check this by using the virsh nodedev-dumpxml command and specifying the device names of physical and virtual functions. For example:# virsh nodedev-dumpxml pci_0000_00_03_0 | grep iommuGroup\r\n    <iommuGroup number='1'>\r\n    </iommuGroup>\r\n# virsh nodedev-dumpxml pci_0000_00_03_1 | grep iommuGroup\r\n    <iommuGroup number='1'>\r\n    </iommuGroup>\nThe device names have the format pci_0000_<bus_number>_<device_number>_<function_number>. These numbers can be obtained via the lspci command:# lspci -nn | grep Ethernet\r\n00:03.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017]\r\n...\nIn this output, 00 is the bus number, 03 is the device number, and 0 is the function number.\nIf the physical and virtual functions belong to the same IOMMU group, you need to detach the physical function from the node by running the pci-helper.py script and specifying its VID and PID. For example:# /usr/libexec/vstorage-ui-agent/bin/pci-helper.py detach 15b3:1017\nSee also\n\nCreating virtual machines with physical GPUs\n\nCreating virtual machines with virtual GPUs\n\nCreating virtual machines with different vGPU types\n\nCreating virtual machines\n\nDisabling PCI passthrough and vGPU support",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-machines-with-sr-iov-network-ports.html"
    },
    {
        "title": "Data redundancy",
        "content": "Data redundancy\nVirtuozzo Hybrid Infrastructure protects every piece of data by making it redundant. It means that copies of each piece of data are stored across different failure domains, to ensure that the data is available even if some of the failure domains are inaccessible.\nVirtuozzo Hybrid Infrastructure automatically maintains a required number of copies within the cluster and ensures that all the copies are up to date. If a storage node becomes inaccessible when the host failure domain is used, copies from this node are replaced by new ones that are distributed among healthy storage nodes. If a storage node becomes accessible again after downtime, the out-of-date copies on it are updated.\nThe redundancy is achieved by one of two methods: replication or erasure coding. The chosen method affects the size of one piece of data and the number of its copies that will be maintained in the cluster. In general, replication offers better performance, while erasure coding leaves more storage space available for data (refer to Redundancy modes).",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/data-redundancy.html"
    },
    {
        "title": "Creating virtual machines with different vGPU types",
        "content": "Creating virtual machines with different vGPU types\nIf you want to use multiple vGPU types in the compute cluster, you need to manually create CUSTOM_NVIDIA_XXX traits and assign them to corresponding vGPU resource providers first. Then, you can proceed to create flavors and virtual machines with the assigned traits.\nLimitations\n\nVirtual machines with attached vGPUs cannot be suspended and live migrated.\n\nThe default QLX driver for the VNC console and the NVIDIA GPU driver are incompatible\n\nAfter installing the NVIDIA GPU driver inside a virtual machine with an attached vGPU, the VNC console stops working. You can use RDP for a remote connection. Alternatively, for templates that already have the NVIDIA GPU driver installed, you can set the hw_use_vgpu_display property, to disable the integrated QLX driver. For example:# openstack --insecure image set --property hw_use_vgpu_display 007db63f-9b41-4918-b572-2c5eef4c8f4b\n\nPrerequisites\n\nThe compute cluster is reconfigured for vGPU support, as described in Enabling PCI passthrough and vGPU support.\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo configure multiple vGPU types for the compute cluster\n\nList resource providers in the compute cluster to obtain their IDs. For example:# openstack --insecure resource provider list\r\n+--------------------------------------+-----------------------------------------+------------+--------------------+----------------------+\r\n| uuid                                 | name                                    | generation | root_provider_uuid | parent_provider_uuid |\r\n+--------------------------------------+-----------------------------------------+------------+--------------------+----------------------+\r\n| 7d2ef259-42df-4ef8-8eaa-66c3b7448fc3 | node001.vstoragedomain_pci_0000_85_00_0 |         62 | 1f08c319-f270-<\u00e2\u0080\u00a6>  | 1f08c319-f270-<\u00e2\u0080\u00a6>    |\r\n| 94a84fc6-2f28-46d5-93e1-e588e347dd3b | node001.vstoragedomain_pci_0000_10_00_0 |         38 | 1f08c319-f270-<\u00e2\u0080\u00a6>  | 1f08c319-f270-<\u00e2\u0080\u00a6>    |\r\n| 41c177e3-6998-4e56-8d29-f98f72fef910 | node002.vstoragedomain_pci_0000_85_00_0 |         13 | 9dbc8c64-0048-<\u00e2\u0080\u00a6>  | 9dbc8c64-0048-<\u00e2\u0080\u00a6>    |\r\n| 7fd1d10f-9ceb-4cd1-acec-a1254755211b | node002.vstoragedomain_pci_0000_10_00_0 |         13 | 9dbc8c64-0048-<\u00e2\u0080\u00a6>  | 9dbc8c64-0048-<\u00e2\u0080\u00a6>    |\r\n+--------------------------------------+-----------------------------------------+------------+--------------------+----------------------+\n\nCreate custom traits that correspond to different GPU types. For example, to create the traits CUSTOM_NVIDIA_231 and CUSTOM_NVIDIA_232, run:# openstack --insecure trait create CUSTOM_NVIDIA_231\r\n# openstack --insecure trait create CUSTOM_NVIDIA_232\n\nAdd the corresponding trait to the resource provider matching the GPU. For example:# openstack --insecure resource provider trait set --trait CUSTOM_NVIDIA_231 7d2ef259-42df-4ef8-8eaa-66c3b7448fc3\r\n+-------------------+\r\n| name              |\r\n+-------------------+\r\n| CUSTOM_NVIDIA_231 |\r\n+-------------------+\r\n# openstack --insecure resource provider trait set --trait CUSTOM_NVIDIA_231 94a84fc6-2f28-46d5-93e1-e588e347dd3b\r\n+-------------------+\r\n| name              |\r\n+-------------------+\r\n| CUSTOM_NVIDIA_231 |\r\n+-------------------+\nNow, the trait CUSTOM_NVIDIA_231 is assigned to the vGPU resource providers of the node node001. To assign the trait CUSTOM_NVIDIA_232 to the vGPU resource providers of the node node002, run:# openstack --insecure resource provider trait set --trait CUSTOM_NVIDIA_232 41c177e3-6998-4e56-8d29-f98f72fef910\r\n+-------------------+\r\n| name              |\r\n+-------------------+\r\n| CUSTOM_NVIDIA_232 |\r\n+-------------------+\r\n# openstack --insecure resource provider trait set --trait CUSTOM_NVIDIA_232 7fd1d10f-9ceb-4cd1-acec-a1254755211b\r\n+-------------------+\r\n| name              |\r\n+-------------------+\r\n| CUSTOM_NVIDIA_232 |\r\n+-------------------+\n\nTo create virtual machines with different vGPU types\n\nCreate flavors with the resources property specifying the number of vGPUs to use. For example, to create the vgpu231-flavor flavor with 2 vCPUs and 4 GiB of RAM and the vgpu232-flavor flavor with 4 vCPUs and 8 GiB of RAM, run:# openstack --insecure flavor create --ram 4096 --vcpus 2 --property resources:VGPU=1 --public vgpu231-flavor\r\n# openstack --insecure flavor create --ram 8192 --vcpus 4 --property resources:VGPU=1 --public vgpu232-flavor\n\nAdd the requested traits to your flavors. For example, to add the traits CUSTOM_NVIDIA_231 and CUSTOM_NVIDIA_232 to the flavors vgpu231-flavor and vgpu232-flavor, run:# openstack --insecure flavor set --property trait:CUSTOM_NVIDIA_231=required vgpu231-flavor\r\n# openstack --insecure flavor set --property trait:CUSTOM_NVIDIA_232=required vgpu232-flavor\n\nCreate virtual machines specifying the prepared flavors. For example, to create the vm vgpu231-vm with the vgpu231-flavor flavor and the vol1 volume, and the vm vgpu232-vm with the vgpu232-flavor flavor and the vol2 volume, run:# openstack --insecure server create --volume vol1 --flavor vgpu231-flavor vgpu231-vm\r\n# openstack --insecure server create --volume vol2 --flavor vgpu232-flavor vgpu232-vm\n\nThe created virtual machines will have virtual GPUs of different types that are configured in the compute cluster.\nSee also\n\nCreating virtual machines with virtual GPUs\n\nCreating virtual machines with physical GPUs\n\nCreating virtual machines with SR-IOV network ports\n\nCreating virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-machines-with-different-vgpu-types.html"
    },
    {
        "title": "Creating virtual routers",
        "content": "Creating virtual routers\nPrerequisites\n\nCompute networks are created, as described in Creating physical compute networks and Creating virtual compute networks.\nThe compute networks that are to be connected to a router have a gateway specified.\n\nTo create a virtual router\n\nAdmin panel\n\nOn the Compute > Network > Routers tab, click Add router.\n\nIn the Add router window:\n\nSpecify a router name.\nFrom the Network drop-down menu, select a physical network through which external access will be provided via an external gateway. The new external gateway will pick an unused IP address from the selected physical network.\nIn the Add internal interfaces section, select one or more virtual networks to connect to a router via internal interfaces. The new internal interfaces will attempt to use the gateway IP address of the selected virtual networks by default.\n\nSelect or deselect the SNAT check box to enable or disable SNAT on the external gateway of the router. With SNAT enabled, the router replaces VM private IP addresses with the public IP address of its external gateway.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra service compute router create [--external-gateway <network>]\r\n                                     [--enable-snat | --disable-snat]\r\n                                     [--fixed-ip <fixid-ip>]\r\n                                     [--internal-interface <network=network,ip-addr=ip-addr> |\r\n                                     <network>] <router-name>\r\n\n\n--external-gateway <network>\n\nSpecify a physical network to be used as the router\u00e2\u0080\u0099s external gateway (name or ID)\n--enable-snat\n\nEnable source NAT on the external gateway\n--disable-snat\n\nDisable source NAT on the external gateway\n--fixed-ip <fixid-ip>\n\nDesired IP on the external gateway\n--internal-interface <network=network,ip-addr=ip-addr>|<network>\n\nSpecify an internal interface. This option can be used multiple times.\n\nnetwork: name of a virtual network.\nip-addr: an unused IP address from the selected virtual network to assign to the interface; specify if the default gateway of the selected virtual network is in use.\n\n<router-name>\n\nVirtual router name\n\nFor example, to create a router myrouter between the physical network public and the virtual network private with enabled SNAT on the external gateway, run:# vinfra service compute router create myrouter --external-gateway public \\\r\n--internal-interface private --enable-snat\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: true                                |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | []                                               |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\r\n\nThe created router will appear in the vinfra service compute router list output:# vinfra service compute router list -c id -c external_gateway_info -c name -c status\r\n+---------------------+---------------------------------+----------+--------+\r\n| id                  | external_gateway_info           | name     | status |\r\n+---------------------+---------------------------------+----------+--------+\r\n| b9d8b000-5d06-<...> | enable_snat: true               | myrouter | ACTIVE |\r\n|                     | ip_addresses:                   |          |        |\r\n|                     | - 10.94.129.76                  |          |        |\r\n|                     | network_id: 720e45bc-4225-<...> |          |        |\r\n+---------------------+---------------------------------+----------+--------+\r\n\n\nSee also\n\nManaging router interfaces\n\nManaging static routes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router create [--external-gateway <network>]\r\n                                     [--enable-snat | --disable-snat]\r\n                                     [--fixed-ip <fixid-ip>]\r\n                                     [--internal-interface <network=network,ip-addr=ip-addr> |\r\n                                     <network>] <router-name>\r\n\n\n--external-gateway <network>\n\nSpecify a physical network to be used as the router\u00e2\u0080\u0099s external gateway (name or ID)\n--enable-snat\n\nEnable source NAT on the external gateway\n--disable-snat\n\nDisable source NAT on the external gateway\n--fixed-ip <fixid-ip>\n\nDesired IP on the external gateway\n--internal-interface <network=network,ip-addr=ip-addr>|<network>\n\n\nSpecify an internal interface. This option can be used multiple times.\n\nnetwork: name of a virtual network.\nip-addr: an unused IP address from the selected virtual network to assign to the interface; specify if the default gateway of the selected virtual network is in use.\n\n\n<router-name>\n\nVirtual router name\n\nFor example, to create a router myrouter between the physical network public and the virtual network private with enabled SNAT on the external gateway, run:# vinfra service compute router create myrouter --external-gateway public \\\r\n--internal-interface private --enable-snat\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: true                                |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | []                                               |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\r\n\nThe created router will appear in the vinfra service compute router list output:# vinfra service compute router list -c id -c external_gateway_info -c name -c status\r\n+---------------------+---------------------------------+----------+--------+\r\n| id                  | external_gateway_info           | name     | status |\r\n+---------------------+---------------------------------+----------+--------+\r\n| b9d8b000-5d06-<...> | enable_snat: true               | myrouter | ACTIVE |\r\n|                     | ip_addresses:                   |          |        |\r\n|                     | - 10.94.129.76                  |          |        |\r\n|                     | network_id: 720e45bc-4225-<...> |          |        |\r\n+---------------------+---------------------------------+----------+--------+\r\n\n",
                "title": "To create a virtual router"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Routers tab, click Add router.\n\nIn the Add router window:\n\nSpecify a router name.\nFrom the Network drop-down menu, select a physical network through which external access will be provided via an external gateway. The new external gateway will pick an unused IP address from the selected physical network.\nIn the Add internal interfaces section, select one or more virtual networks to connect to a router via internal interfaces. The new internal interfaces will attempt to use the gateway IP address of the selected virtual networks by default.\n\nSelect or deselect the SNAT check box to enable or disable SNAT on the external gateway of the router. With SNAT enabled, the router replaces VM private IP addresses with the public IP address of its external gateway.\n\n\n\n\n\n\n\n\nClick Create.\n\n\n",
                "title": "To create a virtual router"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-routers.html"
    },
    {
        "title": "Creating the compute cluster",
        "content": "Creating the compute cluster\nLimitations\n\nThe compute cluster must have at least three nodes to allow self-service users to enable high availability for Kubernetes master nodes.\n\nYou can create only one untagged network over an infrastructure network.\nA physical network MTU cannot exceed that of the underlying network interface.\nAfter the default storage policy is created, its redundancy type cannot be changed.\n\nPrerequisites\n\nThe storage cluster has at least one disk with the Storage role.\nBefore deploying the compute cluster with the metering service, ensure that you have tier 0 in your storage.\n\nTo create the compute cluster\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that these traffic types are added to the networks you intend to use: VM private, VM public, Compute API, VM backups.\nOpen the Compute screen, and then click Create compute cluster.\n\nOn the Nodes step, select the nodes to add to the compute cluster. You can only select nodes with the Configured network state. Nodes in the management node high availability cluster are automatically selected to join the compute cluster. Then, click Next.\n\nOn the Physical network step, do the following:\n\nEnable or disable IP address management:\n\nWith IP address management enabled, VMs connected to the network will automatically be assigned IP addresses from allocation pools by the built-in DHCP server and use custom DNS servers. Additionally, spoofing protection will be enabled for all VM network ports by default. Each VM network interface will be able to accept and send IP packets only if it has IP and MAC addresses assigned. You can disable spoofing protection manually for a VM interface, if required.\nWith IP address management disabled, VMs connected to the network will obtain IP addresses from the DHCP servers in that network, if any. Also, spoofing protection will be disabled for all VM network ports, and you cannot enable it manually. This means that each VM network interface, with or without assigned IP and MAC addresses, will be able to accept and send IP packets.\n\nIn any case, you will be able to manually assign static IP addresses from inside the VMs.\n\nProvide the required details for the physical network:\n\nSelect an infrastructure network to connect the physical network to.\nSelect the physical network type: select VLAN and specify a VLAN ID to create a VLAN-based network, or select Untagged to create a flat physical network.\nThe network MTU is set to 1500 by default. If required, you can adjust this value according to the MTU of the underlying network interface.\nIf you enabled IP address management, the subnet IP range in the CIDR format will be filled in automatically. Optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\n\nClick Next.\n\nThe selected physical network will appear in the list of compute networks on compute cluster\u00e2\u0080\u0099s Network tab. By default, it will be shared between all future projects. You can disable the network access on the network right pane later.\n\nIf you enabled IP address management, you will move on to the DHCP and DNS step, where you can configure the network settings for IP address management:\n\nEnable or disable the built-in DHCP server:\n\nWith the DHCP server enabled, VM network interfaces will automatically be assigned IP addresses: either from allocation pools or, if there are no pools, from the network\u00e2\u0080\u0099s entire IP range. The DHCP server will receive the first two IP addresses from the IP pool. For example:\n\n In a subnet with CIDR 192.168.128.0/24 and without a gateway, the DHCP server will be assigned the IP addresses 192.168.128.1 and 192.168.128.2.\n In a subnet with CIDR 192.168.128.0/24 and the gateway IP address set to 192.168.128.1, the DHCP server will be assigned the IP addresses 192.168.128.2 and 192.168.128.3.\n\nWith the DHCP server disabled, VM network interfaces will still get IP addresses, but you will have to manually assign them inside VMs.\n\nThe virtual DHCP service will work only within the current network and will not be exposed to other networks.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nSpecify DNS servers that will be used by virtual machines. These servers can be delivered to VMs via the built-in DHCP server or by using the cloud-init network configuration (if cloud-init is installed in the VM).\nClick Add.\n\nOn the Add-on services step, enable the additional services that will be installed during the compute cluster deployment. You can also install these services later. Then, click Next.\n\nInstalling Kubernetes automatically installs the load balancer service as well.\n\nOn the Storage policy step, select a redundancy mode, storage tier, and failure domain for the default policy, which will be applied to uploaded images and base volumes created from these images. You can also use the default parameters, which include the first available storage tier, the host failure domain, and the best replication scheme allowed by the number of nodes in the storage cluster:\n\nThe 3 replicas mode is used if the storage cluster has three or more nodes.\nThe 2 replicas mode is used if the storage cluster has two nodes.\nThe No redundancy mode is used for a single-node deployment.\n\nTo discard your changes to the storage policy parameters and reset them to their defaults, click Reset to default parameters.\nThen, click Next.\n\nOn the Summary step, review the configuration, and then click Create cluster.\n\nYou can monitor compute cluster deployment on the Compute screen.\n\nCommand-line interface\nUse the following command:vinfra service compute create [--public-network <network>] [--subnet cidr=CIDR[,key=value,\u00e2\u0080\u00a6]]\r\n                              [--vlan-id <vlan-id>] [--mtu <mtu>] [--force] [--enable-k8saas]\r\n                              [--enable-lbaas] [--enable-metering] [--notification-forwarding <transport-url>]\r\n                              [--disable-notification-forwarding] [--default-storage-policy-tier {0,1,2,3}]\r\n                              [--default-storage-policy-replicas <norm>[:<min>] | --default-storage-policy-encoding <M>+<N>]\r\n                              [--default-storage-policy-failure-domain {0,1,2,3,4}] --nodes <nodes>\n\n--public-network <network>\n\nAn infrastructure network to connect the compute physical network to. It must include the \"VM public\" traffic type.\n--subnet cidr=CIDR[,key=value,\u00e2\u0080\u00a6]\n\nSubnet for IP address management in the compute physical network (the --public-network option is required):\n\ncidr: subnet range in CIDR notation;\ncomma-separated key=value pairs with keys (optional):gateway: gateway IP address.dhcp: enable/disable the virtual DHCP server.allocation-pool: allocation pool of IP addresses from CIDR in the format ip1-ip2, where ip1 and ip2 are starting and ending IP addresses. Specify the key multiple times to create multiple IP pools.dns-server: DNS server IP address, specify multiple times to set multiple DNS servers.\n\nExample: --subnet cidr=192.168.5.0/24,dhcp=enable.\n\n--vlan-id <vlan-id>\n\nCreate VLAN-based physical network by the given VLAN ID.\n--mtu <mtu>\n\nCustom MTU value for the compute physical network.\n--force\n\nSkip checks for minimal hardware requirements.\n--enable-k8saas\n\nEnable Kubernetes-as-a-Service services.\n--enable-lbaas\n\nEnable Load-Balancing-as-a-Service services.\n--enable-metering\n\nEnable metering services.\n--notification-forwarding <transport-url>\n\nEnable notification forwarding through the specified transport URL in the format driver://[user:pass@]host:port[,[userN:passN@]hostN:portN]?query, where\n\ndriver is the supported transport driver (kafka)\nuser:pass are the username and password used for authentication with the messaging broker\nhost:port specifies the hostname or IP address and port number of the messaging broker\nquery are parameters that override those from the broker configuration file:topic specifies the topic namedriver is the messaging driver: messaging, messagingv2, routing, log, test, noop\n\nExample: kafka://10.10.10.10:9092?topic=notifications\n\n--disable-notification-forwarding\n\nDisable notification forwarding\n--default-storage-policy-tier {0,1,2,3}\n\nStorage tier\n--default-storage-policy-replicas <norm>[:<min>]\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n--default-storage-policy-encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n--default-storage-policy-failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--nodes <nodes>\n\nA comma-separated list of node IDs or hostnames.\n\nFor example, to create the compute cluster from five nodes that will use the infrastructure network Public with the IP address pool 10.94.129.64-10.94.129.79 to allocate to VMs, run:# vinfra service compute create --nodes node001,node002,node003,node004,node005 \\\r\n--public-network Public --subnet cidr=10.94.0.0/16,dhcp=enable,gateway=10.94.0.1,\\\r\nallocation-pool=10.94.129.64-10.94.129.79,dns-server=10.30.0.27\nYou can view the compute cluster details in the vinfra service compute show output:# vinfra service compute show\r\n+--------------+-----------------------------------------------------------------------------------------------------------------+\r\n| Field        | Value                                                                                                           |\r\n+--------------+-----------------------------------------------------------------------------------------------------------------+\r\n| capabilities | cpu_models:                                                                                                     |\r\n|              | - EPYC-IBPB                                                                                                     |\r\n|              | - Nehalem                                                                                                       |\r\n|              | - Nehalem-IBRS                                                                                                  |\r\n|              | - SandyBridge                                                                                                   |\r\n|              | - SandyBridge-IBRS                                                                                              |\r\n|              | - IvyBridge                                                                                                     |\r\n|              | - IvyBridge-IBRS                                                                                                |\r\n|              | - Haswell                                                                                                       |\r\n|              | - Haswell-IBRS                                                                                                  |\r\n|              | - Haswell-noTSX                                                                                                 |\r\n|              | - Haswell-noTSX-IBRS                                                                                            |\r\n|              | - Broadwell                                                                                                     |\r\n|              | - Broadwell-IBRS                                                                                                |\r\n|              | - Broadwell-noTSX                                                                                               |\r\n|              | - Broadwell-noTSX-IBRS                                                                                          |\r\n|              | - Skylake-Client                                                                                                |\r\n|              | - Skylake-Client-IBRS                                                                                           |\r\n|              | - Skylake-Server                                                                                                |\r\n|              | - Skylake-Server-IBRS                                                                                           |\r\n|              | - HostPassthrough                                                                                               |\r\n|              | k8saas_capabilities:                                                                                            |\r\n|              |   v1.18.6:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     release: v1.18                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.19.9                                                                                                   |\r\n|              |   v1.19.9:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://v1-19.docs.kubernetes.io/docs/setup/release/notes/#deprecation                       |\r\n|              |     release: v1.19                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.20.7                                                                                                   |\r\n|              |   v1.20.7:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://v1-20.docs.kubernetes.io/docs/setup/release/notes/#deprecation                       |\r\n|              |     release: v1.20                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.21.3                                                                                                   |\r\n|              |   v1.21.3:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation |\r\n|              |     release: v1.21                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.22.2                                                                                                   |\r\n|              |   v1.22.2:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation |\r\n|              |     release: v1.22                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.23.5                                                                                                   |\r\n|              |   v1.23.5:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation |\r\n|              |     release: v1.23                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.24.3                                                                                                   |\r\n|              |   v1.24.3:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation |\r\n|              |     release: v1.24                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.25.7                                                                                                   |\r\n|              |   v1.25.7:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation |\r\n|              |     release: v1.25                                                                                              |\r\n|              |     upgrade: []                                                                                                 |\r\n|              | k8saas_versions:                                                                                                |\r\n|              | - v1.25.7                                                                                                       |\r\n|              | - v1.24.3                                                                                                       |\r\n|              | - v1.23.5                                                                                                       |\r\n|              | - v1.22.2                                                                                                       |\r\n|              | os_distributions:                                                                                               |\r\n|              | - id: linux                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Generic Linux                                                                                          |\r\n|              | - id: rockylinux8                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Rocky Linux 8                                                                                          |\r\n|              | - id: rockylinux9                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Rocky Linux 9                                                                                          |\r\n|              | - id: almalinux8                                                                                                |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Alma Linux 8                                                                                           |\r\n|              | - id: almalinux9                                                                                                |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Alma Linux 9                                                                                           |\r\n|              | - id: centos8                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: CentOS 8                                                                                               |\r\n|              | - id: centos7                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: CentOS 7                                                                                               |\r\n|              | - id: centos6                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: CentOS 6                                                                                               |\r\n|              | - id: rhel9                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Red Hat Enterprise Linux 9                                                                             |\r\n|              | - id: rhel8                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Red Hat Enterprise Linux 8                                                                             |\r\n|              | - id: rhel7                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Red Hat Enterprise Linux 7                                                                             |\r\n|              | - id: ubuntu22.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 22.04                                                                                           |\r\n|              | - id: ubuntu20.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 20.04                                                                                           |\r\n|              | - id: ubuntu18.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 18.04                                                                                           |\r\n|              | - id: ubuntu16.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 16.04                                                                                           |\r\n|              | - id: debian10                                                                                                  |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Debian 10                                                                                              |\r\n|              | - id: debian9                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Debian 9                                                                                               |\r\n|              | - id: oracle7                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Oracle Linux 7                                                                                         |\r\n|              | - id: windows                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Generic Windows                                                                                        |\r\n|              | - id: win2k22                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2022                                                                                    |\r\n|              | - id: win2k19                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2019                                                                                    |\r\n|              | - id: win2k16                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2016                                                                                    |\r\n|              | - id: win2k12r2                                                                                                 |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2012 R2                                                                                 |\r\n|              | - id: win2k12                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2012                                                                                    |\r\n|              | - id: win2k8r2                                                                                                  |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2008 R2                                                                                 |\r\n|              | - id: win2k8                                                                                                    |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2008                                                                                    |\r\n|              | - id: win10                                                                                                     |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows 10                                                                                             |\r\n|              | - id: win8.1                                                                                                    |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows 8.1                                                                                            |\r\n| features     | []                                                                                                              |\r\n| options      | cpu_model: ''                                                                                                   |\r\n|              | custom_params: []                                                                                               |\r\n|              | endpoint_hostname: 10.10.10.10                                                                                  |\r\n|              | notification_forwarding: disabled                                                                               |\r\n|              | scheduler:                                                                                                      |\r\n|              |   cpu_weight_multiplier: 1.0                                                                                    |\r\n|              |   metrics_weight_multiplier: 1.0                                                                                |\r\n|              |   metrics_weight_setting: {}                                                                                    |\r\n|              |   pci_weight_multiplier: 1.0                                                                                    |\r\n|              |   ram_weight_multiplier: 1.0                                                                                    |\r\n|              |   soft_anti_affinity_weight_multiplier: 5.0                                                                     |\r\n| status       | active                                                                                                          |\r\n| storages     | - vstorage                                                                                                      |\r\n+--------------+-----------------------------------------------------------------------------------------------------------------+\n\nSee also\n\nCreating virtual machines\n\nWhat's next\n\nSetting virtual machine CPU model",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute create [--public-network <network>] [--subnet cidr=CIDR[,key=value,\u00e2\u0080\u00a6]]\r\n                              [--vlan-id <vlan-id>] [--mtu <mtu>] [--force] [--enable-k8saas]\r\n                              [--enable-lbaas] [--enable-metering] [--notification-forwarding <transport-url>]\r\n                              [--disable-notification-forwarding] [--default-storage-policy-tier {0,1,2,3}]\r\n                              [--default-storage-policy-replicas <norm>[:<min>] | --default-storage-policy-encoding <M>+<N>]\r\n                              [--default-storage-policy-failure-domain {0,1,2,3,4}] --nodes <nodes>\n\n--public-network <network>\n\nAn infrastructure network to connect the compute physical network to. It must include the \"VM public\" traffic type.\n--subnet cidr=CIDR[,key=value,\u00e2\u0080\u00a6]\n\n\nSubnet for IP address management in the compute physical network (the --public-network option is required):\n\ncidr: subnet range in CIDR notation;\ncomma-separated key=value pairs with keys (optional):gateway: gateway IP address.dhcp: enable/disable the virtual DHCP server.allocation-pool: allocation pool of IP addresses from CIDR in the format ip1-ip2, where ip1 and ip2 are starting and ending IP addresses. Specify the key multiple times to create multiple IP pools.dns-server: DNS server IP address, specify multiple times to set multiple DNS servers.\n\nExample: --subnet cidr=192.168.5.0/24,dhcp=enable.\n\n--vlan-id <vlan-id>\n\nCreate VLAN-based physical network by the given VLAN ID.\n--mtu <mtu>\n\nCustom MTU value for the compute physical network.\n--force\n\nSkip checks for minimal hardware requirements.\n--enable-k8saas\n\nEnable Kubernetes-as-a-Service services.\n--enable-lbaas\n\nEnable Load-Balancing-as-a-Service services.\n--enable-metering\n\nEnable metering services.\n--notification-forwarding <transport-url>\n\n\nEnable notification forwarding through the specified transport URL in the format driver://[user:pass@]host:port[,[userN:passN@]hostN:portN]?query, where\n\ndriver is the supported transport driver (kafka)\nuser:pass are the username and password used for authentication with the messaging broker\nhost:port specifies the hostname or IP address and port number of the messaging broker\nquery are parameters that override those from the broker configuration file:topic specifies the topic namedriver is the messaging driver: messaging, messagingv2, routing, log, test, noop\n\nExample: kafka://10.10.10.10:9092?topic=notifications\n\n--disable-notification-forwarding\n\nDisable notification forwarding\n--default-storage-policy-tier {0,1,2,3}\n\nStorage tier\n--default-storage-policy-replicas <norm>[:<min>]\n\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n\n--default-storage-policy-encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n\n--default-storage-policy-failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--nodes <nodes>\n\nA comma-separated list of node IDs or hostnames.\n\nFor example, to create the compute cluster from five nodes that will use the infrastructure network Public with the IP address pool 10.94.129.64-10.94.129.79 to allocate to VMs, run:# vinfra service compute create --nodes node001,node002,node003,node004,node005 \\\r\n--public-network Public --subnet cidr=10.94.0.0/16,dhcp=enable,gateway=10.94.0.1,\\\r\nallocation-pool=10.94.129.64-10.94.129.79,dns-server=10.30.0.27\nYou can view the compute cluster details in the vinfra service compute show output:# vinfra service compute show\r\n+--------------+-----------------------------------------------------------------------------------------------------------------+\r\n| Field        | Value                                                                                                           |\r\n+--------------+-----------------------------------------------------------------------------------------------------------------+\r\n| capabilities | cpu_models:                                                                                                     |\r\n|              | - EPYC-IBPB                                                                                                     |\r\n|              | - Nehalem                                                                                                       |\r\n|              | - Nehalem-IBRS                                                                                                  |\r\n|              | - SandyBridge                                                                                                   |\r\n|              | - SandyBridge-IBRS                                                                                              |\r\n|              | - IvyBridge                                                                                                     |\r\n|              | - IvyBridge-IBRS                                                                                                |\r\n|              | - Haswell                                                                                                       |\r\n|              | - Haswell-IBRS                                                                                                  |\r\n|              | - Haswell-noTSX                                                                                                 |\r\n|              | - Haswell-noTSX-IBRS                                                                                            |\r\n|              | - Broadwell                                                                                                     |\r\n|              | - Broadwell-IBRS                                                                                                |\r\n|              | - Broadwell-noTSX                                                                                               |\r\n|              | - Broadwell-noTSX-IBRS                                                                                          |\r\n|              | - Skylake-Client                                                                                                |\r\n|              | - Skylake-Client-IBRS                                                                                           |\r\n|              | - Skylake-Server                                                                                                |\r\n|              | - Skylake-Server-IBRS                                                                                           |\r\n|              | - HostPassthrough                                                                                               |\r\n|              | k8saas_capabilities:                                                                                            |\r\n|              |   v1.18.6:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     release: v1.18                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.19.9                                                                                                   |\r\n|              |   v1.19.9:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://v1-19.docs.kubernetes.io/docs/setup/release/notes/#deprecation                       |\r\n|              |     release: v1.19                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.20.7                                                                                                   |\r\n|              |   v1.20.7:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://v1-20.docs.kubernetes.io/docs/setup/release/notes/#deprecation                       |\r\n|              |     release: v1.20                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.21.3                                                                                                   |\r\n|              |   v1.21.3:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation |\r\n|              |     release: v1.21                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.22.2                                                                                                   |\r\n|              |   v1.22.2:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation |\r\n|              |     release: v1.22                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.23.5                                                                                                   |\r\n|              |   v1.23.5:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation |\r\n|              |     release: v1.23                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.24.3                                                                                                   |\r\n|              |   v1.24.3:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation |\r\n|              |     release: v1.24                                                                                              |\r\n|              |     upgrade:                                                                                                    |\r\n|              |     - v1.25.7                                                                                                   |\r\n|              |   v1.25.7:                                                                                                      |\r\n|              |     features:                                                                                                   |\r\n|              |     - nodegroups                                                                                                |\r\n|              |     links:                                                                                                      |\r\n|              |       deprecation: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation |\r\n|              |     release: v1.25                                                                                              |\r\n|              |     upgrade: []                                                                                                 |\r\n|              | k8saas_versions:                                                                                                |\r\n|              | - v1.25.7                                                                                                       |\r\n|              | - v1.24.3                                                                                                       |\r\n|              | - v1.23.5                                                                                                       |\r\n|              | - v1.22.2                                                                                                       |\r\n|              | os_distributions:                                                                                               |\r\n|              | - id: linux                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Generic Linux                                                                                          |\r\n|              | - id: rockylinux8                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Rocky Linux 8                                                                                          |\r\n|              | - id: rockylinux9                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Rocky Linux 9                                                                                          |\r\n|              | - id: almalinux8                                                                                                |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Alma Linux 8                                                                                           |\r\n|              | - id: almalinux9                                                                                                |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Alma Linux 9                                                                                           |\r\n|              | - id: centos8                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: CentOS 8                                                                                               |\r\n|              | - id: centos7                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: CentOS 7                                                                                               |\r\n|              | - id: centos6                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: CentOS 6                                                                                               |\r\n|              | - id: rhel9                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Red Hat Enterprise Linux 9                                                                             |\r\n|              | - id: rhel8                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Red Hat Enterprise Linux 8                                                                             |\r\n|              | - id: rhel7                                                                                                     |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Red Hat Enterprise Linux 7                                                                             |\r\n|              | - id: ubuntu22.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 22.04                                                                                           |\r\n|              | - id: ubuntu20.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 20.04                                                                                           |\r\n|              | - id: ubuntu18.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 18.04                                                                                           |\r\n|              | - id: ubuntu16.04                                                                                               |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Ubuntu 16.04                                                                                           |\r\n|              | - id: debian10                                                                                                  |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Debian 10                                                                                              |\r\n|              | - id: debian9                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Debian 9                                                                                               |\r\n|              | - id: oracle7                                                                                                   |\r\n|              |   os_type: linux                                                                                                |\r\n|              |   title: Oracle Linux 7                                                                                         |\r\n|              | - id: windows                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Generic Windows                                                                                        |\r\n|              | - id: win2k22                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2022                                                                                    |\r\n|              | - id: win2k19                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2019                                                                                    |\r\n|              | - id: win2k16                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2016                                                                                    |\r\n|              | - id: win2k12r2                                                                                                 |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2012 R2                                                                                 |\r\n|              | - id: win2k12                                                                                                   |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2012                                                                                    |\r\n|              | - id: win2k8r2                                                                                                  |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2008 R2                                                                                 |\r\n|              | - id: win2k8                                                                                                    |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows Server 2008                                                                                    |\r\n|              | - id: win10                                                                                                     |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows 10                                                                                             |\r\n|              | - id: win8.1                                                                                                    |\r\n|              |   os_type: windows                                                                                              |\r\n|              |   title: Windows 8.1                                                                                            |\r\n| features     | []                                                                                                              |\r\n| options      | cpu_model: ''                                                                                                   |\r\n|              | custom_params: []                                                                                               |\r\n|              | endpoint_hostname: 10.10.10.10                                                                                  |\r\n|              | notification_forwarding: disabled                                                                               |\r\n|              | scheduler:                                                                                                      |\r\n|              |   cpu_weight_multiplier: 1.0                                                                                    |\r\n|              |   metrics_weight_multiplier: 1.0                                                                                |\r\n|              |   metrics_weight_setting: {}                                                                                    |\r\n|              |   pci_weight_multiplier: 1.0                                                                                    |\r\n|              |   ram_weight_multiplier: 1.0                                                                                    |\r\n|              |   soft_anti_affinity_weight_multiplier: 5.0                                                                     |\r\n| status       | active                                                                                                          |\r\n| storages     | - vstorage                                                                                                      |\r\n+--------------+-----------------------------------------------------------------------------------------------------------------+\n",
                "title": "To create the compute cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that these traffic types are added to the networks you intend to use: VM private, VM public, Compute API, VM backups.\nOpen the Compute screen, and then click Create compute cluster.\n\nOn the Nodes step, select the nodes to add to the compute cluster. You can only select nodes with the Configured network state. Nodes in the management node high availability cluster are automatically selected to join the compute cluster. Then, click Next.\n\n\n\n\n\n\nOn the Physical network step, do the following:\n\n\nEnable or disable IP address management:\n\nWith IP address management enabled, VMs connected to the network will automatically be assigned IP addresses from allocation pools by the built-in DHCP server and use custom DNS servers. Additionally, spoofing protection will be enabled for all VM network ports by default. Each VM network interface will be able to accept and send IP packets only if it has IP and MAC addresses assigned. You can disable spoofing protection manually for a VM interface, if required.\nWith IP address management disabled, VMs connected to the network will obtain IP addresses from the DHCP servers in that network, if any. Also, spoofing protection will be disabled for all VM network ports, and you cannot enable it manually. This means that each VM network interface, with or without assigned IP and MAC addresses, will be able to accept and send IP packets.\n\nIn any case, you will be able to manually assign static IP addresses from inside the VMs.\n\n\nProvide the required details for the physical network:\n\nSelect an infrastructure network to connect the physical network to.\nSelect the physical network type: select VLAN and specify a VLAN ID to create a VLAN-based network, or select Untagged to create a flat physical network.\nThe network MTU is set to 1500 by default. If required, you can adjust this value according to the MTU of the underlying network interface.\nIf you enabled IP address management, the subnet IP range in the CIDR format will be filled in automatically. Optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\n\n\nClick Next.\n\n\n\n\n\nThe selected physical network will appear in the list of compute networks on compute cluster\u00e2\u0080\u0099s Network tab. By default, it will be shared between all future projects. You can disable the network access on the network right pane later.\n\n\nIf you enabled IP address management, you will move on to the DHCP and DNS step, where you can configure the network settings for IP address management:\n\n\nEnable or disable the built-in DHCP server:\n\n\nWith the DHCP server enabled, VM network interfaces will automatically be assigned IP addresses: either from allocation pools or, if there are no pools, from the network\u00e2\u0080\u0099s entire IP range. The DHCP server will receive the first two IP addresses from the IP pool. For example:\n\n In a subnet with CIDR 192.168.128.0/24 and without a gateway, the DHCP server will be assigned the IP addresses 192.168.128.1 and 192.168.128.2.\n In a subnet with CIDR 192.168.128.0/24 and the gateway IP address set to 192.168.128.1, the DHCP server will be assigned the IP addresses 192.168.128.2 and 192.168.128.3.\n\n\nWith the DHCP server disabled, VM network interfaces will still get IP addresses, but you will have to manually assign them inside VMs.\n\nThe virtual DHCP service will work only within the current network and will not be exposed to other networks.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nSpecify DNS servers that will be used by virtual machines. These servers can be delivered to VMs via the built-in DHCP server or by using the cloud-init network configuration (if cloud-init is installed in the VM).\nClick Add.\n\n\n\n\n\n\n\nOn the Add-on services step, enable the additional services that will be installed during the compute cluster deployment. You can also install these services later. Then, click Next.\n\nInstalling Kubernetes automatically installs the load balancer service as well.\n\n\n\n\n\n\n\nOn the Storage policy step, select a redundancy mode, storage tier, and failure domain for the default policy, which will be applied to uploaded images and base volumes created from these images. You can also use the default parameters, which include the first available storage tier, the host failure domain, and the best replication scheme allowed by the number of nodes in the storage cluster:\n\nThe 3 replicas mode is used if the storage cluster has three or more nodes.\nThe 2 replicas mode is used if the storage cluster has two nodes.\nThe No redundancy mode is used for a single-node deployment.\n\nTo discard your changes to the storage policy parameters and reset them to their defaults, click Reset to default parameters.\nThen, click Next.\n\n\n\n\n\nOn the Summary step, review the configuration, and then click Create cluster.\n\nYou can monitor compute cluster deployment on the Compute screen.\n",
                "title": "To create the compute cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-the-compute-cluster.html"
    },
    {
        "title": "Creating the S3 cluster",
        "content": "Creating the S3 cluster\nLimitations\n\nAfter the S3 cluster deployment, you can change only the replication redundancy scheme for data. Changing the encoding redundancy scheme for data is disabled, because it may decrease cluster performance. Re-encoding demands a significant amount of cluster resources for a long period of time. If you still want to change the redundancy scheme for data, contact the technical support team.\n\nPrerequisites\n\nA clear understanding of the concept Storage policies.\nThe storage cluster has at least one disk with the Storage role.\nEnsure that each node to join the object storage cluster has the TCP port 443 (HTTPS) or TCP port 80 (HTTP) open for outgoing and incoming Internet connections.\n\nTo set up object storage services on cluster nodes\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the OSTOR private and S3 public traffic types are added to the networks you intend to use.\nOpen the Storage services > S3 screen, and then click Create S3 storage.\nOn the Nodes step, select nodes to add to the S3 storage, and then click Next. To create highly available S3 storage, select at least three nodes.\n\nOn the Storage policies step, define storage policies for S3 data and metadata:\n\nIn the Data storage policy section, select the desired tier, failure domain, and data redundancy mode for storing S3 data. To benefit from high availability, select a mode other than No redundancy and a failure domain other than Disk.\n\nIn the Metadata storage policy section, select the desired tier for storing S3 metadata, which includes NS and OS journals. It is highly recommended to place metadata on a faster storage tier than is used for data, to improve the service performance.\n\nOn the DNS step, specify an external DNS name for the S3 storage. For example, s3storage.example.com. End users will use this DNS name and the TCP port 443 (HTTPS) or TCP port 80 (HTTP) to access the S3 data. Then, click Next.\n\nDNS load balancing can be used for test purposes only. For production, use an external load balancer.\n\nOn the Protocol step, select an S3 endpoint protocol: HTTP, HTTPS, or both.\n\nIt is recommended to use only HTTPS for production deployments.\n\nIf you selected the HTTPS protocol, do one of the following: \n\nSelect Upload a certificate, specify the prepared SSL certificate, and then specify the SSL key or passphrase (for PKCS#12 files).\nYou need to acquire a key and a trusted wildcard SSL certificate for endpoint\u00e2\u0080\u0099s bottom-level domain. For example, the endpoint s3storage.example.com would need a wildcard certificate for *.s3storage.example.com with the subject alternative name s3storage.example.com.\nIf you acquired an SSL certificate from an intermediate certificate authority (CA)\n\nYou should have an end-user certificate along with a CA bundle that contains the root and intermediate certificates. To be able to use these certificates, you need to merge them into a chain first. A certificate chain includes the end-user certificate, the certificates of intermediate CAs, and the certificate of a trusted root CA. In this case, an SSL certificate can only be trusted if every certificate in the chain is properly issued and valid.\nFor example, if you have an end-user certificate, two intermediate CA certificates, and a root CA certificate, create a new certificate file and add all certificates to it in the following order:# End-user certificate issued by the intermediate CA 1\r\n-----BEGIN CERTIFICATE-----\r\nMIICiDCCAg2gAwIBAgIQNfwmXNmET8k9Jj1X<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 1 certificate issued by the intermediate CA 2\r\n-----BEGIN CERTIFICATE-----\r\nMIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9ON9<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 2 certificate issued by the root CA\r\n-----BEGIN CERTIFICATE-----\r\nMIIC8jCCAdqgAwIBAgICZngwDQYJKoZIhvcN<...>\r\n-----END CERTIFICATE-----\r\n# Root CA certificate\r\n-----BEGIN CERTIFICATE-----\r\nMIIDODCCAiCgAwIBAgIGIAYFFnACMA0GCSqG<...>\r\n-----END CERTIFICATE-----\r\n\n\nSelect Generate a certificate, to get a self-signed certificate for HTTPS evaluation purposes.\n\nS3 geo-replication requires a certificate from a trusted authority. It does not work with self-signed certificates.\nTo access the data in the S3 cluster via a browser, add the self-signed certificate to browser\u00e2\u0080\u0099s exceptions.\n\nThen, click Next.\n\nOn the Summary step, review the configuration, and then click Create.\n\nTo check if the S3 storage is successfully deployed and can be accessed by users, visit https://<S3_DNS_name> or http://<S3_DNS_name> in your browser. You should receive the following XML response:<Error>\r\n<Code>AccessDenied</Code>\r\n<Message/>\r\n</Error>\r\n\nTo start using the S3 storage, you will also need to create at least one S3 user.\n\nCommand-line interface\nUse the following command:vinfra service s3 cluster create [--tier {0,1,2,3}] [--failure-domain {0,1,2,3,4}]\r\n                                 [--replicas <norm> | --encoding <M>+<N>] [--metadata-tier {0,1,2,3}]\r\n                                 [--self-signed | --no-ssl | --cert-file <cert_file>]\r\n                                 [--insecure] [--key-file <key_file>] [--password]\r\n                                 --nodes <nodes> --s3gw-domain <domain> --s3gw-count <s3gw_count>\r\n                                 --os-count <os_count> --ns-count <ns_count>\n\n--tier {0,1,2,3}\n\nStorage tier (default: 0)\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain (default: 0)\n--replicas <norm>\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain (default: 1)\n\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: the number of data blocks\nN: the number of parity blocks\n\n--metadata-tier {0,1,2,3}\n\nStorage tier\n--self-signed\n\n        Generate a new self-signed certificate (default)\n--no-ssl\n\nDo not generate a self-signed certificate\n--cert-file <cert_file>\n\nPath to a file with the new certificate\n--insecure\n\nAllow insecure connections in addition to secure ones (only used with the --cert-file and --self-signed options)\n--key-file <key_file>\n\nPath to a file with the private key (only used with the --cert-file option)\n--password\n\nRead certificate password from stdin (only used with the --cert-file option)\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n--s3gw-domain <domain>\n\nDNS name S3 endpoint\n--s3gw-count <s3gw_count>\n\nNumber of S3 gateways\n--os-count <os_count>\n\n  Amount of OS services in S3 cluster\n--ns-count <ns_count>\n\n  Amount of NS services in S3 cluster\n\nIncreasing the number of gateways, NS, and OS services also increases the resource requirements. To learn more about CPU and RAM reservations for the S3 services, refer to General requirements.\n\nFor example, to create the S3 cluster from nodes node001 and node002 with a self-signed certificate, run:# vinfra service s3 cluster create --nodes node001,node002 --tier 0 --failure-domain 1 --encoding 1+2 \\\r\n--metadata-tier 1 --self-signed --s3gw-domain dns.example.com\nThis command also specifies the tier, failure domain, redundancy mode, and domain name.\nYou can view the S3 storage details in the vinfra service s3 show output:# vinfra service s3 show\r\n+-----------------+--------------------------------------------+\r\n| Field           | Value                                      |\r\n+-----------------+--------------------------------------------+\r\n| failure_domain  | 1                                          |\r\n| id              | 0100000000000002                           |\r\n| metadata_policy | failure_domain: 1                          |\r\n|                 | redundancy:                                |\r\n|                 |   m: 1                                     |\r\n|                 |   n: 2                                     |\r\n|                 |   type: raid6                              |\r\n|                 | tier: 1                                    |\r\n| name            | cluster1                                   |\r\n| nodes           | - id: ca334b1d-20a1-1241-96a5-eb9acadb8ecd |\r\n|                 | - id: ab36b523-91dc-e78d-53a7-88baed44541e |\r\n| np              |                                            |\r\n| nusers          | 0                                          |\r\n| protocol        | scheme: https                              |\r\n| redundancy      | m: 1                                       |\r\n|                 | n: 2                                       |\r\n|                 | type: raid6                                |\r\n| s3gw_domain     | dns.example.com                            |\r\n| tier            | 0                                          |\r\n+-----------------+--------------------------------------------+\n\nWhat's next\n\nAdding S3 users",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service s3 cluster create [--tier {0,1,2,3}] [--failure-domain {0,1,2,3,4}]\r\n                                 [--replicas <norm> | --encoding <M>+<N>] [--metadata-tier {0,1,2,3}]\r\n                                 [--self-signed | --no-ssl | --cert-file <cert_file>]\r\n                                 [--insecure] [--key-file <key_file>] [--password]\r\n                                 --nodes <nodes> --s3gw-domain <domain> --s3gw-count <s3gw_count>\r\n                                 --os-count <os_count> --ns-count <ns_count>\n\n--tier {0,1,2,3}\n\nStorage tier (default: 0)\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain (default: 0)\n--replicas <norm>\n\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain (default: 1)\n\n\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: the number of data blocks\nN: the number of parity blocks\n\n\n--metadata-tier {0,1,2,3}\n\nStorage tier\n--self-signed\n\n        Generate a new self-signed certificate (default)\n--no-ssl\n\nDo not generate a self-signed certificate\n--cert-file <cert_file>\n\nPath to a file with the new certificate\n--insecure\n\nAllow insecure connections in addition to secure ones (only used with the --cert-file and --self-signed options)\n--key-file <key_file>\n\nPath to a file with the private key (only used with the --cert-file option)\n--password\n\nRead certificate password from stdin (only used with the --cert-file option)\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n--s3gw-domain <domain>\n\nDNS name S3 endpoint\n--s3gw-count <s3gw_count>\n\nNumber of S3 gateways\n--os-count <os_count>\n\n  Amount of OS services in S3 cluster\n--ns-count <ns_count>\n\n  Amount of NS services in S3 cluster\n\n\nIncreasing the number of gateways, NS, and OS services also increases the resource requirements. To learn more about CPU and RAM reservations for the S3 services, refer to General requirements.\n\nFor example, to create the S3 cluster from nodes node001 and node002 with a self-signed certificate, run:# vinfra service s3 cluster create --nodes node001,node002 --tier 0 --failure-domain 1 --encoding 1+2 \\\r\n--metadata-tier 1 --self-signed --s3gw-domain dns.example.com\nThis command also specifies the tier, failure domain, redundancy mode, and domain name.\nYou can view the S3 storage details in the vinfra service s3 show output:# vinfra service s3 show\r\n+-----------------+--------------------------------------------+\r\n| Field           | Value                                      |\r\n+-----------------+--------------------------------------------+\r\n| failure_domain  | 1                                          |\r\n| id              | 0100000000000002                           |\r\n| metadata_policy | failure_domain: 1                          |\r\n|                 | redundancy:                                |\r\n|                 |   m: 1                                     |\r\n|                 |   n: 2                                     |\r\n|                 |   type: raid6                              |\r\n|                 | tier: 1                                    |\r\n| name            | cluster1                                   |\r\n| nodes           | - id: ca334b1d-20a1-1241-96a5-eb9acadb8ecd |\r\n|                 | - id: ab36b523-91dc-e78d-53a7-88baed44541e |\r\n| np              |                                            |\r\n| nusers          | 0                                          |\r\n| protocol        | scheme: https                              |\r\n| redundancy      | m: 1                                       |\r\n|                 | n: 2                                       |\r\n|                 | type: raid6                                |\r\n| s3gw_domain     | dns.example.com                            |\r\n| tier            | 0                                          |\r\n+-----------------+--------------------------------------------+\n",
                "title": "To set up object storage services on cluster nodes"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, make sure that the OSTOR private and S3 public traffic types are added to the networks you intend to use.\nOpen the Storage services > S3 screen, and then click Create S3 storage.\nOn the Nodes step, select nodes to add to the S3 storage, and then click Next. To create highly available S3 storage, select at least three nodes.\n\nOn the Storage policies step, define storage policies for S3 data and metadata:\n\n\nIn the Data storage policy section, select the desired tier, failure domain, and data redundancy mode for storing S3 data. To benefit from high availability, select a mode other than No redundancy and a failure domain other than Disk.\n\n\n\n\n\n\nIn the Metadata storage policy section, select the desired tier for storing S3 metadata, which includes NS and OS journals. It is highly recommended to place metadata on a faster storage tier than is used for data, to improve the service performance.\n\n\n\n\n\n\n\n\nOn the DNS step, specify an external DNS name for the S3 storage. For example, s3storage.example.com. End users will use this DNS name and the TCP port 443 (HTTPS) or TCP port 80 (HTTP) to access the S3 data. Then, click Next.\n\nDNS load balancing can be used for test purposes only. For production, use an external load balancer.\n\n\n\nOn the Protocol step, select an S3 endpoint protocol: HTTP, HTTPS, or both.\n\nIt is recommended to use only HTTPS for production deployments.\n\n\n\n\n\nIf you selected the HTTPS protocol, do one of the following: \n\n\nSelect Upload a certificate, specify the prepared SSL certificate, and then specify the SSL key or passphrase (for PKCS#12 files).\nYou need to acquire a key and a trusted wildcard SSL certificate for endpoint\u00e2\u0080\u0099s bottom-level domain. For example, the endpoint s3storage.example.com would need a wildcard certificate for *.s3storage.example.com with the subject alternative name s3storage.example.com.\nIf you acquired an SSL certificate from an intermediate certificate authority (CA)\n\nYou should have an end-user certificate along with a CA bundle that contains the root and intermediate certificates. To be able to use these certificates, you need to merge them into a chain first. A certificate chain includes the end-user certificate, the certificates of intermediate CAs, and the certificate of a trusted root CA. In this case, an SSL certificate can only be trusted if every certificate in the chain is properly issued and valid.\nFor example, if you have an end-user certificate, two intermediate CA certificates, and a root CA certificate, create a new certificate file and add all certificates to it in the following order:# End-user certificate issued by the intermediate CA 1\r\n-----BEGIN CERTIFICATE-----\r\nMIICiDCCAg2gAwIBAgIQNfwmXNmET8k9Jj1X<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 1 certificate issued by the intermediate CA 2\r\n-----BEGIN CERTIFICATE-----\r\nMIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9ON9<...>\r\n-----END CERTIFICATE-----\r\n# Intermediate CA 2 certificate issued by the root CA\r\n-----BEGIN CERTIFICATE-----\r\nMIIC8jCCAdqgAwIBAgICZngwDQYJKoZIhvcN<...>\r\n-----END CERTIFICATE-----\r\n# Root CA certificate\r\n-----BEGIN CERTIFICATE-----\r\nMIIDODCCAiCgAwIBAgIGIAYFFnACMA0GCSqG<...>\r\n-----END CERTIFICATE-----\r\n\n\n\n\n\nSelect Generate a certificate, to get a self-signed certificate for HTTPS evaluation purposes.\n\n\nS3 geo-replication requires a certificate from a trusted authority. It does not work with self-signed certificates.\nTo access the data in the S3 cluster via a browser, add the self-signed certificate to browser\u00e2\u0080\u0099s exceptions.\n\n\n\n\nThen, click Next.\n\nOn the Summary step, review the configuration, and then click Create.\n\nTo check if the S3 storage is successfully deployed and can be accessed by users, visit https://<S3_DNS_name> or http://<S3_DNS_name> in your browser. You should receive the following XML response:<Error>\r\n<Code>AccessDenied</Code>\r\n<Message/>\r\n</Error>\r\n\nTo start using the S3 storage, you will also need to create at least one S3 user.\n",
                "title": "To set up object storage services on cluster nodes"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-the-s3-cluster.html"
    },
    {
        "title": "Creating VLAN interfaces",
        "content": "Creating VLAN interfaces\nAccording to the requirements listed in Compute cluster requirements, it is recommended to configure VLAN interfaces over network bonds for the compute cluster. You can create these logical interfaces on the Network interfaces tab of the node screen. Additionally, VLAN network interfaces on the compute nodes can be created with the automated procedure outlined in Connecting virtual switches to trunk interfaces.\nTo create a VLAN interface\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click Create.\nIn the Create network interface window, select the VLAN type and a network interface to create a VLAN interface from, and then click Next.\nSpecify a number for VLAN in the VLAN ID field. You can choose a number in the range from 1 to 4094.\n\nSelect a network to assign the VLAN to, and then specify the network parameters:\n\nSelect Automatically (DHCP) to obtain the IP address, DNS, and routing settings from the DHCP server.\nSelect Automatically (DHCP address only) to obtain only the IP address from the DHCP server.\nSelect Manually, and then specify the IP address in CIDR notation by clicking Add.\n\nDynamic IP address allocation will cause network issues as soon as the IP addresses of cluster nodes change. Configure static IP addresses from the start or as soon as possible.\n\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\n\nEnter the desired MTU value in the MTU field. If you leave Auto, the parent interface MTU will be used.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra node iface create-vlan [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                              [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                              [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                              [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                              [--network <network>] [--node <node>] --iface <iface> --tag <tag>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n--iface <iface>\n\nInterface name\n--tag <tag>\n\nVLAN tag number\n\nFor example, to create a VLAN interface with the tag 100 on the network interface eth2 on the node node002, run:# vinfra node iface create-vlan --iface eth2 --tag 100 --dhcp4 --node node002\n\nSee also\n\nChanging network interface parameters\n\nManaging network interfaces\n\nWhat's next\n\nAdding external DNS servers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface create-vlan [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                              [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                              [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                              [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                              [--network <network>] [--node <node>] --iface <iface> --tag <tag>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n--iface <iface>\n\nInterface name\n--tag <tag>\n\nVLAN tag number\n\nFor example, to create a VLAN interface with the tag 100 on the network interface eth2 on the node node002, run:# vinfra node iface create-vlan --iface eth2 --tag 100 --dhcp4 --node node002\n",
                "title": "To create a VLAN interface"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click Create.\nIn the Create network interface window, select the VLAN type and a network interface to create a VLAN interface from, and then click Next.\nSpecify a number for VLAN in the VLAN ID field. You can choose a number in the range from 1 to 4094.\n\nSelect a network to assign the VLAN to, and then specify the network parameters:\n\nSelect Automatically (DHCP) to obtain the IP address, DNS, and routing settings from the DHCP server.\nSelect Automatically (DHCP address only) to obtain only the IP address from the DHCP server.\nSelect Manually, and then specify the IP address in CIDR notation by clicking Add.\n\n\nDynamic IP address allocation will cause network issues as soon as the IP addresses of cluster nodes change. Configure static IP addresses from the start or as soon as possible.\n\n\n\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\n\n\nEnter the desired MTU value in the MTU field. If you leave Auto, the parent interface MTU will be used.\n\nClick Create.\n\n\n\n\n\n",
                "title": "To create a VLAN interface"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-vlan-interfaces.html"
    },
    {
        "title": "Creating virtual machines",
        "content": "Creating virtual machines\nLimitations\n\nVirtual machines are created with the host CPU model, by default. Having compute nodes with different CPUs may lead to live migration issues. To avoid them, you can manually set the CPU model for all new VMs, as described in Setting virtual machine CPU model. Alternatively, you can create a placement for each group of compute nodes with the same CPU model by using the instructions in Managing placements for compute nodes.\n\nUEFI boot is not supported for CentOS 7.x virtual machines with less than 1 GiB of RAM.\n\nPrerequisites\n\nYou have a guest OS source prepared, as described in Preparing boot media for virtual machines.\nOne or more compute networks are created automatically during the compute cluster deployment or manually by using the instructions in Creating physical compute networks and Creating virtual compute networks.\n\nCustom security groups are configured, as instructed in Managing security groups.\n\nYou have a custom flavor created, as described in Creating custom flavors for virtual machines. You can also use preconfigured flavors.\n\nAn SSH key is added to the compute cluster, as outlined in Adding SSH keys for virtual machines. You can specify an SSH key only when creating VMs from a template or boot volume.\n\nA custom storage policy is created for volumes, as described in Managing storage policies.\n\nTo create a virtual machine\n\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click Create virtual machine. A window will open where you will need to specify the VM parameters.\n\nSpecify a name for the new VM.\n\nSelect the VM boot media:\n\nIf you have an ISO image or a template\n\nSelect Image in the Deploy from section, and then click Specify in the Image section.\n\nIn the Images window, select the ISO image or template, and then click Done.\n\nIf you have a compute boot volume\n\nSelect Volume in the Deploy from section, and then click Specify in the Volumes section.\nIn the Volumes window, click Attach.\n\nIn the Attach volume window, find and select the volume, and then click Attach.\n\nIf you attach more than one volume, the first attached volume becomes the boot volume, by default. To select another volume as bootable, place it first in the list by clicking the up arrow button next to it.\n\nIf you select an image or volume with an assigned placement, the created VM will also inherit this placement.\n\nAfter selecting the boot media, volumes required for this media to boot will be automatically added to the Volumes section.\n\nConfigure the VM disks:\n\nIn the Volumes window, make sure the default boot volume is large enough to accommodate the guest OS. Otherwise, click the ellipsis icon next to it, and then Edit. Change the volume size and click Save.\n\nAdd more disks to the VM by creating or attaching volumes. To do this, click the pencil icon in the Volumes section, and then Add or Attach in the Volumes window.\n\nSelect volumes that will be removed during the VM deletion. To do this, click the pencil icon in the Volumes section, click the ellipsis icon next to the needed volume, and then Edit. Enable Delete on termination and click Save.\nWhen you finish configuring the VM disks, click Done.\n\nChoose the amount of RAM and CPU resources that will be allocated to the VM in the Flavor section. In the Flavor window, select a flavor, and then click Done.\n\nWhen choosing a flavor for a VM, ensure it satisfies the hardware requirements of the guest OS.\n\nTo select a flavor with an assigned placement, you can filter flavors by placement. The VM created from such a flavor will also inherit this placement.\n\nAdd network interfaces to the VM in the Networks section:\n\nIn the Network interfaces window, click Add to attach a network interface.\n\nIn the Add network interface window, select a compute network to connect to, and then specify MAC address, IPv4 and/or IPv6 addresses, and security groups. By default, MAC and primary IP addresses are assigned automatically. To specify them manually, clear the Assign automatically check boxes, and enter the desired addresses. Optionally, assign additional IP addresses to the network interface in the Secondary IP addresses section. Note that a secondary IPv6 address is not available for an IPv6 subnet that works in the SLAAC or DHCPv6 stateless mode.\n\nSecondary IP addresses, unlike the primary one, will not be automatically assigned to the network interface inside the virtual machine guest OS. You should assign them manually.\n\nIf you selected a network with enabled IP address management\n\nIn this case, spoofing protection is enabled and the default security group is selected by default. This security group allows all incoming and outgoing traffic on all the VM ports. If required, you can select another security group or multiple security groups.\nTo disable spoofing protection, clear all of the check boxes and turn off the toggle switch. Security groups cannot be configured with disabled spoofing protection.\n\nIf you selected a network with disabled IP address management\nIn this case, spoofing protection is disabled by default and cannot be enabled. Security groups cannot be configured for such a network.\n\nAfter specifying the network interface parameters, click Add. The network interface will appear in the Network interfaces list.\n\nIf required, edit IP addresses and security groups of newly added network interfaces. To do this, click the ellipsis icon, click Edit, and then set the parameters.\n\nWhen you finish configuring the VM network interfaces, click Done.\n\nIf you have chosen to boot from a template or volume, which has cloud-init and OpenSSH installed:\n\nAs cloud images have no default password, you can access VMs deployed from them only by using the key authentication method with SSH.\n\nAdd an SSH key to the VM, to be able to access it via SSH without a password. \n\nIn the Select an SSH key window, select an SSH key  and then click Done.\n\nAdd user data to customize the VM after launch, for example, change a user password. \n\nWrite a cloud-config or shell script in the Customization script field or browse a file on your local server to load the script from.\n\nTo inject a script in a Windows VM, refer to the Cloudbase-Init documentation. For example, you can set a new password for the account using the following script:#ps1\r\nnet user <username> <new_password>\r\n\n\nEnable CPU and RAM hot plug for the VM in Advanced options, to be able to change its flavor when the VM is running. You can also enable hot plug after the VM is created.\n\nIf you have chosen to boot from an ISO image, enable UEFI boot in Advanced options, to be able to boot the VM in the UEFI mode. This option cannot be configured after the VM is created.\n\nYou cannot configure UEFI boot if you have selected a template as the VM boot media. If your template has UEFI boot enabled, the option is automatically enabled for the VM, and vice versa.\n\nAfter configuring all of the VM parameters, click Deploy to create and boot the VM.\n\nIf you are deploying the VM from an ISO image, you need to install the guest OS inside the VM by using the built-in VNC console. For VMs with UEFI boot enabled, open the VNC console, and then press any key to boot from the chosen ISO image. Virtual machines created from a template or a boot volume already have a preinstalled guest OS.\n\nCommand-line interface\nUse the following command:vinfra service compute server create [--description <description>]\r\n                                     [--metadata <metadata>]\r\n                                     [--user-data <user-data>]\r\n                                     [--key-name <key-name>]\r\n                                     [--config-drive] [--count <count>]\r\n                                     [--ha-enabled {true,false}]\r\n                                     [--placements <placements>]\r\n                                     [--allow-live-resize] [--uefi]\r\n                                     --network id|<id=id[,key=value,\u00e2\u0080\u00a6]>\r\n                                     --volume <source=source\r\n                                     [,key=value,\u00e2\u0080\u00a6]>\r\n                                     --flavor <flavor> <server-name>\r\n\n\n--description <description>\n\nVirtual machine description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--metadata <metadata>\n\nVirtual machine metadata\n--user-data <user-data>\n\nUser data file\n--key-name <key-name>\n\nKey pair to inject\n--config-drive\n\nUse an ephemeral drive\n--count <count>\n\nIf count is specified and greater than 1, the name argument is treated as a naming pattern.\n--ha-enabled {true,false}\n\nEnable or disable HA for the virtual machine.\n--placements <placements>\n\nNames or IDs of placements to add the virtual machine to.\n--allow-live-resize\n\nAllow online resize for the virtual machine.\n--uefi\n\nAllow UEFI boot for the virtual machine. This option can be used for VMs created from ISO images.\n--network id|<id=id[,key=value,\u00e2\u0080\u00a6]>\n\nCreate a virtual machine with a specified network. Specify this option multiple times to create multiple networks.\n\nid: attach network interface to a specified network (ID or name)\ncomma-separated key=value pairs with keys (optional):mac: MAC address for network interfacefixed-ip: fixed IP address or None to automatically allocate an IP address. This option can be used multiple times.spoofing-protection-enable: enable spoofing protection for network interfacespoofing-protection-disable: disable spoofing protection for network interfacesecurity-group: security group ID or name. This option can be used multiple times.no-security-group: do not use a security group\n\n--volume <source=source[,key=value,\u00e2\u0080\u00a6]>\n\nCreate a virtual machine with a specified volume. Specify this option multiple times to create multiple volumes.\n\nsource: source type (volume, image, snapshot, or blank)\ncomma-separated key=value pairs with keys (optional):id: resource ID or name for the specified source type (required for source types volume, image, and snapshot)size: block device size, in gigabytes (required for source types image and blank)boot-index: block device boot index (required for multiple volumes with source type volume)bus: block device controller type (scsi)type: block device type (disk or cdrom)rm: remove block device on virtual machine termination (yes or no)storage-policy: block device storage policy\n\n--flavor <flavor>\n\nFlavor ID or name\n<server-name>\n\nA new name for the virtual machine\n\nFor example, to create a virtual machine myvm based on the image cirros and the flavor tiny, connect it to the virtual network private with the fixed IP address 192.168.128.100, run:# vinfra service compute server create myvm --network id=private,fixed-ip=192.168.128.100 \\\r\n--volume source=image,id=cirros,size=1 --flavor tiny\r\n+--------------+--------------------------------------+\r\n| Field        | Value                                |\r\n+--------------+--------------------------------------+\r\n| config_drive |                                      |\r\n| created      | 2019-05-29T11:24:04Z                 |\r\n| description  |                                      |\r\n| flavor       | disk: 0                              |\r\n|              | ephemeral: 0                         |\r\n|              | extra_specs: {}                      |\r\n|              | original_name: tiny                  |\r\n|              | ram: 512                             |\r\n|              | swap: 0                              |\r\n|              | vcpus: 1                             |\r\n| ha_enabled   | True                                 |\r\n| host         |                                      |\r\n| id           | 8cd29296-8bee-4efb-828d-0e522d816c6e |\r\n| key_name     |                                      |\r\n| metadata     | {}                                   |\r\n| name         | myvm                                 |\r\n| networks     | []                                   |\r\n| power_state  | NOSTATE                              |\r\n| project_id   | b4267de6fd0c442da99542cd20f5932c     |\r\n| status       | BUILD                                |\r\n| task_state   | scheduling                           |\r\n| updated      | 2019-05-29T11:24:21Z                 |\r\n| user_data    |                                      |\r\n| vm_state     | building                             |\r\n| volumes      | []                                   |\r\n+--------------+--------------------------------------+\r\n\nThe new virtual machine will appear in the vinfra service compute server list output:# vinfra service compute server list\r\n+-------------+------+--------+------------------------+---------------------------+\r\n| id          | name | status | host                   | networks                  |\r\n+-------------+------+--------+------------------------+---------------------------+\r\n| 8cd29296<\u00e2\u0080\u00a6> | myvm | ACTIVE | node002.vstoragedomain | - private=192.168.128.100 |\r\n+-------------+------+--------+------------------------+---------------------------+\n\nSee also\n\nCreating virtIO disks for virtual machines\n\nAttaching host devices to virtual machines\n\nManaging virtual machine power state\n\nMonitoring virtual machines\n\nTroubleshooting virtual machines\n\nWhat's next\n\nConnecting to virtual machines\n\nManaging guest tools",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server create [--description <description>]\r\n                                     [--metadata <metadata>]\r\n                                     [--user-data <user-data>]\r\n                                     [--key-name <key-name>]\r\n                                     [--config-drive] [--count <count>]\r\n                                     [--ha-enabled {true,false}]\r\n                                     [--placements <placements>]\r\n                                     [--allow-live-resize] [--uefi]\r\n                                     --network id|<id=id[,key=value,\u00e2\u0080\u00a6]>\r\n                                     --volume <source=source\r\n                                     [,key=value,\u00e2\u0080\u00a6]>\r\n                                     --flavor <flavor> <server-name>\r\n\n\n--description <description>\n\n\nVirtual machine description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--metadata <metadata>\n\nVirtual machine metadata\n--user-data <user-data>\n\nUser data file\n--key-name <key-name>\n\nKey pair to inject\n--config-drive\n\nUse an ephemeral drive\n--count <count>\n\nIf count is specified and greater than 1, the name argument is treated as a naming pattern.\n--ha-enabled {true,false}\n\nEnable or disable HA for the virtual machine.\n--placements <placements>\n\nNames or IDs of placements to add the virtual machine to.\n--allow-live-resize\n\nAllow online resize for the virtual machine.\n--uefi\n\nAllow UEFI boot for the virtual machine. This option can be used for VMs created from ISO images.\n--network id|<id=id[,key=value,\u00e2\u0080\u00a6]>\n\n\nCreate a virtual machine with a specified network. Specify this option multiple times to create multiple networks.\n\nid: attach network interface to a specified network (ID or name)\ncomma-separated key=value pairs with keys (optional):mac: MAC address for network interfacefixed-ip: fixed IP address or None to automatically allocate an IP address. This option can be used multiple times.spoofing-protection-enable: enable spoofing protection for network interfacespoofing-protection-disable: disable spoofing protection for network interfacesecurity-group: security group ID or name. This option can be used multiple times.no-security-group: do not use a security group\n\n\n--volume <source=source[,key=value,\u00e2\u0080\u00a6]>\n\n\nCreate a virtual machine with a specified volume. Specify this option multiple times to create multiple volumes.\n\nsource: source type (volume, image, snapshot, or blank)\ncomma-separated key=value pairs with keys (optional):id: resource ID or name for the specified source type (required for source types volume, image, and snapshot)size: block device size, in gigabytes (required for source types image and blank)boot-index: block device boot index (required for multiple volumes with source type volume)bus: block device controller type (scsi)type: block device type (disk or cdrom)rm: remove block device on virtual machine termination (yes or no)storage-policy: block device storage policy\n\n\n--flavor <flavor>\n\nFlavor ID or name\n<server-name>\n\nA new name for the virtual machine\n\nFor example, to create a virtual machine myvm based on the image cirros and the flavor tiny, connect it to the virtual network private with the fixed IP address 192.168.128.100, run:# vinfra service compute server create myvm --network id=private,fixed-ip=192.168.128.100 \\\r\n--volume source=image,id=cirros,size=1 --flavor tiny\r\n+--------------+--------------------------------------+\r\n| Field        | Value                                |\r\n+--------------+--------------------------------------+\r\n| config_drive |                                      |\r\n| created      | 2019-05-29T11:24:04Z                 |\r\n| description  |                                      |\r\n| flavor       | disk: 0                              |\r\n|              | ephemeral: 0                         |\r\n|              | extra_specs: {}                      |\r\n|              | original_name: tiny                  |\r\n|              | ram: 512                             |\r\n|              | swap: 0                              |\r\n|              | vcpus: 1                             |\r\n| ha_enabled   | True                                 |\r\n| host         |                                      |\r\n| id           | 8cd29296-8bee-4efb-828d-0e522d816c6e |\r\n| key_name     |                                      |\r\n| metadata     | {}                                   |\r\n| name         | myvm                                 |\r\n| networks     | []                                   |\r\n| power_state  | NOSTATE                              |\r\n| project_id   | b4267de6fd0c442da99542cd20f5932c     |\r\n| status       | BUILD                                |\r\n| task_state   | scheduling                           |\r\n| updated      | 2019-05-29T11:24:21Z                 |\r\n| user_data    |                                      |\r\n| vm_state     | building                             |\r\n| volumes      | []                                   |\r\n+--------------+--------------------------------------+\r\n\nThe new virtual machine will appear in the vinfra service compute server list output:# vinfra service compute server list\r\n+-------------+------+--------+------------------------+---------------------------+\r\n| id          | name | status | host                   | networks                  |\r\n+-------------+------+--------+------------------------+---------------------------+\r\n| 8cd29296<\u00e2\u0080\u00a6> | myvm | ACTIVE | node002.vstoragedomain | - private=192.168.128.100 |\r\n+-------------+------+--------+------------------------+---------------------------+\n",
                "title": "To create a virtual machine"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOn the Compute > Virtual machines > Virtual machines tab, click Create virtual machine. A window will open where you will need to specify the VM parameters.\n\nSpecify a name for the new VM.\n\nSelect the VM boot media:\n\n\nIf you have an ISO image or a template\n\n\nSelect Image in the Deploy from section, and then click Specify in the Image section.\n\nIn the Images window, select the ISO image or template, and then click Done.\n\n\n\n\n\n\n\n\n\n\nIf you have a compute boot volume\n\n\nSelect Volume in the Deploy from section, and then click Specify in the Volumes section.\nIn the Volumes window, click Attach.\n\nIn the Attach volume window, find and select the volume, and then click Attach.\n\n\n\n\nIf you attach more than one volume, the first attached volume becomes the boot volume, by default. To select another volume as bootable, place it first in the list by clicking the up arrow button next to it.\n\n\n\n\n\n\n\nIf you select an image or volume with an assigned placement, the created VM will also inherit this placement.\n\nAfter selecting the boot media, volumes required for this media to boot will be automatically added to the Volumes section.\n\n\nConfigure the VM disks:\n\nIn the Volumes window, make sure the default boot volume is large enough to accommodate the guest OS. Otherwise, click the ellipsis icon next to it, and then Edit. Change the volume size and click Save.\n\nAdd more disks to the VM by creating or attaching volumes. To do this, click the pencil icon in the Volumes section, and then Add or Attach in the Volumes window.\n\nSelect volumes that will be removed during the VM deletion. To do this, click the pencil icon in the Volumes section, click the ellipsis icon next to the needed volume, and then Edit. Enable Delete on termination and click Save.\nWhen you finish configuring the VM disks, click Done.\n\n\n\nChoose the amount of RAM and CPU resources that will be allocated to the VM in the Flavor section. In the Flavor window, select a flavor, and then click Done.\n\nWhen choosing a flavor for a VM, ensure it satisfies the hardware requirements of the guest OS.\n\n\nTo select a flavor with an assigned placement, you can filter flavors by placement. The VM created from such a flavor will also inherit this placement.\n\n\n\n\n\n\n\nAdd network interfaces to the VM in the Networks section:\n\nIn the Network interfaces window, click Add to attach a network interface.\n\nIn the Add network interface window, select a compute network to connect to, and then specify MAC address, IPv4 and/or IPv6 addresses, and security groups. By default, MAC and primary IP addresses are assigned automatically. To specify them manually, clear the Assign automatically check boxes, and enter the desired addresses. Optionally, assign additional IP addresses to the network interface in the Secondary IP addresses section. Note that a secondary IPv6 address is not available for an IPv6 subnet that works in the SLAAC or DHCPv6 stateless mode.\n\nSecondary IP addresses, unlike the primary one, will not be automatically assigned to the network interface inside the virtual machine guest OS. You should assign them manually.\n\n\n\nIf you selected a network with enabled IP address management\n\nIn this case, spoofing protection is enabled and the default security group is selected by default. This security group allows all incoming and outgoing traffic on all the VM ports. If required, you can select another security group or multiple security groups.\nTo disable spoofing protection, clear all of the check boxes and turn off the toggle switch. Security groups cannot be configured with disabled spoofing protection.\n\n\n\n\nIf you selected a network with disabled IP address management\nIn this case, spoofing protection is disabled by default and cannot be enabled. Security groups cannot be configured for such a network.\n\n\n\n\n\n\n\nAfter specifying the network interface parameters, click Add. The network interface will appear in the Network interfaces list.\n\n\nIf required, edit IP addresses and security groups of newly added network interfaces. To do this, click the ellipsis icon, click Edit, and then set the parameters.\n\nWhen you finish configuring the VM network interfaces, click Done.\n\n\n\nIf you have chosen to boot from a template or volume, which has cloud-init and OpenSSH installed:\n\nAs cloud images have no default password, you can access VMs deployed from them only by using the key authentication method with SSH.\n\n\n\nAdd an SSH key to the VM, to be able to access it via SSH without a password. \n\nIn the Select an SSH key window, select an SSH key  and then click Done.\n\n\n\n\n\n\n\n\nAdd user data to customize the VM after launch, for example, change a user password. \n\nWrite a cloud-config or shell script in the Customization script field or browse a file on your local server to load the script from.\n\n\n\n\nTo inject a script in a Windows VM, refer to the Cloudbase-Init documentation. For example, you can set a new password for the account using the following script:#ps1\r\nnet user <username> <new_password>\r\n\n\n\n\n\n\n\nEnable CPU and RAM hot plug for the VM in Advanced options, to be able to change its flavor when the VM is running. You can also enable hot plug after the VM is created.\n\n\nIf you have chosen to boot from an ISO image, enable UEFI boot in Advanced options, to be able to boot the VM in the UEFI mode. This option cannot be configured after the VM is created.\n\nYou cannot configure UEFI boot if you have selected a template as the VM boot media. If your template has UEFI boot enabled, the option is automatically enabled for the VM, and vice versa.\n\n\nAfter configuring all of the VM parameters, click Deploy to create and boot the VM.\n\nIf you are deploying the VM from an ISO image, you need to install the guest OS inside the VM by using the built-in VNC console. For VMs with UEFI boot enabled, open the VNC console, and then press any key to boot from the chosen ISO image. Virtual machines created from a template or a boot volume already have a preinstalled guest OS.\n",
                "title": "To create a virtual machine"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-machines.html"
    },
    {
        "title": "Creating virtual compute networks",
        "content": "Creating virtual compute networks\nLimitations\n\nIPv6 subnets are not available for virtual compute networks.\n\nPrerequisites\n\nA clear understanding of the compute architecture, which is explained in Compute network architecture.\n\nTo add a virtual compute network\n\nAdmin panel\n\nOn the Compute > Network > Networks tab, click Create network.\n\nOn the Network configuration step:\n\nEnable or disable IP address management:\n\nWith IP address management enabled, VMs connected to the network will automatically be assigned IP addresses from allocation pools by the built-in DHCP server and use custom DNS servers. Additionally, spoofing protection will be enabled for all VM network ports by default. Each VM network interface will be able to accept and send IP packets only if it has IP and MAC addresses assigned. You can disable spoofing protection manually for a VM interface, if required.\nWith IP address management disabled, VMs connected to the network will obtain IP addresses from the DHCP servers in that network, if any. Also, spoofing protection will be disabled for all VM network ports, and you cannot enable it manually. This means that each VM network interface, with or without assigned IP and MAC addresses, will be able to accept and send IP packets.\n\nIn any case, you will be able to manually assign static IP addresses from inside the VMs.\n\nSelect the Virtual network type.\nSpecify a network name.\nClick Next.\n\nIf you enabled IP address management, you will move on to the IP address management step, where you can add an IPv4 subnet:\n\nIn the Subnets section, click Add and select IPv4 subnet.\nIn the Add IPv4 subnet window, specify the network\u00e2\u0080\u0099s IPv4 address range and, optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\n\nEnable or disable the built-in DHCP server:\n\nWith the DHCP server enabled, VM network interfaces will automatically be assigned IP addresses: either from allocation pools or, if there are no pools, from the network\u00e2\u0080\u0099s entire IP range. The DHCP server will receive the first two IP addresses from the IP pool. For example:\n\n In a subnet with CIDR 192.168.128.0/24 and without a gateway, the DHCP server will be assigned the IP addresses 192.168.128.1 and 192.168.128.2.\n In a subnet with CIDR 192.168.128.0/24 and the gateway IP address set to 192.168.128.1, the DHCP server will be assigned the IP addresses 192.168.128.2 and 192.168.128.3.\n\nWith the DHCP server disabled, VM network interfaces will still get IP addresses, but you will have to manually assign them inside VMs.\n\nThe virtual DHCP service will work only within the current network and will not be exposed to other networks.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nSpecify DNS servers that will be used by virtual machines. These servers can be delivered to VMs via the built-in DHCP server or by using the cloud-init network configuration (if cloud-init is installed in the VM).\nClick Add.\n\nOn the Summary step, review the configuration, and then click Create network.\n\nCommand-line interface\nUse the following command:vinfra service compute network create [--dhcp | --no-dhcp]\r\n                                      [--dns-nameserver <dns-nameserver>]\r\n                                      [--allocation-pool <allocation-pool>]\r\n                                      [--gateway <gateway> | --no-gateway]\r\n                                      [--rbac-policies <rbac-policies>]\r\n                                      [--physical-network <physical-network>]\r\n                                      [--vlan-network <vlan-network>]\r\n                                      [--vlan <vlan>] [--cidr <cidr>]\r\n                                      [--ipv6-address-mode <ipv6-address-mode>]\r\n                                      <network-name>\r\n\n\n--dhcp\n\nEnable DHCP.\n--no-dhcp\n\nDisable DHCP.\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used  multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: ip_addr_start-ip_addr_end. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n--rbac-policies <rbac-policies>\n\nComma-separated list of RBAC policies in the format: <target>:<target_id>:<action> | none. Valid targets: project, domain. Valid actions: direct, full, routed. \u00e2\u0080\u0098*\u00e2\u0080\u0099 is valid target_id for all targets. Pass none to clear out all existing policies.\nExample: domain:default:routed,project:uuid1:full\n\n--physical-network <physical-network>\n\nAn infrastructure network to link to a physical network\n--vlan-network <vlan-network>\n\nA VLAN network to link\n--vlan <vlan>\n\nVirtual network VLAN ID\n--cidr <cidr>\n\nSubnet range in CIDR notation\n--ipv6-address-mode <ipv6-address-mode>\n\nIPv6 address mode: dhcpv6-stateful, dhcpv6-stateless, slaac\n<network-name>\n\nNetwork name\n\nFor example, to create a virtual network myprivnet with enabled IP management and specified network settings, run:# vinfra service compute network create myprivnet \r\n--cidr 192.168.128.0/24 \\\r\n--gateway 192.168.128.1 --dns-nameserver 8.8.8.8+---------------------+----------------------------------------------------+\r\n| Field               | Value                                              |\r\n+---------------------+----------------------------------------------------+\r\n| allocation_pools    | - end: 192.168.128.254                             |\r\n|                     |   start: 192.168.128.2                             |\r\n| cidr                | 192.168.128.0/24                                   |\r\n| dns_nameservers     | - 8.8.8.8                                          |\r\n| enable_dhcp         | True                                               |\r\n| gateway_ip          | 192.168.128.1                                      |\r\n| id                  | fa6d0ead-32de-4ce2-b620-5529a15eb52a               |\r\n| ip_version          | 4                                                  |\r\n| ipam_enabled        | True                                               |\r\n| name                | myprivnet                                          |\r\n| physical_network    |                                                    |\r\n| project_id          | b906404c55bb44729da99987536ac5bc                   |\r\n| rbac_policies       | []                                                 |\r\n| router_external     | False                                              |\r\n| shared              | False                                              |\r\n| spoofing_protection | True                                               |\r\n| subnet              | allocation_pools:                                  |\r\n|                     | - end: 192.168.128.254                             |\r\n|                     |   start: 192.168.128.2                             |\r\n|                     | cidr: 192.168.128.0/24                             |\r\n|                     | dns_nameservers:                                   |\r\n|                     | - 8.8.8.8                                          |\r\n|                     | enable_dhcp: true                                  |\r\n|                     | gateway_ip: 192.168.128.1                          |\r\n|                     | id: e607dd29-ffe1-46d8-a189-1baf392d1520           |\r\n|                     | ip_version: 4                                      |\r\n|                     | ipv6_address_mode: null                            |\r\n|                     | ipv6_ra_mode: null                                 |\r\n|                     | network_id: fa6d0ead-32de-4ce2-b620-5529a15eb52a   |\r\n| subnets             | - allocation_pools:                                |\r\n|                     |   - end: 192.168.128.254                           |\r\n|                     |     start: 192.168.128.2                           |\r\n|                     |   cidr: 192.168.128.0/24                           |\r\n|                     |   dns_nameservers:                                 |\r\n|                     |   - 8.8.8.8                                        |\r\n|                     |   enable_dhcp: true                                |\r\n|                     |   gateway_ip: 192.168.128.1                        |\r\n|                     |   id: e607dd29-ffe1-46d8-a189-1baf392d1520         |\r\n|                     |   ip_version: 4                                    |\r\n|                     |   ipv6_address_mode: null                          |\r\n|                     |   ipv6_ra_mode: null                               |\r\n|                     |   network_id: fa6d0ead-32de-4ce2-b620-5529a15eb52a |\r\n| tags                | []                                                 |\r\n| type                | virtual                                            |\r\n| vlan_id             |                                                    |\r\n+---------------------+----------------------------------------------------+\r\n\nThe new compute network will appear in the vinfra service compute network list output:# vinfra service compute network list -c id -c name -c cidr -c allocation_pools\r\n+----------------+---------------+------------------+-------------------------------+\r\n| id             | name          | cidr             | allocation_pools              |\r\n+----------------+---------------+------------------+-------------------------------+\r\n| 22674f9d-<...> | mypubnet      | 10.136.16.0/22   | - 10.136.18.141-10.136.18.148   |\r\n| 8f0dc747-<...> | mypubnet_vlan | 10.136.16.0/22   | - 10.136.18.131-10.136.18.138   |\r\n| a0019b43-<...> | myprivnet     | 192.168.128.0/24 | - 192.168.128.2-192.168.128.254 |\r\n+----------------+---------------+------------------+-------------------------------+\n\nSee also\n\nCreating physical compute networks\n\nEditing and deleting compute networks\n\nCreating virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute network create [--dhcp | --no-dhcp]\r\n                                      [--dns-nameserver <dns-nameserver>]\r\n                                      [--allocation-pool <allocation-pool>]\r\n                                      [--gateway <gateway> | --no-gateway]\r\n                                      [--rbac-policies <rbac-policies>]\r\n                                      [--physical-network <physical-network>]\r\n                                      [--vlan-network <vlan-network>]\r\n                                      [--vlan <vlan>] [--cidr <cidr>]\r\n                                      [--ipv6-address-mode <ipv6-address-mode>]\r\n                                      <network-name>\r\n\n\n--dhcp\n\nEnable DHCP.\n--no-dhcp\n\nDisable DHCP.\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used  multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: ip_addr_start-ip_addr_end. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n--rbac-policies <rbac-policies>\n\n\nComma-separated list of RBAC policies in the format: <target>:<target_id>:<action> | none. Valid targets: project, domain. Valid actions: direct, full, routed. \u00e2\u0080\u0098*\u00e2\u0080\u0099 is valid target_id for all targets. Pass none to clear out all existing policies.\nExample: domain:default:routed,project:uuid1:full\n\n--physical-network <physical-network>\n\nAn infrastructure network to link to a physical network\n--vlan-network <vlan-network>\n\nA VLAN network to link\n--vlan <vlan>\n\nVirtual network VLAN ID\n--cidr <cidr>\n\nSubnet range in CIDR notation\n--ipv6-address-mode <ipv6-address-mode>\n\nIPv6 address mode: dhcpv6-stateful, dhcpv6-stateless, slaac\n<network-name>\n\nNetwork name\n\nFor example, to create a virtual network myprivnet with enabled IP management and specified network settings, run:# vinfra service compute network create myprivnet \r\n--cidr 192.168.128.0/24 \\\r\n--gateway 192.168.128.1 --dns-nameserver 8.8.8.8+---------------------+----------------------------------------------------+\r\n| Field               | Value                                              |\r\n+---------------------+----------------------------------------------------+\r\n| allocation_pools    | - end: 192.168.128.254                             |\r\n|                     |   start: 192.168.128.2                             |\r\n| cidr                | 192.168.128.0/24                                   |\r\n| dns_nameservers     | - 8.8.8.8                                          |\r\n| enable_dhcp         | True                                               |\r\n| gateway_ip          | 192.168.128.1                                      |\r\n| id                  | fa6d0ead-32de-4ce2-b620-5529a15eb52a               |\r\n| ip_version          | 4                                                  |\r\n| ipam_enabled        | True                                               |\r\n| name                | myprivnet                                          |\r\n| physical_network    |                                                    |\r\n| project_id          | b906404c55bb44729da99987536ac5bc                   |\r\n| rbac_policies       | []                                                 |\r\n| router_external     | False                                              |\r\n| shared              | False                                              |\r\n| spoofing_protection | True                                               |\r\n| subnet              | allocation_pools:                                  |\r\n|                     | - end: 192.168.128.254                             |\r\n|                     |   start: 192.168.128.2                             |\r\n|                     | cidr: 192.168.128.0/24                             |\r\n|                     | dns_nameservers:                                   |\r\n|                     | - 8.8.8.8                                          |\r\n|                     | enable_dhcp: true                                  |\r\n|                     | gateway_ip: 192.168.128.1                          |\r\n|                     | id: e607dd29-ffe1-46d8-a189-1baf392d1520           |\r\n|                     | ip_version: 4                                      |\r\n|                     | ipv6_address_mode: null                            |\r\n|                     | ipv6_ra_mode: null                                 |\r\n|                     | network_id: fa6d0ead-32de-4ce2-b620-5529a15eb52a   |\r\n| subnets             | - allocation_pools:                                |\r\n|                     |   - end: 192.168.128.254                           |\r\n|                     |     start: 192.168.128.2                           |\r\n|                     |   cidr: 192.168.128.0/24                           |\r\n|                     |   dns_nameservers:                                 |\r\n|                     |   - 8.8.8.8                                        |\r\n|                     |   enable_dhcp: true                                |\r\n|                     |   gateway_ip: 192.168.128.1                        |\r\n|                     |   id: e607dd29-ffe1-46d8-a189-1baf392d1520         |\r\n|                     |   ip_version: 4                                    |\r\n|                     |   ipv6_address_mode: null                          |\r\n|                     |   ipv6_ra_mode: null                               |\r\n|                     |   network_id: fa6d0ead-32de-4ce2-b620-5529a15eb52a |\r\n| tags                | []                                                 |\r\n| type                | virtual                                            |\r\n| vlan_id             |                                                    |\r\n+---------------------+----------------------------------------------------+\r\n\nThe new compute network will appear in the vinfra service compute network list output:# vinfra service compute network list -c id -c name -c cidr -c allocation_pools\r\n+----------------+---------------+------------------+-------------------------------+\r\n| id             | name          | cidr             | allocation_pools              |\r\n+----------------+---------------+------------------+-------------------------------+\r\n| 22674f9d-<...> | mypubnet      | 10.136.16.0/22   | - 10.136.18.141-10.136.18.148   |\r\n| 8f0dc747-<...> | mypubnet_vlan | 10.136.16.0/22   | - 10.136.18.131-10.136.18.138   |\r\n| a0019b43-<...> | myprivnet     | 192.168.128.0/24 | - 192.168.128.2-192.168.128.254 |\r\n+----------------+---------------+------------------+-------------------------------+\n",
                "title": "To add a virtual compute network"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Networks tab, click Create network.\n\nOn the Network configuration step:\n\n\nEnable or disable IP address management:\n\nWith IP address management enabled, VMs connected to the network will automatically be assigned IP addresses from allocation pools by the built-in DHCP server and use custom DNS servers. Additionally, spoofing protection will be enabled for all VM network ports by default. Each VM network interface will be able to accept and send IP packets only if it has IP and MAC addresses assigned. You can disable spoofing protection manually for a VM interface, if required.\nWith IP address management disabled, VMs connected to the network will obtain IP addresses from the DHCP servers in that network, if any. Also, spoofing protection will be disabled for all VM network ports, and you cannot enable it manually. This means that each VM network interface, with or without assigned IP and MAC addresses, will be able to accept and send IP packets.\n\nIn any case, you will be able to manually assign static IP addresses from inside the VMs.\n\nSelect the Virtual network type.\nSpecify a network name.\nClick Next.\n\n\n\n\n\n\n\nIf you enabled IP address management, you will move on to the IP address management step, where you can add an IPv4 subnet:\n\nIn the Subnets section, click Add and select IPv4 subnet.\nIn the Add IPv4 subnet window, specify the network\u00e2\u0080\u0099s IPv4 address range and, optionally, specify a gateway. If you leave the Gateway field blank, the gateway will be omitted from network settings.\n\nEnable or disable the built-in DHCP server:\n\n\nWith the DHCP server enabled, VM network interfaces will automatically be assigned IP addresses: either from allocation pools or, if there are no pools, from the network\u00e2\u0080\u0099s entire IP range. The DHCP server will receive the first two IP addresses from the IP pool. For example:\n\n In a subnet with CIDR 192.168.128.0/24 and without a gateway, the DHCP server will be assigned the IP addresses 192.168.128.1 and 192.168.128.2.\n In a subnet with CIDR 192.168.128.0/24 and the gateway IP address set to 192.168.128.1, the DHCP server will be assigned the IP addresses 192.168.128.2 and 192.168.128.3.\n\n\nWith the DHCP server disabled, VM network interfaces will still get IP addresses, but you will have to manually assign them inside VMs.\n\nThe virtual DHCP service will work only within the current network and will not be exposed to other networks.\n\nSpecify one or more allocation pools (ranges of IP addresses that will be automatically assigned to VMs).\nSpecify DNS servers that will be used by virtual machines. These servers can be delivered to VMs via the built-in DHCP server or by using the cloud-init network configuration (if cloud-init is installed in the VM).\nClick Add.\n\n\n\n\n\n\nOn the Summary step, review the configuration, and then click Create network.\n\n",
                "title": "To add a virtual compute network"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/creating-virtual-compute-networks.html"
    },
    {
        "title": "Default outbound firewall rules",
        "content": "Default outbound firewall rules\nAll networks in the cluster have the default outbound allow rules, which are specified in the format: <address>:<protocol>:<port>:<description>. These rules are the following:\n\n0.0.0.0:udp:500:IKE\n\nTraffic encryption\n0.0.0.0:udp:4500:IKE\n\nTraffic encryption\n0.0.0.0:tcp:8888:Admin panel\n\nUsed by the cluster API\n0.0.0.0:tcp:80:HTTP\n\nConnection to the update repository and the S3 backend when configured to serve HTTP requests\n0.0.0.0:tcp:443:HTTPS\n\nCommunication with Acronis Cyber Protect Cloud and the S3 services\n0.0.0.0:udp:53:DNS\n\nDNS name resolution\n0.0.0.0:tcp:53:DNS\n\nDNS name resolution\n0.0.0.0:udp:123:NTP\n\nTime syncronization\n0.0.0.0:tcp:8443:ABGW registration\n\nData control for the Acronis Cyber Protect agents and Management server\n0.0.0.0:tcp:44445:ABGW Geo-replication\n\nBackup data replication between clusters\n0.0.0.0:tcp:9877:Acronis Cyber Protect\n\nRegistration with Acronis Cyber Protect Management server in on-premise installations\n0.0.0.0:tcp:5900-6079:VM VNC Legacy\n\nLegacy ports for VNC console access to virtual machines\n0.0.0.0:udp:4789:VXLAN\n\nNetwork traffic between virtual machines in private virtual networks\n0.0.0.0:tcp:15900-16900:VM VNC\n\nVNC console access to virtual machines in the compute cluster\n0.0.0.0:tcp:7050:KA license\n\nConnection to the Key Authentication (KA) licensing server\n0.0.0.0:tcp:5224:KA report\n\nSending reports to the KA server\n0.0.0.0:udp:2049:NFS\n\nData exchange with the NFS access point\n0.0.0.0:tcp:2049:NFS\n\nData exchange with the NFS access point\n0.0.0.0:tcp:111:NFS Rpcbind\n\nRPC request mapping from NFS clients to the correct port\n0.0.0.0:any:0:Allow all\n\nAllows all outbound traffic\n\nSee also\n\nCreating outbound firewall rules\n\nRestoring default outbound firewall rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/default-outbound-firewall-rules.html"
    },
    {
        "title": "Deleting registrations",
        "content": "Deleting registrations\nYou can unregister backup storage from your Acronis backup software by deleting its registrations. When you delete a registration, backup data remains in backup storage. You can delete your backup data in Acronis Cyber Protect Cloud. If you want to delete the backup data in Virtuozzo Hybrid Infrastructure, destroy the backup storage cluster.\nPrerequisites\n\nThe backup storage cluster is created and registered in the Cloud Management Panel, as described in Provisioning backup storage space.\n\nTo delete a backup storage registration\n\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Registrations tab. \nSelect a backup storage registration, and then click Delete.\n\nIn the Delete registration window:\n\nTo delete the registration and unregister the backup storage from your Acronis backup software, select (Recommended) Gracefully, and then specify the credentials of your administrator account in your backup software.\n\nTo delete the registration but do not unregister the backup storage from your Acronis backup software, select Forcibly, and then enter Delete in the input field.\n\nSelect this option only if you are sure that the backup storage is already unregistered from your Acronis backup software.\n\nClick Delete.\n\nCommand-line interface\nUse the following command:vinfra service backup registration delete [--username <username>] [--stdin] [--force]\r\n                                          <registration>\n\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--stdin\n\nUse for setting registration password from stdin.\n--force\n\nForcibly delete the registration but do not unregister the backup storage from your Acronis backup software.\n\nChoose this option only if you are sure that the cluster has already been unregistered from your backup software.\n\n<registration>\n\nRegistration ID or name\n\nFor example, to delete the backup storage registration registration2, run:# vinfra service backup registration delete --username account@example.com --stdin registration2\nSpecify the registration password when prompted.\n\nSee also\n\nUpdating registration certificates\n\nWhat's next\n\nReleasing nodes from backup storage",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup registration delete [--username <username>] [--stdin] [--force]\r\n                                          <registration>\n\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server.\n--stdin\n\nUse for setting registration password from stdin.\n--force\n\n\nForcibly delete the registration but do not unregister the backup storage from your Acronis backup software.\n\nChoose this option only if you are sure that the cluster has already been unregistered from your backup software.\n\n\n<registration>\n\nRegistration ID or name\n\nFor example, to delete the backup storage registration registration2, run:# vinfra service backup registration delete --username account@example.com --stdin registration2\nSpecify the registration password when prompted.\n",
                "title": "To delete a backup storage registration"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Registrations tab. \nSelect a backup storage registration, and then click Delete.\n\nIn the Delete registration window:\n\nTo delete the registration and unregister the backup storage from your Acronis backup software, select (Recommended) Gracefully, and then specify the credentials of your administrator account in your backup software.\n\nTo delete the registration but do not unregister the backup storage from your Acronis backup software, select Forcibly, and then enter Delete in the input field.\n\nSelect this option only if you are sure that the backup storage is already unregistered from your Acronis backup software.\n\n\n\n\nClick Delete.\n\n",
                "title": "To delete a backup storage registration"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/deleting-registrations.html"
    },
    {
        "title": "Deleting virtual machines",
        "content": "Deleting virtual machines\nLimitations\n\nA VM is removed along with its disks that have the Delete on termination option enabled during the VM deployment.\n\nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.\n\nTo remove one virtual machine\n\nAdmin panel\n\nClick the ellipsis button next to a VM you want to delete, and then click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service compute server delete <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to delete the virtual machine myvm, run:# vinfra service compute server delete myvm\n\nTo remove multiple virtual machines\n\nSelect the check boxes next to VMs you want to delete.\nOver the VM list, click Delete.\nClick Delete in the confirmation window.",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server delete <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to delete the virtual machine myvm, run:# vinfra service compute server delete myvm\n",
                "title": "To remove one virtual machine"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nClick the ellipsis button next to a VM you want to delete, and then click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To remove one virtual machine"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/deleting-virtual-machines.html"
    },
    {
        "title": "Detaching external storages",
        "content": "Detaching external storages\nLimitations\n\nAfter detaching an external NFS storage, the associated NFS share is left mounted.\n\nPrerequisites\n\nAn external storage is attached to the compute cluster, as described in Attaching external iSCSI storage or Attaching external NFS storage.\n\nTo detach an external storage\n\nDisconnect the external storage from the compute cluster. For example, to remove the external storage pure-storage, run:# vinfra service compute storage remove pure-storage\n\n[For an external iSCSI storage] On each compute node, stop and disable the multipathing service:\r\n# systemctl stop multipathd; systemctl disable multipathd\n\nAfter detaching your external storage, you will have to manually delete storage policies associated with it.\nSee also\n\nManaging storage policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/detaching-external-storages.html"
    },
    {
        "title": "Deleting target groups",
        "content": "Deleting target groups\nPrerequisites\n\nA target group is created, as described in Creating target groups.\n\nTo delete a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups. \nClick the ellipsis icon of the desired target group, and then click Delete.\nIf the target group has active connections, select Force.\nClick Delete in the confirmation window. \n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group delete [--force] <target-group>\n\n--force\n\nForcibly remove a target group\n<target-group>\n\nTarget group name or ID\n\nFor example, to delete the target group tg1, run:# vinfra service block-storage target-group delete tg1",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group delete [--force] <target-group>\n\n--force\n\nForcibly remove a target group\n<target-group>\n\nTarget group name or ID\n\nFor example, to delete the target group tg1, run:# vinfra service block-storage target-group delete tg1\n",
                "title": "To delete a target group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > Target groups. \nClick the ellipsis icon of the desired target group, and then click Delete.\nIf the target group has active connections, select Force.\nClick Delete in the confirmation window. \n\n",
                "title": "To delete a target group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/deleting-target-groups.html"
    },
    {
        "title": "Defining object storage classes",
        "content": "Defining object storage classes\nYou can use up to four object storage classes for applications with different performance and redundancy requirements. The first storage class is set automatically during the S3 cluster creation. The other three classes you can define manually by using the ostor-ctl set-storage-class command.\nOnce a storage class is created, use the ostor-ctl cfg-storage command to change its redundancy settings. With this command, you can modify all storage classes, including the first one.\nPrerequisites\n\nThe S3 cluster is created, as described in Creating the S3 cluster.\n\nTo create a storage class\n\nObtain the password for your storage cluster. For example:# vinfra cluster password show\r\n+----------+---------+\r\n| Field    | Value   |\r\n+----------+---------+\r\n| id       | 1       |\r\n| name     | cluster |\r\n| password | W3HMNq  |\r\n+----------+---------+\r\n\n\nFind out the ID of the object storage volume. For example:# ostor-ctl get-config -V\r\nVOL_ID             TYPE     STATE\r\n0100000000000002   OBJ     READY\n\nDefine a storage class specifying its numeric ID, the number of object servers it will include, and the required redundancy settings. When prompted, enter the password obtained in step 1. For example, to create the storage class 1 with 2 object servers  and the redundancy scheme of 2 replicas for tier 1, run:# ostor-ctl set-storage-class -s /mnt/vstorage/vols/ostor/ -V 0100000000000002 \\\r\n-C 1 -O 2 --vstorage-attr \"replicas=2:1 tier=1\"\r\nPlease enter password for 'ostor-private.svc.vstoragedomain.':\r\nStorage 1 class is successfully assigned to services\nThis command requires the following parameters:\n\n-s, --storage <path>\nThe path to the object storage directory\n-V, --vol <id>\nThe volume ID obtained in step 2\n-C, --storage-class {1,2,3}\nThe storage class ID. Can accept the following values: 1, 2, 3. \n-O, --os-count <number>\nThe number of object servers to create\n--vstorage-attr <attribute>\n\nRedundancy settings, where you can specify the desired tier, failure domain, and data redundancy scheme. Refer to the vstorage set-attr help message.\n\nThe S3 APIs that use the x-amz-storage-class header can specify one of the following storage classes: standard, type_1, type_2, or type_3.\n\nCheck that the new storage class is set. For example, for the storage class 1, run:# vstorage get-attr /mnt/vstorage/vols/ostor/0100000000000002/services/sc1/\r\nconnected to MDS#1\r\nPath: 'vstorage://hciHeat/vols/ostor/0100000000000002/services/sc1'\r\nAttributes:\r\n  directory\r\n  client-ssd-cache=1\r\n  replicas=2:1\r\n  failure-domain=host\r\n  failure-domain.int=1\r\n  tier=1\r\n  chunk-size=268435456\r\n\n\nTo change redundancy settings of a storage class\nUse the ostor-ctl cfg-storage command. For example:# ostor-ctl cfg-storage -r /mnt/vstorage/vols/ostor/0100000000000002/ -C 1 \\\r\n--vstorage-attr \"replicas=3:2 tier=0\"\nThis command requires the following parameters:\n\n-r, --root <path>\nThe path to the initialized shared storage\n-C, --storage-class {0,1,2,3}\nThe storage class ID. Can accept the following values: 0, 1, 2, 3. The default value is 0.\n--vstorage-attr <attributes>\n\nRedundancy settings, where you can specify the desired tier, failure domain, and data redundancy scheme. Refer to the vstorage set-attr help message.\n\nSee also\n\nSupported Amazon S3 features\n\nManaging S3 users\n\nManaging S3 buckets\n\nMonitoring object storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/defining-object-storage-classes.html"
    },
    {
        "title": "Deployment and configuration",
        "content": "Deployment and configuration\nThe Virtuozzo Hybrid Infrastructure workflow  includes the infrastructure setup and service provisioning. After the infrastructure setup, you will have a storage cluster with configured network and the highly available management node. On top of the storage cluster, you can deploy and configure different services for provisioning to end users. You can provision backup, block, object, and file storage space, as well as compute resources. All these tasks can be performed either in the admin panel or via the vinfra command-line tool.\nPrerequisites\n\nVirtuozzo Hybrid Infrastructure is installed on each server, as described in Installation.\n\nInfrastructure setup overview\n\nIf you plan to use the command-line interface, provide your credentials to the vinfra tool.\nSet up your networks, depending on the service you wish to provision. \nConfigure your node network interfaces.\nConfigure an external DNS server.\nEnable RDMA, if supported.\nIf your infrastructure nodes are equipped with NVMe or SSD disks, enable NVMe performance.\nCreate and configure new locations, if required, and move nodes to them.\nDeploy the storage cluster.\nIf you have three and more cluster nodes, enable high availability of the management node.\n\nAfter the infrastructure is set up, you can proceed to provision backup, block, object, file storage space, or compute resources.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/deployment-and-configuration.html"
    },
    {
        "title": "Disabling geo-replication",
        "content": "Disabling geo-replication\nPrerequisites\n\nGeo-replication is enabled, as described in Enabling geo-replication.\n\nTo disable geo-replication\n\nAdmin panel\n\nOn the primary cluster, go to Storage services > Backup storage > Geo-replication, and then click Disable replication. \nClick Disable in the confirmation window.\nOn the primary cluster, go to Storage services > Backup storage > Registrations, and then delete Geo-replication registration.\n\nOn the secondary cluster, do one of the following:\n\nIf you plan to use this backup storage, cancel geo-replication for it by running:# vinfra service backup geo-replication secondary cancel\n\nIf you do not need this backup storage, reinstall Virtuozzo Hybrid Infrastructure on each node from scratch.\n\nCommand-line interface\n\nDisable geo-replication on the primary cluster by running vinfra service backup geo-replication primary disable. For example:# vinfra service backup geo-replication primary disable\n\nDelete Geo-replication registration on the primary cluster by running vinfra service backup registration delete. For example:# vinfra service backup registration delete --username account@example.com --stdin \"Geo-replication registration\"\n\nCancel geo-replication on the secondary cluster by running vinfra service backup geo-replication secondary cancel. For example:# vinfra service backup geo-replication secondary cancel\n\nDestroy the secondary cluster to delete the backup data that has been replicated from the primary cluster. For example:# vinfra service backup cluster release\n\nWhat's next\n\nReleasing nodes from backup storage",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nDisable geo-replication on the primary cluster by running vinfra service backup geo-replication primary disable. For example:# vinfra service backup geo-replication primary disable\n\n\nDelete Geo-replication registration on the primary cluster by running vinfra service backup registration delete. For example:# vinfra service backup registration delete --username account@example.com --stdin \"Geo-replication registration\"\n\n\nCancel geo-replication on the secondary cluster by running vinfra service backup geo-replication secondary cancel. For example:# vinfra service backup geo-replication secondary cancel\n\n\nDestroy the secondary cluster to delete the backup data that has been replicated from the primary cluster. For example:# vinfra service backup cluster release\n\n\n",
                "title": "To disable geo-replication"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the primary cluster, go to Storage services > Backup storage > Geo-replication, and then click Disable replication. \nClick Disable in the confirmation window.\nOn the primary cluster, go to Storage services > Backup storage > Registrations, and then delete Geo-replication registration.\n\nOn the secondary cluster, do one of the following:\n\n\nIf you plan to use this backup storage, cancel geo-replication for it by running:# vinfra service backup geo-replication secondary cancel\n\nIf you do not need this backup storage, reinstall Virtuozzo Hybrid Infrastructure on each node from scratch.\n\n\n\n",
                "title": "To disable geo-replication"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/disabling-geo-replication.html"
    },
    {
        "title": "Deploying the appliance virtual machine",
        "content": "Deploying the appliance virtual machine\nTo create a virt-v2v appliance VM\n\nDownload the virt-v2v appliance image from the official repository.\n\nUpload the image to Virtuozzo Hybrid Infrastructure. For example:# vinfra service compute image create virt-v2v-img --file vmware_to_vip.qcow2\n\nCreate an SSH key for the appliance if you do not have one. For example:# vinfra service compute key create publickey --public-key virt-v2v-app-key.pub\r\n\n\nCreate a virtual machine and deploy the uploaded image in it. The VM needs at least two CPUs, 4 GiB RAM, and enough storage space to accommodate the largest VM to be migrated to Virtuozzo Hybrid Infrastructure. It must also be connected to the network that handles the Compute API traffic type and the network with access to VMware vCenter API. For example:# vinfra service compute server create virt-v2v-appliance --flavor medium --key-name <key>\r\n--network id=<compute_API> --network id=<vcenter_API> --volume source=image,id=virt-v2v-img,size=<size>\r\n\nWhere:\n\n<key> is the SSH key to authorize in the appliance VM.\n<compute_API> is the network that handles the traffic type Compute API.\n<vcenter_API> is the network that can access the VMware vCenter API.\n<size> is the disk size. For online migration, it must be enough to accommodate the largest VM of all you are going to migrate. For offline migration, it must be enough to accommodate twice as much.\n\nWhat's next\n\nSetting up authentication in the appliance virtual machine",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/deploying-the-appliance-virtual-machine.html"
    },
    {
        "title": "Disabling PCI passthrough and vGPU support",
        "content": "Disabling PCI passthrough and vGPU support\nIf you want to stop using PCI devices on a node, you need to remove them from the configuration file, and then use this file to reconfigure the compute cluster.\nPrerequisites\n\nThe compute cluster is reconfigured for PCI passthrough or vGPU support, as described in Enabling PCI passthrough and vGPU support.\n Before disabling PCI passthrough or vGPU support for a host device, stop using this device inside virtual machines. To do this, stop the VMs that are currently using this device, and then change their flavor to the one without PCI passthrough and vGPU-related properties.\n\nTo disable PCI passthrough and vGPU for all devices on a node\n\nCreate the configuration file with an empty device list:# cat config-empty.yaml\r\n- node_id: c3b2321a-7c12-8456-42ce-8005ff937e12\r\n  devices: []\r\n- node_id: 1d6481c2-1fd5-406b-a0c7-330f24bd0e3d\r\n  devices: []\n\nReconfigure the compute cluster by using the new configuration file. For example:# vinfra service compute set --pci-passthrough-config config-empty.yaml\n\nTo disable PCI\u00a0passthrough or vGPU for a particular device on a node\n\nRemove the information about the host device that you want to stop using from the configuration file. For example, to disable passthrough of the physical GPU with the 1b36:0100 VID and PID, remove these lines from config.yaml:- device_type: generic\r\n  device: 1b36:0100\r\n  alias: gpu\n\nReconfigure the compute cluster by using the updated configuration file. For example:# vinfra service compute set --pci-passthrough-config config.yaml\n\nSee also\n\nEnabling PCI passthrough and vGPU support",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/disabling-pci-passthrough-and-vgpu-support.html"
    },
    {
        "title": "Disk health analyzers",
        "content": "Disk health analyzers\nThe core part of calculating disk health are analyzers. Each analyzer calculates disks health based on its own algorithm. The overall disk health is a product of disk health values from all of the analyzers.\nFor example:\n\nAccording to the S.M.A.R.T. attributes, the disk health is 0.9.\nThe slow disk analyzer reports that the disk health is 0.4.\nThe slow CS analyzer reports that the disk health is 0.5.\nAccording to SCSI errors, the disk health is 1.0.\n\nThe overall disk health is calculated as 0.9*0.4*0.5*1.0 and equals 0.18 or 18%.\nS.M.A.R.T. attributes\nThe following table contains the S.M.A.R.T. attributes that affect the health value:\n\nID\nS.M.A.R.T. attribute\nWeight1 Defines by how many percent the attribute value decreases the total disk health value.\nLimit2 Defines the maximum impact the attribute may have on the total disk health., in percent\n\n05\nReallocated Sectors Count\n2\n70\n\n187\nReported Uncorrectable Errors\n1\n70\n\n188\nCommand Timeout\n1\n20\n\n197\nCurrent Pending Sectors Count\n2\n70\n\n198\nOffline uncorrectable Sectors Count\n2\n70\n\n233\nMedia Wearout Indicator\n1\n100\n\nThe  disk health is calculated by using the following formula:Disk health (%) = K * \u00d0\u009f (100% - D)\nwhere:\n\nK is the reduction coefficient. A disk is considered less healthy if it reports more then one type of S.M.A.R.T. errors. The coefficient formula is 0.8^({Number of S.M.A.R.T. attributes with error} \u00e2\u0080\u0093 1). Possible values are 0\u00e2\u0080\u00931.\n\u00d0\u009f is a product of minimums  calculated for each critical S.M.A.R.T. attribute.\n100% is the initial health of the disk.\nD is a minimum from the limit and attribute value with its weight. Its formula is (min(limit, attribute_value * weight)).\nlimit is a limit of each critical S.M.A.R.T. attribute.\nattribute_value is the current attribute value.\nweight is weight of each critical S.M.A.R.T. attribute.\n\nFor example:\n\nReallocated Sectors Count: attribute value = 30, weight = 2, limit = 70\nCommand Timeout: attribute value = 23, weight = 1, limit = 20\n\nK = 0.8 * (2\u00e2\u0080\u00931) = 0.8\n\nThe S.M.A.R.T. disk health is calculated as follows: 0.8 * (100% \u00e2\u0080\u0093 (min(30*2, 70))) * (100% - min(23*1, 20))) = 0.8 * 0.4 * 0.8 = 0.256 (26%)\nSlow disk and slow CS analyzers\nSlow disk and CS analyzers calculate disk health according to the average I/O latency over time (15 minutes).\nThe following table shows the default thresholds:\n\nAnalyzer\nOK latency3 Maximum latency value to consider disk health to be 100%., in seconds\nFATAL latency4  Latency value to consider disk health to be 0%., in seconds\n\nSlow CS\n0.1\n60\n\nSlow HDD Disk\n0.1\n30\n\nSlow SSD Disk\n0.01\n10\n\nIf disk latency is less than OK latency, the disk health is considered to be 100%. If disk latency exceeds FATAL latency, the disk health is considered to be 0%. Disk latency that lies within these two thresholds will vary linearly from 100% to 0%.\n\nWhen disk health becomes 0%, the service generates an alert and marks this CS as unresponsive. Such a CS does not trigger automatic replication but is no longer available for chunk allocation. \nSCSI errors\nBy default, each SCSI failure decrease the overall disk heath by 4%. The maximum health impact is set to 70.\nSee also\n\nDisk-related metrics in Prometheus",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/disk-health-analyzers.html"
    },
    {
        "title": "Disk requirements",
        "content": "Disk requirements\nDisk types and roles\n\nUsing SATA HDDs with one SSD for caching is more cost effective than using only SAS HDDs without such an SSD.\nUsing NVMe or SAS SSDs for write caching improves random I/O performance and is highly recommended for all workloads with heavy random access (for example, iSCSI volumes). In turn, SATA disks are best suited for SSD-only configurations but not write caching.\nRunning metadata services on SSDs improves cluster performance. To also minimize CAPEX, the same SSDs can be used for write caching.\nUsing shingled magnetic recording (SMR) HDDs is available only for storage purposes and only if the node has an SSD disk for cache.\nIf capacity is the main goal and you need to store infrequently accessed data, select SATA disks over SAS ones. If performance is the main goal, select NVMe or SAS disks over SATA ones.\nDisk block size (for example, 512b or 4K) is not important and has no effect on performance.\nThe maximum supported physical partition size is 254 TiB.\n\nDisk capacity\n\nThe system disk must have at least 100 GB of space.\nIt is possible to use disks of different size in the same cluster. However, keep in mind that, given the same IOPS, smaller disks will offer higher performance per terabyte of data compared to bigger disks. It is recommended to group disks with the same IOPS per terabyte in the same tier.\nThe capacity of HDD and SSD is measured and specified with decimal, not binary prefixes, so \u00e2\u0080\u009cTB\u00e2\u0080\u009d in disk specifications usually means \u00e2\u0080\u009cterabyte.\u00e2\u0080\u009d The operating system, however, displays a drive capacity using binary prefixes meaning that \u00e2\u0080\u009cTB\u00e2\u0080\u009d is \u00e2\u0080\u009ctebibyte\u00e2\u0080\u009d which is a noticeably larger number. As a result, disks may show a capacity smaller than the one marketed by the vendor. For example, a disk with 6 TB in specifications may be shown to have 5.45 TB of actual disk space in Virtuozzo Hybrid Infrastructure. 5 percent of disk space is reserved for emergency needs. Therefore, if you add a 6 TB disk to a cluster, the available physical space should increase by about 5.2 TB.\nPerformance of SSD disks may depend on their size. Lower-capacity drives (100 to 400 GB) may perform much slower (sometimes up to ten times slower) than higher-capacity ones (1.9 to 3.8 TB). Check the drive performance and endurance specifications before purchasing hardware.\nThin provisioning is always enabled for all data and cannot be configured otherwise.\n\nConsumer-grade SSD drives\n\nConsumer-grade SSD drives can withstand a very low number of rewrites. SSD drives intended for storage clusters must offer at least 1 DWPD endurance (10 DWPD is recommended). The higher the endurance, the less often SSDs will need to be replaced, and this will improve TCO.\nConsumer-grade SSD drives usually have unstable performance and are not suited to withstand sustainable enterprise workloads. For this reason, pay attention to sustainable load tests when choosing SSDs.\nMany consumer-grade SSD drives can ignore disk flushes and falsely report to operating systems that data was written while it, in fact, was not. Examples of such drives include OCZ Vertex 3, Intel 520, Intel X25-E, and Intel X-25-M G2. These drives are known to be unsafe in terms of data commits, they should not be used with databases, and they may easily corrupt the file system in case of a power failure. It is recommended to use enterprise-grade SSD drives with power loss protection, as described in Protecting data during a power outage.\n\nRAID and HBA controllers\n\nCreate hardware or software RAID1 volumes for system disks by using RAID or HBA controllers, respectively, to ensure its high performance and availability.\nUse HBA controllers, as they are less expensive and easier to manage than RAID controllers.\nDisable all RAID controller caches for SSD drives. Modern SSDs have good performance that can be reduced by a RAID controller\u00e2\u0080\u0099s write and read cache. It is recommended to disable caching for SSD drives and leave it enabled only for HDD drives.\nIf you use RAID controllers, do not create RAID volumes from HDDs intended for storage. Each storage HDD needs to be recognized by Virtuozzo Hybrid Infrastructure as a separate device.\nIf you use RAID controllers with caching, equip them with backup battery units (BBUs), to protect against cache loss during power outages.\n\nSee also\n\nHardware recommendations\n\nServer requirements\n\nNetwork requirements and recommendations\n\nAdmin panel requirements",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/disk-requirements.html"
    },
    {
        "title": "Deploying the storage cluster",
        "content": "Deploying the storage cluster\nCreate the storage cluster on one (the first) node, then populate it with more nodes.\nLimitations\n\nYou can assign a role to a disk only if its size is greater than 1 GiB.\nYou can assign an additional role to a system disk only if its size is at least 100 GiB.\nIt is recommended to assign the System and Metadata roles to either an SSD disk or different HDDs. Assigning both of these roles to the same HDD disk will result in mediocre performance suitable only for cold data (for example, archiving).\nThe System role cannot be combined with the Cache and Metadata+Cache roles. The reason is that the I/O generated by the operating system and applications would contend with the I/O generated by journaling, thus negating its performance benefits.\nYou can use shingled magnetic recording (SMR) HDDs only with the Storage role and only if the node has an SSD disk with the Cache role.\nYou cannot use SMR and standard disks in the same tier.\nYou cannot assign roles to system and non-system disks at a time.\n\nPrerequisites\n\nA clear understanding of the storage cluster architecture and disk roles, which are explained in About the storage cluster.\n A clear understanding of the concept Storage tiers.\nYour infrastructure networks are set up, as described in Setting up networks.\nThe node network interfaces are configured by following the instructions in Configuring node network interfaces.\n If  supported, RDMA  is enabled, as described in Enabling RDMA.\nIf your infrastructure nodes are equipped with NVMe or SSD disks, it is recommended to enable NVMe performance, as described in Configuring NVMe performance.\nExternal DNS servers are added automatically during the installation or manually, as described in Adding external DNS servers.\nLocations for your nodes are configured, as explained in Configuring node locations.\nAll of the nodes are shown in the admin panel on the Infrastructure > Nodes screen with the Unassigned status.\n\nTo create the storage cluster on the first node\n\nAdmin panel\n\nOpen the Infrastructure > Nodes screen, and then click Create storage cluster.\nIn the Create storage cluster window, enter a name for the cluster. The cluster name may only contain Latin letters (a-z, A-Z), numbers (0-9), and hyphens (\"-\"). It must start with a letter and end with a letter or number.\n\nEnable disk encryption for tiers. You can also enable it later.\n\nSelect one node to create the storage cluster from, and then click Next.\n\nIn the next window, check the default disk configuration. If it is correct, proceed to create the storage cluster.\nAlso, you can assign roles to your disks manually or use Disk actions to work with the disks.\n\nTo assign roles to disks manually, do the following:\n\n[Only for SSD drives] To store write cache\n\nSelect the Cache role.\nSelect a storage tier that you want to cache.\n\nFor storage disks to use cache, the Cache role must be assigned before the Storage role. You can also assign both of these roles to disks at the same time, and the system will configure the cache disk first.\n\nTo store data\n\nSelect the Storage role.\nSelect a storage tier where to store your data. To make better use of data redundancy, do not assign all of the disks on a node to the same tier. Instead, make sure that each tier is evenly distributed across the cluster.\n\nEnable data caching and checksumming:\n\nEnable SSD caching and checksumming. Available and recommended only for nodes with SSDs.\nEnable checksumming (default). Recommended for nodes with HDDs as it provides better reliability.\nDisable checksumming. Not recommended for production. For an evaluation or testing environment, you can disable checksumming for nodes with HDDs, to provide better performance.\n\nTo store cluster metadata\n\nSelect the Metadata role.\n\nIt is recommended to have only one disk with the Metadata role per node and maximum five such disks in a cluster.\n\n[Only for SSD drives] To store both metadata and write cache\n\nSelect the Metadata+Cache role.\nSelect a storage tier that you want to cache.\n\nTo assign roles to disks automatically, click Disk actions > Configure automatically.\nTo assign a role to multiple disks at a time, click Disk actions > Bulk disk management, select disks, and then click Assign role. Choose the desired role for the selected disks, and then click Assign.\nTo reset the disk configuration, click Disk actions > Clear configuration.\n\nOnce you finish configuring the disks, click Create, to create the storage cluster.\n\nYou can monitor cluster creation on the Infrastructure > Nodes screen. The creation might take some time, depending on the number of disks to be configured. Once the configuration is complete, the cluster is created.\n\nCommand-line interface\nUse the following command:vinfra cluster create [--disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]] [--tier-encryption {0,1,2,3}]\r\n                      --node <node> <cluster-name>\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n--tier-encryption {0,1,2,3}\n\nEnable encryption for storage cluster tiers. Encryption is disabled by default. This option can be used multiple times.\n--node <node>\n\nNode ID or hostname\n<cluster-name>\n\nStorage cluster name\n\nFor example, to create the storage cluster stor1 on the node node001, run:# vinfra cluster create stor1 --node node001\nAs disk roles are not explicitly specified, they are assigned automatically: mds-system to the system disk, and cs to all other disks.\nYou can view the storage cluster details in the vinfra cluster show output:# vinfra cluster show\r\n+-------+--------------------------------------------+\r\n| Field | Value                                      |\r\n+-------+--------------------------------------------+\r\n| id    | 1                                          |\r\n| name  | stor1                                      |\r\n| nodes | - host: node001.vstoragedomain             |\r\n|       |   id: f59dabdb-bd1c-4944-8af2-26b8fe9ff8d4 |\r\n|       |   is_installing: false                     |\r\n|       |   is_releasing: false                      |\r\n+-------+--------------------------------------------+\r\n\n\nTo add nodes to the cluster\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click an unassigned node.\nOn the node right pane, click Join to cluster.\n\nIn the Join node to storage cluster window, check the default disk configuration. If it is correct, proceed to join the node to the storage cluster.\nAlso, you can assign roles to your disks manually or use Disk actions to work with the disks. Alternatively, you can copy the disk configuration from another node by clicking Copy configuration from and selecting the desired node.\n\nOnce you finish configuring the disks, click Join, to add the node to the storage cluster.\n\nCommand-line interface\nUse the following command:vinfra node join [--disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]] <node>\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n<node>\n\nNode ID or hostname\n\nFor example, to add the node node002 to the storage cluster and assign roles to disks: mds-system to sda, cs to sdb and sdc, run:# vinfra node join f59dabdb-bd1c-4944-8af2-26b8fe9ff8d4 --disk sda:mds-system \\\r\n--disk sdb:cs --disk sdc:cs\nThe added node will appear in the vinfra node list output:# vinfra node list\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| id           | host         | is_primary | is_online | is_assigned | is_in_ha |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| 09bb6b8<...> | node001<...> | True       | True      | True        | False    |\r\n| 187edb1<...> | node002<...> | False      | True      | True        | False    |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n\n\nSee also\n\nScaling the storage cluster\n\nMonitoring the storage cluster\n\nWhat's next\n\nEnabling management node high availability\n\nProvisioning backup storage space\n\nProvisioning object storage space\n\nProvisioning block storage space\n\nProvisioning file storage space\n\nProvisioning compute resources",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster create [--disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]] [--tier-encryption {0,1,2,3}]\r\n                      --node <node> <cluster-name>\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n--tier-encryption {0,1,2,3}\n\nEnable encryption for storage cluster tiers. Encryption is disabled by default. This option can be used multiple times.\n--node <node>\n\nNode ID or hostname\n<cluster-name>\n\nStorage cluster name\n\nFor example, to create the storage cluster stor1 on the node node001, run:# vinfra cluster create stor1 --node node001\nAs disk roles are not explicitly specified, they are assigned automatically: mds-system to the system disk, and cs to all other disks.\nYou can view the storage cluster details in the vinfra cluster show output:# vinfra cluster show\r\n+-------+--------------------------------------------+\r\n| Field | Value                                      |\r\n+-------+--------------------------------------------+\r\n| id    | 1                                          |\r\n| name  | stor1                                      |\r\n| nodes | - host: node001.vstoragedomain             |\r\n|       |   id: f59dabdb-bd1c-4944-8af2-26b8fe9ff8d4 |\r\n|       |   is_installing: false                     |\r\n|       |   is_releasing: false                      |\r\n+-------+--------------------------------------------+\r\n\n",
                "title": "To create the storage cluster on the first node"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node join [--disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]] <node>\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n<node>\n\nNode ID or hostname\n\nFor example, to add the node node002 to the storage cluster and assign roles to disks: mds-system to sda, cs to sdb and sdc, run:# vinfra node join f59dabdb-bd1c-4944-8af2-26b8fe9ff8d4 --disk sda:mds-system \\\r\n--disk sdb:cs --disk sdc:cs\nThe added node will appear in the vinfra node list output:# vinfra node list\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| id           | host         | is_primary | is_online | is_assigned | is_in_ha |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| 09bb6b8<...> | node001<...> | True       | True      | True        | False    |\r\n| 187edb1<...> | node002<...> | False      | True      | True        | False    |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n\n",
                "title": "To add nodes to the cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen the Infrastructure > Nodes screen, and then click Create storage cluster.\nIn the Create storage cluster window, enter a name for the cluster. The cluster name may only contain Latin letters (a-z, A-Z), numbers (0-9), and hyphens (\"-\"). It must start with a letter and end with a letter or number.\n\nEnable disk encryption for tiers. You can also enable it later.\n\n\nSelect one node to create the storage cluster from, and then click Next.\n\n\n\n\n\n\nIn the next window, check the default disk configuration. If it is correct, proceed to create the storage cluster.\nAlso, you can assign roles to your disks manually or use Disk actions to work with the disks.\n\n\nTo assign roles to disks manually, do the following:\n\n\n[Only for SSD drives] To store write cache\n\n\nSelect the Cache role.\nSelect a storage tier that you want to cache.\n\n\nFor storage disks to use cache, the Cache role must be assigned before the Storage role. You can also assign both of these roles to disks at the same time, and the system will configure the cache disk first.\n\n\n\n\n\nTo store data\n\n\nSelect the Storage role.\nSelect a storage tier where to store your data. To make better use of data redundancy, do not assign all of the disks on a node to the same tier. Instead, make sure that each tier is evenly distributed across the cluster.\n\nEnable data caching and checksumming:\n\nEnable SSD caching and checksumming. Available and recommended only for nodes with SSDs.\nEnable checksumming (default). Recommended for nodes with HDDs as it provides better reliability.\nDisable checksumming. Not recommended for production. For an evaluation or testing environment, you can disable checksumming for nodes with HDDs, to provide better performance.\n\n\n\n\n\n\n\nTo store cluster metadata\n\nSelect the Metadata role.\n\nIt is recommended to have only one disk with the Metadata role per node and maximum five such disks in a cluster.\n\n\n\n\n\n[Only for SSD drives] To store both metadata and write cache\n\n\nSelect the Metadata+Cache role.\nSelect a storage tier that you want to cache.\n\n\n\n\n\n\n\n\n\n\nTo assign roles to disks automatically, click Disk actions > Configure automatically.\nTo assign a role to multiple disks at a time, click Disk actions > Bulk disk management, select disks, and then click Assign role. Choose the desired role for the selected disks, and then click Assign.\nTo reset the disk configuration, click Disk actions > Clear configuration.\n\n\nOnce you finish configuring the disks, click Create, to create the storage cluster.\n\nYou can monitor cluster creation on the Infrastructure > Nodes screen. The creation might take some time, depending on the number of disks to be configured. Once the configuration is complete, the cluster is created.\n",
                "title": "To create the storage cluster on the first node"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click an unassigned node.\nOn the node right pane, click Join to cluster.\n\nIn the Join node to storage cluster window, check the default disk configuration. If it is correct, proceed to join the node to the storage cluster.\nAlso, you can assign roles to your disks manually or use Disk actions to work with the disks. Alternatively, you can copy the disk configuration from another node by clicking Copy configuration from and selecting the desired node.\n\nOnce you finish configuring the disks, click Join, to add the node to the storage cluster.\n\n",
                "title": "To add nodes to the cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/deploying-the-storage-cluster.html"
    },
    {
        "title": "Disk-related metrics in Prometheus",
        "content": "Disk-related metrics in Prometheus\nThe Prometheus service stores the following disk-related metrics:\n\nCS-related metrics\n\ncsd_io_op_time_seconds\n\nMean time per I/O request\r\n\n\nmaster:mdsd_cs_status\n\nCS status on master MDS\r\n\n\nDisk-related metrics in /proc/diskstats\n\nnode_disk_read_time_seconds\n\nTotal time, in seconds, spent on read requests\n\nnode_disk_reads_completed\n\nTotal number of completed read requests\r\n\n\nnode_disk_write_time_seconds\n\nTotal time, in seconds, spent on write requests\n\nnode_disk_writes_completed\n\nTotal number of completed write requests\n\nS.M.A.R.T. metrics\n\nsmart_device_smart_healthy\n\nS.M.A.R.T. status is healthy\n\nsmart_reallocated_sector_ct\n\nTotal number of reallocated disk sectors (05)\n\nsmart_reported_uncorrect\n\nTotal number of errors that could not be recovered using hardware ECC (187)\n\nsmart_command_timeout\n\nTotal number of aborted operations due to a timeout (188)\n\nsmart_current_pending_sector\n\nTotal number of unstable sectors (197)\n\nsmart_offline_uncorrectable\n\nTotal number of uncorrectable errors when reading/writing a sector (198)\n\nsmart_media_wearout_indicator\n\nMedia Wearout Indicator for SSD (233)\n\nsmart_nvme_intel_wear_leveling\n\nMedia Wearout Indicator for Intel NVME (233)\n\nsmart_scsi_read_errors_uncorrected\n\nTotal number of uncorrectable errors when reading a sector\n\nsmart_scsi_reallocated_sector_ct\n\nTotal number of reallocated disk sectors\n\nsmart_scsi_verify_errors_uncorrected\n\nTotal number of uncorrectable errors when verifying a sector\n\nsmart_scsi_write_errors_uncorrected\n\nTotal number of uncorrectable errors when writing a sector\n\nKernel SCSI errors\n\nkernel_scsi_failures_total\n\nTotal number of SCSI failures reported by the kernel\n\nDisk health metric from vstorage-disks-monitor\n\ndiskmon_cs_disk_health\n\nDisk health reported by the vstorage-disks-monitor service. Possible values are 0.0\u00e2\u0080\u00931.0. The 1.0 value means that the disk is 100% healthy.\n\nSee also\n\nDisk health analyzers",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/disk-related-metrics-in-prometheus.html"
    },
    {
        "title": "Editing and deleting compute networks",
        "content": "Editing and deleting compute networks\nYou can edit the network name and network access, as well as delete a compute network that is not used by virtual machines.\nLimitations\n\nYou cannot change IP management of a compute network.\n\nPrerequisites\n\nCompute networks are created automatically during the compute cluster deployment or manually, as described in Creating physical compute networks and Creating virtual compute networks.\nTo be able to delete a compute network, no virtual machines must be connected to it.\n\nTo view and edit parameters of a compute network\n\nAdmin panel\n\nOn the Compute > Networks tab, click the network you want to edit. \nOn the network right pane, click the pencil icon next to the required section, and then make your changes.\n\nCommand-line interface\nUse the following command:vinfra service compute network set [--rbac-policies <rbac-policies>]\r\n                                   [--name <name>] <network>\r\n\n\n--rbac-policies <rbac-policies>\n\nComma-separated list of RBAC policies in the format: <target>:<target_id>:<action> | none. Valid targets: project, domain. Valid actions: direct, full, routed. \u00e2\u0080\u0098*\u00e2\u0080\u0099 is valid target_id for all targets. Pass none to clear out all existing policies.\nExample: domain:default:routed,project:uuid1:full\n\n--name <name>\n\nA new name for the network\n<network>\n\nNetwork ID or name\n\nFor example, to  disable network access for the compute network mypubnet, run:# vinfra service compute network set mypubnet --rbac-policies none\r\n+------------------+--------------------------------------+\r\n| Field            | Value                                |\r\n+------------------+--------------------------------------+\r\n| allocation_pools | 10.136.18.141-10.136.18.148          |\r\n| cidr             | 10.136.16.0/22                       |\r\n| dns_nameservers  | 10.35.11.7                           |\r\n| enable_dhcp      | True                                 |\r\n| gateway_ip       | 10.136.16.1                          |\r\n| id               | 22674f9d-1c94-4953-b79b-7f6029ee9bd0 |\r\n| ip_version       | 4                                    |\r\n| ipam_enabled     | True                                 |\r\n| name             | mypubnet                             |\r\n| physical_network | Public                               |\r\n| project_id       | c22613639b3147e0b22ef057b87698fe     |\r\n| rbac_policies    | []                                   |\r\n| router_external  | False                                |\r\n| shared           | False                                |\r\n| tags             | []                                   |\r\n| type             | physical                             |\r\n| vlan_id          |                                      |\r\n+------------------+--------------------------------------+\r\n\n\nTo delete a compute network\n\nAdmin panel\n\nOn the Compute > Networks tab, click the network you want to delete. \nOn the network right pane, click Delete.\n\nCommand-line interface\nUse the following command:vinfra service compute network delete <network>\r\n\n\n<network>\n\nNetwork ID or name\n\nFor example, to delete the compute network myprivnet, run:# vinfra service compute network delete myprivnet\n\nSee also\n\nManaging security groups\n\nManaging virtual routers\n\nManaging floating IP addresses\n\nManaging load balancers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute network set [--rbac-policies <rbac-policies>]\r\n                                   [--name <name>] <network>\r\n\n\n--rbac-policies <rbac-policies>\n\n\nComma-separated list of RBAC policies in the format: <target>:<target_id>:<action> | none. Valid targets: project, domain. Valid actions: direct, full, routed. \u00e2\u0080\u0098*\u00e2\u0080\u0099 is valid target_id for all targets. Pass none to clear out all existing policies.\nExample: domain:default:routed,project:uuid1:full\n\n--name <name>\n\nA new name for the network\n<network>\n\nNetwork ID or name\n\nFor example, to  disable network access for the compute network mypubnet, run:# vinfra service compute network set mypubnet --rbac-policies none\r\n+------------------+--------------------------------------+\r\n| Field            | Value                                |\r\n+------------------+--------------------------------------+\r\n| allocation_pools | 10.136.18.141-10.136.18.148          |\r\n| cidr             | 10.136.16.0/22                       |\r\n| dns_nameservers  | 10.35.11.7                           |\r\n| enable_dhcp      | True                                 |\r\n| gateway_ip       | 10.136.16.1                          |\r\n| id               | 22674f9d-1c94-4953-b79b-7f6029ee9bd0 |\r\n| ip_version       | 4                                    |\r\n| ipam_enabled     | True                                 |\r\n| name             | mypubnet                             |\r\n| physical_network | Public                               |\r\n| project_id       | c22613639b3147e0b22ef057b87698fe     |\r\n| rbac_policies    | []                                   |\r\n| router_external  | False                                |\r\n| shared           | False                                |\r\n| tags             | []                                   |\r\n| type             | physical                             |\r\n| vlan_id          |                                      |\r\n+------------------+--------------------------------------+\r\n\n",
                "title": "To view and edit parameters of a compute network"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute network delete <network>\r\n\n\n<network>\n\nNetwork ID or name\n\nFor example, to delete the compute network myprivnet, run:# vinfra service compute network delete myprivnet\n",
                "title": "To delete a compute network"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Networks tab, click the network you want to edit. \nOn the network right pane, click the pencil icon next to the required section, and then make your changes.\n\n",
                "title": "To view and edit parameters of a compute network"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Networks tab, click the network you want to delete. \nOn the network right pane, click Delete.\n\n",
                "title": "To delete a compute network"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/editing-and-deleting-compute-networks.html"
    },
    {
        "title": "Editing and deleting identity providers",
        "content": "Editing and deleting identity providers\nFor an existing identity provider, you can change the configuration and authorization flow type. You also can enable or disable an identity provider, thus allowing or prohibiting login for its federated users in the management panel. After deleting an identity provider, all of its federated users are removed along with it.\nPrerequisites\n\nIdentity providers are added to the admin panel, as described in Adding  identity providers.\n\nTo edit an identity provider\n\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, click the ellipsis icon next to the identity provider, and then click Edit.\nMake the required changes, and then click Save.\n\nAfter modifying the identity provider parameters, all of its federated users will be logged out of the management panel.\n\nCommand-line interface\nUse the following command:vinfra domain idp set [--issuer <issuer>] [--scope <issuer>] [--metadata-url <metadata-url>]\r\n                      [--client-id <client-id>] [--client-secret <client-secret>]\r\n                      [--mapping <path>] [--verify-ssl | --dont-verify-ssl] [--request-timeout <seconds>]\r\n                      [--name <name>] --domain <domain> <idp>\n\n--issuer <issuer>\n\nIdentity provider issuer\n--scope <scope>\n\nScope that define what user identity data will be shared by the identity provider during authentication\n--metadata-url <metadata-url>\n\nMetadata URL of the identity provider's discovery endpoint\n--client-id <client-id>\n\nClient ID to access the identity provider\n--client-secret <client-secret>\n\nClient secret to access the identity provider\n--mapping <path>\n\nPath to the mapping configuration file.\nA mapping file may look as follows:# cat mapping.json\r\n[\r\n    {\r\n        \"local\": [\r\n            {\r\n                \"user\": {\r\n                    \"name\": \"{0}\"\r\n                },\r\n                \"group\": {\r\n                    \"name\":\"users\"\r\n                }\r\n            }\r\n        ],\r\n        \"remote\": [{\"type\": \"email\"}]\r\n    }\r\n]\nIn this example, all users that have the attribute email will be mapped to the group users within the default domain. For details on creating a mapping file, refer to the OpenStack documentation.\n\n--verify-ssl\n\nEnable identity provider endpoints SSL verification\n--dont-verify-ssl\n\nDisable identity provider endpoints SSL verification\n--request-timeout <seconds>\n\nIdentity provider API request timeout (default: 5)\n--name <name>\n\nA new name for the identity provider\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to change the mapping rules of the identity provider My ADFS within the mydomain domain by using the mapping file new_mapping.json, run:# vinfra domain idp set \"My ADFS\" --domain mydomain --mapping new_mapping.json\nAfter modifying the identity provider parameters, all of its federated users will be logged out of the management panel.\n\nTo change the authorization flow type\nUse the following command:vinfra domain idp set --response-type <response-type> --domain <domain> <idp>\n\n--response-type <response-type>\n\nResponse type to be used in the authorization flow: \n\ncode: use the Authorization Code Flow\nid_token: use the Implicit Flow\n\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to change the authorization flow type of the identity provider My ADFS within the mydomain domain to the Authorization Code Flow, run:# vinfra domain idp set \"My ADFS\" --domain mydomain --response-type code\nTo change the authorization flow back to Implicit Flow, run:# vinfra domain idp set \"My ADFS\" --domain mydomain --response-type id_token\nTo enable or disable an identity provider\n\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, click the ellipsis icon next to the identity provider, and then click Enable or Disable.\n\nCommand-line interface\nUse the following command:vinfra domain idp set [--enable] [--disable] --domain <domain> <idp>\n\n--enable\n\nEnable identity provider\n--disable\n\nDisable identity provider\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to disable the identity provider My ADFS within the mydomain, run:# vinfra domain idp set \"My ADFS\" --domain mydomain --disable\n\nTo delete an identity provider\n\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, click the ellipsis icon next to the identity provider, and then click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra domain idp delete --domain <domain> <idp>\n\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to delete the identity provider My ADFS within the mydomain, run:# vinfra domain idp delete \"My ADFS\" --domain mydomain\n\nSee also\n\nSigning in through identity providers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain idp set [--issuer <issuer>] [--scope <issuer>] [--metadata-url <metadata-url>]\r\n                      [--client-id <client-id>] [--client-secret <client-secret>]\r\n                      [--mapping <path>] [--verify-ssl | --dont-verify-ssl] [--request-timeout <seconds>]\r\n                      [--name <name>] --domain <domain> <idp>\n\n--issuer <issuer>\n\nIdentity provider issuer\n--scope <scope>\n\nScope that define what user identity data will be shared by the identity provider during authentication\n--metadata-url <metadata-url>\n\nMetadata URL of the identity provider's discovery endpoint\n--client-id <client-id>\n\nClient ID to access the identity provider\n--client-secret <client-secret>\n\nClient secret to access the identity provider\n--mapping <path>\n\n\nPath to the mapping configuration file.\nA mapping file may look as follows:# cat mapping.json\r\n[\r\n    {\r\n        \"local\": [\r\n            {\r\n                \"user\": {\r\n                    \"name\": \"{0}\"\r\n                },\r\n                \"group\": {\r\n                    \"name\":\"users\"\r\n                }\r\n            }\r\n        ],\r\n        \"remote\": [{\"type\": \"email\"}]\r\n    }\r\n]\nIn this example, all users that have the attribute email will be mapped to the group users within the default domain. For details on creating a mapping file, refer to the OpenStack documentation.\n\n--verify-ssl\n\nEnable identity provider endpoints SSL verification\n--dont-verify-ssl\n\nDisable identity provider endpoints SSL verification\n--request-timeout <seconds>\n\nIdentity provider API request timeout (default: 5)\n--name <name>\n\nA new name for the identity provider\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to change the mapping rules of the identity provider My ADFS within the mydomain domain by using the mapping file new_mapping.json, run:# vinfra domain idp set \"My ADFS\" --domain mydomain --mapping new_mapping.json\nAfter modifying the identity provider parameters, all of its federated users will be logged out of the management panel.\n",
                "title": "To edit an identity provider"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain idp set [--enable] [--disable] --domain <domain> <idp>\n\n--enable\n\nEnable identity provider\n--disable\n\nDisable identity provider\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to disable the identity provider My ADFS within the mydomain, run:# vinfra domain idp set \"My ADFS\" --domain mydomain --disable\n",
                "title": "To enable or disable an identity provider"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain idp delete --domain <domain> <idp>\n\n--domain <domain>\n\nDomain name or ID\n<idp>\n\nIdentity provider name or ID\n\nFor example, to delete the identity provider My ADFS within the mydomain, run:# vinfra domain idp delete \"My ADFS\" --domain mydomain\n",
                "title": "To delete an identity provider"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, click the ellipsis icon next to the identity provider, and then click Edit.\nMake the required changes, and then click Save.\n\nAfter modifying the identity provider parameters, all of its federated users will be logged out of the management panel.\n",
                "title": "To edit an identity provider"
            },
            {
                "example": "\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, click the ellipsis icon next to the identity provider, and then click Enable or Disable.\n\n",
                "title": "To enable or disable an identity provider"
            },
            {
                "example": "\nAdmin panel\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, click the ellipsis icon next to the identity provider, and then click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete an identity provider"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/editing-and-deleting-identity-providers.html"
    },
    {
        "title": "Editing and deleting domain groups",
        "content": "Editing and deleting domain groups\nYou can edit the following parameters of a domain group: its name, description, user role, system and domain permissions. When you delete a domain group, all of the assigned users are automatically logged out of the management panel.\nPrerequisites\n\nDomain groups are created, as described in Creating domain groups.\n\nTo edit a domain group\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a group.\nGo to the Domain groups tab, click the ellipsis icon next to the group, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nCommand-line interface\nUse the following command:vinfra domain group set [--name <name>] [--description <description>]\r\n                        [--domain-permissions <domain_permissions>]\r\n                        [--system-permissions <system_permissions>]\r\n                        --domain <domain> <group>\r\n\n\n--name <name>\n\nNew name for a group\n--description <description>\n\nGroup description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--system-permissions <system_permissions>\n\nA comma-separated list of system permissions. View the list of available system permissions using vinfra domain user list-available-roles | grep system.\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n\nFor example, to change the name of the domain group users to myusers, run:# vinfra domain group set users --domain mydomain --name myusers\n\nTo delete a domain group\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to delete a domain group.\nGo to the Domain groups tab, click the ellipsis icon next to the group, and then click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra domain group delete --domain <domain> <group>\r\n\n\n--domain <domain>\n\nDomain ID or name\n<group>\n\nGroup ID or name\n\nFor example, to delete the group users from the domain mydomain:# vinfra domain group delete users --domain mydomain\n\nSee also\n\nManaging user assignment to domain groups\n\nManaging project assignment to domain groups",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain group set [--name <name>] [--description <description>]\r\n                        [--domain-permissions <domain_permissions>]\r\n                        [--system-permissions <system_permissions>]\r\n                        --domain <domain> <group>\r\n\n\n--name <name>\n\nNew name for a group\n--description <description>\n\n\nGroup description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--system-permissions <system_permissions>\n\nA comma-separated list of system permissions. View the list of available system permissions using vinfra domain user list-available-roles | grep system.\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n\nFor example, to change the name of the domain group users to myusers, run:# vinfra domain group set users --domain mydomain --name myusers\n",
                "title": "To edit a domain group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain group delete --domain <domain> <group>\r\n\n\n--domain <domain>\n\nDomain ID or name\n<group>\n\nGroup ID or name\n\nFor example, to delete the group users from the domain mydomain:# vinfra domain group delete users --domain mydomain\n",
                "title": "To delete a domain group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a group.\nGo to the Domain groups tab, click the ellipsis icon next to the group, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n",
                "title": "To edit a domain group"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to delete a domain group.\nGo to the Domain groups tab, click the ellipsis icon next to the group, and then click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a domain group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/editing-and-deleting-domain-groups.html"
    },
    {
        "title": "Enabling and disabling data-in-transit encryption",
        "content": "Enabling and disabling data-in-transit encryption\nPrerequisites\n\nEnsure that an infrastructure network and network switches are configured to use jumbo frames (MTU 9000). Otherwise, enabling data-in-transit encryption will significantly decrease the cluster performance.\nIf you want to encrypt an infrastructure network with the Storage traffic type, ensure that IPv6 traffic and IPv6 multicast addresses are allowed on your network equipment.\nIf you want to encrypt an infrastructure network with the VM private traffic type, keep in mind that the encryption increases the default MTU overhead for virtual networks from 50 to 87 bytes. If you have virtual networks created in version 5.2 or earlier, or virtual networks with a custom MTU setting, you need to manually adjust the MTU of virtual machines connected to such virtual networks before enabling the encryption.\nIf you want to encrypt an infrastructure network with the VM backups traffic type, note that the encryption may break agentless backup integration. To avoid this, reassign the VM backups traffic type to an unencrypted network.\n\nTo enable data-in-transit encryption\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Enable encryption.\nIn the Enable encryption window, review the important information about the encryption requirements, and click Enable.\nWait until the operation is finished. You cannot manage infrastructure networks while the operation is in progress.\n\nCommand-line interface\nUse the following command:vinfra cluster network encryption enable [--no-switch-storage-ipv6] <network1> [<network2> ...]\n\n--no-switch-storage-ipv6\n\nDo not switch chunk services to IPv6 addresses\n<network>\n\nNetwork ID or name\n\nFor example, to enable data-in-transit encryption for the Private network, run:# vinfra cluster network encryption enable Private\nTo check the encryption status of your networks, use the vinfra cluster network encryption status command:# vinfra cluster network encryption status\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n| id                                   | name    | status   | subnets                               |\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n| fa0d118e-2ec5-43e5-8813-41ab95d7f1f1 | Private | enabled  | - enabled    192.168.128.0/24         |\r\n|                                      |         |          | - enabled    fd48:e4ee:f220:2808::/64 |\r\n| 178f54ac-7040-40db-95cb-099cc3e8394e | Public  | disabled | - disabled   10.136.16.0/20           |\r\n+--------------------------------------+---------+----------+---------------------------------------+\n\nTo disable data-in-transit encryption\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Disable encryption.\nIn the confirmation window, click Disable.\nWait until the operation is finished. You cannot manage infrastructure networks while the operation is in progress.\n\nCommand-line interface\nUse the following command:vinfra cluster network encryption disable <network1> [<network2> ...]\n\n<network>\n\nNetwork ID or name\n\nFor example, to disable data-in-transit encryption for the Private network, run:# vinfra cluster network encryption disable Private\nTo check the encryption status of your networks, use the vinfra cluster network encryption status command:+--------------------------------------+---------+----------+---------------------------------------+\r\n| id                                   | name    | status   | subnets                               |\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n| fa0d118e-2ec5-43e5-8813-41ab95d7f1f1 | Private | disabled | - disabled   192.168.128.0/24         |\r\n|                                      |         |          | - disabled   fd48:e4ee:f220:2808::/64 |\r\n| 178f54ac-7040-40db-95cb-099cc3e8394e | Public  | disabled | - disabled   10.136.16.0/20           |\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n\n\nSee also\n\nManaging encryption exceptions\n\nRenewing encryption certificates",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network encryption enable [--no-switch-storage-ipv6] <network1> [<network2> ...]\n\n--no-switch-storage-ipv6\n\nDo not switch chunk services to IPv6 addresses\n<network>\n\nNetwork ID or name\n\nFor example, to enable data-in-transit encryption for the Private network, run:# vinfra cluster network encryption enable Private\nTo check the encryption status of your networks, use the vinfra cluster network encryption status command:# vinfra cluster network encryption status\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n| id                                   | name    | status   | subnets                               |\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n| fa0d118e-2ec5-43e5-8813-41ab95d7f1f1 | Private | enabled  | - enabled    192.168.128.0/24         |\r\n|                                      |         |          | - enabled    fd48:e4ee:f220:2808::/64 |\r\n| 178f54ac-7040-40db-95cb-099cc3e8394e | Public  | disabled | - disabled   10.136.16.0/20           |\r\n+--------------------------------------+---------+----------+---------------------------------------+\n",
                "title": "To enable data-in-transit encryption"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network encryption disable <network1> [<network2> ...]\n\n<network>\n\nNetwork ID or name\n\nFor example, to disable data-in-transit encryption for the Private network, run:# vinfra cluster network encryption disable Private\nTo check the encryption status of your networks, use the vinfra cluster network encryption status command:+--------------------------------------+---------+----------+---------------------------------------+\r\n| id                                   | name    | status   | subnets                               |\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n| fa0d118e-2ec5-43e5-8813-41ab95d7f1f1 | Private | disabled | - disabled   192.168.128.0/24         |\r\n|                                      |         |          | - disabled   fd48:e4ee:f220:2808::/64 |\r\n| 178f54ac-7040-40db-95cb-099cc3e8394e | Public  | disabled | - disabled   10.136.16.0/20           |\r\n+--------------------------------------+---------+----------+---------------------------------------+\r\n\n",
                "title": "To disable data-in-transit encryption"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Enable encryption.\nIn the Enable encryption window, review the important information about the encryption requirements, and click Enable.\nWait until the operation is finished. You cannot manage infrastructure networks while the operation is in progress.\n\n",
                "title": "To enable data-in-transit encryption"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Disable encryption.\nIn the confirmation window, click Disable.\nWait until the operation is finished. You cannot manage infrastructure networks while the operation is in progress.\n\n",
                "title": "To disable data-in-transit encryption"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-and-disabling-data-in-transit-encryption.html"
    },
    {
        "title": "Editing and deleting placements",
        "content": "Editing and deleting placements\nLimitations\n\nYou cannot delete a placement that has nodes, images, or flavors.\nAfter deleting a placement, it is not automatically unassigned from virtual machines and volumes. To clean up placements from VMs and volumes, use the --no-placements option for the vinfra service compute server set and vinfra service compute volume set commands.\n\nPrerequisites\n\nPlacements for compute nodes are created, as described in Creating placements.\nBefore deleting a placement, ensure that its assignments are removed by following instructions in Changing placement assignment.\n\nTo edit a placement\n\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, select the required placement, and then click Edit on its right pane.\n\nEnter a new name or change the description, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nCommand-line interface\nUse the following command:vinfra service compute placement update [--name <name>] [--description <description>]\r\n                                        [--non-isolated | --isolated] <placement>\r\n\n\n--name <placement-name>\n\nA new name for the placement\n--description <placement-description>\n\nA new description for the placement\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--non-isolated\n\nMake the placement non-isolated (soft policy)\n--isolated\n\nMake the placement isolated (hard policy)\n\nFor example, to rename the placement placement1 to placement2, run:# vinfra service compute placement update --name placement2 placement1\n\nTo delete a placement\n\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, select the required placement.\nOn the placement right pane, click Delete.\nIn the confirmation window, click Delete placement.\n\nCommand-line interface\nUse the following command:vinfra service compute placement delete <placement>\r\n\n\n<placement>\n\nPlacement ID or name\n\nFor example, to delete the placement placement1, run:# vinfra service compute placement delete placement1\n\nSee also\n\nManaging virtual machines in placements",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute placement update [--name <name>] [--description <description>]\r\n                                        [--non-isolated | --isolated] <placement>\r\n\n\n--name <placement-name>\n\nA new name for the placement\n--description <placement-description>\n\n\nA new description for the placement\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--non-isolated\n\nMake the placement non-isolated (soft policy)\n--isolated\n\nMake the placement isolated (hard policy)\n\nFor example, to rename the placement placement1 to placement2, run:# vinfra service compute placement update --name placement2 placement1\n",
                "title": "To edit a placement"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute placement delete <placement>\r\n\n\n<placement>\n\nPlacement ID or name\n\nFor example, to delete the placement placement1, run:# vinfra service compute placement delete placement1\n",
                "title": "To delete a placement"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, select the required placement, and then click Edit on its right pane.\n\nEnter a new name or change the description, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n",
                "title": "To edit a placement"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Nodes > Placements tab, select the required placement.\nOn the placement right pane, click Delete.\nIn the confirmation window, click Delete placement.\n\n",
                "title": "To delete a placement"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/editing-and-deleting-placements.html"
    },
    {
        "title": "Enabling data encryption",
        "content": "Enabling data encryption\nVirtuozzo Hybrid Infrastructure can encrypt data stored on disks by using the AES-256 standard, so if a disk gets lost or stolen the data will be safe. Virtuozzo Hybrid Infrastructure stores disk encryption keys in cluster\u00e2\u0080\u0099s metadata (MDS).\nEncryption can be enabled or disabled only for the newly created chunk services (CS). Once tier encryption is enabled, you can decrypt disks (CSs) by manually releasing them from encrypted tiers. Correspondingly, simply enabling encryption on the disk\u00e2\u0080\u0099s tier will not encrypt its data (CS). To encrypt a disk, you must assign it to an encrypted tier.\nLimitations\n\nVirtuozzo Hybrid Infrastructure does not encrypt data transmitted over the internal network.\nEnabled encryption slightly decreases performance.\n\nTo enable tier encryption\n\nAdmin panel\n\nGo to Settings > System settings > Storage encryption.\nTurn on the toggle switch Enable AES-256 encryption for data stored on disks.\n\nSelect the tiers that you want to encrypt, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra cluster settings encryption set [--tier-enable {0,1,2,3}] [--tier-disable {0,1,2,3}]\r\n\n\n--tier-enable {0,1,2,3}\n\nEnable encryption for storage tiers. This option can be used multiple times.\n--tier-disable {0,1,2,3}\n\nDisable encryption for storage tiers. This option can be used multiple times.\n\nFor example, to enable encryption for the storage tier 2, run:# vinfra cluster settings encryption set --tier-enable 2\nYou can view the encryption status of each storage tier in the vinfra cluster settings encryption show output:# vinfra cluster settings encryption show\r\n+-------+-------+\r\n| Field | Value |\r\n+-------+-------+\r\n| tier0 | False |\r\n| tier1 | False |\r\n| tier2 | True  |\r\n| tier3 | False |\r\n+-------+-------+\r\n\n\nSee also\n\nBest practices for cluster security\n\nAccessing the admin panel via SSL\n\nSecuring root access to cluster nodes over SSH",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster settings encryption set [--tier-enable {0,1,2,3}] [--tier-disable {0,1,2,3}]\r\n\n\n--tier-enable {0,1,2,3}\n\nEnable encryption for storage tiers. This option can be used multiple times.\n--tier-disable {0,1,2,3}\n\nDisable encryption for storage tiers. This option can be used multiple times.\n\nFor example, to enable encryption for the storage tier 2, run:# vinfra cluster settings encryption set --tier-enable 2\nYou can view the encryption status of each storage tier in the vinfra cluster settings encryption show output:# vinfra cluster settings encryption show\r\n+-------+-------+\r\n| Field | Value |\r\n+-------+-------+\r\n| tier0 | False |\r\n| tier1 | False |\r\n| tier2 | True  |\r\n| tier3 | False |\r\n+-------+-------+\r\n\n",
                "title": "To enable tier encryption"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to Settings > System settings > Storage encryption.\nTurn on the toggle switch Enable AES-256 encryption for data stored on disks.\n\nSelect the tiers that you want to encrypt, and then click Save.\n\n\n\n\n\n\n",
                "title": "To enable tier encryption"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-data-encryption.html"
    },
    {
        "title": "Enabling and disabling RAM overcommitment",
        "content": "Enabling and disabling RAM overcommitment\nRAM overcommitment for virtual machines is only available if all of the compute nodes have enough swap space.\nPrerequisites\n\nA swap file is created, as described in Adding swap space.\n\nTo enable RAM overcommitment for virtual machines\nUse the command vinfra service compute create or vinfra service compute set with the --custom-param or --nova-compute-ram-allocation-ratio option. For example, to set the RAM overcommitment ratio to 1.5, run:# vinfra service compute set --nova-compute-ram-allocation-ratio 1.5\nTo check that the ratio is successfully modified, execute the vinfra service compute show command:# vinfra service compute show\r\n+--------------+-------------------------------------------+\r\n| Field        | Value                                     |\r\n+--------------+-------------------------------------------+\r\n| <...>        | <...>                                     |\r\n| options      | cpu_model: ''                             |\r\n|              | custom_params:                            |\r\n|              | - config_file: nova.conf                  |\r\n|              |   property: ram_allocation_ratio          |\r\n|              |   section: DEFAULT                        |\r\n|              |   service_name: nova-compute              |\r\n|              |   value: 1.5                              |\r\n| <...>        | <...>                                     |\r\n+--------------+-------------------------------------------+\r\n\nTo disable RAM overcommitment for virtual machines\nChange the ratio to 1 by running:# vinfra service compute set --nova-compute-ram-allocation-ratio 1\nIn this case, the swap space is not required anymore and the swap file can be removed:# /usr/libexec/vstorage-ui-agent/bin/configure-swap.sh --remove-all\nRun this script on each node in the compute cluster to remove the swap file from all of the compute nodes.\nSee also\n\nConnecting to OpenStack command-line interface\n\nChanging parameters in OpenStack configuration files\n\nChanging the default project quotas\n\nConfiguring scheduling of virtual machines\n\nConfiguring CPU features for virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-and-disabling-ram-overcommitment.html"
    },
    {
        "title": "Editing network interfaces",
        "content": "Editing network interfaces\nLimitations\n\nA network can only be assigned to one network interface per node.\n\nPrerequisites\n\nIf you want to change the default MTU, it must be configured on the network hardware.\n\nTo edit a network interface\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface.\nOn the interface right pane, click Edit.\n\nIn the Edit network interface window, select a network to assign the interface to, and then specify the network parameters:\n\nSelect Automatically (DHCP) to obtain the IP address, DNS, and routing settings from the DHCP server.\nSelect Automatically (DHCP address only) to obtain only the IP address from the DHCP server.\nSelect Manually, and then specify the IP address in CIDR notation by clicking Add.\n\nDynamic IP address allocation will cause network issues as soon as the IP addresses of cluster nodes change. Configure static IP addresses from the start or as soon as possible.\n\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\n\nIf you have set a custom maximum transmission unit (MTU) on the network hardware, enter the same value in the MTU field.\n\nSetting a custom MTU in the admin panel prior to configuring it on the network hardware will result in network failure on the node and require manual resetting. Setting an MTU that differs from the one configured on the network hardware may result in a network outage or poor performance.\n\nClick Save to apply your changes.\n\nCommand-line interface\nUse the following command:vinfra node iface set [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                      [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                      [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                      [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                      [--network <network> | --no-network] [--ifaces <ifaces>]\r\n                      [--bond-type <bond-type>] [--node <node>] <iface>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--no-network\n\nRemove a network from the interface\n--ifaces <ifaces>\n\nA comma-separated list of network interface names, for example, iface1,iface2,...,ifaceN\n--bond-type <bond-type>\n\nBond type (balance-rr, balance-xor, broadcast, 802.3ad, balance-tlb, balance-alb)\nBond type for an OVS interface (balance-tcp, active-backup)\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nExample 1. To assign the network MyNet to the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --network MyNet --node node002\nExample 2. To unassign a network from the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --node node002 --no-network\nExample 3. To enable IP address allocation via DCHP for the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --node node002 --dhcp4\nExample 4. To disable DHCP and set the IP address 192.168.30.20/24 for the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --node node002 --no-dhcp4 --ipv4 192.168.30.20/24\r\n\nExample 5. To change the bond type of the network bond bond0 located on the node node002 to balance-xor, run:# vinfra node iface set bond0 --node node002 --bond-type balance-xor\nThe updated interface will be listed in the vinfra node iface list output:# vinfra node iface list --node node002\r\n+------+----------------+--------------------+-------+---------+\r\n| name | node_id        | ipv4               | state | network |\r\n+------+----------------+--------------------+-------+---------+\r\n| eth0 | 4f96acf5-<...> | - 10.94.29.218/16  | up    | Public  |\r\n| eth1 | 4f96acf5-<...> | - 10.37.130.101/24 | up    | Private |\r\n| eth2 | 4f96acf5-<...> | - 192.168.30.20/24 | up    | MyNet   |\r\n+------+----------------+--------------------+-------+---------+\r\n\n\nSee also\n\nChanging network interface parameters\n\nManaging network interfaces\n\nWhat's next\n\nAdding external DNS servers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface set [--ipv4 <ipv4>] [--ipv6 <ipv6>] [--gw4 <gw4>] [--gw6 <gw6>]\r\n                      [--mtu <mtu>] [--dhcp4 | --no-dhcp4] [--dhcp6 | --no-dhcp6]\r\n                      [--auto-routes-v4 | --ignore-auto-routes-v4]\r\n                      [--auto-routes-v6 | --ignore-auto-routes-v6]\r\n                      [--network <network> | --no-network] [--ifaces <ifaces>]\r\n                      [--bond-type <bond-type>] [--node <node>] <iface>\r\n\n\n--ipv4 <ipv4>\n\nA comma-separated list of IPv4 addresses\n--ipv6 <ipv6>\n\nA comma-separated list of IPv6 addresses\n--gw4 <gw4>\n\nGateway IPv4 address\n--gw6 <gw6>\n\nGateway IPv6 address\n--mtu <mtu>\n\nMTU interface value\n--dhcp4\n\nEnable DHCPv4\n--no-dhcp4\n\nDisable DHCPv4\n--dhcp6\n\nEnable DHCPv6\n--no-dhcp6\n\nDisable DHCPv6\n--auto-routes-v4\n\nEnable automatic IPv4 routes\n--ignore-auto-routes-v4\n\nIgnore automatic IPv4 routes\n--auto-routes-v6\n\nEnable automatic IPv6 routes\n--ignore-auto-routes-v6\n\nIgnore automatic IPv6 routes\n--network <network>\n\nNetwork ID or name\n--no-network\n\nRemove a network from the interface\n--ifaces <ifaces>\n\nA comma-separated list of network interface names, for example, iface1,iface2,...,ifaceN\n--bond-type <bond-type>\n\n\nBond type (balance-rr, balance-xor, broadcast, 802.3ad, balance-tlb, balance-alb)\nBond type for an OVS interface (balance-tcp, active-backup)\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nExample 1. To assign the network MyNet to the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --network MyNet --node node002\nExample 2. To unassign a network from the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --node node002 --no-network\nExample 3. To enable IP address allocation via DCHP for the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --node node002 --dhcp4\nExample 4. To disable DHCP and set the IP address 192.168.30.20/24 for the network interface eth2 located on the node node002, run:# vinfra node iface set eth2 --node node002 --no-dhcp4 --ipv4 192.168.30.20/24\r\n\nExample 5. To change the bond type of the network bond bond0 located on the node node002 to balance-xor, run:# vinfra node iface set bond0 --node node002 --bond-type balance-xor\nThe updated interface will be listed in the vinfra node iface list output:# vinfra node iface list --node node002\r\n+------+----------------+--------------------+-------+---------+\r\n| name | node_id        | ipv4               | state | network |\r\n+------+----------------+--------------------+-------+---------+\r\n| eth0 | 4f96acf5-<...> | - 10.94.29.218/16  | up    | Public  |\r\n| eth1 | 4f96acf5-<...> | - 10.37.130.101/24 | up    | Private |\r\n| eth2 | 4f96acf5-<...> | - 192.168.30.20/24 | up    | MyNet   |\r\n+------+----------------+--------------------+-------+---------+\r\n\n",
                "title": "To edit a network interface"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface.\nOn the interface right pane, click Edit.\n\nIn the Edit network interface window, select a network to assign the interface to, and then specify the network parameters:\n\nSelect Automatically (DHCP) to obtain the IP address, DNS, and routing settings from the DHCP server.\nSelect Automatically (DHCP address only) to obtain only the IP address from the DHCP server.\nSelect Manually, and then specify the IP address in CIDR notation by clicking Add.\n\n\nDynamic IP address allocation will cause network issues as soon as the IP addresses of cluster nodes change. Configure static IP addresses from the start or as soon as possible.\n\n\n\nSpecify a gateway. The provided gateway will become the node\u00e2\u0080\u0099s default.\n\n\nIf you have set a custom maximum transmission unit (MTU) on the network hardware, enter the same value in the MTU field.\n\nSetting a custom MTU in the admin panel prior to configuring it on the network hardware will result in network failure on the node and require manual resetting. Setting an MTU that differs from the one configured on the network hardware may result in a network outage or poor performance.\n\n\nClick Save to apply your changes.\n\n\n\n\n\n",
                "title": "To edit a network interface"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/editing-network-interfaces.html"
    },
    {
        "title": "Enabling geo-replication",
        "content": "Enabling geo-replication\nPrerequisites\n\nTwo backup storage clusters are deployed, as described in Provisioning backup storage space.\nBefore you configure geo-replication for the secondary cluster, delete all of its registrations, as explained in Deleting registrations.\nAll storage clusters are updated to the latest version.\nAll storage clusters can reach each other via domain names on TCP port 44445.\nEach storage cluster can reach itself via its own domain name on TCP port 44445. This is especially important if the clusters are situated behind NAT.\n\nTo set up geo-replication between two storage clusters\n\nAdmin panel\n\nOn the cluster that will be configured as secondary, go to Storage services > Backup storage > Geo-replication. Click the copy icon next to the UID field to copy its UID to clipboard.\n\nOn the cluster that will be configured as primary, go to Storage services > Backup storage > Geo-replication. Click Configure replication, and then do the following in the Configure replication window:\n\nSpecify external DNS names for the primary and secondary backup storage clusters, and then paste the UID of the secondary cluster. Click Next.\n\nIt is recommended to provide DNS names different from those of backup storage registrations. Otherwise, once you reconfigure the backup storage DNS name, the clusters will be unable to communicate and geo-replication will be stopped. If you cannot provide a dedicated DNS name, specify the DNS name used for backup storage creation.\n\nSpecify the following information for your Acronis product:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\nClick Configure.\n\nThe primary cluster is now configured and ready to be connected to the secondary one, which needs to be configured next. The configuration file of the primary cluster will be automatically downloaded to your local server. Alternatively, you can do it manually by clicking Download configuration file.\n\nOn the secondary cluster, click Configure replication, and then do the following in the Configure replication window:\n\nSelect the Secondary cluster configuration type.\n\nUpload the configuration file of the primary cluster from your local server.\n\nClick Configure.\n\nThe secondary cluster is now also configured and ready to be connected to the primary one.\n\nBack on the primary cluster, click Connect to enable replication between the two datacenters.\n\nCommand-line interface\n\nOn the cluster that will be configured as secondary, run vinfra service backup geo-replication show to learn its UID. For example:# vinfra service backup geo-replication show\r\n+-------+-----------------------------+\r\n| Field | Value                       |\r\n+-------+-----------------------------+\r\n| self  | address: no address         |\r\n|       | datacenter_uid: e63a6738<\u00e2\u0080\u00a6> |\r\n+-------+-----------------------------+\r\n\n\nOn the cluster that will be configured as primary, run vinfra service backup geo-replication primary setup, using the DNS names for the primary cluster, as well as the DNS name and UID of the secondary cluster. Additionally, specify the URL of the cloud management portal or the hostname/IP address and port of the local management server and the account credentials for the registration. For example:# vinfra service backup geo-replication primary setup --primary-cluster-address \r\nprimary.example.com \\\r\n--secondary-cluster-address secondary.example.com --secondary-cluster-uid e63a6738<\u00e2\u0080\u00a6> \\\r\n--username account@example.com --account-server https://cloud.acronis.com --stdin\n\nOn the primary cluster, run vinfra service backup geo-replication primary download-configs to generate the configuration file of the primary cluster. For example:# vinfra service backup geo-replication primary download-configs --conf-file-path primary_dc.conf\r\n\n\nMove the configuration file of the primary cluster to the secondary cluster using the standard Linux command-line tool. For example, by using scp:# scp primary_dc.conf <secondary_cluster_IP_address>:/root/primary_dc.conf\r\n\n\nOn the secondary cluster, run vinfra service backup geo-replication secondary setup to upload the configuration file of the primary cluster. For example:# vinfra service backup geo-replication secondary setup --dc-config-file primary_dc.conf\n\nIf for some reason, you need to cancel geo-replication, run vinfra service backup geo-replication primary cancel on the primary cluster and vinfra service backup geo-replication secondary cancel on the secondary cluster.\n\nOn the primary cluster, run vinfra service backup geo-replication primary establish to establish a connection between the primary and secondary clusters. For example:# vinfra service backup geo-replication primary establish\r\n\n\nSee also\n\nMonitoring backup storage\n\nImporting registrations to the secondary cluster\n\nPerforming a failover\n\nDisabling geo-replication",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nOn the cluster that will be configured as secondary, run vinfra service backup geo-replication show to learn its UID. For example:# vinfra service backup geo-replication show\r\n+-------+-----------------------------+\r\n| Field | Value                       |\r\n+-------+-----------------------------+\r\n| self  | address: no address         |\r\n|       | datacenter_uid: e63a6738<\u00e2\u0080\u00a6> |\r\n+-------+-----------------------------+\r\n\n\n\nOn the cluster that will be configured as primary, run vinfra service backup geo-replication primary setup, using the DNS names for the primary cluster, as well as the DNS name and UID of the secondary cluster. Additionally, specify the URL of the cloud management portal or the hostname/IP address and port of the local management server and the account credentials for the registration. For example:# vinfra service backup geo-replication primary setup --primary-cluster-address \r\nprimary.example.com \\\r\n--secondary-cluster-address secondary.example.com --secondary-cluster-uid e63a6738<\u00e2\u0080\u00a6> \\\r\n--username account@example.com --account-server https://cloud.acronis.com --stdin\n\n\nOn the primary cluster, run vinfra service backup geo-replication primary download-configs to generate the configuration file of the primary cluster. For example:# vinfra service backup geo-replication primary download-configs --conf-file-path primary_dc.conf\r\n\n\n\nMove the configuration file of the primary cluster to the secondary cluster using the standard Linux command-line tool. For example, by using scp:# scp primary_dc.conf <secondary_cluster_IP_address>:/root/primary_dc.conf\r\n\n\n\nOn the secondary cluster, run vinfra service backup geo-replication secondary setup to upload the configuration file of the primary cluster. For example:# vinfra service backup geo-replication secondary setup --dc-config-file primary_dc.conf\n\n\nIf for some reason, you need to cancel geo-replication, run vinfra service backup geo-replication primary cancel on the primary cluster and vinfra service backup geo-replication secondary cancel on the secondary cluster.\n\n\nOn the primary cluster, run vinfra service backup geo-replication primary establish to establish a connection between the primary and secondary clusters. For example:# vinfra service backup geo-replication primary establish\r\n\n\n\n",
                "title": "To set up geo-replication between two storage clusters"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOn the cluster that will be configured as secondary, go to Storage services > Backup storage > Geo-replication. Click the copy icon next to the UID field to copy its UID to clipboard.\n\n\n\n\n\n\nOn the cluster that will be configured as primary, go to Storage services > Backup storage > Geo-replication. Click Configure replication, and then do the following in the Configure replication window:\n\n\nSpecify external DNS names for the primary and secondary backup storage clusters, and then paste the UID of the secondary cluster. Click Next.\n\nIt is recommended to provide DNS names different from those of backup storage registrations. Otherwise, once you reconfigure the backup storage DNS name, the clusters will be unable to communicate and geo-replication will be stopped. If you cannot provide a dedicated DNS name, specify the DNS name used for backup storage creation.\n\n\n\n\n\n\n\nSpecify the following information for your Acronis product:\n\nThe URL of the cloud management portal (for example, https://cloud.acronis.com/) or the hostname/IP address and port of the local management server (for example, http://192.168.1.2:9877)\nThe credentials of a partner account in the cloud or of an organization administrator on the local management server\n\n\n\n\n\n\nClick Configure.\n\nThe primary cluster is now configured and ready to be connected to the secondary one, which needs to be configured next. The configuration file of the primary cluster will be automatically downloaded to your local server. Alternatively, you can do it manually by clicking Download configuration file.\n\n\nOn the secondary cluster, click Configure replication, and then do the following in the Configure replication window:\n\nSelect the Secondary cluster configuration type.\n\nUpload the configuration file of the primary cluster from your local server.\n\n\n\n\n\nClick Configure.\n\nThe secondary cluster is now also configured and ready to be connected to the primary one.\n\n\nBack on the primary cluster, click Connect to enable replication between the two datacenters.\n\n\n\n\n\n\n",
                "title": "To set up geo-replication between two storage clusters"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-geo-replication.html"
    },
    {
        "title": "Enabling RDMA",
        "content": "Enabling RDMA\nVirtuozzo Hybrid Infrastructure supports remote direct memory access (RDMA) over InfiniBand (IB) or Converged Ethernet (RoCE) for the storage backend network. The RDMA technology allows servers in this network to exchange data in main memory without involving their processors, cache or operating systems, thus freeing up resources and improving throughput and performance.\nBy default, RDMA is disabled. To configure it automatically, you can enable it in the admin panel or by using the vinfra tool. However, this is only possible before the storage cluster is created. Before enabling the feature, check the RDMA\u00a0network first.\nLimitations\n\nWe recommend using NVIDIA Mellanox ConnectX-5 adapters for the RDMA mode. If you want to use other adapters in the RDMA mode, contact the technical support team for recommendations.\n\nTo be used for the RDMA traffic, a network bond can only be configured across different interfaces of the same NIC.\n\nRDMA is not supported for the compute service. Therefore, the compute and storage networks must be physically separated on different NICs. If you use the recommended approach with bonded network interfaces, you should have one network card with two bonded network interfaces for the storage network and one network card with two bonded network interfaces for the compute network. To learn how to use a compute trunk network, refer to Connecting virtual switches to trunk interfaces.\n\nEnabling or disabling RDMA may temporarily affect cluster availability.\n\nPrerequisites\n\nYour RDMA network infrastructure must be ready before you install Virtuozzo Hybrid Infrastructure.\nEach network adapter connected to a network with the Storage traffic type supports RDMA.\nInfiniBand devices are configured on all of your nodes, as described in Configuring InfiniBand devices.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-rdma.html"
    },
    {
        "title": "Enabling S3 cross-region replication",
        "content": "Enabling S3 cross-region replication\nCross-region replication (CRR) enables automatic, asynchronous copying of objects across S3 buckets in different regions. Buckets that are configured for CRR can be owned by the same user. Objects may be replicated to a single destination bucket or multiple destination buckets.\nTo enable CRR, you need to add replication configuration to your source bucket. The minimum configuration must provide the destination buckets where you want to replicate objects and a user with the role that enables replicating objects on your behalf.\nLimitations\n\nCRR only supports copying new S3 objects after it is enabled.\n\nPrerequisites\n\nS3 clusters are created, as described in Creating the S3 cluster.\n\nTo set up CRR replication\n\nCreate two or more S3 buckets, one source bucket and one or multiple destination buckets. You can also use buckets that already exist in the S3 cluster. For example, to create the source and destination buckets, use:\r\naws s3api create-bucket --bucket source --endpoint-url http://s3.ostor --profile ostor\r\naws s3api create-bucket --bucket destination --endpoint-url http://s3.ostor --profile ostor\n\nEnable versioning for these buckets. For example:aws s3api  put-bucket-versioning --bucket source --endpoint-url http://s3.ostor --profile ostor \\\r\n--versioning-configuration 'Status=Enabled'\r\naws s3api  put-bucket-versioning --bucket destination --endpoint-url http://s3.ostor --profile ostor \\\r\n--versioning-configuration 'Status=Enabled'\n\nCreate a replication configuration file. For example, the replication.conf file may look as follows:<?xml version=\"1.0\"?>\r\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\r\n  <Rule>\r\n    <Status>Enabled</Status>\r\n    <Filter/>\r\n    <DeleteMarkerReplication>\r\n      <Status>Disabled</Status>\r\n    </DeleteMarkerReplication>\r\n    <Destination>\r\n      <Bucket>arn:aws:s3:::destination</Bucket>\r\n    </Destination>\r\n    <Priority>1</Priority>\r\n    <ID>rep-rule</ID>\r\n  </Rule>\r\n  <Role>arn:aws:iam::ccb13f7bc586ace0:role/service-role/s3crr_role</Role>\r\n</ReplicationConfiguration>\nwhere:\n\nRule\n\nSpecifies which objects to replicate and where to store the replicas.\nDeleteMarkerReplication\n\nSpecifies whether to replicate delete markers. If the Filter\u00a0element is specified, you must also include the DeleteMarkerReplication\u00a0element.\nDestination\n\nA container for information about the replication destination and its configuration.\nPriority\n\nIndicates which rule has precedence whenever two or more replication rules conflict.\nBucket\n\nThe name of the bucket where you want to store the results.\nStatus\n\nSpecifies whether the rule is enabled. Valid values\u00a0are Enabled or Disabled.\nID\n\nA unique identifier for the rule. The value must be up to 255 characters long.\nRole\n\nThe ID of the user that is used to replicate objects on your behalf. In the example above, it is ccb13f7bc586ace0.\n\nConfigure the source bucket for CRR by specifying the replication configuration file. For example:aws s3api put-bucket-replication --bucket source --endpoint-url http://s3.ostor --profile ostor \\\r\n--replication-configuration file://replication.conf\n\nSee also\n\nEnabling S3 geo-replication",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-s3-cross-region-replication.html"
    },
    {
        "title": "Enabling PCI passthrough and vGPU support",
        "content": "Enabling PCI passthrough and vGPU support\nTo enable PCI passthrough and vGPU support for the compute cluster, you need to create a configuration file in the YAML format, and then use it to reconfigure the compute cluster.\nPrerequisites\n\nThe compute nodes are prepared for GPU passthrough, vGPU support, or SR-IOV, as described in Preparing nodes for GPU passthrough, Preparing nodes for GPU virtualization, and Preparing nodes for SR-IOV.\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo create the PCI passthrough and vGPU configuration file\nSpecify the identifier of a compute node that hosts PCI devices, and then add host devices that you want to pass through or virtualize:\n\nTo create virtual functions for a network adapter, add these lines:- device_type: sriov\r\n  device: enp2s0\r\n  physical_network: sriovnet\r\n  num_vfs: 8\nwhere:\n\nsriov is the device type for a network adapter\nenp2s0 is the device name of a network adapter\nsriovnet is an arbitrary name that will be used as an alias for a network adapter\nnum_vfs is the number of virtual functions to create for a network adapter\n\nThe maximum number of virtual functions supported by a PCI device is specified in the /sys/class/net/<device_name>/device/sriov_totalvfs file. For example:# cat /sys/class/net/enp2s0/device/sriov_totalvfs\r\n63\n\nTo enable GPU passthrough, add these lines:- device_type: generic\r\n  device: 1b36:0100\r\n  alias: gpu\nwhere:\n\ngeneric is the device type for a physical GPU that will be passed through\n1b36:0100 is the VID and PID of a physical GPU\ngpu is an arbitrary name that will be used as an alias for a physical GPU\n\nTo enable vGPU, add these lines:- device_type: pgpu\r\n  device: \"0000:01:00.0\"\r\n  vgpu_type: nvidia-224\nwhere:\n\npgpu is the device type for a physical GPU that will be virtualized\n\"0000:01:00.0\" is the PCI address of a physical GPU\nnvidia-232 is the vGPU type that will be enabled for a physical GPU\n\nThe entire configuration file may look as follows:# cat config.yaml\r\n- node_id: c3b2321a-7c12-8456-42ce-8005ff937e12\r\n  devices:\r\n    - device_type: sriov\r\n      device: enp2s0\r\n      physical_network: sriovnet\r\n      num_vfs: 8\r\n    - device_type: generic\r\n      device: 1b36:0100\r\n      alias: gpu\r\n    - device_type: pgpu\r\n      device: \"0000:01:00.0\"\r\n      vgpu_type: nvidia-232\r\n- node_id: 1d6481c2-1fd5-406b-a0c7-330f24bd0e3d\r\n  devices:\r\n    - device_type: generic\r\n      device: 10de:1eb8\r\n      alias: gpu\r\n    - device_type: pgpu\r\n      device: \"0000:03:00.0\"\r\n      vgpu_type: nvidia-224\r\n    - device_type: pgpu\r\n      device: \"0000:81:00.0\"\r\n      vgpu_type: nvidia-228\nTo configure the compute cluster for PCI passthrough and vGPU support\nPass the configuration file to the vinfra service compute set command. For example:# vinfra service compute set --pci-passthrough-config config.yaml\nIf the compute configuration fails\nCheck whether the following error appears in /var/log/vstorage-ui-backend/ansible.log:2021-09-23 16:42:59,796 p=32130 u=vstoradmin | fatal: [32c8461b-92ec-48c3-ae02-\r\n4d12194acd02]: FAILED! => {\"changed\": true, \"cmd\": \"echo 4 > /sys/class/net/\r\nenp103s0f1/device/sriov_numvfs\", \"delta\": \"0:00:00.127417\", \"end\": \"2021-09-23 \r\n19:42:59.784281\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2021-09-23 \r\n19:42:59.656864\", \"stderr\": \"/bin/sh: line 0: echo: write error: Cannot allocate \r\nmemory\", \"stderr_lines\": [\"/bin/sh: line 0: echo: write error: Cannot allocate memory\"], \r\n\"stdout\": \"\", \"stdout_lines\": []}\nIn this case, run the the pci-helper.py script, and reboot the node:# /usr/libexec/vstorage-ui-agent/bin/pci-helper.py enable-iommu --pci-realloc\r\n# reboot\nWhen the node is up again, repeat the vinfra service compute set command.\nTo check that a node has vGPU resources for allocation\nList resource providers in the compute cluster to obtain their IDs. For example:# openstack --insecure resource provider list\r\n+--------------------------------------+-----------------------------------------+------------+--------------------------------------+--------------------------------------+\r\n| uuid                                 | name                                    | generation | root_provider_uuid                   | parent_provider_uuid                 |\r\n+--------------------------------------+-----------------------------------------+------------+--------------------------------------+--------------------------------------+\r\n| 359cccf7-9c64-4edc-a35d-f4673e485a04 | node001.vstoragedomain_pci_0000_01_00_0 |          1 | 4936695a-4711-425a-b0e4-fdab5e4688d6 | 4936695a-4711-425a-b0e4-fdab5e4688d6 |\r\n| b8443d1b-b941-4bf5-ab4b-2dc7c64ac7d1 | node001.vstoragedomain_pci_0000_81_00_0 |          1 | 4936695a-4711-425a-b0e4-fdab5e4688d6 | 4936695a-4711-425a-b0e4-fdab5e4688d6 |\r\n| 4936695a-4711-425a-b0e4-fdab5e4688d6 | node001.vstoragedomain                  |        823 | 4936695a-4711-425a-b0e4-fdab5e4688d6 | None                                 |\r\n+--------------------------------------+-----------------------------------------+------------+--------------------------------------+--------------------------------------+\r\n\nIn this output, the resource provider with the ID 4936695a-4711-425a-b0e4-fdab5e4688d6 has two child resource providers for two physical GPUs with PCI addresses 0000_01_00_0 and 0000_81_00_0.\nUse the obtained ID of a child resource provider to list its inventory. For example:# openstack --insecure resource provider inventory list 359cccf7-9c64-4edc-a35d-f4673e485a04\r\n+----------------+------------------+----------+----------+-----------+----------+-------+\r\n| resource_class | allocation_ratio | max_unit | reserved | step_size | min_unit | total |\r\n+----------------+------------------+----------+----------+-----------+----------+-------+\r\n| VGPU           |              1.0 |        8 |        0 |         1 |        1 |     8 |\r\n+----------------+------------------+----------+----------+-----------+----------+-------+\nThe child resource provider has vGPU resources that can be allocated to virtual machines.\nSee also\n\nChanging the vGPU type for physical GPUs\n\nSwitching between GPU passthrough and vGPU\n\nDisabling PCI passthrough and vGPU support\n\nWhat's next\n\nCreating virtual machines with physical GPUs\n\nCreating virtual machines with virtual GPUs\n\nCreating virtual machines with different vGPU types\n\nCreating virtual machines with SR-IOV network ports",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-pci-passthrough-and-vgpu-support.html"
    },
    {
        "title": "Failure domains",
        "content": "Failure domains\nThe idea behind failure domains is to define a scope (for example, a rack) which can fail, while its data will still be available. If we choose the rack failure domain, the cluster data will tolerate a failure of one rack: the other racks will provide for the data availability. If we choose the host failure domain, the loss of an entire server would not result in the loss of data availability.\nTo provide high availability, Virtuozzo Hybrid Infrastructure spreads data replicas evenly across failure domains, according to a replica placement policy. The following policies are available:\n\nDisk, the smallest possible failure domain. Under this policy, Virtuozzo Hybrid Infrastructure never places more than one data replica per disk. While protecting against disk failure, this option may still result in data loss if data replicas happen to be on different disks of the same host and it fails. This policy should be used with one-node clusters.\nHost as a failure domain. Under this policy, Virtuozzo Hybrid Infrastructure never places more than one data replica per host. So if a storage node fails (an operating system crash) and all its disks become unavailable, the data is still accessible from the healthy nodes.\nRack as a failure domain. Under this policy, Virtuozzo Hybrid Infrastructure never places more than one data replica per rack. So if a single rack fails (a failure of top-of-rack switch) and all the nodes in it become unavailable, the data is still accessible from the other racks.\nRow as a failure domain. Under this policy, Virtuozzo Hybrid Infrastructure never places more than one data replica per row. So if a single row fails (a failure of a single power source) and all the racks in it become unavailable, the data is still accessible from the other rows.\nRoom as a failure domain. Under this policy, Virtuozzo Hybrid Infrastructure never places more than one data replica per room. So if a single room fails (a power outage) and all the rows in it become unavailable, the data is still accessible from the other rooms.\n\nWhen selecting a failure domain, consider the following recommendations:\n\nMake sure the metadata services are distributed among the locations. For example, if you choose a room as the failure domain, and distribute the data across several rooms evenly, you must distribute metadata services too. If you put all the metadata services in one room and it fails due to a power outage, the cluster will not function properly.\nTo select a location as the failure domain, you need to have several locations of the kind so that a service or the data can move from one failure domain to another, such as from one rack to another. For example, if you want to choose the rack failure domain with the redundancy 2 replicas or Encoding 1+1, make sure you have at least two racks with healthy nodes assigned to the cluster.\nThe disk space should be distributed evenly between the failure domains. For example, if you select the rack failure domain, equal disk space should be available on each of the racks. The allocatable disk space in each rack is set to the disk space on the smallest rack. The reason is that each rack should store one replica for a data chunk. So once the disk space on the smallest rack runs out, no more chunks in the cluster can be created until a new rack is added or the replication factor is decreased. Huge failure domains are more sensitive to total disk space imbalance. For example, if a domain has 5 racks, with 10 TB, 20 TB, 30 TB, 100 TB, and 100 TB total disk space, it will not be possible to allocate (10+20+30+100+100)/3 = 86 TB of data in 3 replicas. Instead, only 60 TB will be allocatable, as the low-capacity racks will be exhausted sooner. At that, the largest racks (the 100 TB ones) will still have free unallocatable space.\n\nSee also\n\nStorage policies\n\nData redundancy\n\nStorage tiers\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/failure-domains.html"
    },
    {
        "title": "Enabling SNMP access",
        "content": "Enabling SNMP access\nTo  enable the SNMP access on the node\n\nOpen UDP port 161 on the management node as follows:\n\nOn the Infrastructure > Networks screen, and then click Edit.\nAdd the SNMP traffic type to your public network by selecting the corresponding check box.\nClick Save.\n\nGo to Settings > System settings > SNMP, and then turn on the toggle switch Enable SNMP on the management node. The network management system (SNMP monitor) will be enabled, giving you access to the cluster via the SNMP protocol.\n\nClick the provided link to download the MIB file, and then set it up in your SNMP monitor.\n\nEnable sending SNMP traps to your SNMP monitor as follows:\n\nSelect Send SNMP traps to this network management system.\n\nSpecify the IP address, Port, and Community of the network management system.\nBy default, the snmptrapd daemon uses port 162. The default community is public.\n\nIf required, click Send test trap to test the service.\n\nClick Save to apply the changes.\n\nWhat's next\n\nAccessing cluster information objects via SNMP\n\nListening to SNMP traps\n\nMonitoring the cluster with Zabbix",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-snmp-access.html"
    },
    {
        "title": "Hardware recommendations",
        "content": "Hardware recommendations\n\nVirtuozzo Hybrid Infrastructure works on the same hardware that is recommended for Red Hat Enterprise Linux 9, including AMD EPYC processors: servers, components.\nEven though a cluster can be created on top of varied hardware, by using nodes with similar hardware in each node will yield better cluster performance, capacity, and overall balance. \nIt is not recommended for production to run Virtuozzo Hybrid Infrastructure on top of SAN/NAS hardware that has its own redundancy mechanisms. Doing so may negatively affect performance and data availability.\nIt is recommended to use UEFI instead of BIOS if this supported by your hardware. This is recommended particularly if you use NVMe drives.\nAny cluster infrastructure must be tested extensively before it is deployed to production. Such common points of failure as SSD drives and network adapter bonds must always be thoroughly verified.\n\nSee also\n\nServer requirements\n\nDisk requirements\n\nNetwork requirements and recommendations\n\nAdmin panel requirements",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/hardware-recommendations.html"
    },
    {
        "title": "Enabling S3 geo-replication",
        "content": "Enabling S3 geo-replication\nVirtuozzo Hybrid Infrastructure can store replicas of S3 cluster data and keep them up to date in multiple geographically distributed datacenters. Geo-replication reduces the response time for local S3 users accessing the data in a remote S3 cluster, or remote S3 users accessing the data in a local S3 cluster, as they do not need an Internet connection. \nGeo-replication schedules the update of the replicas as soon as any data is modified. Its performance depends on the Internet connection speed, the redundancy mode, and cluster performance. \nIf you have multiple datacenters with enough free space, it is recommended to set up geo-replication between S3 clusters residing in these datacenters.\nPrerequisites\n\nS3 clusters are created, as described in Creating the S3 cluster.\nEach cluster has its own SSL certificate signed by a global certificate authority.\n\nTo set up geo-replication between S3 clusters\n\nAdmin panel\n\nIn the admin panel of a remote datacenter, open the Storage services > S3 > Geo-replication screen, and then click the home S3 storage.\nOn the right pane, click Get token.\n\nIn the Get token window, click Copy token below the Token section.\n\nIn the admin panel of the local datacenter, open the Storage services > S3 > Geo-replication screen, and then click Add datacenter.\n\nIn the Add datacenter window, enter the copied token, and then click Add.\n\nConfigure the remote S3 storage the same way.\n\nAfter you enable geo-replication for the clusters, you can replicate their data per bucket.\n\nCommand-line interface\n\nIn the remote datacenter, run vinfra service s3 replication show token to get its token, and then copy this token. For example:# vinfra service s3 replication show token --max-value-length -1\r\n+-------+---------------------------------------------------------------------------+\r\n| Field | Value                                                                     |\r\n+-------+---------------------------------------------------------------------------+\r\n| token | eyJyZWFkYWJsZV9uYW1lIjogImhjaUhlYXQiLCAiaXNfc2VsZiI6IHRydWUsICJ1c2VyX3NlY |\r\n|       | 3JldF9rZXkiOiAiTnM3eWZOcVJ1RGQxRzVUc0ZCZ0VlcjNtWGgyRGJIMG1wanB1NkhVNyIsIC |\r\n|       | J1aWQiOiAiZGQ3MTZjY2VmNDE5OTNiZiIsICJ1cmwiOiAiaHR0cHM6Ly9zM3N0b3JhZ2UuZXh |\r\n|       | hbXBsZS5jb206NDQzIiwgInVzZXJfa2V5X2lkIjogIjgyNmYwYmUyMWNjMDcwZjJGUDhRIn0= |\r\n+-------+---------------------------------------------------------------------------+\n\nIn the local datacenter, run vinfra service s3 replication add, using the token of the remote datacenter. For example:# vinfra service s3 replication add --token eyJ1c2VyX3<\u00e2\u0080\u00a6>\n\nConfigure the remote S3 storage the same way.\n\nTo check that S3 geo-replication is enabled, run vinfra service s3 replication list:# vinfra service s3 replication list\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| uid          | readable_name | url                           | is_self | user_key_id  | user_secret_key |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| dd716cc<...> | cluster1      | http://s3stor1.example.com:80 | True    | 826f0be<...> | Ns7yfNq<...>    |\r\n| eff4d48<...> | cluster2      | http://s3stor2.example.com:80 | False   | cd3f6ae<...> | UmSuxYI<...>    |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\n\nTo replicate a bucket\n\n Open the Storage services > S3 > Buckets screen, and then select the bucket.\nClick Enable geo-replication on the right pane. \n\nThe Geo-replication column for this bucket will display Enabled and the bucket will be copied to the connected cluster.\nTo disable geo-replication of a bucket\n\n Open the Storage services > S3 > Buckets screen, and then select the bucket.\nClick Disable geo-replication on the right pane. \n\n The Geo-replication column for this bucket will display Disabled. After geo-replication is disabled for a bucket, the data copied beforehand will remain, but the changes to it will no longer be replicated to the other S3 cluster.\nTo disable geo-replication between S3 clusters\n\nAdmin panel\n\nIn the admin panel of a local datacenter, open the Storage services > S3 > Geo-replication screen, and then click the remote S3 storage.\nOn the right pane, click Delete.\nClick Delete in the confirmation window.\nConfigure the remote S3 storage the same way.\n\nCommand-line interface\n\nIn the local datacenter, run vinfra service s3 replication delete using the UID of the remote datacenter. For example:# vinfra service s3 replication delete --id eff4d48<\u00e2\u0080\u00a6>\n\nConfigure the remote S3 storage the same way.\n\nTo check that S3 geo-replication is disabled, run vinfra service s3 replication list:# vinfra service s3 replication list\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| uid          | readable_name | url                           | is_self | user_key_id  | user_secret_key |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| dd716cc<...> | cluster1      | http://s3stor1.example.com:80 | True    | 826f0be<...> | Ns7yfNq<...>    |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\n\nSee also\n\nEnabling S3 cross-region replication",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nIn the remote datacenter, run vinfra service s3 replication show token to get its token, and then copy this token. For example:# vinfra service s3 replication show token --max-value-length -1\r\n+-------+---------------------------------------------------------------------------+\r\n| Field | Value                                                                     |\r\n+-------+---------------------------------------------------------------------------+\r\n| token | eyJyZWFkYWJsZV9uYW1lIjogImhjaUhlYXQiLCAiaXNfc2VsZiI6IHRydWUsICJ1c2VyX3NlY |\r\n|       | 3JldF9rZXkiOiAiTnM3eWZOcVJ1RGQxRzVUc0ZCZ0VlcjNtWGgyRGJIMG1wanB1NkhVNyIsIC |\r\n|       | J1aWQiOiAiZGQ3MTZjY2VmNDE5OTNiZiIsICJ1cmwiOiAiaHR0cHM6Ly9zM3N0b3JhZ2UuZXh |\r\n|       | hbXBsZS5jb206NDQzIiwgInVzZXJfa2V5X2lkIjogIjgyNmYwYmUyMWNjMDcwZjJGUDhRIn0= |\r\n+-------+---------------------------------------------------------------------------+\n\n\nIn the local datacenter, run vinfra service s3 replication add, using the token of the remote datacenter. For example:# vinfra service s3 replication add --token eyJ1c2VyX3<\u00e2\u0080\u00a6>\n\n\nConfigure the remote S3 storage the same way.\n\n\nTo check that S3 geo-replication is enabled, run vinfra service s3 replication list:# vinfra service s3 replication list\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| uid          | readable_name | url                           | is_self | user_key_id  | user_secret_key |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| dd716cc<...> | cluster1      | http://s3stor1.example.com:80 | True    | 826f0be<...> | Ns7yfNq<...>    |\r\n| eff4d48<...> | cluster2      | http://s3stor2.example.com:80 | False   | cd3f6ae<...> | UmSuxYI<...>    |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\n",
                "title": "To set up geo-replication between S3 clusters"
            },
            {
                "example": "\nCommand-line interface\n\n\nIn the local datacenter, run vinfra service s3 replication delete using the UID of the remote datacenter. For example:# vinfra service s3 replication delete --id eff4d48<\u00e2\u0080\u00a6>\n\n\nConfigure the remote S3 storage the same way.\n\n\nTo check that S3 geo-replication is disabled, run vinfra service s3 replication list:# vinfra service s3 replication list\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| uid          | readable_name | url                           | is_self | user_key_id  | user_secret_key |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\r\n| dd716cc<...> | cluster1      | http://s3stor1.example.com:80 | True    | 826f0be<...> | Ns7yfNq<...>    |\r\n+--------------+---------------+-------------------------------+---------+--------------+-----------------+\n",
                "title": "To disable geo-replication between S3 clusters"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nIn the admin panel of a remote datacenter, open the Storage services > S3 > Geo-replication screen, and then click the home S3 storage.\nOn the right pane, click Get token.\n\nIn the Get token window, click Copy token below the Token section.\n\n\n\n\n\nIn the admin panel of the local datacenter, open the Storage services > S3 > Geo-replication screen, and then click Add datacenter.\n\nIn the Add datacenter window, enter the copied token, and then click Add.\n\n\n\n\n\nConfigure the remote S3 storage the same way.\n\nAfter you enable geo-replication for the clusters, you can replicate their data per bucket.\n",
                "title": "To set up geo-replication between S3 clusters"
            },
            {
                "example": "\nAdmin panel\n\nIn the admin panel of a local datacenter, open the Storage services > S3 > Geo-replication screen, and then click the remote S3 storage.\nOn the right pane, click Delete.\nClick Delete in the confirmation window.\nConfigure the remote S3 storage the same way.\n\n",
                "title": "To disable geo-replication between S3 clusters"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/enabling-s3-geo-replication.html"
    },
    {
        "title": "Getting technical support",
        "content": "Getting technical support\nIf you need technical support, you can send a problem report to and contact the technical support team. When generated, a problem report is assigned an ID. Make sure to mention this ID in the support ticket. \nIn the case of connection problems with the report server or if the report is too large to send via email, you can find the report in the /var/cache/problem-reports/ directory on the management node.\nTo generate and send a problem report\n\nAdmin panel\n\nOn any screen, click the user icon in the top right corner, and then select Report a problem.\n\nEnter your contact email and describe the problem, and then click Generate and send. The report status will be shown in the bottom right corner.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nYou can hide the pop-up window without interrupting the report generation. The report ID will be available in the notification center.\n\nCommand-line interface\nUse the following command:vinfra cluster problem-report [--email <email>] [--description <description>] [--send]\r\n\n\n--email <email>\n\nContact email address\n--description <description>\n\nProblem description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--send\n\nGenerate the problem report archive and send it to the technical support team\n\nFor example, to send a problem report with the description \u00e2\u0080\u009cTest report\u00e2\u0080\u009d to the technical support team and use test@example.com as a reply-to address, run:# vinfra cluster problem-report --email test@example.com --description \"Test report\" --send\r\n+---------+--------------------------------------+\r\n| Field   | Value                                |\r\n+---------+--------------------------------------+\r\n| task_id | 8bcfb92f-f02b-4de8-8e44-3426047630e3 |\r\n+---------+--------------------------------------+\r\n# vinfra task show 8bcfb92f-f02b-4de8-8e44-3426047630e3\r\n+---------+-------------------------------------------------------------+\r\n| Field   | Value                                                       |\r\n+---------+-------------------------------------------------------------+\r\n| details |                                                             |\r\n| name    | backend.presentation.reports.tasks.ReportProblemTask        |\r\n| result  | id: '1001923113'                                            |\r\n|         | path: /var/cache/problem-reports/report-<...>.391329.tar.gz |\r\n| state   | success                                                     |\r\n| task_id | 8bcfb92f-f02b-4de8-8e44-3426047630e3                        |\r\n+---------+-------------------------------------------------------------+\r\n\nNote the problem report ID in the task details. You will need to mention it in the support ticket.\n\nTo contact the technical support team\nVisit the support page at https://www.virtuozzo.com/support/.\nSee also\n\nManaging notifications",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster problem-report [--email <email>] [--description <description>] [--send]\r\n\n\n--email <email>\n\nContact email address\n--description <description>\n\n\nProblem description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--send\n\nGenerate the problem report archive and send it to the technical support team\n\nFor example, to send a problem report with the description \u00e2\u0080\u009cTest report\u00e2\u0080\u009d to the technical support team and use test@example.com as a reply-to address, run:# vinfra cluster problem-report --email test@example.com --description \"Test report\" --send\r\n+---------+--------------------------------------+\r\n| Field   | Value                                |\r\n+---------+--------------------------------------+\r\n| task_id | 8bcfb92f-f02b-4de8-8e44-3426047630e3 |\r\n+---------+--------------------------------------+\r\n# vinfra task show 8bcfb92f-f02b-4de8-8e44-3426047630e3\r\n+---------+-------------------------------------------------------------+\r\n| Field   | Value                                                       |\r\n+---------+-------------------------------------------------------------+\r\n| details |                                                             |\r\n| name    | backend.presentation.reports.tasks.ReportProblemTask        |\r\n| result  | id: '1001923113'                                            |\r\n|         | path: /var/cache/problem-reports/report-<...>.391329.tar.gz |\r\n| state   | success                                                     |\r\n| task_id | 8bcfb92f-f02b-4de8-8e44-3426047630e3                        |\r\n+---------+-------------------------------------------------------------+\r\n\nNote the problem report ID in the task details. You will need to mention it in the support ticket.\n",
                "title": "To generate and send a problem report"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOn any screen, click the user icon in the top right corner, and then select Report a problem.\n\n\n\n\n\n\nEnter your contact email and describe the problem, and then click Generate and send. The report status will be shown in the bottom right corner.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n\n\n\n\nYou can hide the pop-up window without interrupting the report generation. The report ID will be available in the notification center.\n",
                "title": "To generate and send a problem report"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/getting-technical-support.html"
    },
    {
        "title": "Fencing compute nodes",
        "content": "Fencing compute nodes\nA compute node might fail due to a kernel crash, power outage, or might become unreachable over the network. A failed node is automatically fenced from scheduling new virtual machines on it. That is, this node is isolated from the compute cluster, and new VMs are created on other compute nodes. Once a failed node becomes available again, the system runs a health check, and if successful, automatically tries to return the node to operation. If a node cannot be unfenced within 30 minutes after a crash or fails three times within an hour, you need to check its hardware, and then return it to operation manually.\nIn case there is an ongoing VM evacuation when the node is back to operation, the process continues for those VMs already scheduled for evacuation. The remaining VMs are retained on the node.\nIf you experience problems launching VMs on a particular node, you can fence this node manually for troubleshooting. Such nodes need to be returned to operation manually\nLimitations\n\nWith a three-node configuration, the compute cluster can survive a failure of only one node. Having at least five nodes will keep the compute cluster operational even if two compute nodes fail simultaneously.\nCompute nodes that are in maintenance cannot be fenced.\nCompute nodes with the single Controller role cannot be fenced.\nIf a compute node was fenced and then placed into the maintenance mode, it will return to the \"Fenced\" state after exiting maintenance.\nFenced nodes cannot be released from the compute cluster.\n\nPrerequisites\n\nBefore fencing a node, ensure it has no running virtual machines. You can either stop such VMs or migrate them live to other nodes.\n\nTo fence a compute node\n\nAdmin panel\n\nGo to the Compute > Nodes screen, and then click a node.\nOn the node right pane, click Fence.\n\nIn the Fence node window, optionally specify the fencing reason, and then click Fence.\n\nA fencing reason should not contain any personally identifiable information or sensitive business data.\n\nOnce, the node is fenced, you can check the fencing reason in its details. If the node hosts stopped virtual machines, you can move them to healthy nodes by clicking Evacuate on the VM right pane.\n\nCommand-line interface\nUse the following command:vinfra service compute node fence [--reason <reason>] <node>\r\n\n\n--reason <reason>\n\nThe reason for disabling the compute node\n\nA fencing reason should not contain any personally identifiable information or sensitive business data.\n\n<node>\n\nNode ID or hostname\n\nFor example, to fence the node node003.vstoragedomain, run:# vinfra service compute node fence node003.vstoragedomain\r\n\nYou can check that the node is successfully fenced in the vinfra service compute node list output:# vinfra service compute node list\r\n+--------------------------------------+------------------------+----------+--------------+\r\n| id                                   | host                   | state    | roles        |\r\n+--------------------------------------+------------------------+----------+--------------+\r\n| 52565ca3-5893-8f6b-62ce-2f07b175b549 | node001.vstoragedomain | healthy  | - controller |\r\n|                                      |                        |          | - compute    |\r\n| 578ccd91-dd8d-50b0-e3a2-f6ccb5959159 | node002.vstoragedomain | healthy  | - compute    |\r\n| 3ccf40b2-9437-b393-f02b-5282b188a4b3 | node003.vstoragedomain | disabled | - compute    |\r\n+--------------------------------------+------------------------+----------+--------------+\nIn the command-line output, the fenced node has the disabled state.\n\nTo return a fenced node back to operation\n\nAdmin panel\n\nGo to the Compute > Nodes screen, and then click a fenced node.\nOn the node right pane, click Return to operation.\nIn the confirmation window, click Return.\n\nCommand-line interface\nUse the following command:vinfra service compute node unfence <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to return the node node003.vstoragedomain to operation, run:# vinfra service compute node unfence node003.vstoragedomain\r\n\n\nSee also\n\nConfiguring virtual machine high availability\n\nManaging placements for compute nodes\n\nMonitoring compute nodes\n\nReleasing nodes from the compute cluster",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute node fence [--reason <reason>] <node>\r\n\n\n--reason <reason>\n\n\nThe reason for disabling the compute node\n\nA fencing reason should not contain any personally identifiable information or sensitive business data.\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to fence the node node003.vstoragedomain, run:# vinfra service compute node fence node003.vstoragedomain\r\n\nYou can check that the node is successfully fenced in the vinfra service compute node list output:# vinfra service compute node list\r\n+--------------------------------------+------------------------+----------+--------------+\r\n| id                                   | host                   | state    | roles        |\r\n+--------------------------------------+------------------------+----------+--------------+\r\n| 52565ca3-5893-8f6b-62ce-2f07b175b549 | node001.vstoragedomain | healthy  | - controller |\r\n|                                      |                        |          | - compute    |\r\n| 578ccd91-dd8d-50b0-e3a2-f6ccb5959159 | node002.vstoragedomain | healthy  | - compute    |\r\n| 3ccf40b2-9437-b393-f02b-5282b188a4b3 | node003.vstoragedomain | disabled | - compute    |\r\n+--------------------------------------+------------------------+----------+--------------+\nIn the command-line output, the fenced node has the disabled state.\n",
                "title": "To fence a compute node"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute node unfence <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to return the node node003.vstoragedomain to operation, run:# vinfra service compute node unfence node003.vstoragedomain\r\n\n",
                "title": "To return a fenced node back to operation"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Compute > Nodes screen, and then click a node.\nOn the node right pane, click Fence.\n\nIn the Fence node window, optionally specify the fencing reason, and then click Fence.\n\nA fencing reason should not contain any personally identifiable information or sensitive business data.\n\n\n\nOnce, the node is fenced, you can check the fencing reason in its details. If the node hosts stopped virtual machines, you can move them to healthy nodes by clicking Evacuate on the VM right pane.\n",
                "title": "To fence a compute node"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Compute > Nodes screen, and then click a fenced node.\nOn the node right pane, click Return to operation.\nIn the confirmation window, click Return.\n\n",
                "title": "To return a fenced node back to operation"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/fencing-compute-nodes.html"
    },
    {
        "title": "General requirements",
        "content": "General requirements\nVerify that all servers to be joined to the cluster meet the following general requirements.\nStorage requirements\nThe following table lists the minimum and recommended storage requirements according to the disk roles (refer to About the storage cluster):\n\nDisk role\nQuantity\nDisk size\nType\nEndurance\n\nSystem\nOne disk per node\n\n100 GB minimum\n250 GB recommended\n\nSATA/SAS HDD minimum\nSATA/SAS SSD recommended\n\n\u00e2\u0080\u0094\n\nMetadata\n\nOne disk per node\nFive disks recommended for one cluster\n\n100 GB\nEnterprise-grade SSD with power loss protection\n1 DWPD minimum\n\nCache\n\nOptional\nOne SSD disk per 4-12 HDDs\n\n100+ GB\nEnterprise-grade SSD with power loss\r\nprotection and 75 MB/s sequential write performance per serviced HDD\n\n1 DWPD minimum\n10 DWPD recommended\n\nStorage\nAt least one per cluster\n\n100 GB minimum\nUnlimited size on a physical server; 10 TB maximum recommended inside a virtual machine\n\n SATA/SAS HDD or enterprise-grade SATA/SAS/NVMe SSD with power loss protection\n1 DWPD minimum\n\nCPU and RAM reservations\nThe following table lists the amount of RAM and CPU cores that will be reserved on one node, according to the services you will use:\n\nService\nManagement node \nSecondary node\n\nRAM1 Use only Error correction code (ECC) memory, to avoid data corruption.\nCPU cores2 A CPU core here is a physical core in a multicore processor (hyperthreading is not taken into account).\nRAM3 Use only Error correction code (ECC) memory, to avoid data corruption.\nCPU cores4 A CPU core here is a physical core in a multicore processor (hyperthreading is not taken into account).\n\nSystem\n4.5 GB\n3.3 cores\n1.5 GB\n1.1 cores\n\nStorage services: each disk with Metadata, Storage, or Cache role (any size)5 For clusters larger than 1 PB of physical space, add 0.5 GB of RAM per Metadata service.\n0.5 GB\n0.2 cores\n0.5 GB\n0.2 cores\n\nCompute service6 The recommended configuration for a compute cluster node starts with 64+ GB and 16+ cores.\n10 GB\n4 cores\n\u00a0\n\u00a0\n\nLoad balancer\n1.5 GB\n0.5 cores\n\u00a0\n\u00a0\n\nKubernetes\n1 GB\n0.5 cores\n\u00a0\n\u00a0\n\nBackup Gateway7 When working with public clouds and NFS, Backup Gateway consumes as much RAM and CPU as with a local storage.\n1 GB\n0.5 cores\n1 GB\n0.5 cores\n\nS38 By default, each S3 node runs 4 S3 gateways and can run up to 10 NS and 10 OS instances, but the entire S3 cluster cannot host more than 24 OS and 16 NS instances. The number of OS and NS services is defined during the initial S3 cluster setup. Adding more nodes to the S3 cluster does not affect it. The CPU and RAM reservations depend on the number of S3 nodes. Generally, the larger the S3 cluster, the less resources are reserved on each node.\nEach S3 gateway\n256 MB\n1 core\n256 MB\n1 core\n\nEach OS service\n256 MB\n0.1 cores\n256 MB\n0.1 cores\n\nEach NS service\n512 MB\n0.2 cores\n512 MB\n0.2 cores\n\nNFS\nService\n4 GB\n1 core\n4 GB\n1 core\n\nEach share9 The RAM reservation for an NFS share depends on the number of cluster nodes. The larger the NFS cluster, the less RAM is reserved on each node.\nup to 9 GB\nup to 8 cores\nup to 9 GB\nup to 8 cores\n\niSCSI\n1 GB\n1 core\n1 GB\n1 core\n\nThese reserved values correspond to absolute minimum requirements. In general, the more resources you provide for your cluster, the better it works. All extra RAM is used to cache disk reads, while extra CPU cores increase the performance and reduce latency. \nCPU and RAM limits\nThe following table lists the current RAM and CPU limits for Virtuozzo Hybrid Infrastructure servers:\n\nHardware\nTheoretical\nCertified\n\nRAM\n64 TB\n1 TB\n\nCPU\n5120 logical CPUs10 A logical CPU is a core (thread) in a multicore (multithreading) processor.\n384 logical CPUs11 A logical CPU is a core (thread) in a multicore (multithreading) processor.\n\nCPU recommendations\n\nThe minimum CPU frequency for Virtuozzo Hybrid Infrastructure servers is 2.0 GHz. The recommended CPU frequency is at least 2.4 GHz.\nCPU frequency affects the cluster performance. Higher CPU frequency reduces latency almost linearly, thus increasing the performance. If other CPU parameters are the same, higher CPU frequency is preferable to a larger number of CPU cores.\nNodes running the metadata service should have high CPU frequency, because this service is CPU intensive.\n\nTo improve erasure coding performance, it is recommended to use CPUs that support the AVX-512 instruction set, such as Intel Xeon Silver 4110, Intel Xeon Gold 5115, Intel Xeon Platinum 8171, or newer.\n\nNetwork interface requirements\nAt least 2 x 10 GbE interfaces are recommended, for internal and external traffic; 25 GbE, 40 GbE, and 100 GbE are even better. Bonding is recommended. However, for external traffic, you can start with 1 GbE links, but they can limit cluster throughput on modern loads. \nSee also\n\nQuantity of servers\n\nBackup storage requirements\n\nCompute cluster requirements\n\nObject storage requirements",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/general-requirements.html"
    },
    {
        "title": "High availability",
        "content": "High availability\nHigh availability (HA) keeps Virtuozzo Hybrid Infrastructure services operational even if the node they are located on fails. In such cases, services from a failed node are relocated to healthy nodes, according to the Raft consensus algorithm. High availability is ensured by:\n\nMetadata redundancy. For a storage cluster to function, not all but just the majority of metadata servers must be up. By setting up multiple metadata servers in the cluster, you will make sure that if an metadata server fails, other metadata servers will continue controlling the cluster.\nThe required number of metadata servers is deployed automatically, based on recommended hardware configurations.\n\nData redundancy. Copies of each piece of data are stored across different failure domains, to ensure that the data is available even if some of the failure domains are inaccessible. For more information, refer to Data redundancy.\nMonitoring of node health.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/high-availability.html"
    },
    {
        "title": "Importing registrations to the secondary cluster",
        "content": "Importing registrations to the secondary cluster\nAfter geo-replication is enabled, new registrations that you add to the primary cluster are not automatically replicated to the secondary cluster. You need to import such registrations manually.\nPrerequisites\n\nGeo-replication is enabled, as described in Enabling geo-replication.\nOne or more registrations are added to the primary cluster, as explained in Adding registrations.\n\nTo import a registration to the secondary cluster\n\nOn the primary cluster, find out the registration ID in the uuid field. For example:# vstorage-abgw-register list\r\n[\r\n    {\r\n        \"uuid\": \"68ae9bd1-ae34-4ff2-938b-cd576d9610a5\",\r\n        \"type\": \"ABC\",\r\n        \"description\": \"registration1\",\r\n        ...\r\n    },\r\n    ...\r\n]\n\nExport the registration to a configuration file specifying the registration ID and the file name. For example:# vstorage-abgw-register export -R \"68ae9bd1-ae34-4ff2-938b-cd576d9610a5\" -f registration1.txt\n\nMove the configuration file from the primary cluster to the secondary cluster using the standard Linux command-line tool. For example, by using scp:# scp registration1.txt <secondary_cluster_IP_address>:/root/registration1.txt\n\nOn the secondary cluster, import the registration from the configuration file specifying the registration ID and the file name. For example:# vstorage-abgw-register import -R \"68ae9bd1-ae34-4ff2-938b-cd576d9610a5\" -f registration1.txt\n\nList all registrations to check that the registration is successfully imported. For example:# vstorage-abgw-register list\r\n[\r\n    {\r\n        \"uuid\": \"68ae9bd1-ae34-4ff2-938b-cd576d9610a5\",\r\n        \"type\": \"ABC\",\r\n        \"description\": \"registration1\",\r\n        ...\r\n    },\r\n    ...\r\n]\n\nRestart the Backup Gateway services on each node of the secondary backup storage cluster:# systemctl restart vstorage-abgw.service\n\nSee also\n\nPerforming a failover\n\nDisabling geo-replication",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/importing-registrations-to-the-secondary-cluster.html"
    },
    {
        "title": "Installation",
        "content": "Installation\nThis section describes the installation process of Virtuozzo Hybrid Infrastructure.\nLimitations\n\nA display for the installation program must have a minimum screen resolution of 800x600. With 800x600, however, you may experience issues with the user interface. For example, some elements can be inaccessible. The recommended screen resolution is at least 1024x768. \nOne node can be a part of only one cluster.\n\nPrerequisites\n\nA clear understanding of the infrastructure, which is explained in About the infrastructure.\nYour infrastructure hardware meets the requirements listed in System requirements. \n\nInstallation overview\n\nObtain the distribution ISO image. To do that, visit the product page and submit a request for the trial version.\n\nPrepare the bootable media by using the distribution ISO image:  create a bootable USB drive, mount the distribution image to an IPMI virtual drive, or set up a PXE server.\nIf the server's display resolution is lower than  800x600, connect to the server from a remote machine  via VNC.\nIf you plan to perform an unattended installation, create a kickstart file.\n\nInstall Virtuozzo Hybrid Infrastructure on each server in the attended  or unattented mode.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/installation.html"
    },
    {
        "title": "Installing guest tools",
        "content": "Installing guest tools\nYou can install the guest tools inside a virtual machine by using ISO images stored either in the official repository or on a compute node. The latter case includes additional steps performed by  a user with the system administrator role.\nAs a system administrator\nUpload the guest tools ISO files located in the /usr/share/vz-guest-tools/ directory on any compute node to the compute cluster:\n\nfor a Windows guest, upload vz-guest-tools-win.iso by running:# vinfra service compute image create vz-guest-tools-win --file /usr/share/vz-guest-tools/vz-guest-tools-win.iso --public\n\nfor a Linux guest, upload vz-guest-tools-lin.iso by running:# vinfra service compute image create vz-guest-tools-lin --file /usr/share/vz-guest-tools/vz-guest-tools-lin.iso --public\n\nAs a virtual machine user\n\nAdmin panel\n\nCreate a compute volume from the vz-guest-tools-win or vz-guest-tools-lin image, depending on the VM operating system:\n\nOn the Compute > Virtual machines > Images tab, click the vz-guest-tools-win or vz-guest-tools-lin image.\nOn the image right pane, click Create volume.\nIn the Create volume from image window, specify a name for the volume, and then click Create.\n\nAttach the volume with the guest tools to the virtual machine:\n\nOn the Compute > Virtual machines > Virtual machines tab, click the required VM.\nOn the VM right pane, click the pencil icon in the Volumes field.\nIn the Volumes window, click Attach.\nIn the Attach volume window, select the created volume with the guest tools, and then click Attach. The attached volume will be marked as ISO.\nIn the Volumes window, click Done, to save your changes.\n\nLog in to the virtual machine.\n\nInside the VM, do the following:\n\nInside a Windows VM, go to the mounted optical drive in Explorer and install the guest tools by running setup.exe. After the installation is complete, restart the VM.\n\nInside a Linux VM, create a mount point for the optical drive with the guest tools image and run the installer:# mkdir /mnt/cdrom\r\n# mount <path_to_guest_tools_iso> /mnt/cdrom\r\n# bash /mnt/cdrom/install\t\t\t\t\t\t\t\t\n\nCommand-line interface\n\nCreate a compute volume from the vz-guest-tools-win or vz-guest-tools-lin image, depending on the VM operating system. For example:# vinfra service compute volume create guest-tools-lin --image vz-guest-tools-lin \\\r\n--storage-policy default --size 1\n\nAttach the volume with the guest tools to the required virtual machine. For example:# vinfra service compute server volume attach guest-tools-lin --server centos7\r\n+--------+--------------------------------------+\r\n| Field  | Value                                |\r\n+--------+--------------------------------------+\r\n| device | /dev/sda                             |\r\n| id     | 132908e4-3543-419f-a4bf-c219f74e2640 |\r\n+--------+--------------------------------------+\r\n\n\nLog in to the virtual machine.\n\nInside the VM, do the following:\n\nInside a Windows VM, go to the mounted optical drive in Explorer and install the guest tools by running setup.exe. After the installation is complete, restart the VM.\n\nInside a Linux VM, create a mount point for the optical drive with the guest tools image and run the installer:# mkdir /mnt/cdrom\r\n# mount <path_to_guest_tools_iso> /mnt/cdrom\r\n# bash /mnt/cdrom/install\t\n\nSee also\n\nSetting a password inside virtual machines\n\nRunning commands in virtual machines without network connectivity\n\nManaging volume snapshots\n\nUninstalling guest tools",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nCreate a compute volume from the vz-guest-tools-win or vz-guest-tools-lin image, depending on the VM operating system. For example:# vinfra service compute volume create guest-tools-lin --image vz-guest-tools-lin \\\r\n--storage-policy default --size 1\n\n\nAttach the volume with the guest tools to the required virtual machine. For example:# vinfra service compute server volume attach guest-tools-lin --server centos7\r\n+--------+--------------------------------------+\r\n| Field  | Value                                |\r\n+--------+--------------------------------------+\r\n| device | /dev/sda                             |\r\n| id     | 132908e4-3543-419f-a4bf-c219f74e2640 |\r\n+--------+--------------------------------------+\r\n\n\nLog in to the virtual machine.\n\nInside the VM, do the following:\n\nInside a Windows VM, go to the mounted optical drive in Explorer and install the guest tools by running setup.exe. After the installation is complete, restart the VM.\n\nInside a Linux VM, create a mount point for the optical drive with the guest tools image and run the installer:# mkdir /mnt/cdrom\r\n# mount <path_to_guest_tools_iso> /mnt/cdrom\r\n# bash /mnt/cdrom/install\t\n\n\n\n\n",
                "title": "As a virtual machine user"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nCreate a compute volume from the vz-guest-tools-win or vz-guest-tools-lin image, depending on the VM operating system:\n\nOn the Compute > Virtual machines > Images tab, click the vz-guest-tools-win or vz-guest-tools-lin image.\nOn the image right pane, click Create volume.\nIn the Create volume from image window, specify a name for the volume, and then click Create.\n\n\n\nAttach the volume with the guest tools to the virtual machine:\n\nOn the Compute > Virtual machines > Virtual machines tab, click the required VM.\nOn the VM right pane, click the pencil icon in the Volumes field.\nIn the Volumes window, click Attach.\nIn the Attach volume window, select the created volume with the guest tools, and then click Attach. The attached volume will be marked as ISO.\nIn the Volumes window, click Done, to save your changes.\n\n\nLog in to the virtual machine.\n\nInside the VM, do the following:\n\nInside a Windows VM, go to the mounted optical drive in Explorer and install the guest tools by running setup.exe. After the installation is complete, restart the VM.\n\nInside a Linux VM, create a mount point for the optical drive with the guest tools image and run the installer:# mkdir /mnt/cdrom\r\n# mount <path_to_guest_tools_iso> /mnt/cdrom\r\n# bash /mnt/cdrom/install\t\t\t\t\t\t\t\t\n\n\n\n\n",
                "title": "As a virtual machine user"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/installing-guest-tools.html"
    },
    {
        "title": "Installing in the unattended mode",
        "content": "Installing in the unattended mode\nIf you plan to perform an unattended installation of Virtuozzo Hybrid Infrastructure, you can use a kickstart file. It will automatically supply the installer with the options you would normally choose by hand. \nPrerequisites\n\nA PXE server is prepared, as outlined in Setting up a PXE server.\nA kickstart file is created by using the options and scripts described in Creating a kickstart file.\n\nTo install in the unattended mode\n\nMake the kickstart file accessible over the network:\n\nCopy the kickstart file to the same directory on the HTTP server where the Virtuozzo Hybrid Infrastructure installation files are stored (for example, to /var/www/html/stor).\n\nAdd the following string to the /tftpboot/pxelinux.cfg/default file on the PXE server:inst.ks=<HTTP_server_address>/<path_to_kickstart_file>\nFor EFI-based systems, the file you need to edit has the name of /tftpboot/pxelinux.cfg/efidefault or /tftpboot/pxelinux.cfg/<PXE_server_IP_address>.\nAssuming that the HTTP server has the IP address of 198.123.123.198, the DocumentRoot directory is set to /var/www/html, and the full path to your kickstart file on this server is /var/www/html/stor/ks.cfg, your default file may look like the following:default menu.c32prompt 0timeout 100ontimeout ASTORmenu title Boot Menulabel ASTOR        menu label Install        kernel vmlinuz        append initrd=initrd.img ip=dhcp inst.repo=http://198.123.123.198/stor inst.ks=http://198.123.123.198/stor/ks.cfg\n\nConfigure the server to boot from the chosen media.\nBoot the server and wait for the Welcome screen.\nOn the Welcome screen, select Install Virtuozzo Hybrid Infrastructure and press E to edit the menu entry.\n\nAppend the kickstart file location to the linux /images/pxeboot/vmlinuz line, and press Ctrl+X. For example:linux /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=<ISO_img> quiet ip=dhcp logo.nologo=1 inst.ks=<URL>\r\n\n\nThe installation will proceed automatically. Once it is complete, the node will reboot automatically. The admin panel IP address will be shown in the welcome prompt.\nWhat's next\n\nAfter installation on the primary node\n\nIf you installed the admin panel and storage components without exposing the superadmin password and storage token in the kickstart file, do the following:\n\nOn the node, run the following command as the root user to configure the admin panel component:echo <password> | /usr/libexec/vstorage-ui-backend/bin/configure-backend.sh -i <private_iface> -x <public_iface>\r\n\nwhere\n\n<password> is the password of the superadmin account for the admin panel\n<private_iface> is the name of the private network interface (the one you would select for the management network during attended installation)\n<public_iface> is the name of the public network interface (the one you would select for the admin panel network during attended installation)\n\nStart the admin panel service on the node:# systemctl start vstorage-ui-backend\r\n\n\nRegister the node in the admin panel:# /usr/libexec/vstorage-ui-agent/bin/register-storage-node.sh -m <MN_IP_address> -x <public_iface>\r\n\nwhere \n\n<MN_IP_address> is the IP address of the node's private network interface\n<public_iface> is the name of the public network interface\n\nNow you can proceed to deploying secondary nodes.\n\nAfter installation on a secondary node\n\nIf you installed the storage component without exposing the storage token in the kickstart file, run the following script on the node,  to register it in the admin panel:# /usr/libexec/vstorage-ui-agent/bin/register-storage-node.sh -m <MN_IP_address> -t <token>\r\n\nwhere\n\n<MN_IP_address> is the IP address of the private network interface on the node with the admin panel\n<token> is the token obtained in the admin panel or from the vinfra node token show output\n\nWhen all the required nodes are displayed in the admin panel on the Infrastructure > Nodes screen as Unassigned, proceed to create the storage cluster, as outlined in Deployment and configuration.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/installing-in-the-unattended-mode.html"
    },
    {
        "title": "Listening to SNMP traps",
        "content": "Listening to SNMP traps\nPrerequisites\n\nThe SNMP access is enabled, as described in Enabling SNMP access.\n\nTo start listening to SNMP traps\n\nConfigure the snmptrapd daemon to log SNMP traps, allow them to trigger executable actions, and resend data to the network. To do this, uncomment the following public community string in the /etc/snmp/snmptrapd.conf file:authCommunity log,execute,net public\r\n\n\nConfigure the firewall to allow inbound traffic on UDP port 162.\nDownload the VSTORAGE-MIB.txt file and place it in the /usr/share/snmp/mibs directory.\n\nStart the daemon and specify the MIB file:# snmptrapd -M /usr/share/snmp/mibs -m VSTORAGE-MIB -n -f\r\n\nBy default, traps will be logged to /var/log/messages. You can redirect them to a custom log file with the -Lf <path> option. For example:# snmptrapd -M /usr/share/snmp/mibs -m VSTORAGE-MIB -n -f -Lf /tmp/traps.log\r\n\n\nSend a test trap from the Settings > Advanced settings > SNMP tab in the admin panel.\n\nView the log file:# tail -f /tmp/traps.log\r\n2019-10-14 12:51:50 node001.vstoragedomain [UDP: [10.94.80.22]:40029->\\\r\n[10.94.80.22]:162]:#012DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: \\\r\n(111150521) 12 days, 20:45:05.21#011SNMPv2-MIB::snmpTrapOID.0 = OID: \\\r\nNET-SNMP-MIB::netSnmp.161.3.100#011NET-SNMP-MIB::netSnmp.161.2.1 = STRING: \"TestTrap\"\\\r\n#011NET-SNMP-MIB::netSnmp.161.2.2 = STRING: \"It is the test trap from VStorage\"\\\r\n#011NET-SNMP-MIB::netSnmp.161.2.3 = Counter64: 0\r\n\n\nSee also\n\nAccessing cluster information objects via SNMP\n\nMonitoring the cluster with Zabbix",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/listening-to-snmp-traps.html"
    },
    {
        "title": "Installing updates",
        "content": "Installing updates\nVirtuozzo Hybrid Infrastructure supports non-disruptive rolling updates. Nodes are updated one by one, with the data availability unaffected.\nAn update may have the following impact on a node:\n\nReboot required. During such an update, a node needs to be rebooted to apply a new kernel. In this case, you can place the node into the maintenance mode and evacuate its services and virtual machines to other nodes, to avoid their downtime. During the node maintenance, the compute API services are fenced from processing compute requests. Once the node is updated, it is automatically rebooted. If the node entered maintenance, it returns to operation after the reboot and the migrated workloads, except for VMs, are moved back on the node. The evacuated VMs remain on other nodes.\nMaintenance required. During such an update, a node needs to enter the maintenance mode to install new packages for major services. In this case, you can place the node into the maintenance mode and evacuate its services and virtual machines to other nodes, to avoid their downtime. During the node maintenance, the compute API services are fenced from processing compute requests. Once updated, the node exits maintenance without a reboot and the migrated workloads, except for VMs, are moved back on the node. The evacuated VMs remain on other nodes.\nNo impact. In this case, an update is performed without a node reboot or maintenance.\n\nYou can update different cluster components all together or separately. In either case, the components are updated in the following order:\n\nCluster nodes are updated first. If any of these nodes are included in the compute cluster, the compute services on them are updated at this step.\nManagement nodes are updated only when all of the cluster nodes are up to date. The primary management node is the last one to be updated. If management nodes have the compute services deployed, these services are updated at this step.\nThe management panel (admin and self-service) is updated on management nodes and only when all of the nodes, both cluster and management, are up to date. While updating this component, management nodes do not require a reboot.\n\nLimitations\n\nNodes must be updated only in the admin panel or via the vinfra tool. Do not use yum update.\nUnassigned nodes can be updated.\nUpdates are applied to one node at a time.\nYou can only update management nodes all together, one at a time, and after updating all of the cluster nodes.\nYou can only update the management panel after updating all of the management and cluster nodes.\nIn a single-node deployment, the node does not enter maintenance during an update. If an update requires a node reboot or maintenance, the cluster downtime is expected.\nLive migration is not supported for suspended virtual machines, as well as for virtual machines with attached vGPU or PCI devices.\n\nPrerequisites\n\nThe storage cluster is created by following the instructions in Deploying the storage cluster.\nAny third-party repositories are disabled.\nThe cluster is healthy and each node in the infrastructure is online.\nThe cluster DNS is configured, as described in Adding external DNS servers, and point to a DNS table to resolve external host names.\n\nTo update cluster components\n\nAdmin panel\n\nOpen the Settings > Updates screen. The date of the last check is displayed in the upper right corner. Click the round arrow to check for new updates. If updates are available for a cluster component, its update status changes to Available. Check if a node needs a reboot or  maintenance in the Update impact column.\n\nClick Download in the upper right corner to get the updates. Wait until the updates are downloaded and the update status changes to Ready to install.\n\nOnce the updates are downloaded, the automatic check for updates is disabled until the updates are installed or the operation is canceled. To reset the software updates state and make it possible to check for a newer version at this step, use the vinfra software-updates reset command.\n\nClick Release notes to read the release notes.\n\nSelect components that you want to be updated:\n\nTo update cluster nodes, select the desired cluster nodes.\nTo update management nodes, select all of the management nodes and those cluster nodes that require an update.\nTo update the management panel, select this component and all of the management nodes if they require an update.\n\nClick Update to continue.\nWhen upgrading to a new major version, review the upgrade notes, and then click Next.\n\nIf you have selected nodes that require a reboot or maintenance, do the following:\n\nDecide whether these nodes will enter the maintenance mode. Select Maintenance mode, if you want to place the nodes in the maintenance mode.\n\nIf you have selected nodes with the compute service, choose how to migrate virtual machines running on these nodes:\n\nWith the option Ignore VMs that cannot be live migrated, VMs from a node that enters the maintenance mode will be live migrated to other compute nodes. VMs that cannot be live migrated will be ignored. This applies to suspended VMs, VMs with vGPU or PCI devices attached, or if other compute nodes have insufficient vCPU or RAM resources. During the node update, ignored VMs will be stopped, resulting in downtime. They will be started automatically once the update is complete. After the node exits maintenance, the migrated VMs will not be moved back on the node.\nWith the option Ignore VMs that cannot be or failed to be live migrated, VMs from a node that enters the maintenance mode will be live migrated to other compute nodes. VMs that cannot be live migrated will be ignored. This applies to suspended VMs, VMs with vGPU or PCI devices attached, or if other compute nodes have insufficient vCPU or RAM resources. During the node update, ignored VMs and VMs that failed to be live migrated will be stopped, resulting in downtime. They will be started automatically once the update is complete. After the node exits maintenance, the migrated VMs will not be moved back on the node.\nWith the option Live migrate all VMs, all of the VMs from a node that enters the maintenance mode will be live migrated to other compute nodes. After the node exits maintenance, the migrated VMs will not be moved back on the node.\n\nReview the selected components, and then click Install. The system will start update eligibility checks. In case of a major release, the system will also check for removed and unmaintained hardware:\n\nRemoved hardware means that its support has been discontinued. If removed hardware is detected, review its details. If your hardware is detected correctly, the upgrade will fail because such devices are no longer available in the new version. Otherwise, you can force the upgrade by clicking Force update.\nUnmaintained hardware includes devices (drivers and adapters) that are no longer being tested and updated in the new version. If unmaintained hardware is detected, you cannot continue the upgrade because such devices should no longer be used in production.\n\nYou can export the list of detected hardware to a JSON file by clicking Export to file.\n\nDo not perform any cluster configuration tasks in the admin panel or command-line interface during the update, as this will lead to an update failure and cluster downtime.\n\nWhile the updates are being installed, you can pause or cancel the process. After the update is complete, the component statuses will change to Up to date.\nIf the update fails, click Details to view the issue details and decide how to proceed. You can cancel the update, solve the issues, and retry updating without downtime. Alternatively, you can force the update without putting the nodes into maintenance. However, this will cause a downtime of workloads running on them.\n\nCommand-line interface\nUse the following commands:\n\nCheck if there are updates for the storage cluster:# vinfra software-updates check-for-updates\n\nView the results of the check-up:# vinfra software-updates status\r\n+---------------------------+--------------------------------------------+\r\n| Field                     | Value                                      |\r\n+---------------------------+--------------------------------------------+\r\n| available_storage_release | release: '127'                             |\r\n|                           | version: 5.2.0                             |\r\n| control_plane             | available_storage_release:                 |\r\n|                           |   release: '127'                           |\r\n|                           |   version: 5.2.0                           |\r\n|                           | installed_storage_release:                 |\r\n|                           |   release: '206'                           |\r\n|                           |   version: 5.1.0                           |\r\n|                           | status: available                          |\r\n| last_check_datetime       | 2021-11-01T12:22:10.630818                 |\r\n| nodes                     | - available_storage_release:               |\r\n|                           |     release: '127'                         |\r\n|                           |     version: 5.2.0                         |\r\n|                           |   current_storage_release:                 |\r\n|                           |     release: '206'                         |\r\n|                           |     version: 5.1.0                         |\r\n|                           |   downloaded_storage_release: null         |\r\n|                           |   host: node001.vstoragedomain             |\r\n|                           |   id: 0175ce44-c86d-7818-3259-3182f5fd83f6 |\r\n|                           |   is_in_ha: false                          |\r\n|                           |   is_primary: true                         |\r\n|                           |   maintenance_required: true               |\r\n|                           |   orig_hostname: node001                   |\r\n|                           |   reboot_required: true                    |\r\n|                           |   status: available                        |\r\n|                           | - available_storage_release:               |\r\n|                           |     release: '127'                         |\r\n|                           |     version: 5.2.0                         |\r\n|                           |   current_storage_release:                 |\r\n|                           |     release: '206'                         |\r\n|                           |     version: 5.1.0                         |\r\n|                           |   downloaded_storage_release: null         |\r\n|                           |   host: node002.vstoragedomain             |\r\n|                           |   id: 923926da-a879-5f56-1b24-1462917ed335 |\r\n|                           |   is_in_ha: false                          |\r\n|                           |   is_primary: false                        |\r\n|                           |   maintenance_required: true               |\r\n|                           |   orig_hostname: node002                   |\r\n|                           |   reboot_required: true                    |\r\n|                           |   status: available                        |\r\n|                           | - available_storage_release:               |\r\n|                           |     release: '127'                         |\r\n|                           |     version: 5.2.0                         |\r\n|                           |   current_storage_release:                 |\r\n|                           |     release: '206'                         |\r\n|                           |     version: 5.1.0                         |\r\n|                           |   downloaded_storage_release: null         |\r\n|                           |   host: node003.vstoragedomain             |\r\n|                           |   id: ef24c47c-620d-8726-2677-ed94d853de2e |\r\n|                           |   is_in_ha: false                          |\r\n|                           |   is_primary: false                        |\r\n|                           |   maintenance_required: true               |\r\n|                           |   orig_hostname: node003                   |\r\n|                           |   reboot_required: true                    |\r\n|                           |   status: available                        |\r\n| status                    | available                                  |\r\n+---------------------------+--------------------------------------------+\r\n\nThe output above shows that an update to build 234 is available.\n\nDownload the software update:# vinfra software-updates download\n\nCheck whether the nodes in the storage cluster are eligible for the update:# vinfra software-updates eligibility-check\r\n+---------+--------------------------------------+\r\n| Field   | Value                                |\r\n+---------+--------------------------------------+\r\n| task_id | 88e51115-8f0e-4c6f-b33b-949728d1fb99 |\r\n+---------+--------------------------------------+\r\n# vinfra task show 88e51115-8f0e-4c6f-b33b-949728d1fb99\r\n+---------+------------------------------------------------------------------+\r\n| Field   | Value                                                            |\r\n+---------+------------------------------------------------------------------+\r\n| details |                                                                  |\r\n| name    | backend.presentation.software_updates.tasks.EligibilityCheckTask |\r\n| result  | chunks_rebalancing_rate:                                         |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: info                                                 |\r\n|         | cluster_has_releasing_nodes:                                     |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | cluster_unhealthy:                                               |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | not_enough_space_on_agents:                                      |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | not_enough_space_on_mn:                                          |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | postgres_not_running:                                            |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | request_accept_eula:                                             |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | server_with_pci_devices:                                         |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: info                                                 |\r\n|         | shaman:                                                          |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | tgtd:                                                            |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | too_many_pending_chunks:                                         |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: info                                                 |\r\n| state   | success                                                          |\r\n| task_id | 88e51115-8f0e-4c6f-b33b-949728d1fb99                             |\r\n+---------+------------------------------------------------------------------+\r\n\n\nStart the software update procedure by running the command:vinfra software-updates start [--maintenance enabled={yes,no}[,key=value,\u00e2\u0080\u00a6]]\r\n                              [--nodes <nodes>] [--skip-control-plane]\r\n                              \n\n--maintenance enabled={yes,no}[,key=value,\u00e2\u0080\u00a6]>\n\nSpecify maintenance parameters:\n\nenabled: enter maintenance during the upgrade (yes or no)\n\ncomma-separated key=value pairs with keys (optional):\n\non-fail: choose how to proceed with the update if maintenance fails:\n\nstop (default): stop the update if a node cannot enter maintenance mode. Nodes that have already been updated will remain so.\nskip: skip and do not update nodes that cannot enter maintenance mode\nforce: forcibly update and reboot (if needed) all nodes even if they cannot enter maintenance mode. Using this option may result in downtime.\n\ncompute-mode: choose how to proceed with the update if a VM cannot be live migrated:\n\nstrict: stop the upgrade if a VM cannot be live migrated\nignore: ignore a VM that cannot be live migrated\nignore_ext: ignore a VM that cannot be or failed to be live migrated\n\n--nodes <nodes>\n\nA comma-separated list of node IDs or hostnames\n--skip-control-plane\n\nUpdate the cluster without updating the management panel.\n--accept-eula\n\nAccept EULA\n\nFor example, to start updating the nodes node001, node002, and node003 and put them into maintenance, run:# vinfra software-updates start --nodes node001,node002,node003 \\\r\n--maintenance enabled=yes,on-fail=skip,compute-mode=ignore\r\n\nThose nodes that cannot enter maintenance will be skipped. Virtual machines that cannot be live migrated during maintenance will be ignored.\n\nTo pause software updates, use the command vinfra software-updates pause. To resume the update procedure, run vinfra software-updates resume.\nYou can cancel the update and exit the maintenance mode for a node by using the command:vinfra software-updates cancel [--maintenance-mode {exit,exit-keep-resources,hold}]\n\n--maintenance-mode {exit,exit-keep-resources,hold}\n\nMaintenance mode:\n\nexit: exit maintenance for the node and return evacuated resources back on the node\nexit-keep-resources (default): exit maintenance for the node but keep evacuated resources on another node\nhold: do not exit maintenance\n\nFor example, to cancel the update and return the node to operation, run:# vinfra software-updates cancel exit\nThe evacuated resources from this node will be moved back to it.\n\nSee also\n\nPerforming node maintenance",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following commands:\n\n\nCheck if there are updates for the storage cluster:# vinfra software-updates check-for-updates\n\n\nView the results of the check-up:# vinfra software-updates status\r\n+---------------------------+--------------------------------------------+\r\n| Field                     | Value                                      |\r\n+---------------------------+--------------------------------------------+\r\n| available_storage_release | release: '127'                             |\r\n|                           | version: 5.2.0                             |\r\n| control_plane             | available_storage_release:                 |\r\n|                           |   release: '127'                           |\r\n|                           |   version: 5.2.0                           |\r\n|                           | installed_storage_release:                 |\r\n|                           |   release: '206'                           |\r\n|                           |   version: 5.1.0                           |\r\n|                           | status: available                          |\r\n| last_check_datetime       | 2021-11-01T12:22:10.630818                 |\r\n| nodes                     | - available_storage_release:               |\r\n|                           |     release: '127'                         |\r\n|                           |     version: 5.2.0                         |\r\n|                           |   current_storage_release:                 |\r\n|                           |     release: '206'                         |\r\n|                           |     version: 5.1.0                         |\r\n|                           |   downloaded_storage_release: null         |\r\n|                           |   host: node001.vstoragedomain             |\r\n|                           |   id: 0175ce44-c86d-7818-3259-3182f5fd83f6 |\r\n|                           |   is_in_ha: false                          |\r\n|                           |   is_primary: true                         |\r\n|                           |   maintenance_required: true               |\r\n|                           |   orig_hostname: node001                   |\r\n|                           |   reboot_required: true                    |\r\n|                           |   status: available                        |\r\n|                           | - available_storage_release:               |\r\n|                           |     release: '127'                         |\r\n|                           |     version: 5.2.0                         |\r\n|                           |   current_storage_release:                 |\r\n|                           |     release: '206'                         |\r\n|                           |     version: 5.1.0                         |\r\n|                           |   downloaded_storage_release: null         |\r\n|                           |   host: node002.vstoragedomain             |\r\n|                           |   id: 923926da-a879-5f56-1b24-1462917ed335 |\r\n|                           |   is_in_ha: false                          |\r\n|                           |   is_primary: false                        |\r\n|                           |   maintenance_required: true               |\r\n|                           |   orig_hostname: node002                   |\r\n|                           |   reboot_required: true                    |\r\n|                           |   status: available                        |\r\n|                           | - available_storage_release:               |\r\n|                           |     release: '127'                         |\r\n|                           |     version: 5.2.0                         |\r\n|                           |   current_storage_release:                 |\r\n|                           |     release: '206'                         |\r\n|                           |     version: 5.1.0                         |\r\n|                           |   downloaded_storage_release: null         |\r\n|                           |   host: node003.vstoragedomain             |\r\n|                           |   id: ef24c47c-620d-8726-2677-ed94d853de2e |\r\n|                           |   is_in_ha: false                          |\r\n|                           |   is_primary: false                        |\r\n|                           |   maintenance_required: true               |\r\n|                           |   orig_hostname: node003                   |\r\n|                           |   reboot_required: true                    |\r\n|                           |   status: available                        |\r\n| status                    | available                                  |\r\n+---------------------------+--------------------------------------------+\r\n\nThe output above shows that an update to build 234 is available.\n\n\nDownload the software update:# vinfra software-updates download\n\n\nCheck whether the nodes in the storage cluster are eligible for the update:# vinfra software-updates eligibility-check\r\n+---------+--------------------------------------+\r\n| Field   | Value                                |\r\n+---------+--------------------------------------+\r\n| task_id | 88e51115-8f0e-4c6f-b33b-949728d1fb99 |\r\n+---------+--------------------------------------+\r\n# vinfra task show 88e51115-8f0e-4c6f-b33b-949728d1fb99\r\n+---------+------------------------------------------------------------------+\r\n| Field   | Value                                                            |\r\n+---------+------------------------------------------------------------------+\r\n| details |                                                                  |\r\n| name    | backend.presentation.software_updates.tasks.EligibilityCheckTask |\r\n| result  | chunks_rebalancing_rate:                                         |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: info                                                 |\r\n|         | cluster_has_releasing_nodes:                                     |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | cluster_unhealthy:                                               |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | not_enough_space_on_agents:                                      |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | not_enough_space_on_mn:                                          |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | postgres_not_running:                                            |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | request_accept_eula:                                             |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | server_with_pci_devices:                                         |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: info                                                 |\r\n|         | shaman:                                                          |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | tgtd:                                                            |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: critical                                             |\r\n|         | too_many_pending_chunks:                                         |\r\n|         |   details: null                                                  |\r\n|         |   exception: null                                                |\r\n|         |   message: null                                                  |\r\n|         |   passed: true                                                   |\r\n|         |   severity: info                                                 |\r\n| state   | success                                                          |\r\n| task_id | 88e51115-8f0e-4c6f-b33b-949728d1fb99                             |\r\n+---------+------------------------------------------------------------------+\r\n\n\n\nStart the software update procedure by running the command:vinfra software-updates start [--maintenance enabled={yes,no}[,key=value,\u00e2\u0080\u00a6]]\r\n                              [--nodes <nodes>] [--skip-control-plane]\r\n                              \n\n--maintenance enabled={yes,no}[,key=value,\u00e2\u0080\u00a6]>\n\n\nSpecify maintenance parameters:\n\nenabled: enter maintenance during the upgrade (yes or no)\n\ncomma-separated key=value pairs with keys (optional):\n\n\non-fail: choose how to proceed with the update if maintenance fails:\n\nstop (default): stop the update if a node cannot enter maintenance mode. Nodes that have already been updated will remain so.\nskip: skip and do not update nodes that cannot enter maintenance mode\nforce: forcibly update and reboot (if needed) all nodes even if they cannot enter maintenance mode. Using this option may result in downtime.\n\n\n\n\n\ncompute-mode: choose how to proceed with the update if a VM cannot be live migrated:\n\nstrict: stop the upgrade if a VM cannot be live migrated\nignore: ignore a VM that cannot be live migrated\nignore_ext: ignore a VM that cannot be or failed to be live migrated\n\n\n\n\n\n\n--nodes <nodes>\n\nA comma-separated list of node IDs or hostnames\n--skip-control-plane\n\nUpdate the cluster without updating the management panel.\n--accept-eula\n\nAccept EULA\n\nFor example, to start updating the nodes node001, node002, and node003 and put them into maintenance, run:# vinfra software-updates start --nodes node001,node002,node003 \\\r\n--maintenance enabled=yes,on-fail=skip,compute-mode=ignore\r\n\nThose nodes that cannot enter maintenance will be skipped. Virtual machines that cannot be live migrated during maintenance will be ignored.\n\n\nTo pause software updates, use the command vinfra software-updates pause. To resume the update procedure, run vinfra software-updates resume.\nYou can cancel the update and exit the maintenance mode for a node by using the command:vinfra software-updates cancel [--maintenance-mode {exit,exit-keep-resources,hold}]\n\n--maintenance-mode {exit,exit-keep-resources,hold}\n\n\nMaintenance mode:\n\nexit: exit maintenance for the node and return evacuated resources back on the node\nexit-keep-resources (default): exit maintenance for the node but keep evacuated resources on another node\nhold: do not exit maintenance\n\n\n\nFor example, to cancel the update and return the node to operation, run:# vinfra software-updates cancel exit\nThe evacuated resources from this node will be moved back to it.\n",
                "title": "To update cluster components"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOpen the Settings > Updates screen. The date of the last check is displayed in the upper right corner. Click the round arrow to check for new updates. If updates are available for a cluster component, its update status changes to Available. Check if a node needs a reboot or  maintenance in the Update impact column.\n\n\nClick Download in the upper right corner to get the updates. Wait until the updates are downloaded and the update status changes to Ready to install.\n\nOnce the updates are downloaded, the automatic check for updates is disabled until the updates are installed or the operation is canceled. To reset the software updates state and make it possible to check for a newer version at this step, use the vinfra software-updates reset command.\n\n\n\nClick Release notes to read the release notes.\n\n\nSelect components that you want to be updated:\n\nTo update cluster nodes, select the desired cluster nodes.\nTo update management nodes, select all of the management nodes and those cluster nodes that require an update.\nTo update the management panel, select this component and all of the management nodes if they require an update.\n\n\nClick Update to continue.\nWhen upgrading to a new major version, review the upgrade notes, and then click Next.\n\nIf you have selected nodes that require a reboot or maintenance, do the following:\n\nDecide whether these nodes will enter the maintenance mode. Select Maintenance mode, if you want to place the nodes in the maintenance mode.\n\nIf you have selected nodes with the compute service, choose how to migrate virtual machines running on these nodes:\n\nWith the option Ignore VMs that cannot be live migrated, VMs from a node that enters the maintenance mode will be live migrated to other compute nodes. VMs that cannot be live migrated will be ignored. This applies to suspended VMs, VMs with vGPU or PCI devices attached, or if other compute nodes have insufficient vCPU or RAM resources. During the node update, ignored VMs will be stopped, resulting in downtime. They will be started automatically once the update is complete. After the node exits maintenance, the migrated VMs will not be moved back on the node.\nWith the option Ignore VMs that cannot be or failed to be live migrated, VMs from a node that enters the maintenance mode will be live migrated to other compute nodes. VMs that cannot be live migrated will be ignored. This applies to suspended VMs, VMs with vGPU or PCI devices attached, or if other compute nodes have insufficient vCPU or RAM resources. During the node update, ignored VMs and VMs that failed to be live migrated will be stopped, resulting in downtime. They will be started automatically once the update is complete. After the node exits maintenance, the migrated VMs will not be moved back on the node.\nWith the option Live migrate all VMs, all of the VMs from a node that enters the maintenance mode will be live migrated to other compute nodes. After the node exits maintenance, the migrated VMs will not be moved back on the node.\n\n\n\n\n\nReview the selected components, and then click Install. The system will start update eligibility checks. In case of a major release, the system will also check for removed and unmaintained hardware:\n\nRemoved hardware means that its support has been discontinued. If removed hardware is detected, review its details. If your hardware is detected correctly, the upgrade will fail because such devices are no longer available in the new version. Otherwise, you can force the upgrade by clicking Force update.\nUnmaintained hardware includes devices (drivers and adapters) that are no longer being tested and updated in the new version. If unmaintained hardware is detected, you cannot continue the upgrade because such devices should no longer be used in production.\n\nYou can export the list of detected hardware to a JSON file by clicking Export to file.\n\n\n\nDo not perform any cluster configuration tasks in the admin panel or command-line interface during the update, as this will lead to an update failure and cluster downtime.\n\nWhile the updates are being installed, you can pause or cancel the process. After the update is complete, the component statuses will change to Up to date.\nIf the update fails, click Details to view the issue details and decide how to proceed. You can cancel the update, solve the issues, and retry updating without downtime. Alternatively, you can force the update without putting the nodes into maintenance. However, this will cause a downtime of workloads running on them.\n",
                "title": "To update cluster components"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/installing-updates.html"
    },
    {
        "title": "Installing in the attended mode",
        "content": "Installing in the attended mode\nLimitations\n\nShingled magnetic recording (SMR) HDDs cannot be used for system and are not displayed in the installer.\n\nPrerequisites\n\nA bootable USB drive, an IPMI virtual drive, or a PXE server is prepared, as described in Preparing the bootable media.\nThe time is synchronized via NTP on all nodes in the same cluster and the nodes can access the NTP server.\n\nIf you want to change the default MTU, it must be configured on the network hardware.\n\nTo install in the attended mode\n\nConfigure the server to boot from the chosen media.\nBoot the server and wait for the Welcome screen.\nOn the Welcome screen, select Install Virtuozzo Hybrid Infrastructure.\nOn Step 1, carefully read and accept the End-User License Agreement.\n\nOn Step 2, configure the network:\n\nSpecify a unique host name: either a fully qualified domain name (<hostname>.<domainname>) or a short name (<hostname>).\n\nThe only way to change the host name later is by contacting the technical support.\n\nConfigure your network cards. Usually the network is configured automatically (via DHCP). If manual configuration is required, select a network card, click Configure\u00e2\u0080\u00a6, and specify the necessary parameters.\n\nSet MTU on each network card. By default, MTU is set to 1500, while 9000 is recommended.\n\nThe MTU value must be the same across the entire network.\n\nIf you are integrating Virtuozzo Hybrid Infrastructure into an existing network, adjust the MTU value to that of the network. \nIf you are deploying Virtuozzo Hybrid Infrastructure from scratch alongside a new network, set the MTU value to the recommended 9000.\n\nAccording to network recommendations, you may want to create bonded and virtual local area network (VLAN) interfaces for network security and better performance:\n\nCreate bonded connections\n\nBonded connections increase throughput beyond the capabilities of a single network card, as well as improve redundancy.\n\nClick the plus button at the bottom of the screen, select Bond, and then click Add.\n\nIn the Editing Bond connection<N> window, set the following parameters for an Ethernet bonding interface:\n\nMode to one required by your network\nLink Monitoring to MII (recommended)\nMonitoring frequency, Link up delay, and Link down delay to 300\n\nIt is also recommended to manually set xmit_hash_policy to layer3+4 after the installation.\n\nIn the Bonded connections section on the Bond tab, click Add.\n\nIn the Editing bond<N> slave<N> window, select a network interface to bond in Device.\n\nConfigure MTU, if required, and then click Save.\nRepeat steps 3 to 5 for each network interface you need to add to the bonded connection.\nConfigure IPv4 settings, if required, and then click Save.\n\nCreate VLAN interfaces\n\nClick the plus button at the bottom of the screen, select VLAN, and then click Add.\n\nIn the Editing VLAN connection<N> window:\n\nFrom the Parent interface drop-down list, select a physical adapter or bonded connection that the VLAN adapter will be based on.\nSpecify a VLAN adapter identifier in the VLAN ID field. The value must be between 1 and 4094.\n\nConfigure MTU and  IPv4 settings, if required, and then click Save.\n\nOn Step 3, ensure that the Network Time toggle switch is turned on, and then select your time zone. The date and time will be set via NTP. You will need an Internet connection for synchronization to complete.\n\nWe only support the UTC time zone. Changing the time zone for nodes is not supported and may lead to system unavailability.\n\nOn Step 4, specify what type of node you are installing:\n\nIf you are installing on the primary node\n\nThe primary node, also called the management node, is the first node in the infrastructure. This node will host cluster management services and the admin panel. It will also serve as a storage node. Only one primary node is required.\n\nSelect Yes, create a new cluster. \nIn Internal management network, select a network interface for internal management and configuration purposes.\r\n                    \nIn Admin panel network, select a network interface that will provide access to the admin panel.\nCreate and confirm a password for the superadmin account of the admin panel.\n\nIf you are installing on a secondary node\n\nA secondary node is a node added to an already existing infrastructure. Such nodes will run services related to data storage and will be added to the infrastructure during installation.\n\nSelect No, add it to an existing cluster.\n\nObtain the token and management node address in the admin panel:\n\nLog in to the admin panel on port 8888. The panel\u00e2\u0080\u0099s IP address is shown in the console after deploying the primary node. Use the default user name shown on the login screen and the primary node\u00e2\u0080\u0099s root password.\nIf prompted, add the security certificate to the browser\u00e2\u0080\u0099s exceptions.\n\nIn the admin panel, open Infrastructure > Nodes, and then click Connect node to invoke a screen with the management node address and the token.\n\nA single token can be used to deploy multiple secondary nodes in parallel. You can generate a new token, if needed. Generating a new token invalidates the old one.\n\nOn the installation screen, select the private IP address of the management node and enter the token.\n\nIf you want to register the node in the admin panel manually after the installation, select Skip cluster configuration.\n\nOn Step 5, select a disk for the operating system:\n\nAll information on all disks recognized by the installer will be destroyed.\n\nTo partition your system disk automatically\n\nIn the Device Selection section, select a disk.\nIn the Storage Configuration section, select Automatic.\n\nThis disk will have the supplementary role System, although you will still be able to set it up for data storage in the admin panel.\n\nTo partition your system disk manually\n\nIn the Device Selection section, select a disk.\nIn the Storage Configuration section, select Custom.\nOn the Manual Partitioning screen, create and configure the required mount points for your system disk.\nOn the summary screen, review your changes and accept them.\n\nThis disk will have the supplementary role System, although you will still be able to set it up for data storage in the admin panel.\n\nTo create a software RAID mirror for  your system disk\n\nIn the Device Selection section, select at least two disks.\nIn the Storage Configuration section, select Automatic, and then select Combine disks in a software RAID mirror.\n\nIt is recommended to create a RAID mirror from disks of the same size, as the volume equals the size of the smallest disk.\n\nOn Step 6,  enter and confirm the password for the root account, and then click Start installation.\n\nOnce the installation is complete, the node will reboot automatically. The admin panel IP address will be shown in the welcome prompt.\n\r\n            What's next\n\nAfter deploying the primary node, proceed to deploy the required amount of secondary nodes.\nWhen all the required nodes are displayed in the admin panel on the Infrastructure > Nodes screen as Unassigned, proceed to create the storage cluster, as outlined in Deployment and configuration.\n\nIf you skipped cluster configuration on step 4 and want to add the unassigned node manually:\n\nConfigure the primary node\n\nOn the node, run the following command as the root user to configure the admin panel component:echo <password> | /usr/libexec/vstorage-ui-backend/bin/configure-backend.sh -i <private_iface> -x <public_iface>\r\n\nwhere\n\n<password> is the password of the superadmin account for the admin panel\n<private_iface> is the name of the private network interface (the one you would select for the management network during attended installation)\n<public_iface> is the name of the public network interface (the one you would select for the admin panel network during attended installation)\n\nStart the admin panel service on the node:# systemctl start vstorage-ui-backend\r\n\n\nRegister the node in the admin panel:# /usr/libexec/vstorage-ui-agent/bin/register-storage-node.sh -m <MN_IP_address> -x <public_iface>\r\n\nwhere \n\n<MN_IP_address> is the IP address of the node's private network interface\n<public_iface> is the name of the public network interface\n\nNow you can proceed to deploying secondary nodes.\n\nConfigure a secondary node\n\nRun the following script on the node,  to register it in the admin panel:# /usr/libexec/vstorage-ui-agent/bin/register-storage-node.sh -m <MN_IP_address> -t <token>\r\n\nwhere\n\n<MN_IP_address> is the IP address of the private network interface on the node with the admin panel\n<token> is the token obtained in the admin panel or from the vinfra node token show output",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/installing-in-the-attended-mode.html"
    },
    {
        "title": "Kickstart scripts",
        "content": "Kickstart scripts\nAfter setting the options, add scripts to the kickstart file that will install the mandatory package group and cluster components:\nInstall the required packages\nIn the body of the %packages script, specify the package group hci to be installed on the server:%packages@^hci%end\r\n\nInstall the required components on the primary node\nTo deploy the primary node, you need the admin panel and storage components. You can configure these components during the product installation. However, this will expose the superadmin password and storage token in the kickstart file. In this case, add the %addon com_vstorage script to the kickstart file as follows:%addon com_vstorage --management --internal-iface=<private_iface> --external-iface=<public_iface> --password=<password>%end\nwhere\n\n<password> is the password of the superadmin account for the admin panel.\n<private_iface> is the name of the private network interface (the one you would select for the management network during attended installation).\n<public_iface> is the name of the public network interface (the one you would select for the admin panel network during attended installation).\n\nIf you do not want to expose the sensitive information in the kickstart file, you can configure the components after the installation. In this case, add the %addon com_vstorage script to the kickstart file as follows:%addon com_vstorage --management --bare%end\r\n\nInstall the required components on a secondary node\nTo deploy secondary nodes, you need only the storage component, which is installed by default. However, you will need the storage token and management node IP address to add the node to the infrastructure. \n\nObtain the token and management node address in the admin panel:\n\nLog in to the admin panel on port 8888. The panel\u00e2\u0080\u0099s IP address is shown in the console after deploying the primary node. Use the default user name shown on the login screen and the primary node\u00e2\u0080\u0099s root password.\nIf prompted, add the security certificate to the browser\u00e2\u0080\u0099s exceptions.\n\nIn the admin panel, open Infrastructure > Nodes, and then click Connect node to invoke a screen with the management node address and the token.\n\nWith the token and management node address, you can proceed to registering the node in the admin panel. You can do this during the product installation. However, this will expose the storage token in the kickstart file. In this case, add the %addon com_vstorage script to the kickstart file as follows:%addon com_vstorage --storage --token=<token> --mgmt-node-addr=<MN_IP_address>%end\r\n\nwhere:\n\n<token> is the obtained token. \n<MN_IP_address> is the obtained management node address.\n\nIf you do not want to expose the sensitive information in the kickstart file, you can register the node after the installation. ",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/kickstart-scripts.html"
    },
    {
        "title": "Management",
        "content": "Management\nAfter the infrastructure setup, you can manage nodes, networks, users, the self service, and security, as well as install the license and configure email notifications. In addition, after provisioning backup, block, object, file storage, or compute resources, you can manage them in the admin panel.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/management.html"
    },
    {
        "title": "Kickstart file example",
        "content": "Kickstart file example\nBelow is an example of a kickstart file that you can use to install and configure Virtuozzo Hybrid Infrastructure in the unattended mode. You can use this file as the basis for creating your own kickstart files.\n\nThis kickstart file instructs the installer to erase and automatically partition every disk that it recognizes. Make sure to disconnect any disks with useful data prior to installation.\n# Use the SHA-512 encryption for user passwords and enable shadow passwords.auth --enableshadow --passalgo=sha512# Use the US English keyboard.keyboard --vckeymap=us --xlayouts='us'# Use English as the installer language and the default system language.lang en_US.UTF-8# Specify the encrypted root password for the node.rootpw --iscrypted xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx# Disable SELinux.selinux --disabled# Enable time synchronization via NTP.services --enabled=\"chronyd\"# Set the system time zone.timezone America/New_York# Specify a hostname for the node.# NOTE: The only way to change the host name later is via the technical support.network --hostname=<hostname># Configure network interfaces via DHCP.network --device=<iface1> --activatenetwork --device=<iface2> --activate# Alternatively, assign static addresses to network interfaces.#network --device=<iface1> --activate --bootproto=static --ip=<IP_addr> \\#--netmask=<mask> --gateway=<gw> --nameserver=<ns1>[,<ns2>,...]#network --device=<iface2> --activate --bootproto=static --ip=<IP_addr> \\#--netmask=<mask> --gateway=<gw> --nameserver=<ns1>[,<ns2>,...]# If needed, uncomment and specify network interfaces to create a bond.#network --device=bond0 --bondslaves=<iface1>,<iface2> \\#--bondopts=mode=balance-xor,miimon=100,xmit_hash_policy=layer3+4# Erase all partitions from all recognized disks.# WARNING: Destroys data on all disks that the installer can reach!clearpart --all --initlabelzerombr# Automatically partition the system disk, which is 'sda'.autopart --type=lvm# Install the required packages on the node.%packages@^hci%end# Uncomment to install the admin panel and storage components.# Specify an internal interface for the management network and# an external interface for the admin panel network.#%addon com_vstorage --management --internal-iface=eth0 \\#--external-iface=eth1 --password=xxxxxxxxx#%end# Uncomment to install the storage component. To register the node,# specify the token as well as the IP address of the admin panel.#%addon com_vstorage --storage --token=xxxxxxxxx --mgmt-node-addr=10.37.130.1#%end\r\n\nExample for the system partition on software RAID1\nIt is recommended to create RAID1 from disks of the same size, as the volume equals the size of the smallest disk.\nTo create a system partition on a software RAID1 volume, do not use the autopart option in the kickstart file. Consider the following example for a BIOS-based server, which partitions the disks sda and sdb, assembles the software RAID1 array, and creates expandable swap and root LVM volumes:# Create partitions on sda.part biosboot   --size=1      --ondisk=sda --fstype=biosbootpart raid.sda1  --size=1024   --ondisk=sda --fstype=ext4part raid.sda2  --size=101376 --ondisk=sda --grow# Create partitions on sdb.part biosboot   --size=1      --ondisk=sdb --fstype=biosbootpart raid.sdb1  --size=1024   --ondisk=sdb --fstype=ext4part raid.sdb2  --size=101376 --ondisk=sdb --grow# Create software RAID1 from sda and sdb.raid /boot --level=RAID1 --device=md0 --fstype=ext4 raid.sda1 raid.sdb1raid pv.01 --level=RAID1 --device=md1 --fstype=ext4 raid.sda2 raid.sdb2# Make LVM volumes for swap and root partitions.volgroup vgsys pv.01logvol swap --fstype=swap --name=swap --vgname=vgsys --recommendedlogvol /    --fstype=ext4 --name=root --vgname=vgsys --size=10240 --grow# Set the RAID device md0 as the first drive in the BIOS boot order.bootloader --location=mbr --boot-drive=sda --driveorder=md0bootloader --location=mbr --boot-drive=sdb --driveorder=md0\nFor installation on EFI-based servers, specify the /boot/efi partition instead of biosboot.part /boot/efi --size=200 --ondisk={sda|sdb} --fstype=efi\r\n",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/kickstart-file-example.html"
    },
    {
        "title": "Listing outbound firewall rules",
        "content": "Listing outbound firewall rules\nTo check that all the required outbound allow rules apply to your network\nUse the following command:vinfra cluster network show <network>\n\n<network>\n\nNetwork ID or name\n\nFor example, to view the outbound allow rules of the Public network, run:# vinfra cluster network show Public\r\n+---------------------+---------------------------------------------+\r\n| Field               | Value                                       |\r\n+---------------------+---------------------------------------------+\r\n| encryption          | status: disabled                            |\r\n|                     | subnets:                                    |\r\n|                     | - cidr: 10.100.10.0/20                      |\r\n|                     |   status: disabled                          |\r\n| id                  | c2e799f5-c41d-4865-bcce-06b471affed6        |\r\n| inbound_allow_list  | []                                          |\r\n| inbound_deny_list   | []                                          |\r\n| name                | Public                                      |\r\n| outbound_allow_list | - 0.0.0.0:udp:500:IKE                       |\r\n|                     | - 0.0.0.0:udp:4500:IKE                      |\r\n|                     | - 0.0.0.0:tcp:8888:Admin panel              |\r\n|                     | - 0.0.0.0:tcp:80:HTTP                       |\r\n|                     | - 0.0.0.0:tcp:443:HTTPS                     |\r\n|                     | - 0.0.0.0:udp:53:DNS                        |\r\n|                     | - 0.0.0.0:tcp:53:DNS                        |\r\n|                     | - 0.0.0.0:udp:123:NTP                       |\r\n|                     | - 0.0.0.0:tcp:8443:ABGW registration        |\r\n|                     | - 0.0.0.0:tcp:44445:ABGW Geo-replication    |\r\n|                     | - 0.0.0.0:tcp:9877:Acronis Cyber Protect    |\r\n|                     | - 0.0.0.0:tcp:5900-6079:VM VNC Legacy       |\r\n|                     | - 0.0.0.0:tcp:15900-16900:VM VNC            |\r\n|                     | - 0.0.0.0:udp:4789:VXLAN                    |\r\n|                     | - 0.0.0.0:udp:2049:NFS                      |\r\n|                     | - 0.0.0.0:tcp:2049:NFS                      |\r\n|                     | - 0.0.0.0:tcp:111:NFS Rpcbind               |\r\n|                     | - 0.0.0.0:any:0:Allow all                   |\r\n| traffic_types       | Backup (ABGW) public,Compute API,iSCSI,NFS, |\r\n|                     | S3 public,Self-service ...<truncated>       |\r\n| vlan                | 0                                           |\r\n+---------------------+---------------------------------------------+\nSee also\n\nRestoring default outbound firewall rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/listing-outbound-firewall-rules.html"
    },
    {
        "title": "Kickstart options",
        "content": "Kickstart options\nEven though your kickstart file may include any of the standard options, it is recommended to only use the ones listed in this section. They are mandatory and must be included in your kickstart file.\n\nauth --enableshadow --passalgo=sha512\n\nSpecifies authentication options for the Virtuozzo Hybrid Infrastructure physical server.\nautopart --type=lvm\n\nAutomatically partitions the system disk, which is sda. This option must follow clearpart --all.\nOther disks will be partitioned automatically during cluster creation.\n\nbootloader\n\nSpecifies how the boot loader should be installed.\nclearpart --all\n\nRemoves all partitions from all recognized disks.\n\nThis option will destroy data on all the disks that the installer can reach!\n\nkeyboard <layout>\n\nSets the system keyboard type.\nlang <lang>\n\nSets the language to use during installation and the default language to use on the installed system.\nlogvol\n\nCreates a logical volume for a Logical Volume Management (LVM) group.\nnetwork <options>\n\nConfigures network devices and creates bonds and VLANs.\nraid\n\nCreates a software RAID volume.\npart\n\nCreates a partition on the server.\n\nThe size of the /boot partition must be at least 1 GB.\n\nrootpw --iscrypted <passwd>\n\nSets the root password for the server. The value is your password\u00e2\u0080\u0099s hash obtained with the algorithm specified in the --passalgo parameter. For example, to create a SHA-512 hash of your password, run python -c 'import crypt; print(crypt.crypt(\"yourpassword\"))'.\nselinux --disabled\n\nDisables SElinux, because it prevents virtualization from working correctly.\nservices --enabled=\"chronyd\"\n\nEnables time synchronization via NTP.\ntimezone <timezone>\n\nSets the system time zone. For a list of time zones, run timedatectl list-timezones.\nvolgroup\n\nCreates a Logical Volume Management (LVM) group.\nzerombr\n\nInitializes disks with invalid partition tables.\n\nThis option will destroy data on all the disks that the installer can reach!",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/kickstart-options.html"
    },
    {
        "title": "Logical space chart",
        "content": "Logical space chart\n\nAdmin panel\nThe Logical space chart represents all the space allocated to different services for storing user data. This includes the space occupied exclusively by user data. Replicas and erasure coding metadata are not taken into account.\n\nCommand-line interface\nUse the following command:vinfra cluster overview\nFor example, to view the logical space usage in the cluster cluster1, take a look at these lines from the command output:\r\n+-------------------+-------------------------+\r\n| Field             | Value                   |\r\n+-------------------+-------------------------+\r\n| ...               | ...                     |\r\n| logic_space       | free: 1078130163512     |\r\n|                   | total: 1099511627776    |\r\n|                   | used: 21381464264       |\r\n| ...               | ...                     |\r\n| space_per_service | abgw: 0                 |\r\n|                   | compute: 20477967791    |\r\n|                   | iscsi: 0                |\r\n|                   | nfs: null               |\r\n|                   | other: 903496473        |\r\n|                   | s3: 0                   |\r\n+-------------------+-------------------------+\r\n\n\nHow logical space is calculated\nWhen monitoring disk space information in the cluster, keep in mind that logical space is the amount of free disk space that can be used for storing user data in the form of data chunks and all their replicas. Once this space runs out, no data can be written to the cluster.\nTo better understand how logical disk space is calculated, consider the following example:\n\nThe cluster has three disks with the storage role. The first disk has 200 GB of space, the second one has 500 GB, and the third one has 1 TB.\nIf the redundancy mode is set to three replicas, each data chunk must be stored as three replicas on three different disks with the storage role.\n\nIn this example, the available logical disk space will be 200 GB, that is, equal to the capacity of the smallest disk with the storage role. The reason is that each replica must be stored on a different disk. So once the space on the smallest disk (that is, 200 GB) runs out, no new chunk replicas can be created unless a new disk with the storage role is added or the redundancy mode is changed to two replicas.\nWith the two replicas redundancy mode, the available logical disk space would be 700 GB, because the two smallest disks combined can hold 700 GB of data.\nHow compute and iSCSI logical space is calculated\nBlock storage space used by iSCSI LUNs and compute volumes is not fully thin provisioned. Though storage space is not allocated when you create a block volume, the volume usage grows on demand and cannot be reduced. In this case, used logical space is the space actually used by data. At most, used logical space can reach the volume size. After user data removal, unused storage space is not reclaimed and is reported as actual used space.\nTo better understand how compute and iSCSI logical space is calculated, consider the following example:\n\nA user creates an iSCSI LUN with 100 TB.\nThe user connects the LUN to VMware as a datastore.\nThe user fills up this datastore with data/VMs. The used logical space grows to 100 TB.\nThe user deletes the data, emptying the datastore. But the block storage space is not reclaimed and the used logical space remains 100 TB.\n\nSee also\n\nI/O activity charts\n\nServices chart\n\nChunks chart\n\nPhysical space chart",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/logical-space-chart.html"
    },
    {
        "title": "Maintenance",
        "content": "Maintenance\nRoutine cluster maintenance implies installing updates, replacing failed disks, backing up and restoring the management database. The cluster maintenance also includes gracefully releasing nodes from the storage cluster without data loss and removing them from the infrastructure. You can perform node maintenance by placing a node into the maintenance mode. When changing cluster nodes, you might need to modify the high availability configuration or even destroy and re-create it. Additionally, it is important to know the recommended way to gracefully shut down all nodes and start up the cluster later. ",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/maintenance.html"
    },
    {
        "title": "Making a bootable USB drive",
        "content": "Making a bootable USB drive\nPrerequisites\n\nThe size of a USB drive is at least 4 GB.\n\n To copy the distribution image to a USB drive on Linux\nUse the dd utility:# dd if=image.iso of=/dev/sdb\n\nBe careful to specify the correct drive to transfer the image to.\n\n To copy the distribution image to a USB drive on Windows\n\nGo to https://rufus.ie/ and download the portable version.\nLaunch Rufus.\nIn the Drive Properties section, select your flash drive from the Device drop-down menu, and then click SELECT. Then, select the distribution image from your local machine. You can also change other options, if needed.\n\nClick START.\n\nIn a pop-up window, select Write in DD Image mode and click OK.\n\nWhat's next\n\nInstalling in the attended mode",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/making-a-bootable-usb-drive.html"
    },
    {
        "title": "Infrastructure alerts",
        "content": "Infrastructure alerts\nThe infrastructure alerts that are generated and displayed in the admin panel can be related to the management node and admin panel, license and updates, cluster connectivity, as well as to a specific node, its disks, and network interfaces.\nManagement node and admin panel alerts\n\n High availability for the admin panel\r\nmust be configured\n\nConfigure high availability for the admin panel in Settings > System settings > High availability configuration. Otherwise the admin panel will be a single point of failure.\n\n Management node backup does not exist\n\nManagement node backup is older than <number_of_days> days.\n\n Management node backup does not exist\n\nThe last management node backup has failed, does not exist, or is too old.\n\n Changes to the management database are not replicated\n\nChanges to the management database are not replicated to the node \"<hostname>\" because it is offline. Check the node's state and connectivity.\n\n Changes to the management database are not replicated\n\nChanges to the management database are not replicated to the node \"<hostname>\". Please contact the technical support.\n\n Management node HA has four nodes\n\nThe management node HA configuration has four nodes. It is recommended to have three or five nodes included.\n\nLicense alerts\n\n License is not loaded\n\nLicense is not loaded.\n\nContact the sales representative to obtain a license key.\nRegister the license key, as described in Managing licenses.\n\n License is not updated\n\nThe license cannot be updated automatically and will expire in less than 21 days. Check the cluster connectivity to the license server or contact the technical support.\n\nCalculate the licensed storage capacity that you need. To do it, you can use the previous licensed storage size or check the consumed storage capacity on the Monitoring > Dashboard > Logical space chart.\nContact the sales representative to prolong your license, with the storage capacity you have defined on step 1.\nUpgrade your license key, as described in Managing licenses.\n\n License will expire soon\n\nThe license has not been updated automatically and will expire in less than 7 days. Check the cluster connectivity to the license server and contact the technical support immediately.\n\nCalculate the licensed storage capacity that you need. To do it, you can use the previous licensed storage size or check the consumed storage capacity on the Monitoring > Dashboard > Logical space chart.\nContact the sales representative to prolong your license, with the storage capacity you have defined on step 1.\nUpgrade your license key, as described in Managing licenses.\n\n License expired\n\nThe license of cluster \"<cluster_name>\" has expired. \u00d0\u00a1ontact your reseller to update your license immediately!\n\nCalculate the licensed storage capacity that you need. To do it, you can use the previous licensed storage size or check the consumed storage capacity on the Monitoring > Dashboard > Logical space chart.\nContact the sales representative to prolong your license, with the storage capacity you have defined on step 1.\nUpgrade your license key, as described in Managing licenses.\n\n Licensed storage capacity is low\n\nCluster has reached 80% of licensed storage capacity.\n\nCheck the licensed and consumed storage capacity on the Monitoring > Dashboard > Logical space chart.\nContact the sales representative to add more storage capacity.\nRegister a new license key, as described in Managing licenses.\n\n Licensed storage capacity is critically low\n\nCluster has reached 90% of licensed storage capacity.\n\nCheck the licensed and consumed storage capacity on the Monitoring > Dashboard > Logical space chart.\nContact the sales representative to add more storage capacity.\nRegister a new license key, as described in Managing licenses.\n\n Cluster is out of space\n\n\u00d0\u00a1luster \"<cluster_name>\" has run out of storage space allowed by license. No more data can be written. Please contact your reseller to update your license immediately!\n\nCheck the licensed and consumed storage capacity on the Monitoring > Dashboard > Logical space chart.\nContact the sales representative to add more storage capacity.\nRegister a new license key, as described in Managing licenses.\n\nUpdate alerts\n\n Software updates exist\n\nSoftware updates exist for the node <hostname>. Current version: <current_version>. Available version: <available_version>.\n\nUpdate Virtuozzo Hybrid Infrastructure to a new version, as described in Installing updates.\n\n Update check failed\n\nUpdate check failed on the node <hostname>.\n\nThe connection to the update repository could not be established.\nCheck access to the update repositories:\n\nOpen the terminal on the node where the update check has failed.\n\nEnsure that the hci-base and hci-updates repositories are enabled and the mirrorlist URL matches the current release. To do this, run:# cat /etc/hci-release\r\n# grep -P \"^(mirrorlist|enabled)\" /etc/yum.repos.d/hci.repo\n\nDisable third-party repositories. Only two repositories must be enabled: hci-base and hci-updates. To check the enabled repositories, run:# yum -q repolist enabled\n\nCheck access to the repositories:# yum clean all; yum repoinfo hci-base; yum repoinfo hci-updates\n\nGet the mirrorlist content by running:# curl -L <mirrorlist_URL>\n\nInvestigate the log file /var/log/vstorage-ui-agent/updater.log.\n\n Multiple update checks failed\n\nUpdate checks failed multiple times on the node <hostname>.\n\nThe connection to the update repository could not be established for at least 3 days.\nCheck access to the update repositories:\n\nOpen the terminal on the node where the update check has failed.\n\nEnsure that the hci-base and hci-updates repositories are enabled and the mirrorlist URL matches the current release. To do this, run:# cat /etc/hci-release\r\n# grep -P \"^(mirrorlist|enabled)\" /etc/yum.repos.d/hci.repo\n\nDisable third-party repositories. Only two repositories must be enabled: hci-base and hci-updates. To check the enabled repositories, run:# yum -q repolist enabled\n\nCheck access to the repositories:# yum clean all; yum repoinfo hci-base; yum repoinfo hci-updates\n\nGet the mirrorlist content by running:# curl -L <mirrorlist_URL>\n\nInvestigate the log file /var/log/vstorage-ui-agent/updater.log.\n\n Update download failed\n\nUpdate download failed on the node <hostname>.\n\nThe reason can be one of the following:\n\nThe update check has failed.\nThere is not enough free space on the root file system.\n(Rare) A new version became available during the download, and the version that is currently being downloaded is not available anymore.\n\nTo solve the issue:\n\nCheck access to the update repositories:\n\nOpen the terminal on the node where the update check has failed.\n\nEnsure that the hci-base and hci-updates repositories are enabled and the mirrorlist URL matches the current release. To do this, run:# cat /etc/hci-release\r\n# grep -P \"^(mirrorlist|enabled)\" /etc/yum.repos.d/hci.repo\n\nDisable third-party repositories. Only two repositories must be enabled: hci-base and hci-updates. To check the enabled repositories, run:# yum -q repolist enabled\n\nCheck access to the repositories:# yum clean all; yum repoinfo hci-base; yum repoinfo hci-updates\n\nGet the mirrorlist content by running:# curl -L <mirrorlist_URL>\n\nInvestigate the log file /var/log/vstorage-ui-agent/updater.log.\n\nEnsure that the root file system has more than 1 GB of free space left.\n   Retry the update download.\n\n Node update failed\n\nSoftware update failed on the node <hostname>.\n\nThe reason can be one of the following:\n\nThe node rebooted unexpectedly.\nThere are third-party packages, which conflict with the official packages.\n\nTry updating the node again. If the issue persists, contact the technical support team.\n\n Update failed\n\nUpdate failed for the management panel and compute API.\n\nThe reason can be one of the following:\n\nThe management node rebooted unexpectedly.\nFailed to update the compute service.\n\nTry updating the component again. If the issue persists, contact the technical support team.\n\n Cluster update failed\n\nUpdate failed for the cluster.\n\nTry updating the cluster again. If the issue persists, contact the technical support team.\n\n Entering maintenance for update failed\n\nEntering maintenance failed while updating the node <hostname>.\n\nContact the technical support team.\n\nCluster connectivity alerts\n\n Cluster network connectivity problem\n\nAll nodes have network connectivity problems: unstable connectivity via network \"<network_name>\" due to packet loss.\n\n Cluster network connectivity problem\n\nAll nodes have network connectivity problems: no connectivity via network \"<network_name>\".\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: unstable connectivity via network \"<network_name>\" due to the loss of all MTU-sized packets.\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: unstable connectivity via network \"<network_name>\" due to the loss of some MTU-sized packets.\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: unstable connectivity via network \"<network_name>\" due to packet loss.\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: no connectivity to node \"<hostname>\" with interface \"<iface>\" via interface \"<iface>\".\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: unstable connectivity to node \"<hostname>\" with interface \"<iface>\" via interface \"<iface>\" due to the loss of all MTU-sized packets.\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: unstable connectivity to node \"<hostname>\" with interface \"<iface>\" via interface \"<iface>\" due to packet loss.\n\n Node network connectivity problem\n\nNode \"<hostname>\" has network connectivity problems: unstable connectivity to node \"<hostname>\" with interface \"<iface>\" via interface \"<iface>\" due to the loss of some MTU-sized packets.\n\n MTU mismatch\n\nSome interfaces have MTU that differs from other interfaces in the same network: network \"<network_name>\" interface@host \"<iface> @ <hostname>\".\n\nNode alerts\n\n Node is offline\n\nNode \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d is offline.\n\n Node got offline too many times\n\nNode \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d got offline too many times last hour.\n\n Kernel is outdated\n\nNode \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d is not running the latest kernel.\n\n OOM killer triggered\n\nOOM killer has been triggered on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d.\n\n Time is not synced\n\nTime on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d differs from time on backend node by more than 5 seconds.\n\n No Internet access\n\nCluster node <hostname> cannot reach the repository. Make sure that all cluster nodes have Internet access.\n\n Incompatible hardware detected\n\nIncompatible hardware detected on node \"<hostname>\": <hardware_list>. Using Mellanox and AMD may lead to data loss. Please double check that SR-IOV is properly enabled. Visit https://support.virtuozzo.com/hc/en-us/articles/19764365143953 to learn how to troubleshoot this issue.\n\n Swap space is running low\n\n<swap_proportion>% of swap is used on node \"<hostname>\".\n\n Node has high CPU usage\n\nNode <hostname> has CPU usage higher than 90%. The current value is <value>%.\n\n Node has high memory usage\n\nNode <hostname> has memory usage higher than 95%. The current value is <value>%.\n\n Node has high disk I/O usage\n\nDisk /dev/<disk_name> on node <hostname> has I/O usage higher than 85%. The current value is <value>%.\n\n Node has high receive packet loss rate\n\nNode <hostname> has <value> receive packet loss rate reported by job <job_name>.\n\n Node has high transmit packet loss rate\n\nNode <hostname> has <value> transmit packet loss rate reported by job <job_name>.\n\n Node has high receive packet error rate\n\nNode <hostname> has <value> receive packet error rate reported by job <job_name>.\n\n Node has high transmit packet error rate\n\nNode <hostname> has <value> transmit packet error rate reported by job <job_name>.\n\n Reached \"node crash per hour\" threshold\n\nNode <hostname> with shaman node ID <id> has reached the \"node crash per hour\" threshold.\n\n OOM-kill event detected\n\nOOM-kill event detected on node <hostname> at least once for the last 24 hours. You need to check memory consumption.\n\n Node failed to return to operation\n\nNode <hostname> has failed to automatically return to operation within 30 minutes after a crash. Check the node's hardware, and then try returning it to operation manually.\n\n Node crash detected\n\nNode <hostname> crashed, which started the VM evacuation.\n\nDisk alerts\n\n S.M.A.R.T. warning\n\nDisk \u00e2\u0080\u009c<disk_name>\u00e2\u0080\u009d(<serial>) on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d has failed a S.M.A.R.T. check.\n\n Disk error\n\nDisk \u00e2\u0080\u009c<disk_name>\u00e2\u0080\u009d (<serial>) failed on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d.\n\n Disk is out of space\n\nRoot partition on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d is running out of space (less than 10% of free space).\n\n Software RAID is not fully synced\n\nSoftware RAID <disk_name> on node <hostname> is <value>% synced.\n\n Systemd service is flapping\n\nSystemd service <service_name> on node <hostname> has changed its state more than 5 times in 5 minutes or 15 times in one hour.\n\n SMART Media Wearout warning\n\nDisk <disk_name> on node <hostname> is almost worn out and may fail soon. Consider replacement.\n\nThe alert is based on the smart_media_wearout_indicator metric. For details on the Media Wearout Indicator S.M.A.R.T. attribute, refer to Disk health analyzers.\n\n SMART Media Wearout critical\n\nDisk <disk_name> on node <hostname> is worn out and will fail soon. Consider replacement.\n\nThe alert is based on the smart_media_wearout_indicator metric. For details on the Media Wearout Indicator S.M.A.R.T. attribute, refer to Disk health analyzers.\n\nNetwork interface alerts\n\n Network warning\n\nNetwork interface \u00e2\u0080\u009c<iface_name>\u00e2\u0080\u009d has incorrect settings: <duplex> duplex and <speed> speed.\n\n Network warning\n\nNetwork interface \u00e2\u0080\u009c<iface_name>\u00e2\u0080\u009d on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d is missing important features (or has them disabled): \u00e2\u0080\u009c<feature_name>\u00e2\u0080\u009d.\n\n Network warning\n\nNetwork interface \u00e2\u0080\u009c<iface_name>\u00e2\u0080\u009d on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d is not in the full duplex mode.\n\n Network warning\n\nNetwork interface \u00e2\u0080\u009c<iface_name>\u00e2\u0080\u009d on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d has speed lower than the minimally required 1 Gbps.\n\n Network warning\n\nNetwork interface \u00e2\u0080\u009c<iface_name>\u00e2\u0080\u009d on node \u00e2\u0080\u009c<hostname>\u00e2\u0080\u009d has an undefined speed.\n\n Network interface is flapping\n\nNetwork interface <iface_name> on node <hostname> is flapping.\n\n Network bond is not redundant\n\nNetwork bond <iface_name> on node <hostname> is missing <number_of_ifaces> subordinate interface(s).\n\nData-in-transit encryption alerts\n\n Enabling IPv6 mode takes too much time\n\nOperation to enable the IPv6 mode is running for more than 1 hour. Please contact the technical support.\n\n\u00d0\u00a1ancel the operation to enable the IPv6 mode by using the vinfra cluster network encryption cancel command.\nRetry the operation.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n Enabling traffic encryption takes too much time\n\nOperation to enable traffic encryption is running for more than 1 hour. Please contact the technical support.\n\nCancel the operation to enable traffic encryption by using the vinfra cluster network encryption cancel command.\nRetry the operation.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n System configuration is not optimal for traffic encryption\n\nTraffic encryption is enabled but the storage network is not in the IPv6 mode. Switch on the IPv6 configuration, as described in the product documentation.\n\nDisable traffic encryption for the storage network, and then enable it again, as described in Enabling and disabling data-in-transit encryption.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n Node IPsec certificate will expire in less than 7 days\n\nIPsec certificate for node <hostname> with ID <id> will expire in less than 7 days. Renew the certificate, as described in the product documentation, or contact the technical support.\n\nFollow the instructions in Renewing encryption certificates to renew the certificate.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\n Node IPsec certificate has expired\n\nIPsec certificate for node <hostname> with ID <id> has expired. Renew the certificate, as described in the product documentation, or contact the technical support.\n\nFollow the instructions in Renewing encryption certificates to renew the certificate.\nIf you cannot troubleshoot the problem, contact the technical support team.\n\nWhat's next\n\nGetting technical support\n\nSee also\n\nCore storage alerts\n\nBackup storage alerts\n\nObject storage alerts\n\nBlock storage alerts\n\nCompute alerts",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/infrastructure-alerts.html"
    },
    {
        "title": "Managing backup storage",
        "content": "Managing backup storage\nThis section describes administrator's tasks for backup storage: changing the redundancy scheme, managing multiple registrations for backup storage in Acronis Cyber Protect Cloud or Acronis Cyber Protect, adding and releasing nodes from the backup storage cluster. In addition, this section covers backup storage geo-replication between two geographically distributed data centers or two different backup destinations.  If you need to delete the backup storage, delete all of its registrations, and then release its nodes.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-backup-storage.html"
    },
    {
        "title": "Managing admin panel users",
        "content": "Managing admin panel users\nDuring the primary node deployment, the unique Default domain is created along with the default user account and project:\n\nThe default administrator account is created with the unique Superuser permission. The user name for this account is admin and the password is specified during the primary node deployment. This account cannot be deleted or disabled and its permissions cannot be changed. Other than that, admin does not differ from a user who is assigned the System administrator role.\nThe default admin project is a bootstrap project for initializing the compute cloud. It cannot be deleted or renamed.\n\nThe Default domain with system users and projects is used by the system for different services. System entities are marked with the System tag and cannot be modified or deleted. \nDue to security concerns, you might want to create other system administrators with different permissions to manage the infrastructure. For example, you can create system administrators that are able to monitor the cluster performance and parameters, but cannot change any settings. \nOther users such as domain administrators and project members have access only to the self-service panel and are required to provision multitenant compute resources.\nLimitations\n\nSystem administrators can be created only within the Default domain.\n\nTo create a system administrator\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the Default domain.\nGo to the Domain users tab, and then click Create user.\n\nIn the Create user window, specify the user name, password, and, if required, a user email address and description. The user name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nSelect the System administrator role.\n\nSelect the permissions to be granted to the user account from the System permission set section:\n\nFull (System administrator): has all permissions and can perform all management operations, including creating projects and managing other users.\nCompute: can create and manage the compute cluster.\nISCSI: can create and manage iSCSI targets, LUNs, and CHAP users.\nS3: can create and manage the S3 cluster.\nABGW: can create and manage the Backup Gateway cluster.\nNFS: can create and manage NFS shares and exports.\nCluster: can create the storage cluster, join nodes to it, and manage (assign and release) disks.\nNetwork: can modify networks and traffic types.\nUpdate: can install updates.\nSSH: can add and remove SSH keys for cluster nodes access.\n\nThe view permission is always enabled.\n\nEnable the full Domain permissions set to allow the user to manage virtual objects in all projects within the Default domain and other users in the self-service panel.\n\nEnable Image uploading to allow the user to upload images.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra domain user create [--email <email>] [--description <description>]\r\n                          [--system-permissions <system_permissions>]\r\n                          [--enable | --disable] --domain <domain> <name>\r\n\n\n--email <email>\n\nUser email\n--description <description>\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--system-permissions <system_permissions>\n\nA comma-separated list of system permissions. View the list of available system permissions using vinfra domain user list-available-roles | grep system.\n--enable\n\nEnable user\n--disable\n\nDisable user\n--domain <domain>\n\nDomain name or ID\n<name>\n\nUser name\n\nFor example, to create a system administrator account called mysysadmin within the domain Default, to manage the compute cluster, run:# vinfra domain user create mysysadmin --domain Default --system-permissions compute\r\n\nSpecify the user password when prompted.\nThe created system administrator will appear in the vinfra domain user list output:# vinfra domain user list --domain Default\r\n+----------------------------------+-----------------------+-------+---------+-------------+--------------------+-------------------+\r\n| id                               | name                  | email | enabled | description | domain_permissions | assigned_projects |\r\n+----------------------------------+-----------------------+-------+---------+-------------+--------------------+-------------------+\r\n| 1d207818a205433fabb85d68ff8bd45a | nova                  |       | True    |             | []                 | []                |\r\n| 1eb4cd6272d84d0a824877a8afe16269 | heat                  |       | True    |             | []                 | []                |\r\n| 4ae74e324e7241139e1357c9ce65f0b1 | backup-service-user   |       | False   |             | []                 | []                |\r\n| 4e7db09ec1794aff92cbac0a70159478 | gnocchi               |       | True    |             | []                 | []                |\r\n| 8d54115532ee421a8551ab32910998ad | octavia               |       | True    |             | []                 | []                |\r\n| 8fd6757e10494c399cd8445dd8c83c87 | barbican              |       | True    |             | []                 | []                |\r\n| 9e462afe59a742049970bdbb902569d1 | neutron               |       | True    |             | []                 | []                |\r\n| a2c7eda0ea5a45749d0af7742ace85b0 | glance                |       | True    |             | []                 | []                |\r\n| a91aa030575c474f9753abda3bf7afa0 | cinder                |       | True    |             | []                 | []                |\r\n| c727a901a6444ee1a8ad31e3d5b53b3a | admin                 |       | True    |             | []                 | []                |\r\n| ca92d0b41f354a6882f24e0eb101b4ea | vstorage-service-user |       | True    |             | []                 | []                |\r\n| e03bf89a89ef4a018dbf5aae107beed8 | mysysadmin            |       | True    |             | []                 | []                |\r\n| ed4b3f0b6e61470ba0b79662671679f6 | ceilometer            |       | True    |             | []                 | []                |\r\n| f62f123df20c4b388fefebf058fb185c | placement             |       | True    |             | []                 | []                |\r\n+----------------------------------+-----------------------+-------+---------+-------------+--------------------+-------------------+\r\n\n\nTo change the password\n\nAdmin panel\n\n In the top right corner of the admin panel, click the user icon, and then click Change password.\nIn the Change password window, enter the current password and enter a new password twice.\nClick Save.\n\nCommand-line interface\n\nFor the default administrator account, use the following command:vinfra cluster user change-password\nWhen prompted, enter the current and a new password, and then repeat the new password for confirmation.\n\nFor other accounts, us the following command:vinfra domain user set [--password] --domain <domain> <user>\r\n\n\n--password\n\nRequest the password from stdin\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to change the password for the system administrator mysysadmin, run:# vinfra domain user set mysysadmin --domain Default --password\nWhen prompted, enter a new password, which will replace the old one.\n\nSee also\n\nManaging domain groups\n\nManaging self-service users\n\nManaging security\n\nUnlocking user accounts",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user create [--email <email>] [--description <description>]\r\n                          [--system-permissions <system_permissions>]\r\n                          [--enable | --disable] --domain <domain> <name>\r\n\n\n--email <email>\n\nUser email\n--description <description>\n\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--system-permissions <system_permissions>\n\nA comma-separated list of system permissions. View the list of available system permissions using vinfra domain user list-available-roles | grep system.\n--enable\n\nEnable user\n--disable\n\nDisable user\n--domain <domain>\n\nDomain name or ID\n<name>\n\nUser name\n\nFor example, to create a system administrator account called mysysadmin within the domain Default, to manage the compute cluster, run:# vinfra domain user create mysysadmin --domain Default --system-permissions compute\r\n\nSpecify the user password when prompted.\nThe created system administrator will appear in the vinfra domain user list output:# vinfra domain user list --domain Default\r\n+----------------------------------+-----------------------+-------+---------+-------------+--------------------+-------------------+\r\n| id                               | name                  | email | enabled | description | domain_permissions | assigned_projects |\r\n+----------------------------------+-----------------------+-------+---------+-------------+--------------------+-------------------+\r\n| 1d207818a205433fabb85d68ff8bd45a | nova                  |       | True    |             | []                 | []                |\r\n| 1eb4cd6272d84d0a824877a8afe16269 | heat                  |       | True    |             | []                 | []                |\r\n| 4ae74e324e7241139e1357c9ce65f0b1 | backup-service-user   |       | False   |             | []                 | []                |\r\n| 4e7db09ec1794aff92cbac0a70159478 | gnocchi               |       | True    |             | []                 | []                |\r\n| 8d54115532ee421a8551ab32910998ad | octavia               |       | True    |             | []                 | []                |\r\n| 8fd6757e10494c399cd8445dd8c83c87 | barbican              |       | True    |             | []                 | []                |\r\n| 9e462afe59a742049970bdbb902569d1 | neutron               |       | True    |             | []                 | []                |\r\n| a2c7eda0ea5a45749d0af7742ace85b0 | glance                |       | True    |             | []                 | []                |\r\n| a91aa030575c474f9753abda3bf7afa0 | cinder                |       | True    |             | []                 | []                |\r\n| c727a901a6444ee1a8ad31e3d5b53b3a | admin                 |       | True    |             | []                 | []                |\r\n| ca92d0b41f354a6882f24e0eb101b4ea | vstorage-service-user |       | True    |             | []                 | []                |\r\n| e03bf89a89ef4a018dbf5aae107beed8 | mysysadmin            |       | True    |             | []                 | []                |\r\n| ed4b3f0b6e61470ba0b79662671679f6 | ceilometer            |       | True    |             | []                 | []                |\r\n| f62f123df20c4b388fefebf058fb185c | placement             |       | True    |             | []                 | []                |\r\n+----------------------------------+-----------------------+-------+---------+-------------+--------------------+-------------------+\r\n\n",
                "title": "To create a system administrator"
            },
            {
                "example": "\nCommand-line interface\n\n\nFor the default administrator account, use the following command:vinfra cluster user change-password\nWhen prompted, enter the current and a new password, and then repeat the new password for confirmation.\n\n\nFor other accounts, us the following command:vinfra domain user set [--password] --domain <domain> <user>\r\n\n\n--password\n\nRequest the password from stdin\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to change the password for the system administrator mysysadmin, run:# vinfra domain user set mysysadmin --domain Default --password\nWhen prompted, enter a new password, which will replace the old one.\n\n\n",
                "title": "To change the password"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the Default domain.\nGo to the Domain users tab, and then click Create user.\n\nIn the Create user window, specify the user name, password, and, if required, a user email address and description. The user name must be unique within a domain.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\nSelect the System administrator role.\n\nSelect the permissions to be granted to the user account from the System permission set section:\n\nFull (System administrator): has all permissions and can perform all management operations, including creating projects and managing other users.\nCompute: can create and manage the compute cluster.\nISCSI: can create and manage iSCSI targets, LUNs, and CHAP users.\nS3: can create and manage the S3 cluster.\nABGW: can create and manage the Backup Gateway cluster.\nNFS: can create and manage NFS shares and exports.\nCluster: can create the storage cluster, join nodes to it, and manage (assign and release) disks.\nNetwork: can modify networks and traffic types.\nUpdate: can install updates.\nSSH: can add and remove SSH keys for cluster nodes access.\n\n\nThe view permission is always enabled.\n\n\n\nEnable the full Domain permissions set to allow the user to manage virtual objects in all projects within the Default domain and other users in the self-service panel.\n\n\nEnable Image uploading to allow the user to upload images.\n\nClick Create.\n\n\n\n\n\n",
                "title": "To create a system administrator"
            },
            {
                "example": "\nAdmin panel\n\n In the top right corner of the admin panel, click the user icon, and then click Change password.\nIn the Change password window, enter the current password and enter a new password twice.\nClick Save.\n\n",
                "title": "To change the password"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-admin-panel-users.html"
    },
    {
        "title": "Managing block storage",
        "content": "Managing block storage\nThis section describes how to manage iSCSI target groups, targets, and volumes. It also explains how to restrict access to target groups by using CHAP or ACL.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-block-storage.html"
    },
    {
        "title": "Managing compute networks",
        "content": "Managing compute networks\nVirtuozzo Hybrid Infrastructure provides secure and isolated virtual networking for virtual machines using VXLAN encapsulation. The distributed virtual switching and routing simplify VM network configuration, and the built-in firewall makes it more secure. The integrated DHCP, IP, and DNS management provides for enhanced configuration of the network.\nIn the compute cluster, you can create and manage two types of networks:\n\nVirtual networks are VXLAN-based overlay networks that can be used for intercommunication between virtual machines (VMs). Each virtual network is isolated from other virtual networks, as well as physical networks. Virtual networks support only IPv4 address management.\nPhysical networks use IP address ranges of public infrastructure networks. Such networks can be used to provide Internet access to VMs. Physical networks support both IPv4 and IPv6 address management.\n\nLimitations\n\nWhen you create load balancers or Kubernetes clusters with highly available master nodes, the lb-mgmt-net virtual network appears in the compute cluster. This network is used by the system for load balancing. It is marked with the System tag and cannot be modified or deleted.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-compute-networks.html"
    },
    {
        "title": "Managing balancing pools",
        "content": "Managing balancing pools\nOn the Compute > Network > Load balancers tab, click a load balancer name, to see a list of its balancing pools.\n\nTo manage balancing pools\n\nAdmin panel\n\nTo monitor pool performance and health, open the pool\u00e2\u0080\u0099s panel on the Overview tab.\nTo see the pool parameters, open the pool\u00e2\u0080\u0099s panel and go to the  Properties tab.\nTo manage the pool members, open the pool\u00e2\u0080\u0099s panel and go the Members tab.\nTo remove a balancing pool, click the ellipsis icon next to it, and then click Delete. \n\nCommand-line interface\n\nTo view the pool's details, use vinfra service compute load-balancer pool show. For example:# vinfra service compute load-balancer pool show mypool\r\n+-----------------------+--------------------------------------------+\r\n| Field                 | Value                                      |\r\n+-----------------------+--------------------------------------------+\r\n| backend_protocol      | HTTP                                       |\r\n| backend_protocol_port | 80                                         |\r\n| certificate           |                                            |\r\n| connection_limit      | -1                                         |\r\n| created_at            | 2019-11-18T13:11:27.982129                 |\r\n| description           |                                            |\r\n| enabled               | True                                       |\r\n| healthmonitor         |                                            |\r\n| id                    | fa40e282-b29a-465a-afaa-2c702d2bde17       |\r\n| lb_algorithm          | LEAST_CONNECTIONS                          |\r\n| listener_id           | 66cc714e-af7f-40eb-9db8-67b8b6b6d23c       |\r\n| loadbalancer_id       | 941bf637-2d55-40f0-92c0-e65d6567b468       |\r\n| members               | - address: 192.168.31.153                  |\r\n|                       |   compute_server_id: d51c10a7-6187-<...>   |\r\n|                       |   created_at: '2019-11-18T13:11:59.681101' |\r\n|                       |   enabled: true                            |\r\n|                       |   id: 3fd5dcc5-6e2c-4e22-8d0a-8e94e20a122f |\r\n|                       |   name: ''                                 |\r\n|                       |   pool_id: null                            |\r\n|                       |   status: HEALTHY                          |\r\n|                       |   updated_at: '2019-11-18T13:12:01.467306' |\r\n|                       |   weight: 1                                |\r\n|                       | - address: 192.168.31.22                   |\r\n|                       |   compute_server_id: 54603109-8963-<...>   |\r\n|                       |   created_at: '2019-11-18T13:12:10.176853' |\r\n|                       |   enabled: true                            |\r\n|                       |   id: ccb645b3-63c7-44f8-b861-b197c85506d4 |\r\n|                       |   name: ''                                 |\r\n|                       |   pool_id: null                            |\r\n|                       |   status: HEALTHY                          |\r\n|                       |   updated_at: '2019-11-18T13:12:12.281578' |\r\n|                       |   weight: 1                                |\r\n| name                  | mypool                                     |\r\n| private_key           |                                            |\r\n| project_id            | e4e059c67dee4736851df14d4519a5a5           |\r\n| protocol              | HTTP                                       |\r\n| protocol_port         | 80                                         |\r\n| status                | ACTIVE                                     |\r\n| sticky_session        | True                                       |\r\n| updated_at            | 2019-11-18T13:12:12.305509                 |\r\n+-----------------------+--------------------------------------------+\r\n\n\nTo edit the pool's members, use vinfra service compute load-balancer pool set. For example:# vinfra service compute load-balancer pool set mypool \\\r\n--member address=192.168.31.153 --member address=192.168.31.22 \\\r\n--member address=192.168.31.51\nThis command adds the third member to the balancing pool mypool.\n\nTo delete a balancing pool, use vinfra service compute load-balancer pool delete. For example:# vinfra service compute load-balancer pool delete mypool\n\nSee also\n\nChanging the default load balancer flavor",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nTo view the pool's details, use vinfra service compute load-balancer pool show. For example:# vinfra service compute load-balancer pool show mypool\r\n+-----------------------+--------------------------------------------+\r\n| Field                 | Value                                      |\r\n+-----------------------+--------------------------------------------+\r\n| backend_protocol      | HTTP                                       |\r\n| backend_protocol_port | 80                                         |\r\n| certificate           |                                            |\r\n| connection_limit      | -1                                         |\r\n| created_at            | 2019-11-18T13:11:27.982129                 |\r\n| description           |                                            |\r\n| enabled               | True                                       |\r\n| healthmonitor         |                                            |\r\n| id                    | fa40e282-b29a-465a-afaa-2c702d2bde17       |\r\n| lb_algorithm          | LEAST_CONNECTIONS                          |\r\n| listener_id           | 66cc714e-af7f-40eb-9db8-67b8b6b6d23c       |\r\n| loadbalancer_id       | 941bf637-2d55-40f0-92c0-e65d6567b468       |\r\n| members               | - address: 192.168.31.153                  |\r\n|                       |   compute_server_id: d51c10a7-6187-<...>   |\r\n|                       |   created_at: '2019-11-18T13:11:59.681101' |\r\n|                       |   enabled: true                            |\r\n|                       |   id: 3fd5dcc5-6e2c-4e22-8d0a-8e94e20a122f |\r\n|                       |   name: ''                                 |\r\n|                       |   pool_id: null                            |\r\n|                       |   status: HEALTHY                          |\r\n|                       |   updated_at: '2019-11-18T13:12:01.467306' |\r\n|                       |   weight: 1                                |\r\n|                       | - address: 192.168.31.22                   |\r\n|                       |   compute_server_id: 54603109-8963-<...>   |\r\n|                       |   created_at: '2019-11-18T13:12:10.176853' |\r\n|                       |   enabled: true                            |\r\n|                       |   id: ccb645b3-63c7-44f8-b861-b197c85506d4 |\r\n|                       |   name: ''                                 |\r\n|                       |   pool_id: null                            |\r\n|                       |   status: HEALTHY                          |\r\n|                       |   updated_at: '2019-11-18T13:12:12.281578' |\r\n|                       |   weight: 1                                |\r\n| name                  | mypool                                     |\r\n| private_key           |                                            |\r\n| project_id            | e4e059c67dee4736851df14d4519a5a5           |\r\n| protocol              | HTTP                                       |\r\n| protocol_port         | 80                                         |\r\n| status                | ACTIVE                                     |\r\n| sticky_session        | True                                       |\r\n| updated_at            | 2019-11-18T13:12:12.305509                 |\r\n+-----------------------+--------------------------------------------+\r\n\n\n\nTo edit the pool's members, use vinfra service compute load-balancer pool set. For example:# vinfra service compute load-balancer pool set mypool \\\r\n--member address=192.168.31.153 --member address=192.168.31.22 \\\r\n--member address=192.168.31.51\nThis command adds the third member to the balancing pool mypool.\n\n\nTo delete a balancing pool, use vinfra service compute load-balancer pool delete. For example:# vinfra service compute load-balancer pool delete mypool\n\n\n",
                "title": "To manage balancing pools"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nTo monitor pool performance and health, open the pool\u00e2\u0080\u0099s panel on the Overview tab.\nTo see the pool parameters, open the pool\u00e2\u0080\u0099s panel and go to the  Properties tab.\nTo manage the pool members, open the pool\u00e2\u0080\u0099s panel and go the Members tab.\nTo remove a balancing pool, click the ellipsis icon next to it, and then click Delete. \n\n",
                "title": "To manage balancing pools"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-balancing-pools.html"
    },
    {
        "title": "Managing access control lists",
        "content": "Managing access control lists\nAn access control list (ACL) limits access to chosen LUNs for specific initiators. Initiators not on the list have access to all LUNs in iSCSI target groups.\nPrerequisites\n\nA target group is created, as described in Creating target groups.\n\nTo add an initiator to a target group\u00e2\u0080\u0099s ACL\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, and then click the desired target group in the list (anywhere except the group\u00e2\u0080\u0099s name).\n\nOn the group right pane, open the Access control tab, and then click the pencil icon.\n\nIn the Access control window, select ACL, and then click Add.\n\nIn the Add ACL window, specify the initiator\u00e2\u0080\u0099s IQN, enter an alias, select the LUNs that it will be able to access, and then click Add. The initiator will appear in the ACL.\n\nHaving populated the ACL with initiators, click Save.\n\nCommand-line interface\n\nAdd an initiator  to the ACL of the target group:vinfra service block-storage target-group acl add [--alias <alias>] [--lun <lun>] <target-group> <wwn>\n\n--alias <alias>\n\nInitiator name\n--lun <lun>\n\nLUN ID\n<target-group>\n\nTarget group name or ID\n<wwn>\n\nWorld wide name (WWN) of the target, that is, IQN\n\nFor example, to add the initiator initiator1 to the ACL of the target group tg1, run:# vinfra service block-storage target-group acl add --lun 0 --alias initiator1 tg1 iqn.2014-06.com.vstorage:target1\n\nEnable ACL for the target group:vinfra service block-storage target-group set --enable-acl <target-group>\n\n--enable-acl\n\nEnable ACL\n<target-group>\n\nTarget group name or ID\n\nFor example, to enable ACL for the target group tg1, run:# vinfra service block-storage target-group set --enable-acl tg1\n\nTo edit an initiator in the ACL\n\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon.\nIn the Access control window, click the pencil icon of the desired initiator, and then click Edit.\nHaving changed the ACL, click Save.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group acl set (--lun <lun> | --no-luns) <target-group> <wwn>\n\n--lun <lun>\n\nLUN ID\n--no-luns\n\nNo LUNs\n<target-group>\n\nTarget group name or ID\n<wwn>\n\nWorld wide name (WWN) of the target, that is, IQN\n\nFor example, to change the LUN ID to 1 for the ACL of the target group tg1, run:# vinfra service block-storage target-group acl set --lun 1 tg1 iqn.2014-06.com.vstorage:target1\n\nTo disable the ACL for a target group\n\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon in the ACL section.\n\nIn the Access control window, clear ACL, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group set --disable-acl <target-group>\n\n--disable-acl\n\nDisable ACL\n<target-group>\n\nTarget group name or ID\n\nFor example, to disable the ACL for the target group tg1, run:# vinfra service block-storage target-group set --disable-acl tg1\n\nTo delete an initiator from the ACL\n\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon in the ACL section.\nIn the Access control window, click the pencil icon of the desired initiator, and then click Delete.\nClick Save to apply the changes.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group acl delete <target-group> <wwn>\n\n<target-group>\n\nTarget group name or ID\n<wwn>\n\nWorld wide name (WWN) of the target, that is, IQN\n\nFor example, to remove the initiator from the ACL of the target group tg1, run:# vinfra service block-storage target-group acl delete tg1 iqn.2014-06.com.vstorage:target1\n\nSee also\n\nManaging CHAP users",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nAdd an initiator  to the ACL of the target group:vinfra service block-storage target-group acl add [--alias <alias>] [--lun <lun>] <target-group> <wwn>\n\n--alias <alias>\n\nInitiator name\n--lun <lun>\n\nLUN ID\n<target-group>\n\nTarget group name or ID\n<wwn>\n\nWorld wide name (WWN) of the target, that is, IQN\n\nFor example, to add the initiator initiator1 to the ACL of the target group tg1, run:# vinfra service block-storage target-group acl add --lun 0 --alias initiator1 tg1 iqn.2014-06.com.vstorage:target1\n\n\nEnable ACL for the target group:vinfra service block-storage target-group set --enable-acl <target-group>\n\n--enable-acl\n\nEnable ACL\n<target-group>\n\nTarget group name or ID\n\nFor example, to enable ACL for the target group tg1, run:# vinfra service block-storage target-group set --enable-acl tg1\n\n\n",
                "title": "To add an initiator to a target group\u00e2\u0080\u0099s ACL"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group acl set (--lun <lun> | --no-luns) <target-group> <wwn>\n\n--lun <lun>\n\nLUN ID\n--no-luns\n\nNo LUNs\n<target-group>\n\nTarget group name or ID\n<wwn>\n\nWorld wide name (WWN) of the target, that is, IQN\n\nFor example, to change the LUN ID to 1 for the ACL of the target group tg1, run:# vinfra service block-storage target-group acl set --lun 1 tg1 iqn.2014-06.com.vstorage:target1\n",
                "title": "To edit an initiator in the ACL"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group set --disable-acl <target-group>\n\n--disable-acl\n\nDisable ACL\n<target-group>\n\nTarget group name or ID\n\nFor example, to disable the ACL for the target group tg1, run:# vinfra service block-storage target-group set --disable-acl tg1\n",
                "title": "To disable the ACL for a target group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group acl delete <target-group> <wwn>\n\n<target-group>\n\nTarget group name or ID\n<wwn>\n\nWorld wide name (WWN) of the target, that is, IQN\n\nFor example, to remove the initiator from the ACL of the target group tg1, run:# vinfra service block-storage target-group acl delete tg1 iqn.2014-06.com.vstorage:target1\n",
                "title": "To delete an initiator from the ACL"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, and then click the desired target group in the list (anywhere except the group\u00e2\u0080\u0099s name).\n\nOn the group right pane, open the Access control tab, and then click the pencil icon.\n\n\n\n\n\n\nIn the Access control window, select ACL, and then click Add.\n\n\n\n\n\n\nIn the Add ACL window, specify the initiator\u00e2\u0080\u0099s IQN, enter an alias, select the LUNs that it will be able to access, and then click Add. The initiator will appear in the ACL.\n\n\n\n\n\nHaving populated the ACL with initiators, click Save.\n\n",
                "title": "To add an initiator to a target group\u00e2\u0080\u0099s ACL"
            },
            {
                "example": "\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon.\nIn the Access control window, click the pencil icon of the desired initiator, and then click Edit.\nHaving changed the ACL, click Save.\n\n",
                "title": "To edit an initiator in the ACL"
            },
            {
                "example": "\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon in the ACL section.\n\nIn the Access control window, clear ACL, and then click Save.\n\n\n\n\n\n\n",
                "title": "To disable the ACL for a target group"
            },
            {
                "example": "\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon in the ACL section.\nIn the Access control window, click the pencil icon of the desired initiator, and then click Delete.\nClick Save to apply the changes.\n\n",
                "title": "To delete an initiator from the ACL"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-access-control-lists.html"
    },
    {
        "title": "Managing compute nodes",
        "content": "Managing compute nodes\nCompute nodes run the compute services and virtual machines. You can deploy the compute cluster on top of the storage cluster, thus, making a hyperconverged infrastructure. Alternatively, you can separate the compute services from the core storage services by running the compute services on other infrastructure nodes. Keep in mind that deploying the compute services increases resource consumption and may affect storage cluster performance.\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-compute-nodes.html"
    },
    {
        "title": "Managing CHAP users",
        "content": "Managing CHAP users\nThe Challenge-Handshake Authentication Protocol (CHAP) provides a way to restrict access to targets and their LUNs by requiring a user name and a password from the initiator. CHAP accounts apply to entire target groups.\nLimitations\n\nYou can only delete CHAP users that do not apply to any target group.\n\nTo enable CHAP authentication for a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, and then click the desired target group in the list (anywhere except group\u00e2\u0080\u0099s name).\n\nOn the group right pane, open the Access control tab, and then click the pencil icon.\n\nIn the Access control window, select CHAP, and then click Create user.\n\nIn the Create CHAP user window, enter a user name and a password (12 to 16 characters long), and then click Create.\n\nBack on the Access control screen, select the desired CHAP user, and then click Save.\n\nCommand-line interface\n\nCreate a CHAP user:vinfra service block-storage user create [--description <description>] <name>\n\n--description <description>\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n<name>\n\nUser name\n\nFor example, to create the CHAP user user1 with the description A new user, run:# vinfra service block-storage user create user1 --description \"A new user\"\nSpecify the user password when prompted. The password must be 12-16 characters long.\n\nEnable CHAP authentication for the target group and the created CHAP user:vinfra service block-storage target-group set --enable-chap --chap-user <user-name> <target-group>\n\n--enable-chap\n\nEnable CHAP authentication\n--chap-user <user-name>\n\nCHAP user name\n<target-group>\n\nTarget group name or ID\n\nFor example, to enable CHAP authentication for the target group tg1 and the CHAP user user1, run:# vinfra service block-storage target-group set --enable-chap --chap-user user1 tg1\n\nTo change the password of a CHAP user\n\nAdmin panel\n\nOpen Storage services > Block storage > CHAP users, click a user to open details, and then click the pencil icon. \nIn the Edit CHAP user window, specify a new password, and then click Apply.\n\nCommand-line interface\nUse the following command:vinfra service block-storage user set [--description <description>] [--password] <user>\n\n--description <description>\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--password\n\nChange the user password\n<user>\n\nUser name\n\nFor example, to change the password of the CHAP user user1, run:# vinfra service block-storage user set user1 --password\nWhen prompted, enter a new password, which will replace the old one. The password must be 12-16 characters long.\n\nTo disable CHAP authentication for a target group\n\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon in the CHAP authentication section.\n\nIn the Access control window, clear CHAP, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group set --disable-chap <target-group>\n\n--disable-chap\n\nDisable CHAP authentication\n<target-group>\n\nTarget group name or ID\n\nFor example, to disable CHAP authentication for the target group tg1, run:# vinfra service block-storage target-group set --disable-chap tg1\n\nTo delete a CHAP user\n\nAdmin panel\n\nOpen Storage services > Block storage > CHAP users. \nClick the ellipsis icon of the user, and then click Delete.\n\nCommand-line interface\nUse the following command:vinfra service block-storage user delete <user>\n\n<user>\n\nUser name\n\nFor example, to delete the CHAP user user1, run:# vinfra service block-storage user delete user1\n\nSee also\n\nManaging access control lists",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nCreate a CHAP user:vinfra service block-storage user create [--description <description>] <name>\n\n--description <description>\n\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n<name>\n\nUser name\n\nFor example, to create the CHAP user user1 with the description A new user, run:# vinfra service block-storage user create user1 --description \"A new user\"\nSpecify the user password when prompted. The password must be 12-16 characters long.\n\n\nEnable CHAP authentication for the target group and the created CHAP user:vinfra service block-storage target-group set --enable-chap --chap-user <user-name> <target-group>\n\n--enable-chap\n\nEnable CHAP authentication\n--chap-user <user-name>\n\nCHAP user name\n<target-group>\n\nTarget group name or ID\n\nFor example, to enable CHAP authentication for the target group tg1 and the CHAP user user1, run:# vinfra service block-storage target-group set --enable-chap --chap-user user1 tg1\n\n\n",
                "title": "To enable CHAP authentication for a target group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage user set [--description <description>] [--password] <user>\n\n--description <description>\n\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--password\n\nChange the user password\n<user>\n\nUser name\n\nFor example, to change the password of the CHAP user user1, run:# vinfra service block-storage user set user1 --password\nWhen prompted, enter a new password, which will replace the old one. The password must be 12-16 characters long.\n",
                "title": "To change the password of a CHAP user"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group set --disable-chap <target-group>\n\n--disable-chap\n\nDisable CHAP authentication\n<target-group>\n\nTarget group name or ID\n\nFor example, to disable CHAP authentication for the target group tg1, run:# vinfra service block-storage target-group set --disable-chap tg1\n",
                "title": "To disable CHAP authentication for a target group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage user delete <user>\n\n<user>\n\nUser name\n\nFor example, to delete the CHAP user user1, run:# vinfra service block-storage user delete user1\n",
                "title": "To delete a CHAP user"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, and then click the desired target group in the list (anywhere except group\u00e2\u0080\u0099s name).\n\nOn the group right pane, open the Access control tab, and then click the pencil icon.\n\n\n\n\n\n\nIn the Access control window, select CHAP, and then click Create user.\n\n\n\n\n\n\nIn the Create CHAP user window, enter a user name and a password (12 to 16 characters long), and then click Create.\n\n\n\n\n\n\nBack on the Access control screen, select the desired CHAP user, and then click Save.\n\n\n\n\n\n\n",
                "title": "To enable CHAP authentication for a target group"
            },
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > CHAP users, click a user to open details, and then click the pencil icon. \nIn the Edit CHAP user window, specify a new password, and then click Apply.\n\n",
                "title": "To change the password of a CHAP user"
            },
            {
                "example": "\nAdmin panel\n\nOn the target group right pane, open the Access control tab, and then click the pencil icon in the CHAP authentication section.\n\nIn the Access control window, clear CHAP, and then click Save.\n\n\n\n\n\n\n",
                "title": "To disable CHAP authentication for a target group"
            },
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > CHAP users. \nClick the ellipsis icon of the user, and then click Delete.\n\n",
                "title": "To delete a CHAP user"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-chap-users.html"
    },
    {
        "title": "Managing domain groups",
        "content": "Managing domain groups\nA domain group is a group of users with the same role permissions within a particular domain. Users assigned to a domain group inherit role permissions set to this domain group. Using domain groups allows you to configure permissions of multiple users without the need to do it individually. \nYou can create, edit, and delete the existing domain groups. Also, you can manage user and project assignment to domain groups.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-domain-groups.html"
    },
    {
        "title": "Managing encryption exceptions",
        "content": "Managing encryption exceptions\nWhen enabling data-in-transit encryption for a network, you can add exceptions for particular IP addresses, prefixes, or ports to bypass the encryption. This is required for external communication of services that operate in the same subnet with cluster nodes.  For example, if you use a custom port, instead of TCP 443,  to connect your backup storage to an external S3 storage, you need to add this port to the encryption exceptions.\nPrerequisites\n\nData-in-transit encryption is enabled for an infrastructure network, as described in Enabling and disabling data-in-transit encryption.\n\nTo add exceptions for data-in-transit encryption\nUse the following command:vinfra cluster network encryption bypass add <subnet> <port>\n\n<subnet>\n\nSubnet range in CIDR notation or a single address\n<port>\n\nPort number\n\nFor example, to bypass encryption for the port 700, run:vinfra cluster network encryption bypass add 0.0.0.0/24 700\nTo list all exceptions for data-in-transit encryption, use the vinfra cluster network encryption bypass list command:# vinfra cluster network encryption bypass list\r\n+------------+------+\r\n| subnet     | port |\r\n+------------+------+\r\n| 0.0.0.0/24 | 700  |\r\n+------------+------+\nTo remove exceptions for data-in-transit encryption\nUse the following command:vinfra cluster network encryption bypass delete <subnet> <port>\n\n<subnet>\n\nSubnet range in CIDR notation or a single address\n<port>\n\nPort number\n\nFor example, to enable encryption for the port 700, run:vinfra cluster network encryption bypass delete 0.0.0.0/24 700\nSee also\n\nRenewing encryption certificates",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-encryption-exceptions.html"
    },
    {
        "title": "Managing compute volumes",
        "content": "Managing compute volumes\nA volume in Virtuozzo Hybrid Infrastructure is a virtual disk drive that can be attached to a virtual machine. The integrity of data in volumes is protected by the redundancy mode specified in the storage policy.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-compute-volumes.html"
    },
    {
        "title": "Managing domains",
        "content": "Managing domains\nYou can add more domains, as described in Configuring multitenancy. Also, you can edit domain details and quotas, as well as enable/disable and delete the existing domains. Disabling and enabling domains allows or prohibits access to domains in the self-service panel.\nLimitations\n\nA domain cannot be deleted if it has projects.\n\nTo edit a domain name or description\n\nAdmin panel\n\nGo to the Settings > Projects and users screen.\nClick the ellipsis icon next to the domain, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nCommand-line interface\nUse the following command:vinfra domain set [--description <description>] [--name <name>] <domain>\r\n\n\n--description <description>\n\nDomain description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--name <name>\n\nDomain name\n<domain>\n\nDomain ID or name\n\nFor example, to add a description for the domain mydomain, run:# vinfra domain set mydomain --description \"A custom domain\"\n\nTo edit domain quotas\nUse the following command:vinfra service compute quotas update [--cores <cores>] [--ram-size <ram>]\r\n                                     [--storage-policy <storage_policy>] <domain_id>\n\n--cores <cores>\n\nNumber of cores\n--ram-size <ram>\n\nAmount of RAM\n--storage-policy <storage_policy>\n\nStorage policy in the format <storage_policy>:<size> (this option can be used multiple times)\n<domain_id>\n\nDomain ID\n\nFor example, to update quotas for the domain with the ID 0ed0dac39ba14e89b7d2b8cb7d5337f7 to 30 vCPUs, 60 GiB of RAM, and 1024 GiB of disk space for the default storage policy, run:# vinfra service compute quotas update --cores 30 --ram-size 60G --storage-policy default:1024G \\\r\n0ed0dac39ba14e89b7d2b8cb7d5337f7\nYou can view the updated quotas in the vinfra service compute quotas show output:# vinfra service compute quotas show 0ed0dac39ba14e89b7d2b8cb7d5337f7\r\n+----------------------------------------+---------+\r\n| Field                                  | Value   |\r\n+----------------------------------------+---------+\r\n| compute.cores.limit                    | 30      |\r\n| compute.ram_quota.limit                | 60.0GiB |\r\n| storage.storage_policies.default.limit | 1.0TiB  |\r\n+----------------------------------------+---------+\nTo enable or disable a domain\n\nAdmin panel\n\nGo to the Settings > Projects and users screen.\nClick the ellipsis icon next to the domain, and then click Enable or Disable.\n\nCommand-line interface\nUse the following command:vinfra domain set [--enable | --disable] <domain>\r\n\n\n--enable\n\nEnable domain\n--disable\n\nDisable domain\n<domain>\n\nDomain ID or name\n\nFor example, to disable the domain mydomain, run:# vinfra domain set mydomain --disable\n\nTo delete a domain\n\nAdmin panel\n\nGo to the Settings > Projects and users screen.\nClick the ellipsis icon next to the domain, and then click Delete.\n\nCommand-line interface\nUse the following command:vinfra domain delete <domain>\r\n\n\n<domain>\n\nDomain ID or name\n\nFor example, to delete the domain mydomain, run:# vinfra domain delete mydomain\n\nSee also\n\nManaging domain groups\n\nManaging projects\n\nManaging identity providers\n\nEnabling CPU\u00a0and RAM hot plug per domain",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain set [--description <description>] [--name <name>] <domain>\r\n\n\n--description <description>\n\n\nDomain description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--name <name>\n\nDomain name\n<domain>\n\nDomain ID or name\n\nFor example, to add a description for the domain mydomain, run:# vinfra domain set mydomain --description \"A custom domain\"\n",
                "title": "To edit a domain name or description"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain set [--enable | --disable] <domain>\r\n\n\n--enable\n\nEnable domain\n--disable\n\nDisable domain\n<domain>\n\nDomain ID or name\n\nFor example, to disable the domain mydomain, run:# vinfra domain set mydomain --disable\n",
                "title": "To enable or disable a domain"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain delete <domain>\r\n\n\n<domain>\n\nDomain ID or name\n\nFor example, to delete the domain mydomain, run:# vinfra domain delete mydomain\n",
                "title": "To delete a domain"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Settings > Projects and users screen.\nClick the ellipsis icon next to the domain, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n",
                "title": "To edit a domain name or description"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Settings > Projects and users screen.\nClick the ellipsis icon next to the domain, and then click Enable or Disable.\n\n",
                "title": "To enable or disable a domain"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Settings > Projects and users screen.\nClick the ellipsis icon next to the domain, and then click Delete.\n\n",
                "title": "To delete a domain"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-domains.html"
    },
    {
        "title": "Managing custom traffic types",
        "content": "Managing custom traffic types\nYou can create custom traffic types, add them to multiple networks, edit, and delete.\nLimitations\n\nIf you create allow rules but leave the deny list empty, all incoming traffic will still be allowed.\nYou cannot change the name of a traffic type, if it is assigned to any network.\n\nTo create a custom traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Create traffic type.\nIn the Create traffic type window, specify a traffic type name and port to open. Traffic type names must be alphanumeric and 3 to 32 characters long.\n\nIn the Access rules section, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra cluster traffic-type create --port <port> [--inbound-allow-list <addresses>]\r\n                                   [--inbound-deny-list <addresses>] <traffic-type-name>\r\n\n\n--port <port>\n\nTraffic type port\n--inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses\n<traffic-type-name>\n\nTraffic type name\n\nFor example, to create a custom traffic type MyTrafficType on port 6900, run:# vinfra cluster traffic-type create \"MyTrafficType\" --port 6900\r\n+--------------------+---------------+\r\n| Field              | Value         |\r\n+--------------------+---------------+\r\n| exclusive          | False         |\r\n| hidden             | False         |\r\n| inbound_allow_list | []            |\r\n| inbound_deny_list  | []            |\r\n| name               | MyTrafficType |\r\n| port               | 6900          |\r\n| type               | custom        |\r\n+--------------------+---------------+\r\n\nThe created traffic type will appear in the vinfra cluster traffic-type list output:# vinfra cluster traffic-type list -c name -c type -c exclusive -c port\r\n+-----------------------+------------+-----------+------+\r\n| name                  | type       | exclusive | port |\r\n+-----------------------+------------+-----------+------+\r\n| Storage               | predefined | True      |      |\r\n| Internal management   | predefined | True      |      |\r\n| OSTOR private         | predefined | True      |      |\r\n| S3 public             | predefined | False     |      |\r\n| iSCSI                 | predefined | False     |      |\r\n| NFS                   | predefined | False     |      |\r\n| Backup (ABGW) private | predefined | True      |      |\r\n| Backup (ABGW) public  | predefined | False     |      |\r\n| Admin panel           | predefined | False     |      |\r\n| SSH                   | predefined | False     |      |\r\n| VM public             | predefined | False     |      |\r\n| VM private            | predefined | True      |      |\r\n| Compute API           | predefined | True      |      |\r\n| MyTrafficType         | custom     | False     | 6900 |\r\n+-----------------------+------------+-----------+------+\r\n\n\nTo assign, reassign, or unassign a custom traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Assign to networks next to the Custom traffic types section.\nAdd the needed traffic type to or remove it from your networks by selecting the corresponding check boxes.\nClick Save to apply the changes.\n\nCommand-line interface\nUse the following command:vinfra cluster network set [--traffic-types <traffic-types> | --add-traffic-types <traffic-types> |\r\n                           --del-traffic-types <traffic-types>] <network>\r\n\n\n--traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (overwrites network\u00e2\u0080\u0099s current traffic types)\n--add-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (adds the specified traffic types to the network)\n--del-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (removes the specified traffic types from the network)\n<network>\n\nNetwork ID or name\n\nFor example, to add the traffic type MyTrafficType to the MyNet network, run:# vinfra cluster network set MyNet --add-traffic-types \"MyTrafficType\"\n\nTo edit  a custom traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the ellipsis icon next to the traffic type name, and select Edit.\nIn the Edit traffic type window, change the traffic type name or port, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra cluster traffic-type set [--name <name>] [--port <port>] <traffic-type>\r\n\n\n--name <name>\n\nA new name for the traffic type\n--port <port>\n\nA new port for the traffic type\n<traffic-type>\n\nTraffic type name\n\nFor example, to rename the traffic type MyTrafficType to MyOtherTrafficType and change its port to 6901, run:# vinfra cluster traffic-type set \"MyTrafficType\" --name \"MyOtherTrafficType\" --port 6901\n\nTo delete a custom traffic type\n\nAdmin panel\n\nMake sure it is excluded from all networks.\nOn the Infrastructure > Networks screen, click the ellipsis icon next to the traffic type, and then select Delete.\nIn the Delete traffic type window, confirm your action by clicking Delete.\n\nCommand-line interface\nUse the following command:vinfra cluster traffic-type delete <traffic-type>\r\n\n\n<traffic-type>\n\nTraffic type name\n\nFor example, to delete the custom traffic type MyOtherTrafficType, run:# vinfra cluster traffic-type delete \"MyOtherTrafficType\"\n\nSee also\n\nManaging exclusive traffic types\n\nManaging regular traffic types\n\nConfiguring inbound firewall rules\n\nManaging networks",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster traffic-type create --port <port> [--inbound-allow-list <addresses>]\r\n                                   [--inbound-deny-list <addresses>] <traffic-type-name>\r\n\n\n--port <port>\n\nTraffic type port\n--inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses\n<traffic-type-name>\n\nTraffic type name\n\nFor example, to create a custom traffic type MyTrafficType on port 6900, run:# vinfra cluster traffic-type create \"MyTrafficType\" --port 6900\r\n+--------------------+---------------+\r\n| Field              | Value         |\r\n+--------------------+---------------+\r\n| exclusive          | False         |\r\n| hidden             | False         |\r\n| inbound_allow_list | []            |\r\n| inbound_deny_list  | []            |\r\n| name               | MyTrafficType |\r\n| port               | 6900          |\r\n| type               | custom        |\r\n+--------------------+---------------+\r\n\nThe created traffic type will appear in the vinfra cluster traffic-type list output:# vinfra cluster traffic-type list -c name -c type -c exclusive -c port\r\n+-----------------------+------------+-----------+------+\r\n| name                  | type       | exclusive | port |\r\n+-----------------------+------------+-----------+------+\r\n| Storage               | predefined | True      |      |\r\n| Internal management   | predefined | True      |      |\r\n| OSTOR private         | predefined | True      |      |\r\n| S3 public             | predefined | False     |      |\r\n| iSCSI                 | predefined | False     |      |\r\n| NFS                   | predefined | False     |      |\r\n| Backup (ABGW) private | predefined | True      |      |\r\n| Backup (ABGW) public  | predefined | False     |      |\r\n| Admin panel           | predefined | False     |      |\r\n| SSH                   | predefined | False     |      |\r\n| VM public             | predefined | False     |      |\r\n| VM private            | predefined | True      |      |\r\n| Compute API           | predefined | True      |      |\r\n| MyTrafficType         | custom     | False     | 6900 |\r\n+-----------------------+------------+-----------+------+\r\n\n",
                "title": "To create a custom traffic type"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network set [--traffic-types <traffic-types> | --add-traffic-types <traffic-types> |\r\n                           --del-traffic-types <traffic-types>] <network>\r\n\n\n--traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (overwrites network\u00e2\u0080\u0099s current traffic types)\n--add-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (adds the specified traffic types to the network)\n--del-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (removes the specified traffic types from the network)\n<network>\n\nNetwork ID or name\n\nFor example, to add the traffic type MyTrafficType to the MyNet network, run:# vinfra cluster network set MyNet --add-traffic-types \"MyTrafficType\"\n",
                "title": "To assign, reassign, or unassign a custom traffic type"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster traffic-type set [--name <name>] [--port <port>] <traffic-type>\r\n\n\n--name <name>\n\nA new name for the traffic type\n--port <port>\n\nA new port for the traffic type\n<traffic-type>\n\nTraffic type name\n\nFor example, to rename the traffic type MyTrafficType to MyOtherTrafficType and change its port to 6901, run:# vinfra cluster traffic-type set \"MyTrafficType\" --name \"MyOtherTrafficType\" --port 6901\n",
                "title": "To edit  a custom traffic type"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster traffic-type delete <traffic-type>\r\n\n\n<traffic-type>\n\nTraffic type name\n\nFor example, to delete the custom traffic type MyOtherTrafficType, run:# vinfra cluster traffic-type delete \"MyOtherTrafficType\"\n",
                "title": "To delete a custom traffic type"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Create traffic type.\nIn the Create traffic type window, specify a traffic type name and port to open. Traffic type names must be alphanumeric and 3 to 32 characters long.\n\nIn the Access rules section, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\n\n\n\n\n\nClick Create.\n\n",
                "title": "To create a custom traffic type"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Assign to networks next to the Custom traffic types section.\nAdd the needed traffic type to or remove it from your networks by selecting the corresponding check boxes.\nClick Save to apply the changes.\n\n",
                "title": "To assign, reassign, or unassign a custom traffic type"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the ellipsis icon next to the traffic type name, and select Edit.\nIn the Edit traffic type window, change the traffic type name or port, and then click Save.\n\n",
                "title": "To edit  a custom traffic type"
            },
            {
                "example": "\nAdmin panel\n\nMake sure it is excluded from all networks.\nOn the Infrastructure > Networks screen, click the ellipsis icon next to the traffic type, and then select Delete.\nIn the Delete traffic type window, confirm your action by clicking Delete.\n\n",
                "title": "To delete a custom traffic type"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-custom-traffic-types.html"
    },
    {
        "title": "Managing exclusive traffic types",
        "content": "Managing exclusive traffic types\nExclusive traffic types can only be reassigned from one network to another and only one at a time. Reassignment can be performed even if the related services are already deployed. This can be useful, for example, if the initial network configuration is wrong but the storage cluster is already populated with data and running critical services; or after adding a network card, which requires changing network settings, adding a new network, and assigning traffic types to it.\nLimitations\n\nExclusive traffic types cannot be edited or deleted.\nYou cannot manage access rules for exclusive traffic types.\nIf the management node high availability is enabled, you cannot reassign the Internal management and Compute API traffic types.\n\nPrerequisites\n\nAll of the connected node interfaces are online.\nEach network interface has only one IP address.\nThe number of interfaces on nodes connected to the source and target networks must be the same. Unassigned nodes are also taken into account.\nThe deployed related services are in the healthy state.\nIf you have restricted outbound traffic in your cluster, you need to manually add a rule that will allow outbound traffic on TCP and UDP ports 60000\u00e2\u0080\u009360100, as described in Configuring outbound firewall rules.\n\nTo reassign an exclusive traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Assign to network next to the Exclusive traffic types section, and then select the traffic type you want to reassign.\nReassign the traffic type to another network by selecting the corresponding radio button, and then click Save.\n\nIn the Reassign traffic type window, review the source and target networks, and important information about potential risks, and then click Continue to start the traffic type reassignment.\n\nIf the related services are already deployed, wait until the connected interfaces are tested and the new configuration is created. Then, click Apply.\n\nWhile traffic type reassignment is in progress, users cannot perform other tasks in the admin panel. Moreover, the self-service users may not have access to the portal and will need to wait until the reassignment is complete.\n\nIf the connectivity checks fail, you can revert to your old network configuration by clicking Revert. Then, you need to fix the found issues and try again.\nWait until the reassignment is complete on all the connected interfaces, and then click Done.\nIf you reassign the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\nCommand-line interface\n\nStart the traffic type reassignment by using the following command:vinfra cluster traffic-type assignment start --traffic-type <traffic-type>\r\n                                             --target-network <target-network>\r\n\n\n--traffic-type <traffic-type>\n\nTraffic type name\n--target-network <target-network>\n\nTarget network ID or name\n\nFor example:# vinfra cluster traffic-type assignment start --traffic-type Storage --target-network Public\r\n+---------------+---------------------------------------------------------------+\r\n| Field         | Value                                                         |\r\n+---------------+---------------------------------------------------------------+\r\n| configuration | target_network: 69ad1db5-512f-4994-ab08-7d643fdb7b39          |\r\n|               | traffic_type: Storage                                         |\r\n| link          | href: /api/v2/network/traffic-type-assignment/285be91b-<...>/ |\r\n|               | method: GET                                                   |\r\n|               | rel: traffic-type-assignment-details                          |\r\n| operation     | traffic-type-assignment                                       |\r\n| progress      | 0.0                                                           |\r\n| state         | preparing                                                     |\r\n| task_id       | 285be91b-77ee-4f8f-a118-8410ab792148                          |\r\n| transitions   | 0                                                             |\r\n+---------------+---------------------------------------------------------------+\r\n\n\nView the traffic type reassignment details. For example:# vinfra cluster traffic-type assignment show\r\n+-------------+---------------------------------------------------------------+\r\n| Field       | Value                                                         |\r\n+-------------+---------------------------------------------------------------+\r\n| link        | href: /api/v2/network/traffic-type-assignment/285be91b-<...>/ |\r\n|             | method: GET                                                   |\r\n|             | rel: traffic-type-assignment-details                          |\r\n| operation   | traffic-type-assignment                                       |\r\n| progress    | 1.0                                                           |\r\n| state       | test-passed                                                   |\r\n| task_id     | 285be91b-77ee-4f8f-a118-8410ab792148                          |\r\n| transitions | 3                                                             |\r\n+-------------+---------------------------------------------------------------+\nThe output shows that the new network configuration has been tested and can be applied.\n\nContinue the traffic type reassignment and apply the new network configuration. For example:# vinfra cluster traffic-type assignment apply\n\nIf you reassign the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\nIf the connectivity checks fail, you need to fix the found issues and try again by running:# vinfra cluster traffic-type assignment retry\nAlternatively, you can revert to your old network configuration with vinfra cluster traffic-type assignment revert, fix the issue, and try again.\n\nTo troubleshoot a failed reassignment\n\nConnect to your cluster via SSH.\nInvestigate /var/log/vstorage-ui-backend/celery.log to find the root cause.\nFix the issue.\nGo back to the wizard screen and click Retry.\n\nSee also\n\nManaging regular traffic types\n\nManaging custom traffic types\n\nChanging network configuration\n\nManaging networks",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nStart the traffic type reassignment by using the following command:vinfra cluster traffic-type assignment start --traffic-type <traffic-type>\r\n                                             --target-network <target-network>\r\n\n\n--traffic-type <traffic-type>\n\nTraffic type name\n--target-network <target-network>\n\nTarget network ID or name\n\nFor example:# vinfra cluster traffic-type assignment start --traffic-type Storage --target-network Public\r\n+---------------+---------------------------------------------------------------+\r\n| Field         | Value                                                         |\r\n+---------------+---------------------------------------------------------------+\r\n| configuration | target_network: 69ad1db5-512f-4994-ab08-7d643fdb7b39          |\r\n|               | traffic_type: Storage                                         |\r\n| link          | href: /api/v2/network/traffic-type-assignment/285be91b-<...>/ |\r\n|               | method: GET                                                   |\r\n|               | rel: traffic-type-assignment-details                          |\r\n| operation     | traffic-type-assignment                                       |\r\n| progress      | 0.0                                                           |\r\n| state         | preparing                                                     |\r\n| task_id       | 285be91b-77ee-4f8f-a118-8410ab792148                          |\r\n| transitions   | 0                                                             |\r\n+---------------+---------------------------------------------------------------+\r\n\n\n\nView the traffic type reassignment details. For example:# vinfra cluster traffic-type assignment show\r\n+-------------+---------------------------------------------------------------+\r\n| Field       | Value                                                         |\r\n+-------------+---------------------------------------------------------------+\r\n| link        | href: /api/v2/network/traffic-type-assignment/285be91b-<...>/ |\r\n|             | method: GET                                                   |\r\n|             | rel: traffic-type-assignment-details                          |\r\n| operation   | traffic-type-assignment                                       |\r\n| progress    | 1.0                                                           |\r\n| state       | test-passed                                                   |\r\n| task_id     | 285be91b-77ee-4f8f-a118-8410ab792148                          |\r\n| transitions | 3                                                             |\r\n+-------------+---------------------------------------------------------------+\nThe output shows that the new network configuration has been tested and can be applied.\n\n\nContinue the traffic type reassignment and apply the new network configuration. For example:# vinfra cluster traffic-type assignment apply\n\n\nIf you reassign the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\n\nIf the connectivity checks fail, you need to fix the found issues and try again by running:# vinfra cluster traffic-type assignment retry\nAlternatively, you can revert to your old network configuration with vinfra cluster traffic-type assignment revert, fix the issue, and try again.\n",
                "title": "To reassign an exclusive traffic type"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Assign to network next to the Exclusive traffic types section, and then select the traffic type you want to reassign.\nReassign the traffic type to another network by selecting the corresponding radio button, and then click Save.\n\nIn the Reassign traffic type window, review the source and target networks, and important information about potential risks, and then click Continue to start the traffic type reassignment.\n\n\n\n\n\n\nIf the related services are already deployed, wait until the connected interfaces are tested and the new configuration is created. Then, click Apply.\n\n\n\n\n\nWhile traffic type reassignment is in progress, users cannot perform other tasks in the admin panel. Moreover, the self-service users may not have access to the portal and will need to wait until the reassignment is complete.\n\n\nIf the connectivity checks fail, you can revert to your old network configuration by clicking Revert. Then, you need to fix the found issues and try again.\nWait until the reassignment is complete on all the connected interfaces, and then click Done.\nIf you reassign the Internal management or VM private traffic type, manually restart all running virtual machines, to be able to access them via VNC console.\n\n",
                "title": "To reassign an exclusive traffic type"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-exclusive-traffic-types.html"
    },
    {
        "title": "Managing compute subnets",
        "content": "Managing compute subnets\nYou can add, edit, and delete subnets of physical compute networks. Also, if you exhaust all public IP addresses in a physical compute network, you can add more subnets to this network by using the vinfra command-line tool. The new subnets will be available in the admin and self-service panel for IP address allocation and management.\nLimitations\n\nFor an IPv6 subnet, you cannot change the IPv6 address mode.\nA compute network with enabled IP address management must have at least one subnet.\n\nPrerequisites\n\nCompute networks are created automatically during the compute cluster deployment or manually, as described in Creating physical compute networks and Creating virtual compute networks.\n\nTo add a subnet to a physical compute network\n\nAdmin panel\n\nOn the Compute > Networks tab, click the desired physical network. \nIn the Subnets section, click Add, and then select IPv4 subnet or IPv6 subnet, depending on the available option.\nIn the new window, specify the subnet settings, and then click Add.\n\nCommand-line interface\nUse the following command:vinfra service compute subnet create [--dhcp | --no-dhcp] [--dns-nameserver <dns-nameserver>]\r\n                                     [--allocation-pool <allocation-pool>] [--gateway <gateway> | --no-gateway]\r\n                                     [--ipv6-address-mode {dhcpv6-stateful,dhcpv6-stateless,slaac}]\r\n                                     --network <network> --cidr <cidr>\n\n--dhcp\n\nEnable DHCP\n--no-dhcp\n\nDisable DHCP\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: <starting_ip_address>-<ending_ip_address>. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n--ipv6-address-mode {dhcpv6-stateful,dhcpv6-stateless,slaac}\n\nIPv6 address mode. Valid modes are dhcpv6-stateful, dhcpv6-stateless, and slaac.\n--network <network>\n\nNetwork ID or name\n--cidr <cidr>\n\nSubnet range in CIDR notation\n\nFor example, to add an IPv6 subnet to the physical network public, run:# vinfra service compute subnet create --network public --cidr 2001:bd8::/64 --ipv6-address-mode dhcpv6-stateful \\\r\n--dhcp --allocation-pool 2001:bd8::100-2001:bd8::200 --dns-nameserver 2001:4860:4860::8888\nThe created subnet will appear in the vinfra service compute subnet list output:# vinfra service compute subnet list --network public -c id -c network_id -c cidr\r\n+--------------------------------------+--------------------------------------+----------------+\r\n| id                                   | network_id                           | cidr           |\r\n+--------------------------------------+--------------------------------------+----------------+\r\n| 8d2169a1-3459-4e87-9f5c-6d2d20353eea | d94545f0-c974-49ac-99f6-db6da73cd23d | 10.136.16.0/20 |\r\n| afa4710a-a82c-4273-a1b0-72d5d2884be6 | d94545f0-c974-49ac-99f6-db6da73cd23d | 2001:bd8::/64  |\r\n+--------------------------------------+--------------------------------------+----------------+\n\nTo add more subnets to a physical compute network\n\nIdentify the required network by listing all of the existing networks:# vinfra service compute network list -c id -c name -c physical_network -c cidr\r\n+--------------------------------------+---------+------------------+------------------+\r\n| id                                   | name    | physical_network | cidr             |\r\n+--------------------------------------+---------+------------------+------------------+\r\n| 3cbd594b-a1ff-4ee4-a771-1010822607c4 | private |                  | 192.168.128.0/24 |\r\n| d94545f0-c974-49ac-99f6-db6da73cd23d | public  | Public           | 10.136.16.0/20   |\r\n+--------------------------------------+---------+------------------+------------------+\n\nCreate a new subnet in the network by using the vinfra service compute subnet create command. For example:# vinfra service compute subnet create --network public --cidr 10.164.132.0/24 --gateway 10.164.132.1 \\\r\n--dhcp --allocation-pool 10.164.132.201-10.164.132.250 --dns-nameserver 8.8.8.8\n\nTo edit a subnet of a physical compute network\n\nAdmin panel\n\nOn the Compute > Networks tab, click the desired physical network. \nIn the Subnets section, click the pencil icon next to the subnet you want to edit.\nIn the Edit subnet window, change the subnet settings, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra service compute subnet set [--dhcp | --no-dhcp] [--dns-nameserver <dns-nameserver>]\r\n                                  [--allocation-pool <allocation-pool>]\r\n                                  [--gateway <gateway> | --no-gateway] <subnet>\n\n--dhcp\n\nEnable DHCP\n--no-dhcp\n\nDisable DHCP\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: <starting_ip_address>-<ending_ip_address>. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n<subnet>\n\nSubnet ID\n\nFor example, to add more IP addresses to the subnet allocation pool, run:# vinfra service compute subnet set 8d2169a1-3459-4e87-9f5c-6d2d20353eea --allocation-pool 10.136.18.201-10.136.18.250\n\nTo delete a subnet from a physical compute network\n\nAdmin panel\n\nOn the Compute > Networks tab, click the desired physical network. \n\nIn the Subnets section, click the bin icon next to the subnet you want to remove.\n\nYou cannot delete the only subnet.\n\nCommand-line interface\nUse the following command:vinfra service compute subnet delete <subnet>\n\n<subnet>\n\nSubnet ID\n\nFor example, to delete the IPv6 subnet from the physical network public, run:# vinfra service compute subnet delete afa4710a-a82c-4273-a1b0-72d5d2884be6\n\nSee also\n\nManaging security groups\n\nManaging virtual routers\n\nManaging floating IP addresses\n\nManaging load balancers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute subnet create [--dhcp | --no-dhcp] [--dns-nameserver <dns-nameserver>]\r\n                                     [--allocation-pool <allocation-pool>] [--gateway <gateway> | --no-gateway]\r\n                                     [--ipv6-address-mode {dhcpv6-stateful,dhcpv6-stateless,slaac}]\r\n                                     --network <network> --cidr <cidr>\n\n--dhcp\n\nEnable DHCP\n--no-dhcp\n\nDisable DHCP\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: <starting_ip_address>-<ending_ip_address>. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n--ipv6-address-mode {dhcpv6-stateful,dhcpv6-stateless,slaac}\n\nIPv6 address mode. Valid modes are dhcpv6-stateful, dhcpv6-stateless, and slaac.\n--network <network>\n\nNetwork ID or name\n--cidr <cidr>\n\nSubnet range in CIDR notation\n\nFor example, to add an IPv6 subnet to the physical network public, run:# vinfra service compute subnet create --network public --cidr 2001:bd8::/64 --ipv6-address-mode dhcpv6-stateful \\\r\n--dhcp --allocation-pool 2001:bd8::100-2001:bd8::200 --dns-nameserver 2001:4860:4860::8888\nThe created subnet will appear in the vinfra service compute subnet list output:# vinfra service compute subnet list --network public -c id -c network_id -c cidr\r\n+--------------------------------------+--------------------------------------+----------------+\r\n| id                                   | network_id                           | cidr           |\r\n+--------------------------------------+--------------------------------------+----------------+\r\n| 8d2169a1-3459-4e87-9f5c-6d2d20353eea | d94545f0-c974-49ac-99f6-db6da73cd23d | 10.136.16.0/20 |\r\n| afa4710a-a82c-4273-a1b0-72d5d2884be6 | d94545f0-c974-49ac-99f6-db6da73cd23d | 2001:bd8::/64  |\r\n+--------------------------------------+--------------------------------------+----------------+\n",
                "title": "To add a subnet to a physical compute network"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute subnet set [--dhcp | --no-dhcp] [--dns-nameserver <dns-nameserver>]\r\n                                  [--allocation-pool <allocation-pool>]\r\n                                  [--gateway <gateway> | --no-gateway] <subnet>\n\n--dhcp\n\nEnable DHCP\n--no-dhcp\n\nDisable DHCP\n--dns-nameserver <dns-nameserver>\n\nDNS server IP address. This option can be used multiple times.\n--allocation-pool <allocation-pool>\n\nAllocation pool to create inside the network in the format: <starting_ip_address>-<ending_ip_address>. This option can be used multiple times.\n--gateway <gateway>\n\nGateway IP address\n--no-gateway\n\nDo not configure a gateway for this network.\n<subnet>\n\nSubnet ID\n\nFor example, to add more IP addresses to the subnet allocation pool, run:# vinfra service compute subnet set 8d2169a1-3459-4e87-9f5c-6d2d20353eea --allocation-pool 10.136.18.201-10.136.18.250\n",
                "title": "To edit a subnet of a physical compute network"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute subnet delete <subnet>\n\n<subnet>\n\nSubnet ID\n\nFor example, to delete the IPv6 subnet from the physical network public, run:# vinfra service compute subnet delete afa4710a-a82c-4273-a1b0-72d5d2884be6\n",
                "title": "To delete a subnet from a physical compute network"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Networks tab, click the desired physical network. \nIn the Subnets section, click Add, and then select IPv4 subnet or IPv6 subnet, depending on the available option.\nIn the new window, specify the subnet settings, and then click Add.\n\n",
                "title": "To add a subnet to a physical compute network"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Networks tab, click the desired physical network. \nIn the Subnets section, click the pencil icon next to the subnet you want to edit.\nIn the Edit subnet window, change the subnet settings, and then click Save.\n\n",
                "title": "To edit a subnet of a physical compute network"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Networks tab, click the desired physical network. \n\nIn the Subnets section, click the bin icon next to the subnet you want to remove.\n\nYou cannot delete the only subnet.\n\n\n\n",
                "title": "To delete a subnet from a physical compute network"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-compute-subnets.html"
    },
    {
        "title": "Managing external storages",
        "content": "Managing external storages\nBesides the local compute storage, you can integrate your compute cluster with a third-party storage compatible with OpenStack. This will allow you to use an enterprise-grade storage solution, as well as isolate storage from your compute nodes.\nOpenStack provides multiple drivers for different storages. A set of available operations on volumes is limited by the driver capabilities and can be checked in the Cinder Driver Support Matrix.\nLimitations\n\nThe currently supported storages only include those that use the Pure Storage iSCSI Driver and Generic NFS Reference Driver.\nVolumes with external storage cannot be reverted from snapshots.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-external-storages.html"
    },
    {
        "title": "Managing guest tools",
        "content": "Managing guest tools\nThis section explains how to install and uninstall the guest tools. This functionality is required for running commands in virtual machines without network connectivity and setting a password inside virtual machines, as well as for creating consistent snapshots of a running VM\u00e2\u0080\u0099s disks.\nLimitations\n\nGuest tools rely on the QEMU guest agent that is installed alongside the tools. The agent service must be running for the tools to work.\n\nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.\nThe virtual machine has a guest operating system installed.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-guest-tools.html"
    },
    {
        "title": "Managing file storage",
        "content": "Managing file storage\nThis section outlines common administrator's tasks for NFS nodes, shares and exports.\nPrerequisites\n\nNFS exports are created, as described in Creating NFS exports.\nIf you have restricted outbound traffic in your cluster, you need to manually add a rule that will allow outbound traffic on TCP ports 88 and 749, and UDP port 88, as described in Configuring outbound firewall rules.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-file-storage.html"
    },
    {
        "title": "Managing identity providers",
        "content": "Managing identity providers\nBesides creating local users manually, you can add users from external identity providers and automatically map them to local domain groups.  User authentication can be based either on the Implicit Flow or Authorization Code Flow of the OpenID Connect (OIDC) protocol. The Implicit Flow is the default option.\nUsers imported from identity providers are called Federated, that is, shared between different identity management systems. Unlike local users, federated users do not have credentials set in Virtuozzo Hybrid Infrastructure. They log in to the admin or self-service panels  by using their respective credentials from the primary identity management system. The set of actions available to federated users is defined by the roles you assign to their local domain groups.\nLimitations\n\nOnly Microsoft Active Directory Federation Services (AD FS) identity providers are supported.\nWhen federated users are removed by their identity provider, they are not automatically deleted from the infrastructure.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-identity-providers.html"
    },
    {
        "title": "Managing floating IP addresses",
        "content": "Managing floating IP addresses\nA virtual machine connected to a virtual network can be accessed from public networks, such as the Internet, by means of a floating IP address. Such an address is picked from a physical network and mapped to the VM\u00e2\u0080\u0099s private IP address. The floating and private IP addresses are used at the same time on the VM\u00e2\u0080\u0099s network interface. The private IP address is used to communicate with other VMs on the virtual network. The floating IP address is used to access the VM from public networks. The VM guest operating system is unaware of the assigned floating IP address.\nPrerequisites\n\nYou have a virtual router created, as described in Creating virtual routers.\nThe virtual machine to assign a floating IP to has a fixed private IP address.\nThe virtual router connects the physical network, from which a floating IP will be picked, with the VM\u00e2\u0080\u0099s virtual network.\n\nTo create a floating IP address and assign it to a virtual machine\n\nAdmin panel\n\nOn the Compute > Network > Floating IPs tab, click Add floating IP.\n\nIn the Add floating IP address, select a physical network, from which a floating IP will be picked, and a VM network interface with a fixed private IP address.\n\nClick Add.\n\nCommand-line interface\nUse the following command:vinfra service compute floatingip create [--floating-ip <floating-ip>]\r\n                                         [--port-id <port-id>]\r\n                                         [--fixed-ip <fixed-ip>]\r\n                                         [--description <description>]\r\n                                         --network <network>\r\n\n\n--floating-ip <floating-ip>\n\nFloating IP address\n--port-id <port-id>\n\nID of the port to be associated with the floating IP address. To learn the port ID of the selected virtual machine, use the command vinfra service compute server iface list.\n--fixed-ip <fixed-ip>\n\nPort IP address (required only if the port has multiple IP addresses)\n--description <description>\n\nDescription of the floating IP address\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--network <network>\n\nID or name of the network from which to allocate the floating IP\n\nFor example, to create a floating IP address from the physical network public and assign it to a virtual machine on port with the ID 418c8c9e-aaa5-42f2-8da7-24bfead6f28b and the virtual IP address 192.168.128.5, run:# vinfra service compute floatingip create public --port-id 418c8c9e-aaa5-42f2-8da7-24bfead6f28b \\\r\n--fixed-ip-address 192.168.128.5\r\n+---------------------+--------------------------------------+\r\n| Field               | Value                                |\r\n+---------------------+--------------------------------------+\r\n| attached_to         | a172cb6a-1c7b-4157-9e86-035f3077646f |\r\n| description         |                                      |\r\n| fixed_ip_address    | 192.168.128.5                        |\r\n| floating_ip_address | 10.94.129.72                         |\r\n| floating_network_id | 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                  | a709f884-c43f-4a9a-a243-a340d7682ef8 |\r\n| port_id             | 418c8c9e-aaa5-42f2-8da7-24bfead6f28b |\r\n| project_id          | 894696133031439f8aaa7e4868dcbd4d     |\r\n| router_id           | f7f86029-a553-4d61-b7ec-6f581d9c5f5f |\r\n| status              | DOWN                                 |\r\n+---------------------+--------------------------------------+\r\n\nThe created floating IP address will appear in the vinfra service compute floatingip list output:# vinfra service compute floatingip list -c id -c fixed_ip_address -c port_id -c floating_ip_address\r\n+----------------+------------------+----------------+---------------------+\r\n| id             | fixed_ip_address | port_id        | floating_ip_address |\r\n+----------------+------------------+----------------+---------------------+\r\n| a709f884-<...> | 192.168.128.5    | 418c8c9e-<...> | 10.94.129.72        |\r\n+----------------+------------------+----------------+---------------------+\n\nTo re-assign a floating IP address to another virtual machine\n\nAdmin panel\n\nClick the ellipsis icon next to the floating IP address, and then click Unassign.\nOnce the VM name disappears in the Assigned to column, click the ellipsis icon again, and then select Assign.\nIn the Assign floating IP address window, select a VM network interface with a fixed private IP address.\nClick Assign.\n\nCommand-line interface\nUse the following command:vinfra service compute floatingip set [--port-id <port-id>] [--fixed-ip <fixed-ip>]\r\n                                      [--description <description>] <floating-ip>\r\n\n\n--port-id <port-id>\n\nID of the port to be associated with the floating IP address\n--fixed-ip <fixed-ip>\n\nPort IP address (required only if the port has multiple IP addresses)\n--description <description>\n\nDescription of the floating IP address\n<floating-ip>\n\nID of the floating IP address\n\nFor example, to assign the floating IP address with the ID a709f884-c43f-4a9a-a243-a340d7682ef8 to a virtual machine on port with the ID 8c11c29b-9a73-4017-baff-1e872b18b54b and the virtual IP address 192.128.30.15, run:# vinfra service compute floatingip set a709f884-c43f-4a9a-a243-a340d7682ef8 \\\r\n--port-id 8c11c29b-9a73-4017-baff-1e872b18b54b --fixed-ip-address 192.128.30.15\r\n+---------------------+--------------------------------------+\r\n| Field               | Value                                |\r\n+---------------------+--------------------------------------+\r\n| attached_to         | 3a092f6f-bbaf-47a9-bcc7-f86223aacb55 |\r\n| description         |                                      |\r\n| fixed_ip_address    | 192.128.30.15                        |\r\n| floating_ip_address | 10.94.129.72                         |\r\n| floating_network_id | 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                  | a709f884-c43f-4a9a-a243-a340d7682ef8 |\r\n| port_id             | 8c11c29b-9a73-4017-baff-1e872b18b54b |\r\n| project_id          | 894696133031439f8aaa7e4868dcbd4d     |\r\n| router_id           | f7f86029-a553-4d61-b7ec-6f581d9c5f5f |\r\n| status              | ACTIVE                               |\r\n+---------------------+--------------------------------------+\r\n\n\nTo remove a floating IP address\n\nAdmin panel\n\nUnassign it from a virtual machine. Click the ellipsis icon next to the floating IP address, and then click Unassign.\nClick the ellipsis icon again, and then select Delete.\n\nCommand-line interface\nUse the following command:vinfra service compute floatingip delete <floating-ip>\r\n\n\n<floating-ip>\n\nID of the floating IP address\n\nFor example, to delete the floating IP address with the ID a709f884-c43f-4a9a-a243-a340d7682ef8, run:# vinfra service compute floatingip delete a709f884-c43f-4a9a-a243-a340d7682ef8\r\nOperation successful\r\n\n\nSee also\n\nManaging compute networks\n\nManaging virtual routers\n\nManaging load balancers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute floatingip create [--floating-ip <floating-ip>]\r\n                                         [--port-id <port-id>]\r\n                                         [--fixed-ip <fixed-ip>]\r\n                                         [--description <description>]\r\n                                         --network <network>\r\n\n\n--floating-ip <floating-ip>\n\nFloating IP address\n--port-id <port-id>\n\nID of the port to be associated with the floating IP address. To learn the port ID of the selected virtual machine, use the command vinfra service compute server iface list.\n--fixed-ip <fixed-ip>\n\nPort IP address (required only if the port has multiple IP addresses)\n--description <description>\n\n\nDescription of the floating IP address\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--network <network>\n\nID or name of the network from which to allocate the floating IP\n\nFor example, to create a floating IP address from the physical network public and assign it to a virtual machine on port with the ID 418c8c9e-aaa5-42f2-8da7-24bfead6f28b and the virtual IP address 192.168.128.5, run:# vinfra service compute floatingip create public --port-id 418c8c9e-aaa5-42f2-8da7-24bfead6f28b \\\r\n--fixed-ip-address 192.168.128.5\r\n+---------------------+--------------------------------------+\r\n| Field               | Value                                |\r\n+---------------------+--------------------------------------+\r\n| attached_to         | a172cb6a-1c7b-4157-9e86-035f3077646f |\r\n| description         |                                      |\r\n| fixed_ip_address    | 192.168.128.5                        |\r\n| floating_ip_address | 10.94.129.72                         |\r\n| floating_network_id | 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                  | a709f884-c43f-4a9a-a243-a340d7682ef8 |\r\n| port_id             | 418c8c9e-aaa5-42f2-8da7-24bfead6f28b |\r\n| project_id          | 894696133031439f8aaa7e4868dcbd4d     |\r\n| router_id           | f7f86029-a553-4d61-b7ec-6f581d9c5f5f |\r\n| status              | DOWN                                 |\r\n+---------------------+--------------------------------------+\r\n\nThe created floating IP address will appear in the vinfra service compute floatingip list output:# vinfra service compute floatingip list -c id -c fixed_ip_address -c port_id -c floating_ip_address\r\n+----------------+------------------+----------------+---------------------+\r\n| id             | fixed_ip_address | port_id        | floating_ip_address |\r\n+----------------+------------------+----------------+---------------------+\r\n| a709f884-<...> | 192.168.128.5    | 418c8c9e-<...> | 10.94.129.72        |\r\n+----------------+------------------+----------------+---------------------+\n",
                "title": "To create a floating IP address and assign it to a virtual machine"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute floatingip set [--port-id <port-id>] [--fixed-ip <fixed-ip>]\r\n                                      [--description <description>] <floating-ip>\r\n\n\n--port-id <port-id>\n\nID of the port to be associated with the floating IP address\n--fixed-ip <fixed-ip>\n\nPort IP address (required only if the port has multiple IP addresses)\n--description <description>\n\nDescription of the floating IP address\n<floating-ip>\n\nID of the floating IP address\n\nFor example, to assign the floating IP address with the ID a709f884-c43f-4a9a-a243-a340d7682ef8 to a virtual machine on port with the ID 8c11c29b-9a73-4017-baff-1e872b18b54b and the virtual IP address 192.128.30.15, run:# vinfra service compute floatingip set a709f884-c43f-4a9a-a243-a340d7682ef8 \\\r\n--port-id 8c11c29b-9a73-4017-baff-1e872b18b54b --fixed-ip-address 192.128.30.15\r\n+---------------------+--------------------------------------+\r\n| Field               | Value                                |\r\n+---------------------+--------------------------------------+\r\n| attached_to         | 3a092f6f-bbaf-47a9-bcc7-f86223aacb55 |\r\n| description         |                                      |\r\n| fixed_ip_address    | 192.128.30.15                        |\r\n| floating_ip_address | 10.94.129.72                         |\r\n| floating_network_id | 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                  | a709f884-c43f-4a9a-a243-a340d7682ef8 |\r\n| port_id             | 8c11c29b-9a73-4017-baff-1e872b18b54b |\r\n| project_id          | 894696133031439f8aaa7e4868dcbd4d     |\r\n| router_id           | f7f86029-a553-4d61-b7ec-6f581d9c5f5f |\r\n| status              | ACTIVE                               |\r\n+---------------------+--------------------------------------+\r\n\n",
                "title": "To re-assign a floating IP address to another virtual machine"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute floatingip delete <floating-ip>\r\n\n\n<floating-ip>\n\nID of the floating IP address\n\nFor example, to delete the floating IP address with the ID a709f884-c43f-4a9a-a243-a340d7682ef8, run:# vinfra service compute floatingip delete a709f884-c43f-4a9a-a243-a340d7682ef8\r\nOperation successful\r\n\n",
                "title": "To remove a floating IP address"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Floating IPs tab, click Add floating IP.\n\nIn the Add floating IP address, select a physical network, from which a floating IP will be picked, and a VM network interface with a fixed private IP address.\n\n\n\n\n\nClick Add.\n\n",
                "title": "To create a floating IP address and assign it to a virtual machine"
            },
            {
                "example": "\nAdmin panel\n\nClick the ellipsis icon next to the floating IP address, and then click Unassign.\nOnce the VM name disappears in the Assigned to column, click the ellipsis icon again, and then select Assign.\nIn the Assign floating IP address window, select a VM network interface with a fixed private IP address.\nClick Assign.\n\n",
                "title": "To re-assign a floating IP address to another virtual machine"
            },
            {
                "example": "\nAdmin panel\n\nUnassign it from a virtual machine. Click the ellipsis icon next to the floating IP address, and then click Unassign.\nClick the ellipsis icon again, and then select Delete.\n\n",
                "title": "To remove a floating IP address"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-floating-ip-addresses.html"
    },
    {
        "title": "Managing images",
        "content": "Managing images\nImages  stored in the compute cluster can be downloaded to a client's machine, edited, and removed.\nLimitations\n\nImages are stored according to the default storage policy.\nWhen you install the load balancer or Kubernetes service, special images appear in the compute cluster, which are used by the system for creating service VMs. Such images are marked with the System tag and cannot be modified or deleted in the admin panel.\n\nPrerequisites\n\nImages are added to the compute cluster, as described in Uploading images for virtual machines.\n\nTo download an image\n\nAdmin panel\n\nGo to the Compute > Virtual machines > Images tab, and then click the ellipsis button next to the required image.\nClick Download image.\n\nThe image will be downloaded to the your machine.\n\nCommand-line interface\nUse the following command:vinfra service compute image save [--file <filename>] <image>\r\n\n\n--file <filename>\n\nFile to save the image to (default: stdout)\n<image>\n\nImage ID or name\n\nFor example, to download the image cirros to the local disk as cirros.qcow2, run:# vinfra service compute image save cirros --file cirros.qcow2\n\nTo edit an image\n\nAdmin panel\n\nGo to the Compute > Virtual machines > Images tab, and then click the required image.\nOn the image right pane, click the pencil icon next to a parameter you need to change. You can change the image name, OS type, and network access. For templates, you can also edit the minimum volume size.\n\nCommand-line interface\nUse the following command:vinfra service compute image set [--min-disk <size-gb>] [--min-ram <size-mb>] [--os-distro <os-distro>]\r\n                                 [--protected | --unprotected] [--public] [--private] [--name <name>] <image>\r\n\n\n--min-disk <size-gb>\n\nMinimum disk size required to boot from image, in gigabytes\n--min-ram <size-mb>\n\nMinimum RAM size required to boot from image, in megabytes\n--os-distro <os-distro>\n\nOS distribution. To list available distributions, run vinfra service compute cluster show.\n--protected\n\nProtect image from deletion.\n--unprotected\n\nAllow image to be deleted.\n--public\n\nMake image accessible to all users.\n--private\n\nMake image accessible only to the owners.\n--name <name>\n\nImage name\n<image>\n\nImage ID or name\n\nFor example, to make the image cirros accessible to all users and set the minimum RAM size for it to 1 GB, run:# vinfra service compute image set cirros --public --min-ram 1\n\nTo remove an image\n\nAdmin panel\n\nGo to the Compute > Virtual machines > Images tab, and then click the ellipsis button next to the required image.\nClick Delete.\n\nCommand-line interface\nUse the following command:vinfra service compute image delete <image>\r\n\n\n<image>\n\nImage ID or name\n\nFor example, to delete the image cirros, run:# vinfra service compute image delete cirros\r\n",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute image save [--file <filename>] <image>\r\n\n\n--file <filename>\n\nFile to save the image to (default: stdout)\n<image>\n\nImage ID or name\n\nFor example, to download the image cirros to the local disk as cirros.qcow2, run:# vinfra service compute image save cirros --file cirros.qcow2\n",
                "title": "To download an image"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute image set [--min-disk <size-gb>] [--min-ram <size-mb>] [--os-distro <os-distro>]\r\n                                 [--protected | --unprotected] [--public] [--private] [--name <name>] <image>\r\n\n\n--min-disk <size-gb>\n\nMinimum disk size required to boot from image, in gigabytes\n--min-ram <size-mb>\n\nMinimum RAM size required to boot from image, in megabytes\n--os-distro <os-distro>\n\nOS distribution. To list available distributions, run vinfra service compute cluster show.\n--protected\n\nProtect image from deletion.\n--unprotected\n\nAllow image to be deleted.\n--public\n\nMake image accessible to all users.\n--private\n\nMake image accessible only to the owners.\n--name <name>\n\nImage name\n<image>\n\nImage ID or name\n\nFor example, to make the image cirros accessible to all users and set the minimum RAM size for it to 1 GB, run:# vinfra service compute image set cirros --public --min-ram 1\n",
                "title": "To edit an image"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute image delete <image>\r\n\n\n<image>\n\nImage ID or name\n\nFor example, to delete the image cirros, run:# vinfra service compute image delete cirros\r\n\n",
                "title": "To remove an image"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Compute > Virtual machines > Images tab, and then click the ellipsis button next to the required image.\nClick Download image.\n\nThe image will be downloaded to the your machine.\n",
                "title": "To download an image"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Compute > Virtual machines > Images tab, and then click the required image.\nOn the image right pane, click the pencil icon next to a parameter you need to change. You can change the image name, OS type, and network access. For templates, you can also edit the minimum volume size.\n\n",
                "title": "To edit an image"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Compute > Virtual machines > Images tab, and then click the ellipsis button next to the required image.\nClick Delete.\n\n",
                "title": "To remove an image"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-images.html"
    },
    {
        "title": "Managing infrastructure networks",
        "content": "Managing infrastructure networks\nIn Virtuozzo Hybrid Infrastructure, you can manage infrastructure networks and assign different types of traffic to them. In addition, you can change nodes' IP addresses and network configuration if the cluster is moved to another location or the network topology needs to be modified.\nPrerequisites\n\nYour infrastructure networks are set up, as described in Setting up networks.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-infrastructure-networks.html"
    },
    {
        "title": "Managing NFS shares",
        "content": "Managing NFS shares\nOnce an NFS share is created, it is started automatically. You can modify its size and redundancy scheme, stop it to change its IP address, and finally delete it.\nLimitations\n\nAfter creating an NFS share, you can change only the replication redundancy scheme. Changing the encoding redundancy scheme is disabled, because it may decrease cluster performance. Re-encoding demands a significant amount of cluster resources for a long period of time. If you still want to change the redundancy scheme, contact the technical support team.\nYou can change the IP address only for stopped NFS shares.\n\nPrerequisites\n\nNFS shares are created, as described in Creating NFS shares.\n\nTo stop an NFS share\n\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nClick the line with a running share, and then click Stop on the right pane.\n\nCommand-line interface\nUse the following command:vinfra service nfs share stop [--force] <name>\n\n--force\n\nStop the NFS share forcibly\n<name>\n\nNFS share name\n\nFor example, to stop the share share1, run:# vinfra service nfs share stop share1\n\nTo restart an NFS share\n\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nClick the line with a stopped share, and then click Run on the right pane.\n\nCommand-line interface\nUse the following command:vinfra service nfs share start <name>\n\n<name>\n\nNFS share name\n\nFor example, to start the share share1, run:# vinfra service nfs share start share1\n\nTo reconfigure an NFS share\n\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nIf you want to change the IP address of a share, stop the share first. To do it, click the line with the share, and then click Stop on the right pane.\nTo change the share parameters, click Edit on the right pane.\nIn the Edit window, change the IP address, size, or replication redundancy parameters, and then click Save. \n\nCommand-line interface\nUse the following command:vinfra service nfs share set [--tier {0,1,2,3}] [--replicas <norm>] [--failure-domain {0,1,2,3,4}]\r\n                             [--size <size>] <name>\n\n--tier {0,1,2,3}\n\nStorage tier\n--replicas <norm>\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--size <size>\n\nNFS share size, in bytes. You can also specify the following units: KiB for kibibytes, MiB for mebibytes, GiB for gibibytes, TiB for tebibytes, and PiB for pebibytes.\n<name>\n\nNFS share name\n\nFor example, to increase the size of the share share1 to 200 GiB, run:# vinfra service nfs share set share1 --size 200GiB\n\nTo delete an NFS share\n\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nClick the line with a share to remove, and then click Delete on the right pane.\nIf a share has exports, it can only be deleted after deleting all of them. To delete such a share, select Delete all exports in this share, and then click Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service nfs share delete <name>\n\n<name>\n\nNFS share name\n\nFor example, to delete the share share1, run:# vinfra service nfs share delete share1\n\nSee also\n\nAuthenticating NFS share users via Kerberos\n\nManaging NFS exports\n\nMonitoring file storage",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs share stop [--force] <name>\n\n--force\n\nStop the NFS share forcibly\n<name>\n\nNFS share name\n\nFor example, to stop the share share1, run:# vinfra service nfs share stop share1\n",
                "title": "To stop an NFS share"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs share start <name>\n\n<name>\n\nNFS share name\n\nFor example, to start the share share1, run:# vinfra service nfs share start share1\n",
                "title": "To restart an NFS share"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs share set [--tier {0,1,2,3}] [--replicas <norm>] [--failure-domain {0,1,2,3,4}]\r\n                             [--size <size>] <name>\n\n--tier {0,1,2,3}\n\nStorage tier\n--replicas <norm>\n\n\nStorage replication mapping in the format:\n\nnorm: the number of replicas to maintain\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--size <size>\n\nNFS share size, in bytes. You can also specify the following units: KiB for kibibytes, MiB for mebibytes, GiB for gibibytes, TiB for tebibytes, and PiB for pebibytes.\n<name>\n\nNFS share name\n\nFor example, to increase the size of the share share1 to 200 GiB, run:# vinfra service nfs share set share1 --size 200GiB\n",
                "title": "To reconfigure an NFS share"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs share delete <name>\n\n<name>\n\nNFS share name\n\nFor example, to delete the share share1, run:# vinfra service nfs share delete share1\n",
                "title": "To delete an NFS share"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nClick the line with a running share, and then click Stop on the right pane.\n\n",
                "title": "To stop an NFS share"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nClick the line with a stopped share, and then click Run on the right pane.\n\n",
                "title": "To restart an NFS share"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nIf you want to change the IP address of a share, stop the share first. To do it, click the line with the share, and then click Stop on the right pane.\nTo change the share parameters, click Edit on the right pane.\nIn the Edit window, change the IP address, size, or replication redundancy parameters, and then click Save. \n\n",
                "title": "To reconfigure an NFS share"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab.\nClick the line with a share to remove, and then click Delete on the right pane.\nIf a share has exports, it can only be deleted after deleting all of them. To delete such a share, select Delete all exports in this share, and then click Delete in the confirmation window.\n\n",
                "title": "To delete an NFS share"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-nfs-shares.html"
    },
    {
        "title": "Managing infrastructure nodes",
        "content": "Managing infrastructure nodes\nThis section describes how to manage network interfaces and change their parameters, as well as connect remote iSCSI devices. ",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-infrastructure-nodes.html"
    },
    {
        "title": "Managing network interfaces",
        "content": "Managing network interfaces\nAfter configuring your node network interfaces, you can bring them up and down, as well as delete logical interfaces, bonds and VLANs.\nPrerequisites\n\nA network interface is configured, as described in Configuring node network interfaces.\n\nLimitations\n\nOpen vSwitch-based bridge interfaces cannot be brought down or deleted after creation.\nYou can only delete bonded and VLAN interfaces.\n\nTo bring a network interface up\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface with the Disabled status. \nOn the interface right pane, click Bring up.\n\nCommand-line interface\nUse the following command:vinfra node iface up [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname\n<iface>\n\nNetwork interface name\n\nFor example, to bring up the network interface eth2 located on the node node003, run:# vinfra node iface up eth2 --node node003\n\nTo bring a network interface down\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface with the Connected status. \nOn the interface right pane, click Bring down.\nClick Bring down in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra node iface down [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname\n<iface>\n\nNetwork interface name\n\nFor example, to bring down the network interface eth2 located on the node node003, run:# vinfra node iface down eth2 --node node003\n\nTo delete a network interface\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the logical network interface that you want to delete. \nOn the interface right pane, click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra node iface delete [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname\n<iface>\n\nNetwork interface name\n\nFor example, to delete the network interface eth2.100 located on the node node003, run:# vinfra node iface delete eth2.100 --node node003\n\nSee also\n\nChanging network interface parameters\n\nChanging network configuration",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface up [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname\n<iface>\n\nNetwork interface name\n\nFor example, to bring up the network interface eth2 located on the node node003, run:# vinfra node iface up eth2 --node node003\n",
                "title": "To bring a network interface up"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface down [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname\n<iface>\n\nNetwork interface name\n\nFor example, to bring down the network interface eth2 located on the node node003, run:# vinfra node iface down eth2 --node node003\n",
                "title": "To bring a network interface down"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface delete [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname\n<iface>\n\nNetwork interface name\n\nFor example, to delete the network interface eth2.100 located on the node node003, run:# vinfra node iface delete eth2.100 --node node003\n",
                "title": "To delete a network interface"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface with the Disabled status. \nOn the interface right pane, click Bring up.\n\n",
                "title": "To bring a network interface up"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the network interface with the Connected status. \nOn the interface right pane, click Bring down.\nClick Bring down in the confirmation window.\n\n",
                "title": "To bring a network interface down"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node, go to the Network interfaces tab, and then click the logical network interface that you want to delete. \nOn the interface right pane, click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a network interface"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-network-interfaces.html"
    },
    {
        "title": "Managing object storage",
        "content": "Managing object storage\nThis section outlines common administrator's tasks for S3 clusters, users, and buckets. It also provides the best practices for using S3 in Virtuozzo Hybrid Infrastructure and lists the supported Amazon S3 features. In addition, the section explains how to configure S3 geo-replication or cross-region replication between datacenters.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-object-storage.html"
    },
    {
        "title": "Managing licenses",
        "content": "Managing licenses\nVirtuozzo Hybrid Infrastructure comes with a trial license that allows you to evaluate its features. The trial license has no expiration date but limits storage capacity to 1TB.\nVirtuozzo Hybrid Infrastructure uses license keys as a licensing model for production environments. Implementing the provisioning model, keys are time-limited (subscription) or perpetual and grant a certain storage capacity. If a commercial license is already installed, a key augments its expiration date or storage limit.\n\nIf a license expires, all write operations to the storage cluster stop until a valid license is installed.\n\nLimitations\n\nBlock storage space used by iSCSI LUNs and compute volumes is not fully thin provisioned. After user data removal, unused storage space is not reclaimed and is reported as actual used space, which is charged according to your licensing model. For more details, refer to Logical space chart.\n\nPrerequisites\n\nThe storage cluster is created by following the instructions in Deploying the storage cluster.\n\nTo install a license key\n\nAdmin panel\n\nOn the Settings > License screen, click Register key.\n\nIn the Register key window, paste the license key, and then click Activate.\n\nThe expiration date or storage capacity will change according to what the key grants.\n\nCommand-line interface\nUse the following command:vinfra cluster license load <license-key>\n\n<license-key>\n\nLicense key to register.\n\nFor example, to install the license from the key A38600-3P6W74-RZSK58-Y9ZH05-2X7J48, run:# vinfra cluster license load A38600-3P6W74-RZSK58-Y9ZH05-2X7J48\nYou can view the details of the currently installed license in the vinfra cluster license show output:# vinfra cluster license show\r\n+---------------+------------------------+\r\n| Field         | Value                  |\r\n+---------------+------------------------+\r\n| capacity      | 7036767043584000       |\r\n| expiration_ts | 1549583999             |\r\n| free_size     | 7036766991361913       |\r\n| keynumber     | VZSTOR.74418710.0000   |\r\n| spla          | registered: false      |\r\n|               | registration_url: null |\r\n| status        | active                 |\r\n| total_size    | 7036767043584000       |\r\n| used_size     | 52222087               |\r\n+---------------+------------------------+",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster license load <license-key>\n\n<license-key>\n\nLicense key to register.\n\nFor example, to install the license from the key A38600-3P6W74-RZSK58-Y9ZH05-2X7J48, run:# vinfra cluster license load A38600-3P6W74-RZSK58-Y9ZH05-2X7J48\nYou can view the details of the currently installed license in the vinfra cluster license show output:# vinfra cluster license show\r\n+---------------+------------------------+\r\n| Field         | Value                  |\r\n+---------------+------------------------+\r\n| capacity      | 7036767043584000       |\r\n| expiration_ts | 1549583999             |\r\n| free_size     | 7036766991361913       |\r\n| keynumber     | VZSTOR.74418710.0000   |\r\n| spla          | registered: false      |\r\n|               | registration_url: null |\r\n| status        | active                 |\r\n| total_size    | 7036767043584000       |\r\n| used_size     | 52222087               |\r\n+---------------+------------------------+\n",
                "title": "To install a license key"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOn the Settings > License screen, click Register key.\n\n\n\n\nIn the Register key window, paste the license key, and then click Activate.\n\n\n\n\nThe expiration date or storage capacity will change according to what the key grants.\n",
                "title": "To install a license key"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-licenses.html"
    },
    {
        "title": "Managing networks",
        "content": "Managing networks\nYou can create networks, view network details, edit, and delete them.\nLimitations\n\nIf you create allow rules but leave the deny list empty, all incoming traffic will still be allowed.\nAn infrastructure network cannot be renamed if it is used by a compute virtual network.\nYou can only delete networks that are not assigned to any network adapters. \n\nTo create a network\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Create network.\nIn the New network window, specify a network name. Network names may contain only Latin letters, numbers, and underscores, and must be 3 to 32 characters long.\n\nIn the Access rules section, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra cluster network create [--traffic-types <traffic-types>] [--inbound-allow-list <addresses>]\r\n                              [--inbound-deny-list <addresses>] [--outbound-allow-list <rules>]\r\n                              <network-name>\r\n\n\n--traffic-types <traffic-types>\n\nA comma-separated list of traffic type IDs or names\n--inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses\n--outbound-allow-list <rules>\n\nA comma-separated list of allow rules in the format: <address>:<protocol>:<port>:<description>\n<network-name>\n\nNetwork name\n\nFor example, to create a custom network MyNet and assign the traffic type SSH to it, run:# vinfra cluster network create MyNet --traffic-types ssh\r\n+---------------------+------------------------------------------+\r\n| Field               | Value                                    |\r\n+---------------------+------------------------------------------+\r\n| id                  | b451c5ed-a553-4214-96c4-d926daa6110e     |\r\n| inbound_allow_list  | []                                       |\r\n| inbound_deny_list   | []                                       |\r\n| name                | MyNet                                    |\r\n| outbound_allow_list | - 0.0.0.0:tcp:8888:Internal management   |\r\n|                     | - 0.0.0.0:tcp:80:HTTP                    |\r\n|                     | - 0.0.0.0:tcp:443:HTTPS                  |\r\n|                     | - 0.0.0.0:udp:53:DNS                     |\r\n|                     | - 0.0.0.0:tcp:53:DNS                     |\r\n|                     | - 0.0.0.0:udp:123:NTP                    |\r\n|                     | - 0.0.0.0:tcp:8443:ABGW registration     |\r\n|                     | - 0.0.0.0:tcp:44445:ABGW Geo-replication |\r\n|                     | - 0.0.0.0:tcp:9877:Acronis Cyber Protect |\r\n|                     | - 0.0.0.0:any:0:Allow all                |\r\n| name                | MyNet                                    |\r\n| traffic_types       | SSH                                      |\r\n| vlan                |                                          |\r\n+---------------------+------------------------------------------+\r\n\n\nTo view network details\nClick the cogwheel icon next to the network name. In the network summary window, the following information is available:\n\nThe General section includes the network CIDR and subnet mask.\nThe Connected interfaces section shows the nodes\u00e2\u0080\u0099 network interfaces with their IP addresses.\n\nTo rename a network\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Edit.\nIn the Edit window, enter a new name, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra cluster network set [--name <network-name>] <network>\r\n\n\n--name <network-name>\n\nNetwork name\n<network>\n\nNetwork ID or name\n\nFor example, to rename the network MyNet to MyOtherNet, run:# vinfra cluster network set MyNet --name MyOtherNet\r\n\n\nTo delete a network\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Delete.\nIn the Delete network window, confirm your action by clicking Delete.\n\nCommand-line interface\nUse the following command:vinfra cluster network delete <network>\r\n\n\n<network>\n\nNetwork ID or name\n\nFor example, to delete the network MyOtherNet, run:# vinfra cluster network delete MyOtherNet\n\nSee also\n\nChanging network configuration\n\nConfiguring inbound firewall rules\n\nConfiguring outbound firewall rules\n\nConfiguring data-in-transit encryption\n\nManaging traffic types",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network create [--traffic-types <traffic-types>] [--inbound-allow-list <addresses>]\r\n                              [--inbound-deny-list <addresses>] [--outbound-allow-list <rules>]\r\n                              <network-name>\r\n\n\n--traffic-types <traffic-types>\n\nA comma-separated list of traffic type IDs or names\n--inbound-allow-list <addresses>\n\nA comma-separated list of IP addresses\n--inbound-deny-list <addresses>\n\nA comma-separated list of IP addresses\n--outbound-allow-list <rules>\n\nA comma-separated list of allow rules in the format: <address>:<protocol>:<port>:<description>\n<network-name>\n\nNetwork name\n\nFor example, to create a custom network MyNet and assign the traffic type SSH to it, run:# vinfra cluster network create MyNet --traffic-types ssh\r\n+---------------------+------------------------------------------+\r\n| Field               | Value                                    |\r\n+---------------------+------------------------------------------+\r\n| id                  | b451c5ed-a553-4214-96c4-d926daa6110e     |\r\n| inbound_allow_list  | []                                       |\r\n| inbound_deny_list   | []                                       |\r\n| name                | MyNet                                    |\r\n| outbound_allow_list | - 0.0.0.0:tcp:8888:Internal management   |\r\n|                     | - 0.0.0.0:tcp:80:HTTP                    |\r\n|                     | - 0.0.0.0:tcp:443:HTTPS                  |\r\n|                     | - 0.0.0.0:udp:53:DNS                     |\r\n|                     | - 0.0.0.0:tcp:53:DNS                     |\r\n|                     | - 0.0.0.0:udp:123:NTP                    |\r\n|                     | - 0.0.0.0:tcp:8443:ABGW registration     |\r\n|                     | - 0.0.0.0:tcp:44445:ABGW Geo-replication |\r\n|                     | - 0.0.0.0:tcp:9877:Acronis Cyber Protect |\r\n|                     | - 0.0.0.0:any:0:Allow all                |\r\n| name                | MyNet                                    |\r\n| traffic_types       | SSH                                      |\r\n| vlan                |                                          |\r\n+---------------------+------------------------------------------+\r\n\n",
                "title": "To create a network"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network set [--name <network-name>] <network>\r\n\n\n--name <network-name>\n\nNetwork name\n<network>\n\nNetwork ID or name\n\nFor example, to rename the network MyNet to MyOtherNet, run:# vinfra cluster network set MyNet --name MyOtherNet\r\n\n",
                "title": "To rename a network"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network delete <network>\r\n\n\n<network>\n\nNetwork ID or name\n\nFor example, to delete the network MyOtherNet, run:# vinfra cluster network delete MyOtherNet\n",
                "title": "To delete a network"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Create network.\nIn the New network window, specify a network name. Network names may contain only Latin letters, numbers, and underscores, and must be 3 to 32 characters long.\n\nIn the Access rules section, do the following:\n\nTo block traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Deny list section.\nTo allow traffic from particular IP addresses, IP address ranges, or subnets, specify them in the Allow list section. Additionally, specify 0.0.0.0/0 in the Deny list section, to block all other traffic.\n\n\n\n\n\n\nClick Create.\n\n",
                "title": "To create a network"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Edit.\nIn the Edit window, enter a new name, and then click Save.\n\n",
                "title": "To rename a network"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click the cogwheel icon next to the network name.\nIn the network summary window, click Delete.\nIn the Delete network window, confirm your action by clicking Delete.\n\n",
                "title": "To delete a network"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-networks.html"
    },
    {
        "title": "Managing NFS exports",
        "content": "Managing NFS exports\nYou can reconfigure and delete existing NFS exports.\nPrerequisites\n\nNFS exports are created, as described in Creating NFS exports.\n\nTo reconfigure an NFS export\n\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab, and then click the name of the desired share. This will open the share screen.\nSelect an export, and then click Edit on the right pane.\nIn the Edit export window, change the access or root squashing settings, and then click Save. \n\nCommand-line interface\nUse the following command:vinfra service nfs export set [--path <path>] [--access-type <access-type>] [--security-types <security-types>]\r\n                              [--client <address=ip_addresses:access=access_type:security=security_types>]\r\n                              [--squash <squash>] [--anonymous-gid <anonymous-gid>] [--anonymous-uid <anonymous-uid>]\r\n                              <share-name> <export-name>\n\n--path <path>\n\nPath to the NFS export\n--access-type <access-type>\n\nType of access to the NFS export (none, rw, or ro)\n--security-types <security-types>\n\nTypes of NFS export security (none, sys, krb5, krb5i, or krb5p)\n--client <address=ip_addresses:access=access_type:security=security_types>\n\nClient access list of the NFS export\n--squash <squash>\n\nNFS export squash (root_squash, root_id_squash, all_squash, or none)\n--anonymous-gid <anonymous-gid>\n\nAnonymous GID of the NFS export\n--anonymous-uid <anonymous-uid>\n\nAnonymous UID of the NFS export\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to change the access type of the export export1 from the share share1 to read-only, run:# vinfra service nfs export set share1 export1 --access-type ro\n\nTo delete an NFS export\n\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab, and then click the name of the desired share. This will open the share screen.\nSelect an export, and then click Delete on the right pane.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service nfs export delete <share-name> <export-name>\n\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to delete the export export1 from the share share1, run:# vinfra service nfs export delete share1 export1\n\nSee also\n\nManaging NFS shares\n\nMonitoring file storage\n\nAdding nodes to the NFS cluster",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs export set [--path <path>] [--access-type <access-type>] [--security-types <security-types>]\r\n                              [--client <address=ip_addresses:access=access_type:security=security_types>]\r\n                              [--squash <squash>] [--anonymous-gid <anonymous-gid>] [--anonymous-uid <anonymous-uid>]\r\n                              <share-name> <export-name>\n\n--path <path>\n\nPath to the NFS export\n--access-type <access-type>\n\nType of access to the NFS export (none, rw, or ro)\n--security-types <security-types>\n\nTypes of NFS export security (none, sys, krb5, krb5i, or krb5p)\n--client <address=ip_addresses:access=access_type:security=security_types>\n\nClient access list of the NFS export\n--squash <squash>\n\nNFS export squash (root_squash, root_id_squash, all_squash, or none)\n--anonymous-gid <anonymous-gid>\n\nAnonymous GID of the NFS export\n--anonymous-uid <anonymous-uid>\n\nAnonymous UID of the NFS export\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to change the access type of the export export1 from the share share1 to read-only, run:# vinfra service nfs export set share1 export1 --access-type ro\n",
                "title": "To reconfigure an NFS export"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service nfs export delete <share-name> <export-name>\n\n<share-name>\n\nNFS share name\n<export-name>\n\nNFS export name\n\nFor example, to delete the export export1 from the share share1, run:# vinfra service nfs export delete share1 export1\n",
                "title": "To delete an NFS export"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab, and then click the name of the desired share. This will open the share screen.\nSelect an export, and then click Edit on the right pane.\nIn the Edit export window, change the access or root squashing settings, and then click Save. \n\n",
                "title": "To reconfigure an NFS export"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > NFS > Shares tab, and then click the name of the desired share. This will open the share screen.\nSelect an export, and then click Delete on the right pane.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete an NFS export"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-nfs-exports.html"
    },
    {
        "title": "Managing registrations for backup storage",
        "content": "Managing registrations for backup storage\nAfter deploying backup storage, you can use it as a centralized remote storage for backups by adding multiple registrations in different Acronis Cyber Protect or Acronis Cyber Protect Cloud instances.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-registrations-for-backup-storage.html"
    },
    {
        "title": "Managing load balancers",
        "content": "Managing load balancers\nLoad balancers are created and managed by self-service users, as described in \"Managing load balancers\" in the Self-Service Guide. In the admin panel, you can monitor, manage balancing pools, disable/enable, and delete a load balancer. Additionally, in case of a load balancer failure, it is possible to perform a manual failover in the command-line interface.\nLimitations\n\nIPv6 is supported for load balancers that are connected to physical networks.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\nThe load balancer service is installed during the compute cluster deployment or later, as described in Provisioning load balancers.\n\nTo view load balancer properties \n\nAdmin panel\n\nOn the Compute > Network > Load balancers tab, select the required load balancer.\nOpen the Properties tab. In the Virtual machines field, you can see the name of load balancer instances.\nClick the instance name to open the VM\u00e2\u0080\u0099s panel.\n\nCommand-line interface\nUse the following command:vinfra service compute load-balancer show <load-balancer>\r\n\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to view the details of the load balancer mylbaas, run:# vinfra service compute load-balancer show mylbaas\r\n+---------------+----------------------------------------------------+\r\n| Field         | Value                                              |\r\n+---------------+----------------------------------------------------+\r\n| address       | 192.168.30.230                                     |\r\n| amphorae      | - active: true                                     |\r\n|               |   compute_id: b0c4793f-e1b1-4251-91c2-94e34787f537 |\r\n|               |   created_at: '2019-11-18T12:59:12.742446'         |\r\n|               |   id: b7b23106-a87b-412d-9ce6-7c69b5594342         |\r\n|               |   image_id: 6d1ba6f9-cf86-4ea4-a32d-f138868a9742   |\r\n|               |   role: STANDALONE                                 |\r\n|               |   status: ALLOCATED                                |\r\n|               |   updated_at: '2019-11-18T13:01:07.601184'         |\r\n| created_at    | 2019-11-18T12:59:08.243413                         |\r\n| description   |                                                    |\r\n| enabled       | True                                               |\r\n| floating_ip   | 10.94.129.70                                       |\r\n| ha_enabled    | False                                              |\r\n| id            | 941bf637-2d55-40f0-92c0-e65d6567b468               |\r\n| members_count | 0                                                  |\r\n| name          | mylbaas                                            |\r\n| network_id    | 2b821d00-e428-4a76-b1ae-d181c9f5ae7f               |\r\n| pools         | []                                                 |\r\n| port_id       | 2d8ab88a-847c-4396-857e-11eaa80e1b24               |\r\n| project_id    | e4e059c67dee4736851df14d4519a5a5                   |\r\n| status        | ACTIVE                                             |\r\n| updated_at    | 2019-11-18T13:01:10.983144                         |\r\n+---------------+----------------------------------------------------+\r\n\n\nTo enable/disable a load balancer\n\nAdmin panel\n\nOn the Compute > Network > Load balancers tab, select a load balancer.\nClick the ellipsis icon next to it, and then click Enable or Disable.\n\nCommand-line interface\nUse the following command:vinfra service compute load-balancer set [--enable | --disable] [--description <description>] <load-balancer>\n\n--enable\n\nEnable the load balancer\n--disable\n\nDisable the load balancer\n--description <description>\n\nLoad balancer description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to disable the load balancer mylbaas, run:# vinfra service compute load-balancer set mylbaas --disable \\\r\n--description \"Disabled load balancer\"\r\n+---------------+--------------------------------------+\r\n| Field         | Value                                |\r\n+---------------+--------------------------------------+\r\n| address       | 192.168.30.230                       |\r\n| amphorae      |                                      |\r\n| created_at    | 2019-11-18T12:59:08.243413           |\r\n| description   | Disabled load balancer               |\r\n| enabled       | False                                |\r\n| floating_ip   |                                      |\r\n| ha_enabled    |                                      |\r\n| id            | 941bf637-2d55-40f0-92c0-e65d6567b468 |\r\n| members_count | 0                                    |\r\n| name          | mylbaas                              |\r\n| network_id    | 2b821d00-e428-4a76-b1ae-d181c9f5ae7f |\r\n| pools         | []                                   |\r\n| port_id       | 2d8ab88a-847c-4396-857e-11eaa80e1b24 |\r\n| project_id    | e4e059c67dee4736851df14d4519a5a5     |\r\n| status        | DISABLED                             |\r\n| updated_at    | 2019-11-18T13:09:09.151442           |\r\n+---------------+--------------------------------------+\r\n\n\nTo perform a load balancer failover\nUse the following command:vinfra service compute load-balancer failover <load-balancer>\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to perform a failover of the load balancer mylbaas, run:# vinfra service compute load-balancer failover mylbaas\nTo delete a load balancer\n\nAdmin panel\n\nOn the Compute > Network > Load balancers tab, select a load balancer.\nClick the ellipsis icon next to it, and then click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service compute load-balancer delete <load-balancer>\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to delete the load balancer mylbaas, run:# vinfra service compute load-balancer delete mylbaas\n\nSee also\n\nMonitoring load balancers\n\nManaging compute networks\n\nManaging virtual routers\n\nManaging floating IP addresses",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute load-balancer show <load-balancer>\r\n\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to view the details of the load balancer mylbaas, run:# vinfra service compute load-balancer show mylbaas\r\n+---------------+----------------------------------------------------+\r\n| Field         | Value                                              |\r\n+---------------+----------------------------------------------------+\r\n| address       | 192.168.30.230                                     |\r\n| amphorae      | - active: true                                     |\r\n|               |   compute_id: b0c4793f-e1b1-4251-91c2-94e34787f537 |\r\n|               |   created_at: '2019-11-18T12:59:12.742446'         |\r\n|               |   id: b7b23106-a87b-412d-9ce6-7c69b5594342         |\r\n|               |   image_id: 6d1ba6f9-cf86-4ea4-a32d-f138868a9742   |\r\n|               |   role: STANDALONE                                 |\r\n|               |   status: ALLOCATED                                |\r\n|               |   updated_at: '2019-11-18T13:01:07.601184'         |\r\n| created_at    | 2019-11-18T12:59:08.243413                         |\r\n| description   |                                                    |\r\n| enabled       | True                                               |\r\n| floating_ip   | 10.94.129.70                                       |\r\n| ha_enabled    | False                                              |\r\n| id            | 941bf637-2d55-40f0-92c0-e65d6567b468               |\r\n| members_count | 0                                                  |\r\n| name          | mylbaas                                            |\r\n| network_id    | 2b821d00-e428-4a76-b1ae-d181c9f5ae7f               |\r\n| pools         | []                                                 |\r\n| port_id       | 2d8ab88a-847c-4396-857e-11eaa80e1b24               |\r\n| project_id    | e4e059c67dee4736851df14d4519a5a5                   |\r\n| status        | ACTIVE                                             |\r\n| updated_at    | 2019-11-18T13:01:10.983144                         |\r\n+---------------+----------------------------------------------------+\r\n\n",
                "title": "To view load balancer properties "
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute load-balancer set [--enable | --disable] [--description <description>] <load-balancer>\n\n--enable\n\nEnable the load balancer\n--disable\n\nDisable the load balancer\n--description <description>\n\n\nLoad balancer description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to disable the load balancer mylbaas, run:# vinfra service compute load-balancer set mylbaas --disable \\\r\n--description \"Disabled load balancer\"\r\n+---------------+--------------------------------------+\r\n| Field         | Value                                |\r\n+---------------+--------------------------------------+\r\n| address       | 192.168.30.230                       |\r\n| amphorae      |                                      |\r\n| created_at    | 2019-11-18T12:59:08.243413           |\r\n| description   | Disabled load balancer               |\r\n| enabled       | False                                |\r\n| floating_ip   |                                      |\r\n| ha_enabled    |                                      |\r\n| id            | 941bf637-2d55-40f0-92c0-e65d6567b468 |\r\n| members_count | 0                                    |\r\n| name          | mylbaas                              |\r\n| network_id    | 2b821d00-e428-4a76-b1ae-d181c9f5ae7f |\r\n| pools         | []                                   |\r\n| port_id       | 2d8ab88a-847c-4396-857e-11eaa80e1b24 |\r\n| project_id    | e4e059c67dee4736851df14d4519a5a5     |\r\n| status        | DISABLED                             |\r\n| updated_at    | 2019-11-18T13:09:09.151442           |\r\n+---------------+--------------------------------------+\r\n\n",
                "title": "To enable/disable a load balancer"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute load-balancer delete <load-balancer>\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to delete the load balancer mylbaas, run:# vinfra service compute load-balancer delete mylbaas\n",
                "title": "To delete a load balancer"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Load balancers tab, select the required load balancer.\nOpen the Properties tab. In the Virtual machines field, you can see the name of load balancer instances.\nClick the instance name to open the VM\u00e2\u0080\u0099s panel.\n\n",
                "title": "To view load balancer properties "
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Load balancers tab, select a load balancer.\nClick the ellipsis icon next to it, and then click Enable or Disable.\n\n",
                "title": "To enable/disable a load balancer"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Load balancers tab, select a load balancer.\nClick the ellipsis icon next to it, and then click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a load balancer"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-load-balancers.html"
    },
    {
        "title": "Managing Kubernetes clusters",
        "content": "Managing Kubernetes clusters\nKubernetes clusters are created and managed by self-service users, as described in \"Managing Kubernetes clusters\" in the Self-Service Guide. In the admin panel, you can view Kubernetes cluster details, view master and worker groups, change service parameters, update the Kubernetes version, and delete Kubernetes clusters.\nVirtuozzo Hybrid Infrastructure uses the soft anti-affinity policy for Kubernetes cluster nodes. According to this policy, Kubernetes nodes are distributed across compute nodes by groups: master nodes are distributed separately from workers. In this case, a compute node can host both a master node and a worker node. However, if there are not enough compute nodes to evenly distribute Kubernetes nodes from the same group, some of them can be placed on one compute node.\n\nFor Kubernetes service users to be able to use cluster autoscaling, the cluster must have a valid certificate issued by a trusted certificate authority, instead of a self-signed certificate.\n\nLimitations\n\nKubernetes versions 1.15.x\u00e2\u0080\u00931.21.x are no longer supported. Kubernetes clusters created with these versions are marked with the Deprecated tag.\n\nWhen a Kubernetes cluster is created, its configuration files contain the IP address or DNS name of the compute API endpoint. Modifying this IP address or DNS name will lead to inability to perform Kubernetes management operations. You can have one of the following scenarios:\n\nIf the high availability for the management node is disabled\n\nThe compute API is accessed via the IP address of the management node. In this case, changing this IP address or creating the management node HA is prohibited.\n\nIf the high availability for the management node is enabled\n\nThe compute API is accessed via the virtual IP address. In this case, changing this virtual IP address or destroying the management node HA is prohibited.\n\nIf a DNS name for the compute API is configured\n\nChanging the DNS name is prohibited.\n\nKubernetes cluster certificates are issued for five years. To renew the certificates, use the vinfra service compute k8saas rotate-ca command. Alternatively, you can use the openstack coe ca rotate command, as described in the OpenStack documentation.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\nThe Kubernetes service is installed during the compute cluster deployment or later, as described in Provisioning Kubernetes clusters.\n\nTo view the details of a Kubernetes cluster\n\nAdmin panel\nOn the Compute > Kubernetes screen, click a Kubernetes cluster to open its right pane. \n\nCommand-line interface\nUse the following command:vinfra service compute k8saas show <cluster>\r\n\n\n<cluster>\n\nCluster ID or name\n\nFor example, to view the details of the Kubernetes cluster k8s1, run:# vinfra service compute k8saas show k8s1\r\n+----------------------------------+--------------------------------------------+\r\n| Field                            | Value                                      |\r\n+----------------------------------+--------------------------------------------+\r\n| action_status                    | CREATE_COMPLETE                            |\r\n| boot_volume_size                 | 10                                         |\r\n| boot_volume_storage_policy       | default                                    |\r\n| containers_volume_size           | 10                                         |\r\n| containers_volume_storage_policy | default                                    |\r\n| create_timeout                   | 60                                         |\r\n| external_network_id              | 10cc4d59-adac-4ec1-8e0a-df5015b82c64       |\r\n| id                               | 749737ae-2452-4a98-a057-b59b1c579a85       |\r\n| key_name                         | key1                                       |\r\n| master_flavor                    | medium                                     |\r\n| master_node_count                | 1                                          |\r\n| name                             | k8s1                                       |\r\n| network_id                       | d037623b-0db7-40c2-b38a-9ac34fbd1cc5       |\r\n| nodegroups                       | - action_status: CREATE_COMPLETE           |\r\n|                                  |   flavor: medium                           |\r\n|                                  |   id: c3b4ec41-b8c1-4dae-9e1c-aa586b99a62c |\r\n|                                  |   is_default: true                         |\r\n|                                  |   name: default-master                     |\r\n|                                  |   node_count: 1                            |\r\n|                                  |   role: master                             |\r\n|                                  |   status: ACTIVE                           |\r\n|                                  |   version: v1.22.2                         |\r\n|                                  | - action_status: CREATE_COMPLETE           |\r\n|                                  |   flavor: small                            |\r\n|                                  |   id: 65b80f19-0920-48b7-84e0-d0c63c46e99f |\r\n|                                  |   is_default: true                         |\r\n|                                  |   name: default-worker                     |\r\n|                                  |   node_count: 3                            |\r\n|                                  |   role: worker                             |\r\n|                                  |   status: ACTIVE                           |\r\n|                                  |   version: v1.22.2                         |\r\n| project_id                       | d8a72d59539c431381989af6cb48b05d           |\r\n| status                           | ACTIVE                                     |\r\n| user_id                          | 5846f988280f42199ed030a22970d48e           |\r\n| worker_pools                     | - flavor: small                            |\r\n|                                  |   node_count: 3                            |\r\n+----------------------------------+--------------------------------------------+\r\n\n\nTo view master and worker groups\n\nOn the Compute > Kubernetes screen, click a Kubernetes cluster.\n On the cluster right pane, navigate to the Groups tab.\nList all of the nodes in a group by clicking the arrow icon next to the required node group.\n\nTo renew the Kubernetes cluster certificates\nUse the following command:vinfra service compute k8saas rotate-ca <cluster>\r\n\n\n<cluster>\n\nCluster ID or name\n\nFor example, to renew the CA certificates for the Kubernetes cluster k8s1, run:# vinfra service compute k8saas rotate-ca k8s1\nTo delete a Kubernetes cluster\n\nAdmin panel\n\nOn the Compute > Kubernetes screen, click a Kubernetes cluster.\n On the cluster right pane, click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service compute k8saas delete <cluster>\r\n\n\n<cluster>\n\nCluster ID or name\n\nFor example, to delete the Kubernetes cluster k8s1, run:# vinfra service compute k8saas delete k8s1",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute k8saas show <cluster>\r\n\n\n<cluster>\n\nCluster ID or name\n\nFor example, to view the details of the Kubernetes cluster k8s1, run:# vinfra service compute k8saas show k8s1\r\n+----------------------------------+--------------------------------------------+\r\n| Field                            | Value                                      |\r\n+----------------------------------+--------------------------------------------+\r\n| action_status                    | CREATE_COMPLETE                            |\r\n| boot_volume_size                 | 10                                         |\r\n| boot_volume_storage_policy       | default                                    |\r\n| containers_volume_size           | 10                                         |\r\n| containers_volume_storage_policy | default                                    |\r\n| create_timeout                   | 60                                         |\r\n| external_network_id              | 10cc4d59-adac-4ec1-8e0a-df5015b82c64       |\r\n| id                               | 749737ae-2452-4a98-a057-b59b1c579a85       |\r\n| key_name                         | key1                                       |\r\n| master_flavor                    | medium                                     |\r\n| master_node_count                | 1                                          |\r\n| name                             | k8s1                                       |\r\n| network_id                       | d037623b-0db7-40c2-b38a-9ac34fbd1cc5       |\r\n| nodegroups                       | - action_status: CREATE_COMPLETE           |\r\n|                                  |   flavor: medium                           |\r\n|                                  |   id: c3b4ec41-b8c1-4dae-9e1c-aa586b99a62c |\r\n|                                  |   is_default: true                         |\r\n|                                  |   name: default-master                     |\r\n|                                  |   node_count: 1                            |\r\n|                                  |   role: master                             |\r\n|                                  |   status: ACTIVE                           |\r\n|                                  |   version: v1.22.2                         |\r\n|                                  | - action_status: CREATE_COMPLETE           |\r\n|                                  |   flavor: small                            |\r\n|                                  |   id: 65b80f19-0920-48b7-84e0-d0c63c46e99f |\r\n|                                  |   is_default: true                         |\r\n|                                  |   name: default-worker                     |\r\n|                                  |   node_count: 3                            |\r\n|                                  |   role: worker                             |\r\n|                                  |   status: ACTIVE                           |\r\n|                                  |   version: v1.22.2                         |\r\n| project_id                       | d8a72d59539c431381989af6cb48b05d           |\r\n| status                           | ACTIVE                                     |\r\n| user_id                          | 5846f988280f42199ed030a22970d48e           |\r\n| worker_pools                     | - flavor: small                            |\r\n|                                  |   node_count: 3                            |\r\n+----------------------------------+--------------------------------------------+\r\n\n",
                "title": "To view the details of a Kubernetes cluster"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute k8saas delete <cluster>\r\n\n\n<cluster>\n\nCluster ID or name\n\nFor example, to delete the Kubernetes cluster k8s1, run:# vinfra service compute k8saas delete k8s1\n",
                "title": "To delete a Kubernetes cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nOn the Compute > Kubernetes screen, click a Kubernetes cluster to open its right pane. \n",
                "title": "To view the details of a Kubernetes cluster"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Kubernetes screen, click a Kubernetes cluster.\n On the cluster right pane, click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a Kubernetes cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-kubernetes-clusters.html"
    },
    {
        "title": "Managing project assignment to domain groups",
        "content": "Managing project assignment to domain groups\nOnce you create a domain group with the role Project member, you can assign projects to it. Projects that you assign to a domain group will be automatically assigned to all of the domain group users.\nPrerequisites\n\nDomain groups are created, as described in Creating domain groups.\nProjects are created, as described in Configuring multitenancy.\n\nTo manage projects of a domain group\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a domain group.\nGo to the Domain groups tab, click the ellipsis icon next to the group with the Project member role, and then click Manage projects.\nIn the Manage projects window, select projects to assign to the group or to unassign from the group, and then click Assign.\n\nCommand-line interface\nUse the following command:vinfra domain group set [--assign <project> <role>] [--unassign <project>] --domain <domain> <group>\r\n\n\n--assign <project> <role>\n\nAssign a group to a project with one or more permission sets. Specify this option multiple times to assign the group to multiple projects.\n\n<project>: project ID or name\n<role>: group role in the project (project_admin)\n\n--unassign <project>\n\nUnassign a group from a project. Specify this option multiple times to unassign the group from multiple projects.\n\n<project>: project ID or name\n\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n\nFor example, to assign the group myusers within the domain mydomain to the project myproject as a project administrator, run:# vinfra domain group set myusers --domain mydomain --assign myproject project_admin\n\nSee also\n\nManaging user assignment to domain groups\n\nEditing and deleting domain groups",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain group set [--assign <project> <role>] [--unassign <project>] --domain <domain> <group>\r\n\n\n--assign <project> <role>\n\n\nAssign a group to a project with one or more permission sets. Specify this option multiple times to assign the group to multiple projects.\n\n<project>: project ID or name\n<role>: group role in the project (project_admin)\n\n\n--unassign <project>\n\n\nUnassign a group from a project. Specify this option multiple times to unassign the group from multiple projects.\n\n<project>: project ID or name\n\n\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n\nFor example, to assign the group myusers within the domain mydomain to the project myproject as a project administrator, run:# vinfra domain group set myusers --domain mydomain --assign myproject project_admin\n",
                "title": "To manage projects of a domain group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a domain group.\nGo to the Domain groups tab, click the ellipsis icon next to the group with the Project member role, and then click Manage projects.\nIn the Manage projects window, select projects to assign to the group or to unassign from the group, and then click Assign.\n\n",
                "title": "To manage projects of a domain group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-project-assignment-to-domain-groups.html"
    },
    {
        "title": "Managing projects",
        "content": "Managing projects\nYou can add more projects, as described in Configuring multitenancy. Also, you can edit project details and quotas, manage user assignment, as well as enable/disable and delete the existing projects. Enabling and disabling projects allows or prohibits access to projects in the self-service panel.\nLimitations\n\nA project cannot be deleted if it has virtual objects.\n\nTo edit a project name or description\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nCommand-line interface\nUse the following command:vinfra domain project set [--description <description>] [--name <name>] --domain <domain> <project>\r\n\n\n--description <description>\n\nProject description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--name <name>\n\nProject name\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to change the name of the project myproject within the domain mydomain to newproject, run:# vinfra domain project set myproject --domain mydomain --name newproject\n\nTo assign users to a project\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Manage users.\nIn the Manage users window, select only those users that you want to assign to the project, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra domain user set --assign <project> <role> --domain <domain> <user>\r\n\n\n--assign <project> <role>\n\nAssign a user to a project with one or more permission sets. Specify this option multiple times to assign the user to multiple projects.\n\n<project>: project ID or name\n<role>: user role in the project (project_admin)\n\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to assign the user myuser from the domain mydomain to the project myproject as a project administrator, run:# vinfra domain user set myuser --domain mydomain --assign myproject project_admin\n\nTo unassign users from a project\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab and click the required project.\nOn the Users tab, click the bin icon next to a user that you want to unassign from the project.\n\nCommand-line interface\nUse the following command:vinfra domain project user remove --user <user> --domain <domain> <project>\r\n\n\n--user <user>\n\nUser name or ID\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to remove the user myuser from the project myproject within the domain mydomain, run:# vinfra domain project user remove myproject --domain mydomain --user myuser\n\nTo edit project quotas\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Edit quotas.\nMake the required changes, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra service compute quotas update [--cores <cores>] [--ram-size <ram>] [--floatingip <floating-ip>]\r\n                                     [--storage-policy <storage_policy>:<size>]\r\n                                     [--k8saas-cluster <cluster>] [--lbaas-loadbalancer <load-balancer>]\r\n                                     [--placement <placement>] <project-id>\r\n\n\n--cores <cores>\n\nNumber of cores\n--ram-size <ram>\n\nNumber of RAM. Use the following units: M or MiB for mebibytes, G or GiB for gibibytes, T or TiB for tebibytes, P or PiB for pebibytes, and E or EiB for exbibytes.\n--floatingip <floating-ip>\n\nNumber of floating IP addresses\n--storage-policy <storage_policy>:<size>\n\nComma-separated list of <storage_policy>:<size>. To specify the size, use the following units: M or MiB for mebibytes, G or GiB for gibibytes, T or TiB for tebibytes, P or PiB for pebibytes, and E or EiB for exbibytes.\n--k8saas-cluster <cluster>\n\nNumber of Kubernetes clusters\n--lbaas-loadbalancer <load-balancer>\n\nThe new value for the load balancer quota limit. The value -1 means unlimited.\n--placement <placement>\n\nComma-separated list of <placement-id>:<size>\n<project-id>\n\nProject ID\n\nFor example, to update quotas for the project with the ID 6ef6f48f01b640ccb8ff53117b830fa3 to 10 vCPUs, 20 GiB of RAM, and 512 GiB of disk space for the default storage policy, run:# vinfra service compute quotas update 6ef6f48f01b640ccb8ff53117b830fa3 --cores 10 --ram-size 10G --storage-policy default:512G\nYou can view the updated quotas in the vinfra service compute quotas show output:# vinfra service compute quotas show 79830e3c64c74ded9bac6bffde5d26e4\r\n+----------------------------------------+----------+\r\n| Field                                  | Value    |\r\n+----------------------------------------+----------+\r\n| compute.cores.limit                    | 10       |\r\n| compute.ram.limit                      | 10.0GiB  |\r\n| compute.ram_quota.limit                | 10.0GiB  |\r\n| lbaas.loadbalancer.limit               | -1       |\r\n| network.floatingip.limit               | -1       |\r\n| storage.gigabytes.default.limit        | 512.0GiB |\r\n| storage.storage_policies.default.limit | 512.0GiB |\r\n+----------------------------------------+----------+\r\n\n\nTo enable or disable a project\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Enable or Disable.\n\nCommand-line interface\nUse the following command:vinfra domain project set [--enable | --disable] --domain <domain> <project>\r\n\n\n--enable\n\nEnable project\n--disable\n\nDisable project\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to disable the project myproject within the domain mydomain, run:# vinfra domain project set myproject --domain mydomain --disable\n\nTo delete a project\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Delete.\nIn the confirmation window, click Delete.\n\nCommand-line interface\nUse the following command:vinfra domain project delete --domain <domain> <project>\r\n\n\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to delete the project myproject from the domain mydomain, run:# vinfra domain project delete myproject --domain mydomain\n\nSee also\n\nManaging domains\n\nManaging self-service users",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain project set [--description <description>] [--name <name>] --domain <domain> <project>\r\n\n\n--description <description>\n\n\nProject description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--name <name>\n\nProject name\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to change the name of the project myproject within the domain mydomain to newproject, run:# vinfra domain project set myproject --domain mydomain --name newproject\n",
                "title": "To edit a project name or description"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user set --assign <project> <role> --domain <domain> <user>\r\n\n\n--assign <project> <role>\n\n\nAssign a user to a project with one or more permission sets. Specify this option multiple times to assign the user to multiple projects.\n\n<project>: project ID or name\n<role>: user role in the project (project_admin)\n\n\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to assign the user myuser from the domain mydomain to the project myproject as a project administrator, run:# vinfra domain user set myuser --domain mydomain --assign myproject project_admin\n",
                "title": "To assign users to a project"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain project user remove --user <user> --domain <domain> <project>\r\n\n\n--user <user>\n\nUser name or ID\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to remove the user myuser from the project myproject within the domain mydomain, run:# vinfra domain project user remove myproject --domain mydomain --user myuser\n",
                "title": "To unassign users from a project"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute quotas update [--cores <cores>] [--ram-size <ram>] [--floatingip <floating-ip>]\r\n                                     [--storage-policy <storage_policy>:<size>]\r\n                                     [--k8saas-cluster <cluster>] [--lbaas-loadbalancer <load-balancer>]\r\n                                     [--placement <placement>] <project-id>\r\n\n\n--cores <cores>\n\nNumber of cores\n--ram-size <ram>\n\nNumber of RAM. Use the following units: M or MiB for mebibytes, G or GiB for gibibytes, T or TiB for tebibytes, P or PiB for pebibytes, and E or EiB for exbibytes.\n--floatingip <floating-ip>\n\nNumber of floating IP addresses\n--storage-policy <storage_policy>:<size>\n\nComma-separated list of <storage_policy>:<size>. To specify the size, use the following units: M or MiB for mebibytes, G or GiB for gibibytes, T or TiB for tebibytes, P or PiB for pebibytes, and E or EiB for exbibytes.\n--k8saas-cluster <cluster>\n\nNumber of Kubernetes clusters\n--lbaas-loadbalancer <load-balancer>\n\nThe new value for the load balancer quota limit. The value -1 means unlimited.\n--placement <placement>\n\nComma-separated list of <placement-id>:<size>\n<project-id>\n\nProject ID\n\nFor example, to update quotas for the project with the ID 6ef6f48f01b640ccb8ff53117b830fa3 to 10 vCPUs, 20 GiB of RAM, and 512 GiB of disk space for the default storage policy, run:# vinfra service compute quotas update 6ef6f48f01b640ccb8ff53117b830fa3 --cores 10 --ram-size 10G --storage-policy default:512G\nYou can view the updated quotas in the vinfra service compute quotas show output:# vinfra service compute quotas show 79830e3c64c74ded9bac6bffde5d26e4\r\n+----------------------------------------+----------+\r\n| Field                                  | Value    |\r\n+----------------------------------------+----------+\r\n| compute.cores.limit                    | 10       |\r\n| compute.ram.limit                      | 10.0GiB  |\r\n| compute.ram_quota.limit                | 10.0GiB  |\r\n| lbaas.loadbalancer.limit               | -1       |\r\n| network.floatingip.limit               | -1       |\r\n| storage.gigabytes.default.limit        | 512.0GiB |\r\n| storage.storage_policies.default.limit | 512.0GiB |\r\n+----------------------------------------+----------+\r\n\n",
                "title": "To edit project quotas"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain project set [--enable | --disable] --domain <domain> <project>\r\n\n\n--enable\n\nEnable project\n--disable\n\nDisable project\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to disable the project myproject within the domain mydomain, run:# vinfra domain project set myproject --domain mydomain --disable\n",
                "title": "To enable or disable a project"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain project delete --domain <domain> <project>\r\n\n\n--domain <domain>\n\nDomain name or ID\n<project>\n\nProject ID or name\n\nFor example, to delete the project myproject from the domain mydomain, run:# vinfra domain project delete myproject --domain mydomain\n",
                "title": "To delete a project"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n",
                "title": "To edit a project name or description"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Manage users.\nIn the Manage users window, select only those users that you want to assign to the project, and then click Save.\n\n",
                "title": "To assign users to a project"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab and click the required project.\nOn the Users tab, click the bin icon next to a user that you want to unassign from the project.\n\n",
                "title": "To unassign users from a project"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Edit quotas.\nMake the required changes, and then click Save.\n\n",
                "title": "To edit project quotas"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Enable or Disable.\n\n",
                "title": "To enable or disable a project"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to manage projects.\nGo to the Projects tab, click the ellipsis icon next to the project, and then click Delete.\nIn the confirmation window, click Delete.\n\n",
                "title": "To delete a project"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-projects.html"
    },
    {
        "title": "Managing regular traffic types",
        "content": "Managing regular traffic types\nYou can add a regular traffic type to multiple networks or remove it from any network.\nLimitations\n\nRegular traffic types cannot be edited or deleted.\nYou can unassign traffic types from networks only if the related services are not deployed. For example, you cannot unassign the VM public traffic type if the compute cluster is created.\nYou cannot manage access rules for the VM public traffic type.\nIf the management node high availability is enabled, you cannot reassign the Admin panel traffic type.\n\nTo assign, reassign, or unassign a regular traffic type\n\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Assign to networks next to the Regular traffic types section.\nAdd or remove the needed traffic type from your networks by selecting the corresponding check boxes.\nClick Save to apply the changes.\n\nCommand-line interface\nUse the following command:vinfra cluster network set [--traffic-types <traffic-types> | --add-traffic-types <traffic-types> |\r\n                           --del-traffic-types <traffic-types>] <network>\r\n\n\n--traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (overwrites network\u00e2\u0080\u0099s current traffic types)\n--add-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (adds the specified traffic types to the network)\n--del-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (removes the specified traffic types from the network)\n<network>\n\nNetwork ID or name\n\nFor example, to add the traffic type SNMP to the Public network, run:# vinfra cluster network set Public --add-traffic-types \"SNMP\"\n\nSee also\n\nManaging exclusive traffic types\n\nManaging custom traffic types\n\nConfiguring inbound firewall rules\n\nManaging networks",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster network set [--traffic-types <traffic-types> | --add-traffic-types <traffic-types> |\r\n                           --del-traffic-types <traffic-types>] <network>\r\n\n\n--traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (overwrites network\u00e2\u0080\u0099s current traffic types)\n--add-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (adds the specified traffic types to the network)\n--del-traffic-types <traffic-types>\n\nA comma-separated list of traffic type names (removes the specified traffic types from the network)\n<network>\n\nNetwork ID or name\n\nFor example, to add the traffic type SNMP to the Public network, run:# vinfra cluster network set Public --add-traffic-types \"SNMP\"\n",
                "title": "To assign, reassign, or unassign a regular traffic type"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Networks screen, click Assign to networks next to the Regular traffic types section.\nAdd or remove the needed traffic type from your networks by selecting the corresponding check boxes.\nClick Save to apply the changes.\n\n",
                "title": "To assign, reassign, or unassign a regular traffic type"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-regular-traffic-types.html"
    },
    {
        "title": "Managing notifications",
        "content": "Managing notifications\nThe notification center stores and shows notifications about recent tasks of the current user in the management panel. Notifications are displayed only for tasks performed during the current user session and cleared out when the user logs out.\nA user is informed about each task by a pop-up notification in the bottom right corner of the screen. The same notification also appears in the notification center. After the pop-up window is closed, the notification is available in the notification center.\nThe following table describes all of the supported notification types:\n\nNotification type\nIcon\nDescription\nRetention period of a pop-up window\nRetention period in the notification center\n\nInfo\n\nNotifications about a task launch\n3 seconds\n10 minutes\n\nSuccess\n\nNotifications about successfully completed tasks\n3 seconds\n10 minutes\n\nError\n\nNotifications about failed tasks\n10 seconds\n50 minutes\n\nIn progress\n\nLong-running tasks, such as image upload or problem report creation\nTask time\nTask time\n\nTo view notifications\n\nOn any screen, click the bell icon in the top right corner. Next to the bell icon, you can see the notification counter, or the loading sign if you have a running task.\nTo view notifications of a particular type, click All types, and then select the notification type you wish to be displayed in the notification center.\n\nTo clear notifications\n\nOn any screen, click the bell icon in the top right corner.\nTo clear only one notification, click the cross icon next to it.\nTo clear all of the notifications, click Clear all above the notification list.\n\nTo configure pop-up notifications\n\nOn any screen, click the bell icon in the top right corner.\n\nClick the cogwheel icon, and then clear notification types that you do not wish to be shown in a pop-up window. Only the selected notification types will appear as pop-up messages.\n\nTo mute pop-up notifications\n\nOn any screen, click the bell icon in the top right corner.\n\nClick the cogwheel icon, and then turn on the Do not disturb mode.\n\nThe bell icon will be greyed out, and the notification counter will disappear. While this mode is on, pop-up notifications are disabled. However, all notifications are still available in the notification center.\nTo unmute pop-up notifications\n\nOn any screen, click the greyed out bell icon in the top right corner.\n\nClick Turn off, to turn off the Do not disturb mode.\n\nSee also\n\nSending email notifications\n\nViewing alerts\n\nViewing audit log",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-notifications.html"
    },
    {
        "title": "Managing router interfaces",
        "content": "Managing router interfaces\nPrerequisites\n\nYou have a virtual router created, as described in Creating virtual routers.\n\nTo add an external router interface\n\nAdmin panel\n\nIf you already have an external gateway, remove the existing one first.\nOn the Routers screen, click the router name to open the list of its interfaces.\nClick Add on the toolbar, or click Add interface if there are no interfaces to show.\n\nIn the Add interface window, do the following:\n\nSelect External gateway.\nFrom the Network drop-down menu, select a physical network to connect to the router. The new interface will pick an unused IP address from the selected physical network. You can also provide a specific IP address from the selected physical network to assign to the interface in the IP address field.\n\nSelect or deselect the SNAT check box to enable or disable SNAT on the external gateway of the router. With SNAT enabled, the router replaces VM private IP addresses with the public IP address of its external gateway.\n\nClick Add.\n\nCommand-line interface\nUse the following command:vinfra service compute router iface add [--ip-address <ip-address>]\r\n                                        --interface <network> router\r\n\n\n--ip-address <ip-address>\n\nIP address\n--interface <network>\n\nNetwork name or ID\nrouter\n\nVirtual router name or ID\n\nFor example, to add an external interface from the physical network public to the virtual router myrouter with the IP address 10.94.129.76, run:# vinfra service compute router iface add myrouter --interface public \\\r\n--ip-address 10.94.129.76\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| network_id                           | is_external | ip_addresses    | status |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-49de-9346-26513d8d1262 | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-48b2-9e4f-64a128ce97ae | False       | - 192.168.128.1 | ACTIVE |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n\nThe added interface will appear in the vinfra service compute router iface list output:# vinfra service compute router iface list myrouter\r\n+--------------------------------+-------------+-----------------+--------+\r\n| network_id                     | is_external | ip_addresses    | status |\r\n+--------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-<...> (public)   | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-<...> (private)  | False       | - 192.168.128.1 | ACTIVE |\r\n+--------------------------------+-------------+-----------------+--------+\r\n\n\nTo add an internal router interface\n\nAdmin panel\n\nOn the Routers screen, click the router name to open the list of its interfaces.\nClick Add.\n\nIn the Add interface window, select a network to connect to the router from the Network drop-down menu. The new interface will attempt to use the gateway IP address of the selected virtual network by default. If it is in use, specify an unused IP address from the selected virtual network to assign to the interface in the IP address field.\n\nClick Add.\n\nCommand-line interface\nUse the following command:vinfra service compute router iface add [--ip-address <ip-address>]\r\n                                        --interface <network> router\r\n\n\n--ip-address <ip-address>\n\nIP address\n--interface <network>\n\nNetwork name or ID\nrouter\n\nVirtual router name or ID\n\nFor example, to add an interface from the virtual network private2 to the virtual router myrouter with the IP address 192.168.30.3, run:# vinfra service compute router iface add myrouter --interface private2 \\\r\n--ip-address 192.168.30.3\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| network_id                           | is_external | ip_addresses    | status |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-49de-9346-26513d8d1262 | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-48b2-9e4f-64a128ce97ae | False       | - 192.168.128.1 | ACTIVE |\r\n| 86803e07-a6d7-4809-9566-1cbe4a89adfd | False       | - 192.168.30.3  | DOWN   |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n\nThe added interface will appear in the vinfra service compute router iface list output:# vinfra service compute router iface list myrouter\r\n+--------------------------------+-------------+-----------------+--------+\r\n| network_id                     | is_external | ip_addresses    | status |\r\n+--------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-<...> (public)   | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-<...> (private)  | False       | - 192.168.128.1 | ACTIVE |\r\n| 86803e07-a6d7-<...> (private2) | False       | - 192.168.30.3  | ACTIVE |\r\n+--------------------------------+-------------+-----------------+--------+\r\n\n\nTo edit external interface parameters\n\nAdmin panel\n\nClick the ellipsis icon next to the external interface, and then click Edit. \nIn the Edit interface window, change the  IP address or configure SNAT. \nClick Save to save your changes.\n\nCommand-line interface\nUse the following command:vinfra service compute router set [--external-gateway <network> |\r\n                                  --no-external-gateway]\r\n                                  [--fixed-ip <fixed-ip>]\r\n                                  [--enable-snat | --disable-snat]\r\n                                  <router>\r\n\n\n--external-gateway <network>\n\nSpecify a physical network to be used as the router\u00e2\u0080\u0099s external gateway (name or ID)\n--no-external-gateway\n\nRemove the external gateway from the router\n--enable-snat\n\nEnable source NAT on the external gateway\n--disable-snat\n\nDisable source NAT on the external gateway\n--fixed-ip <fixed-ip>\n\nDesired IP on the external gateway\n<router>\n\nVirtual router name or ID\n\nFor example, to disable SNAT on the external gateway of the virtual router myrouter, run:# vinfra service compute router set myrouter --disable-snat --external-gateway public\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | []                                               |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\r\n\n\nTo remove a router interface \n\nAdmin panel\n\nSelect the interface you want to remove.\nClick the ellipsis icon next to it, and then click Delete.\nIn the confirmation window, click Delete.\n\nCommand-line interface\nUse the following command:vinfra service compute router iface remove --interface <network> router\r\n\n\n--interface <network>\n\nNetwork name or ID\nrouter\n\nVirtual router name or ID\n\nFor example, to remove the interface from the virtual network private2 from the virtual router myrouter, run:# vinfra service compute router iface remove myrouter --interface private2\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| network_id                           | is_external | ip_addresses    | status |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-49de-9346-26513d8d1262 | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-48b2-9e4f-64a128ce97ae | False       | - 192.168.128.1 | ACTIVE |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n\n\nSee also\n\nManaging static routes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router iface add [--ip-address <ip-address>]\r\n                                        --interface <network> router\r\n\n\n--ip-address <ip-address>\n\nIP address\n--interface <network>\n\nNetwork name or ID\nrouter\n\nVirtual router name or ID\n\nFor example, to add an external interface from the physical network public to the virtual router myrouter with the IP address 10.94.129.76, run:# vinfra service compute router iface add myrouter --interface public \\\r\n--ip-address 10.94.129.76\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| network_id                           | is_external | ip_addresses    | status |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-49de-9346-26513d8d1262 | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-48b2-9e4f-64a128ce97ae | False       | - 192.168.128.1 | ACTIVE |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n\nThe added interface will appear in the vinfra service compute router iface list output:# vinfra service compute router iface list myrouter\r\n+--------------------------------+-------------+-----------------+--------+\r\n| network_id                     | is_external | ip_addresses    | status |\r\n+--------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-<...> (public)   | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-<...> (private)  | False       | - 192.168.128.1 | ACTIVE |\r\n+--------------------------------+-------------+-----------------+--------+\r\n\n",
                "title": "To add an external router interface"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router iface add [--ip-address <ip-address>]\r\n                                        --interface <network> router\r\n\n\n--ip-address <ip-address>\n\nIP address\n--interface <network>\n\nNetwork name or ID\nrouter\n\nVirtual router name or ID\n\nFor example, to add an interface from the virtual network private2 to the virtual router myrouter with the IP address 192.168.30.3, run:# vinfra service compute router iface add myrouter --interface private2 \\\r\n--ip-address 192.168.30.3\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| network_id                           | is_external | ip_addresses    | status |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-49de-9346-26513d8d1262 | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-48b2-9e4f-64a128ce97ae | False       | - 192.168.128.1 | ACTIVE |\r\n| 86803e07-a6d7-4809-9566-1cbe4a89adfd | False       | - 192.168.30.3  | DOWN   |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n\nThe added interface will appear in the vinfra service compute router iface list output:# vinfra service compute router iface list myrouter\r\n+--------------------------------+-------------+-----------------+--------+\r\n| network_id                     | is_external | ip_addresses    | status |\r\n+--------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-<...> (public)   | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-<...> (private)  | False       | - 192.168.128.1 | ACTIVE |\r\n| 86803e07-a6d7-<...> (private2) | False       | - 192.168.30.3  | ACTIVE |\r\n+--------------------------------+-------------+-----------------+--------+\r\n\n",
                "title": "To add an internal router interface"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router set [--external-gateway <network> |\r\n                                  --no-external-gateway]\r\n                                  [--fixed-ip <fixed-ip>]\r\n                                  [--enable-snat | --disable-snat]\r\n                                  <router>\r\n\n\n--external-gateway <network>\n\nSpecify a physical network to be used as the router\u00e2\u0080\u0099s external gateway (name or ID)\n--no-external-gateway\n\nRemove the external gateway from the router\n--enable-snat\n\nEnable source NAT on the external gateway\n--disable-snat\n\nDisable source NAT on the external gateway\n--fixed-ip <fixed-ip>\n\nDesired IP on the external gateway\n<router>\n\nVirtual router name or ID\n\nFor example, to disable SNAT on the external gateway of the virtual router myrouter, run:# vinfra service compute router set myrouter --disable-snat --external-gateway public\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | []                                               |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\r\n\n",
                "title": "To edit external interface parameters"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router iface remove --interface <network> router\r\n\n\n--interface <network>\n\nNetwork name or ID\nrouter\n\nVirtual router name or ID\n\nFor example, to remove the interface from the virtual network private2 from the virtual router myrouter, run:# vinfra service compute router iface remove myrouter --interface private2\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| network_id                           | is_external | ip_addresses    | status |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n| 720e45bc-4225-49de-9346-26513d8d1262 | True        | - 10.94.129.76  | ACTIVE |\r\n| e6f146ce-a6d0-48b2-9e4f-64a128ce97ae | False       | - 192.168.128.1 | ACTIVE |\r\n+--------------------------------------+-------------+-----------------+--------+\r\n\n",
                "title": "To remove a router interface "
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nIf you already have an external gateway, remove the existing one first.\nOn the Routers screen, click the router name to open the list of its interfaces.\nClick Add on the toolbar, or click Add interface if there are no interfaces to show.\n\nIn the Add interface window, do the following:\n\nSelect External gateway.\nFrom the Network drop-down menu, select a physical network to connect to the router. The new interface will pick an unused IP address from the selected physical network. You can also provide a specific IP address from the selected physical network to assign to the interface in the IP address field.\n\nSelect or deselect the SNAT check box to enable or disable SNAT on the external gateway of the router. With SNAT enabled, the router replaces VM private IP addresses with the public IP address of its external gateway.\n\n\n\n\n\n\n\nClick Add.\n\n",
                "title": "To add an external router interface"
            },
            {
                "example": "\nAdmin panel\n\nOn the Routers screen, click the router name to open the list of its interfaces.\nClick Add.\n\nIn the Add interface window, select a network to connect to the router from the Network drop-down menu. The new interface will attempt to use the gateway IP address of the selected virtual network by default. If it is in use, specify an unused IP address from the selected virtual network to assign to the interface in the IP address field.\n\n\n\n\n\nClick Add.\n\n",
                "title": "To add an internal router interface"
            },
            {
                "example": "\nAdmin panel\n\nClick the ellipsis icon next to the external interface, and then click Edit. \nIn the Edit interface window, change the  IP address or configure SNAT. \nClick Save to save your changes.\n\n",
                "title": "To edit external interface parameters"
            },
            {
                "example": "\nAdmin panel\n\nSelect the interface you want to remove.\nClick the ellipsis icon next to it, and then click Delete.\nIn the confirmation window, click Delete.\n\n",
                "title": "To remove a router interface "
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-router-interfaces.html"
    },
    {
        "title": "Managing S3 users",
        "content": "Managing S3 users\nThe concept of an S3 user is one of the base concepts of object storage along with those of an object and a bucket (a container for storing objects). The Amazon S3 protocol uses a permission model based on access control lists (ACLs), where each bucket and each object are assigned an ACL that lists all users with access to the given resource and the type of this access (read, write, read ACL, or write ACL). The list of users includes the entity owner assigned to every object and bucket at creation. The entity owner has extra rights compared to other users. For example, the bucket owner is the only one who can delete that bucket.\nUser model and access policies implemented in Virtuozzo Hybrid Infrastructure comply with the Amazon S3 user model and access policies.\nUser management scenarios in Virtuozzo Hybrid Infrastructure are largely based on the Amazon Web Services user management and include the following operations: create, query, and delete users, as well as generate and revoke user access key pairs.\nEach S3 user has one or two key pairs (access key and secret key) for accessing the S3 cloud. You can think of the access key as the login and the secret key as the password. (For more information about S3 key pairs, refer to the Amazon documentation.) The access keys are generated and stored locally in the storage cluster on S3 name servers. Each user can have up to two key pairs. It is recommended to periodically revoke old access key pairs and generate new ones.\nTo access a bucket, a user needs the following information:\n\nAdmin panel IP address\nDNS name of the S3 cluster specified during configuration\nS3 access key ID\nS3 secret access key\nSSL certificate if the HTTPS protocol was chosen during configuration (the certificate file can be found in the /etc/nginx/ssl/ directory on any node hosting the S3 gateway service)\n\nPrerequisites\n\nS3 users are created, as described in Adding S3 users.\nA clear understanding of the best S3 practices, listed in Best practices for using S3 in Virtuozzo Hybrid Infrastructure.\n\nTo copy, add, or revoke the S3 access key pairs for an S3 user\n\n Open the Storage services > S3 > Users screen, and then select a user.\n\nOn the user right pane, browse the S3 access keys section:\n\nTo copy the existing keys, click the copy icon next to them.\nTo revoke keys, click the bin icon next to them.\nTo add new keys, click Add.\n\nWhat's next\n\nManaging S3 buckets\n\nSee also\n\nSupported Amazon S3 features\n\nDefining object storage classes\n\nChanging the TLS configuration for S3\n\nMonitoring object storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-s3-users.html"
    },
    {
        "title": "Managing S3 buckets",
        "content": "Managing S3 buckets\nAll objects in an Amazon S3-like storage are stored in containers called buckets. Buckets are addressed by names that are unique in the given object storage, so an S3 user of that object storage cannot create a bucket that has the same name as a different bucket in the same object storage. Buckets are used to:\n\nGroup and isolate objects from those in other buckets\nProvide ACL management mechanisms for objects in them\nSet per-bucket access policies for tasks such as versioning in the bucket\n\nYou can list S3 buckets and monitor the space used by them on the Storage services > S3 > Buckets screen. You cannot create and manage S3 buckets from Virtuozzo Hybrid Infrastructure admin panel. However, you can do it via the Virtuozzo Hybrid Infrastructure user panel or by using a third-party application.\nThird-party applications listed below allow you to perform the following actions:\n\nIn CyberDuck, you can create and manage buckets, and their contents.\nIn MountainDuck, you can mount an object storage as a disk drive, manage buckets and their contents.\n\nPrerequisites\n\nS3 users are created, as described in Adding S3 users.\nA clear understanding of the best S3 practices, listed in Best practices for using S3 in Virtuozzo Hybrid Infrastructure.\n\nTo access the buckets from the user panel \n\nOpen the Storage services > S3 > Users screen, and then select a user. \nClick Browse. \n\nThe SSL certificate will be used for logging in, so make sure it is valid or add it to the browser\u00e2\u0080\u0099s exceptions if it is a self-signed one.\nTo list bucket contents with a web browser\n\nVisit the URL that consists of the external DNS name for the S3 endpoint that you specified when creating the S3 cluster and the bucket name. For example, s3.example.com/mybucket or mybucket.s3.example.com (depending on DNS configuration).\nCopy the link to the bucket contents by right-clicking it in CyberDuck, and then selecting Copy URL.\n\nSee also\n\nSupported Amazon S3 features\n\nReplicating S3 data between datacenters\n\nChanging S3 protocol settings\n\nDefining object storage classes\n\nConfiguring bucket notifications\n\nMonitoring object storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-s3-buckets.html"
    },
    {
        "title": "Managing self-service users",
        "content": "Managing self-service users\nYou can add more domain administrators and project members, as described in Configuring multitenancy. Also, you can edit user credentials and permissions, manage project assignment, as well as enable/disable and delete the existing users. Enabling and disabling users allows or prohibits user login in the self-service panel.\nTo edit user credentials and permissions\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the ellipsis icon next to the user, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nCommand-line interface\nUse the following command:vinfra domain user set [--password] [--email <email>] [--name <name>] [--description <description>]\r\n                       [--domain-permissions <domain_permissions>] --domain <domain> <user>\r\n\n\n--password\n\nRequest the password from stdin\n--email <email>\n\nUser email\n--name <name>\n\nUser name\n--description <description>\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to change the email of the user myuser to user@example.com, run:# vinfra domain user set myuser --domain mydomain --email user@example.com\n\nTo enable or disable a user\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the ellipsis icon next to the user, and then click Enable or Disable.\n\nCommand-line interface\nUse the following command:vinfra domain user set [--enable | --disable] --domain <domain> <user>\r\n\n\n--enable\n\nEnable user\n--disable\n\nDisable user\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to disable the user myuser within the domain mydomain, run:# vinfra domain user set myuser --domain mydomain --disable\n\nTo assign a user to projects\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the ellipsis icon next to a user with the Project member role, and then click Manage projects.\nIn the Manage projects window, select only those projects that you want to assign the user to, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra domain user set --assign <project> <role> --domain <domain> <user>\r\n\n\n--assign <project> <role>\n\nAssign a user to a project with one or more permission sets. Specify this option multiple times to assign the user to multiple projects.\n\n<project>: project ID or name\n<role>: user role in the project (project_admin)\n\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to assign the user myuser from the domain mydomain to the project myproject as a project administrator, run:# vinfra domain user set myuser --domain mydomain --assign myproject project_admin\n\nTo unassign a user from projects\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the required user with the Project member role.\nOn the Projects tab, click the bin icon next to a project that you want to remove the user from.\n\nCommand-line interface\nUse the following command:vinfra domain user set --unassign <project> --domain <domain> <user>\r\n\n\n--unassign <project>\n\nUnassign a user from a project. Specify this option multiple times to unassign the user from multiple projects.\n\n<project>: project ID or name\n\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to unassign the user myuser from the domain mydomain from the project myproject, run:# vinfra domain user set myuser --domain mydomain --unassign myproject\n\nTo delete a user\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to delete a user.\nGo to the Domain users tab, click the ellipsis icon next to it, and then click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra domain user delete --domain <domain> <user>\r\n\n\n--domain <domain>\n\nDomain ID or name\n<user>\n\nUser ID or name\n\nFor example, to delete the user myuser from the domain mydomain:# vinfra domain user delete myuser --domain mydomain\n\nSee also\n\nAssigning users to multiple domains\n\nManaging projects\n\nManaging domain groups\n\nUnlocking user accounts",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user set [--password] [--email <email>] [--name <name>] [--description <description>]\r\n                       [--domain-permissions <domain_permissions>] --domain <domain> <user>\r\n\n\n--password\n\nRequest the password from stdin\n--email <email>\n\nUser email\n--name <name>\n\nUser name\n--description <description>\n\n\nUser description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--domain-permissions <domain_permissions>\n\nA comma-separated list of domain permissions. View the list of available domain permissions using vinfra domain user list-available-roles | grep domain.\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to change the email of the user myuser to user@example.com, run:# vinfra domain user set myuser --domain mydomain --email user@example.com\n",
                "title": "To edit user credentials and permissions"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user set [--enable | --disable] --domain <domain> <user>\r\n\n\n--enable\n\nEnable user\n--disable\n\nDisable user\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to disable the user myuser within the domain mydomain, run:# vinfra domain user set myuser --domain mydomain --disable\n",
                "title": "To enable or disable a user"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user set --assign <project> <role> --domain <domain> <user>\r\n\n\n--assign <project> <role>\n\n\nAssign a user to a project with one or more permission sets. Specify this option multiple times to assign the user to multiple projects.\n\n<project>: project ID or name\n<role>: user role in the project (project_admin)\n\n\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to assign the user myuser from the domain mydomain to the project myproject as a project administrator, run:# vinfra domain user set myuser --domain mydomain --assign myproject project_admin\n",
                "title": "To assign a user to projects"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user set --unassign <project> --domain <domain> <user>\r\n\n\n--unassign <project>\n\n\nUnassign a user from a project. Specify this option multiple times to unassign the user from multiple projects.\n\n<project>: project ID or name\n\n\n--domain <domain>\n\nDomain name or ID\n<user>\n\nUser ID or name\n\nFor example, to unassign the user myuser from the domain mydomain from the project myproject, run:# vinfra domain user set myuser --domain mydomain --unassign myproject\n",
                "title": "To unassign a user from projects"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra domain user delete --domain <domain> <user>\r\n\n\n--domain <domain>\n\nDomain ID or name\n<user>\n\nUser ID or name\n\nFor example, to delete the user myuser from the domain mydomain:# vinfra domain user delete myuser --domain mydomain\n",
                "title": "To delete a user"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the ellipsis icon next to the user, and then click Edit.\n\nMake the required changes, and then click Save.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n\n",
                "title": "To edit user credentials and permissions"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the ellipsis icon next to the user, and then click Enable or Disable.\n\n",
                "title": "To enable or disable a user"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the ellipsis icon next to a user with the Project member role, and then click Manage projects.\nIn the Manage projects window, select only those projects that you want to assign the user to, and then click Save.\n\n",
                "title": "To assign a user to projects"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a user.\nGo to the Domain users tab, click the required user with the Project member role.\nOn the Projects tab, click the bin icon next to a project that you want to remove the user from.\n\n",
                "title": "To unassign a user from projects"
            },
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to delete a user.\nGo to the Domain users tab, click the ellipsis icon next to it, and then click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a user"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-self-service-users.html"
    },
    {
        "title": "Managing security",
        "content": "Managing security\nThis section provides the best practices for cluster security. Additionally, it explains how to configure SSL access to the admin panel and data encryption for different tiers, as well as secure root access to cluster nodes via SSH.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-security.html"
    },
    {
        "title": "Managing security groups",
        "content": "Managing security groups\nA security group is a set of network access rules that control incoming and outgoing traffic to virtual machines assigned to this group. With security group rules, you can specify the type and direction of traffic that is allowed access to a virtual interface port. Traffic that does not satisfy any rule is dropped.\nFor each project, the default security group is automatically created in the compute cluster. This group allows all traffic on all ports for all protocols and cannot be deleted. When you attach a network interface to a VM, the interface is associated with the default security group, unless you explicitly select a custom security group.\nYou can assign one or more security groups to both new and existing virtual machines. When you add rules to security groups or remove them, the changes are enforced at runtime.\nLimitations\n\nYou can manage only IPv4 security group rules.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-security-groups.html"
    },
    {
        "title": "Managing the compute cluster",
        "content": "Managing the compute cluster\nThis section describes managing the compute cluster, its nodes, and compute networks. It also outlines how to configure placements, images, flavors, and SSH keys for virtual machines. In addition, this section describes operations with the compute storage, its volumes and storage policies.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-the-compute-cluster.html"
    },
    {
        "title": "Managing the compute network",
        "content": "Managing the compute network\nIn Virtuozzo Hybrid Infrastructure, compute networking includes compute networks, security groups, virtual routers, floating public IP addresses, and network load balancers.\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-the-compute-network.html"
    },
    {
        "title": "Managing static routes",
        "content": "Managing static routes\nYou can also configure static routes of a router by manually adding entries into its routing table. This can be useful, for example, if you do not need a mutual connection between two virtual networks and want only one virtual network to be accessible from the other.\nConsider the following example:\n\nThe virtual machine VM1 is connected to the virtual network private1 (192.168.128.0/24) via the network interface with IP address 192.168.128.10.\nThe virtual machine VM2 is connected to the virtual network private2 (192.168.30.0/24) via the network interface with IP address 192.168.30.10.\nThe router router1 connects the network private1 to the physical network via the external gateway with the IP address 10.94.129.73.\nThe router router2 connects the network private2 to the physical network via the external gateway with the IP address 10.94.129.74.\n\nTo be able to access VM2 from VM1, you need to add a static route for router1, specifying the CIDR of private2, that is 192.168.30.0/24, as the destination subnet and the external gateway IP address of router2, that is 10.94.129.74, as the next hop IP address. In this case, when an IP packet for 192.168.30.10 reaches router1, it will be forwarded to router2 and then to VM2.\nPrerequisites\n\nYou have a virtual router created, as described in Creating virtual routers.\n\nTo create a static route for a router\n\nAdmin panel\n\nOn the Routers screen, click the router name. Open the Static routes tab, and then click Add on the right pane. If there are no routes to show, click Add static route.\n\nIn the Add static route window, specify the destination subnet range and mask in CIDR notation and the next hop\u00e2\u0080\u0099s IP address. The next hop\u00e2\u0080\u0099s IP address must belong to one of the networks that the router is connected to.\n\nClick Add.\n\nCommand-line interface\nUse the following command:vinfra service compute router set --route <destination=destination,\r\n                                  nexthop=nexthop> <router>\r\n\n\n--route <destination=destination,nexthop=nexthop>\n\nA static route for the router. This option can be used multiple times.\n\ndestination: destination subnet range in CIDR notation.\nnexthop: next hop IP address from one of the networks that the router is connected to.\n\n<router>\n\nVirtual router name or ID\n\nFor example, to create a static route for the virtual router myrouter with the destination subnet 192.128.30.0/24 and the next top IP address 10.94.129.74, run:# vinfra service compute router set myrouter --route destination=192.128.30.0/24,nexthop=10.94.129.74\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | - destination: 192.128.30.0/24                   |\r\n|                       |   nexthop: 10.94.129.74                          |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\n\nTo edit a static route\n\nAdmin panel\n\nClick the ellipsis icon next to the required static route, and then click Edit. \nIn the Edit static route window, change the desired parameters, and then click Save.\n\nCommand-line interface\nUse the following command:vinfra service compute router set --route <destination=destination,\r\n                                  nexthop=nexthop> <router>\r\n\n\n--route <destination=destination,nexthop=nexthop>\n\nA static route for the router. This option can be used multiple times.\n\ndestination: destination subnet range in CIDR notation.\nnexthop: next hop IP address from one of the networks that the router is connected to.\n\n<router>\n\nVirtual router name or ID\n\nFor example, to edit the destination subnet to 192.168.30.0/24 and the next top IP address 10.94.129.15 for the virtual router myrouter, run:# vinfra service compute router set myrouter --route destination=192.168.30.0/24,nexthop=10.94.129.15\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | - destination: 192.168.30.0/24                   |\r\n|                       |   nexthop: 10.94.129.15                          |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\n\nTo remove a static route\n\nAdmin panel\nClick the ellipsis icon next to the static route you want to remove, and then click Delete. \n\nCommand-line interface\nUse the following command:vinfra service compute router set --no-route <router>\r\n\n\n--no-route\n\nClear routes associated with the router\n<router>\n\nVirtual router name or ID\n\nFor example, to delete all of the static routes for the virtual router myrouter, run:# vinfra service compute router set myrouter --no-route\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | []                                               |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\n\nSee also\n\nManaging router interfaces",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router set --route <destination=destination,\r\n                                  nexthop=nexthop> <router>\r\n\n\n--route <destination=destination,nexthop=nexthop>\n\n\nA static route for the router. This option can be used multiple times.\n\ndestination: destination subnet range in CIDR notation.\nnexthop: next hop IP address from one of the networks that the router is connected to.\n\n\n<router>\n\nVirtual router name or ID\n\nFor example, to create a static route for the virtual router myrouter with the destination subnet 192.128.30.0/24 and the next top IP address 10.94.129.74, run:# vinfra service compute router set myrouter --route destination=192.128.30.0/24,nexthop=10.94.129.74\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | - destination: 192.128.30.0/24                   |\r\n|                       |   nexthop: 10.94.129.74                          |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\n",
                "title": "To create a static route for a router"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router set --route <destination=destination,\r\n                                  nexthop=nexthop> <router>\r\n\n\n--route <destination=destination,nexthop=nexthop>\n\n\nA static route for the router. This option can be used multiple times.\n\ndestination: destination subnet range in CIDR notation.\nnexthop: next hop IP address from one of the networks that the router is connected to.\n\n\n<router>\n\nVirtual router name or ID\n\nFor example, to edit the destination subnet to 192.168.30.0/24 and the next top IP address 10.94.129.15 for the virtual router myrouter, run:# vinfra service compute router set myrouter --route destination=192.168.30.0/24,nexthop=10.94.129.15\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | - destination: 192.168.30.0/24                   |\r\n|                       |   nexthop: 10.94.129.15                          |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\n",
                "title": "To edit a static route"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute router set --no-route <router>\r\n\n\n--no-route\n\nClear routes associated with the router\n<router>\n\nVirtual router name or ID\n\nFor example, to delete all of the static routes for the virtual router myrouter, run:# vinfra service compute router set myrouter --no-route\r\n+-----------------------+--------------------------------------------------+\r\n| Field                 | Value                                            |\r\n+-----------------------+--------------------------------------------------+\r\n| external_gateway_info | enable_snat: false                               |\r\n|                       | ip_addresses:                                    |\r\n|                       | - 10.94.129.76                                   |\r\n|                       | network_id: 720e45bc-4225-49de-9346-26513d8d1262 |\r\n| id                    | b9d8b000-5d06-4768-9f65-2715250cda53             |\r\n| name                  | myrouter                                         |\r\n| project_id            | 894696133031439f8aaa7e4868dcbd4d                 |\r\n| routes                | []                                               |\r\n| status                | ACTIVE                                           |\r\n+-----------------------+--------------------------------------------------+\n",
                "title": "To remove a static route"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Routers screen, click the router name. Open the Static routes tab, and then click Add on the right pane. If there are no routes to show, click Add static route.\n\nIn the Add static route window, specify the destination subnet range and mask in CIDR notation and the next hop\u00e2\u0080\u0099s IP address. The next hop\u00e2\u0080\u0099s IP address must belong to one of the networks that the router is connected to.\n\n\n\n\n\n\nClick Add.\n\n\n",
                "title": "To create a static route for a router"
            },
            {
                "example": "\nAdmin panel\n\nClick the ellipsis icon next to the required static route, and then click Edit. \nIn the Edit static route window, change the desired parameters, and then click Save.\n\n",
                "title": "To edit a static route"
            },
            {
                "example": "\nAdmin panel\nClick the ellipsis icon next to the static route you want to remove, and then click Delete. \n",
                "title": "To remove a static route"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-static-routes.html"
    },
    {
        "title": "Managing user assignment to domain groups",
        "content": "Managing user assignment to domain groups\nOnce you create a domain group, you can assign users to it. You can choose from users that are added to the infrastructure either manually or automatically from external identity providers. Users assigned to a domain group inherit the role set to this domain group, regardless of their original roles. For example, if you assign a user with the role Project member to a domain group with the role Domain administrator, the user will act as the domain administrator within this domain.\nPrerequisites\n\nDomain groups are created, as described in Creating domain groups.\nUsers are created locally, as outlined in Configuring multitenancy or Managing admin panel users, or added from external identity providers, as described in Adding  identity providers.\n\nTo manage users of a domain group\n\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a domain group.\nGo to the Domain groups tab, click the ellipsis icon next to the group, and then click Manage users.\nIn the Manage users window, select users to assign to the group, or deselect them to unassign from the group, and then click Save.\n\nCommand-line interface\n\nTo add a user to a domain group, use the following command:vinfra domain group user add --domain <domain> <group> <user>\n\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n<user>\n\nUser ID or name\n\nFor example, to add the user myuser to the domain group users within the domain mydomain, run:# vinfra domain group user add --domain mydomain users myuser\n\nTo remove a user from a domain group, use the following command:vinfra domain group user remove --domain <domain> <group> <user>\n\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n<user>\n\nUser ID or name\n\nFor example, to remove the user myuser from the domain group users within the domain mydomain, run:# vinfra domain group user remove --domain mydomain users myuser\n\nSee also\n\nManaging project assignment to domain groups\n\nEditing and deleting domain groups",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nTo add a user to a domain group, use the following command:vinfra domain group user add --domain <domain> <group> <user>\n\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n<user>\n\nUser ID or name\n\nFor example, to add the user myuser to the domain group users within the domain mydomain, run:# vinfra domain group user add --domain mydomain users myuser\n\n\nTo remove a user from a domain group, use the following command:vinfra domain group user remove --domain <domain> <group> <user>\n\n--domain <domain>\n\nDomain name or ID\n<group>\n\nGroup ID or name\n<user>\n\nUser ID or name\n\nFor example, to remove the user myuser from the domain group users within the domain mydomain, run:# vinfra domain group user remove --domain mydomain users myuser\n\n\n",
                "title": "To manage users of a domain group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Settings > Projects and users screen, click the domain, within which you want to edit a domain group.\nGo to the Domain groups tab, click the ellipsis icon next to the group, and then click Manage users.\nIn the Manage users window, select users to assign to the group, or deselect them to unassign from the group, and then click Save.\n\n",
                "title": "To manage users of a domain group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-user-assignment-to-domain-groups.html"
    },
    {
        "title": "Managing storage policies",
        "content": "Managing storage policies\nA storage policy is a group of parameters that define how to store VM volumes: a tier, a failure domain, and a redundancy mode. A storage policy can also be used to limit the bandwidth or IOPS of the volume. These limits help customize the allocation of cluster resources between the virtual machines. It is also needed to provide predictable performance levels for virtual machine disks.\nWhen you deploy the compute cluster, you select parameters for the default storage policy, to be automatically created together with the cluster. The default policy cannot be deleted or renamed. By default, it is applied to uploaded images and base volumes created from these images.\nAbout base volumes\n\nA base volume is created from a source image when you deploy a VM. It is not used directly by a VM, but all volumes that a VM actually uses (which are listed on the Volumes tab) are in fact deltas (differences) from the base volume. It is important to keep base volumes available as VM volumes depend on them. For that, you need the default storage policy to enforce multiple replicas.\n\nIf the storage cluster does not have enough nodes to enable multiple replicas (not recommended), you can adjust the default storage policy once you add more nodes to the storage cluster. It will be applied to the images and base volumes that were created with the default policy.\nTo apply custom redundancy schemes to VM volumes, you can create, edit, or clone storage policies for them.\nLimitations\n\nYou cannot change the redundancy type of an existing storage policy.\nYou cannot delete a storage policy that governs existing volumes. If you still want to delete the storage policy, first remove these volumes or select another policy for them.\n\nPrerequisites\n\nA clear understanding of these concepts: Storage policies, Data redundancy, Failure domains, and Storage tiers.\n\nTo create a storage policy\n\nAdmin panel\n\nOn the Compute > Storage > Storage policies tab, click Create storage policy.\n\nIn the Create storage policy window, specify a policy name and select  redundancy settings.\n\nEnable IOPS limit or Bandwidth limit to set the corresponding limits on the volume.\n\nClick Create.\n\nCommand-line interface\nUse the following command:vinfra service compute storage-policy create --tier {0,1,2,3} (--replicas <norm>[:<min>] | --encoding <M>+<N>)\r\n                                             --failure-domain {0,1,2,3,4}\r\n                                             [--write-bytes-sec <limit>] [--read-bytes-sec <limit>]\r\n                                             [--read-iops-sec <limit>] [--write-iops-sec <limit>]\r\n                                             [--total-bytes-sec <limit>] [--total-iops-sec <limit>]\r\n                                             <name>\r\n\n\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--replicas <norm>[:<min>]\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n--write-bytes-sec <limit>\n\nNumber of bytes written per second\n--read-bytes-sec <limit>\n\nNumber of bytes read per second\n--read-iops-sec <limit>\n\nNumber of read operations per second\n--write-iops-sec <limit>\n\nNumber of write operations per second\n--total-bytes-sec <bytes>\n\nTotal number of bytes per second\n--total-iops-sec <iops>\n\nTotal number of I/O operations per second\n<name>\n\nStorage policy name\n\nFor example, to create a storage policy mystorpolicy with the tier 3, erasure coding 3+2 scheme, host failure domain, and the limits of 100 IOPS and 104857600 bytes per second, run:# vinfra service compute storage-policy create mystorpolicy --tier 3 \\\r\n--encoding 3+2 --failure-domain 1 --total-bytes-sec 104857600 --total-iops-sec 100\nThe created storage policy will appear in the vinfra service compute storage-policy list output:# vinfra service compute storage-policy list\r\n+-------------+--------------+------+--------------+----------------+--------------------------------+\r\n| id          | name         | tier | redundancy   | failure_domain | qos                            |\r\n+-------------+--------------+------+--------------+----------------+--------------------------------+\r\n| 97b55811<\u00e2\u0080\u00a6> | mystorpolicy | 3    | encoding=3+2 | 1              | read_bytes_sec: -1             |\r\n|             |              |      |              |                | read_bytes_sec_per_gb: -1      |\r\n|             |              |      |              |                | read_bytes_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | read_iops_sec: -1              |\r\n|             |              |      |              |                | read_iops_sec_per_gb: -1       |\r\n|             |              |      |              |                | read_iops_sec_per_gb_min: -1   |\r\n|             |              |      |              |                | total_bytes_sec: 104857600     |\r\n|             |              |      |              |                | total_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | total_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | total_iops_sec: 100            |\r\n|             |              |      |              |                | total_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | total_iops_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | write_bytes_sec: -1            |\r\n|             |              |      |              |                | write_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | write_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | write_iops_sec: -1             |\r\n|             |              |      |              |                | write_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | write_iops_sec_per_gb_min: -1  |\r\n| 603bd56b<\u00e2\u0080\u00a6> | default      | 0    | replicas=3   | 1              | read_bytes_sec: -1             |\r\n|             |              |      |              |                | read_bytes_sec_per_gb: -1      |\r\n|             |              |      |              |                | read_bytes_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | read_iops_sec: -1              |\r\n|             |              |      |              |                | read_iops_sec_per_gb: -1       |\r\n|             |              |      |              |                | read_iops_sec_per_gb_min: -1   |\r\n|             |              |      |              |                | total_bytes_sec: -1            |\r\n|             |              |      |              |                | total_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | total_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | total_iops_sec: -1             |\r\n|             |              |      |              |                | total_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | total_iops_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | write_bytes_sec: -1            |\r\n|             |              |      |              |                | write_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | write_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | write_iops_sec: -1             |\r\n|             |              |      |              |                | write_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | write_iops_sec_per_gb_min: -1  |\r\n+-------------+--------------+------+--------------+----------------+--------------------------------+\n\nTo edit a storage policy\n\nAdmin panel\n\nOn the Compute > Storage > Storage policies tab, select a policy from the list.\nOn the policy right pane, click Edit.\nChange the required parameters, and then click Save.\n\nKeep in mind, that changes to the storage policy will affect the redundancy and performance of all the volumes covered by it.\n\nCommand-line interface\nUse the following command:vinfra service compute storage-policy set [--name <name>] [--tier {0,1,2,3}]\r\n                                          [--replicas <norm>[:<min>] |\r\n                                          --encoding <M>+<N>]\r\n                                          [--failure-domain {0,1,2,3,4}]\r\n                                          [--write-bytes-sec <limit>] [--read-bytes-sec <limit>]\r\n                                          [--read-iops-sec <limit>] [--write-iops-sec <limit>]\r\n                                          [--total-bytes-sec <limit>] [--total-iops-sec <limit>]\r\n                                          <storage-policy>\r\n\n\n--name <name>\n\nA new name for the storage policy\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--replicas <norm>[:<min>]\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n--write-bytes-sec <limit>\n\nNumber of bytes written per second\n--read-bytes-sec <limit>\n\nNumber of bytes read per second\n--read-iops-sec <limit>\n\nNumber of read operations per second\n--write-iops-sec <limit>\n\nNumber of write operations per second\n--total-bytes-sec <bytes>\n\nTotal number of bytes per second\n--total-iops-sec <iops>\n\nTotal number of I/O operations per second\n<storage-policy>\n\nStorage policy ID or name\n\nFor example, to change the redundancy type for the storage policy mystorpolicy from erasure coding 3+2 to 5+2, run:# vinfra service compute storage-policy set mystorpolicy --encoding 5+2\n\nTo clone a storage policy\n\nOn the Compute > Storage > Storage policies tab, select a policy from the list.\nOn the policy right pane, click Clone.\n\nModify the existing parameters or just leave them as they are, and then click Clone.\n\nTo remove a storage policy\n\nAdmin panel\n\nOn the Compute > Storage > Storage policies tab, select a policy from the list.\nOn the policy right pane, click Delete.\nIn the confirmation window, click Delete policy.\n\nCommand-line interface\nUse the following command:vinfra service compute storage-policy delete <storage-policy>\r\n\n\n<storage-policy>\n\nStorage policy ID or name\n\nFor example, to delete the storage policy mystorpolicy, run:# vinfra service compute storage-policy delete mystorpolicy\n\nSee also\n\nUsing volume QoS policies\n\nManaging compute volumes\n\nManaging external storages",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute storage-policy create --tier {0,1,2,3} (--replicas <norm>[:<min>] | --encoding <M>+<N>)\r\n                                             --failure-domain {0,1,2,3,4}\r\n                                             [--write-bytes-sec <limit>] [--read-bytes-sec <limit>]\r\n                                             [--read-iops-sec <limit>] [--write-iops-sec <limit>]\r\n                                             [--total-bytes-sec <limit>] [--total-iops-sec <limit>]\r\n                                             <name>\r\n\n\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--replicas <norm>[:<min>]\n\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n\n--write-bytes-sec <limit>\n\nNumber of bytes written per second\n--read-bytes-sec <limit>\n\nNumber of bytes read per second\n--read-iops-sec <limit>\n\nNumber of read operations per second\n--write-iops-sec <limit>\n\nNumber of write operations per second\n--total-bytes-sec <bytes>\n\nTotal number of bytes per second\n--total-iops-sec <iops>\n\nTotal number of I/O operations per second\n<name>\n\nStorage policy name\n\nFor example, to create a storage policy mystorpolicy with the tier 3, erasure coding 3+2 scheme, host failure domain, and the limits of 100 IOPS and 104857600 bytes per second, run:# vinfra service compute storage-policy create mystorpolicy --tier 3 \\\r\n--encoding 3+2 --failure-domain 1 --total-bytes-sec 104857600 --total-iops-sec 100\nThe created storage policy will appear in the vinfra service compute storage-policy list output:# vinfra service compute storage-policy list\r\n+-------------+--------------+------+--------------+----------------+--------------------------------+\r\n| id          | name         | tier | redundancy   | failure_domain | qos                            |\r\n+-------------+--------------+------+--------------+----------------+--------------------------------+\r\n| 97b55811<\u00e2\u0080\u00a6> | mystorpolicy | 3    | encoding=3+2 | 1              | read_bytes_sec: -1             |\r\n|             |              |      |              |                | read_bytes_sec_per_gb: -1      |\r\n|             |              |      |              |                | read_bytes_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | read_iops_sec: -1              |\r\n|             |              |      |              |                | read_iops_sec_per_gb: -1       |\r\n|             |              |      |              |                | read_iops_sec_per_gb_min: -1   |\r\n|             |              |      |              |                | total_bytes_sec: 104857600     |\r\n|             |              |      |              |                | total_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | total_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | total_iops_sec: 100            |\r\n|             |              |      |              |                | total_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | total_iops_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | write_bytes_sec: -1            |\r\n|             |              |      |              |                | write_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | write_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | write_iops_sec: -1             |\r\n|             |              |      |              |                | write_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | write_iops_sec_per_gb_min: -1  |\r\n| 603bd56b<\u00e2\u0080\u00a6> | default      | 0    | replicas=3   | 1              | read_bytes_sec: -1             |\r\n|             |              |      |              |                | read_bytes_sec_per_gb: -1      |\r\n|             |              |      |              |                | read_bytes_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | read_iops_sec: -1              |\r\n|             |              |      |              |                | read_iops_sec_per_gb: -1       |\r\n|             |              |      |              |                | read_iops_sec_per_gb_min: -1   |\r\n|             |              |      |              |                | total_bytes_sec: -1            |\r\n|             |              |      |              |                | total_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | total_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | total_iops_sec: -1             |\r\n|             |              |      |              |                | total_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | total_iops_sec_per_gb_min: -1  |\r\n|             |              |      |              |                | write_bytes_sec: -1            |\r\n|             |              |      |              |                | write_bytes_sec_per_gb: -1     |\r\n|             |              |      |              |                | write_bytes_sec_per_gb_min: -1 |\r\n|             |              |      |              |                | write_iops_sec: -1             |\r\n|             |              |      |              |                | write_iops_sec_per_gb: -1      |\r\n|             |              |      |              |                | write_iops_sec_per_gb_min: -1  |\r\n+-------------+--------------+------+--------------+----------------+--------------------------------+\n",
                "title": "To create a storage policy"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute storage-policy set [--name <name>] [--tier {0,1,2,3}]\r\n                                          [--replicas <norm>[:<min>] |\r\n                                          --encoding <M>+<N>]\r\n                                          [--failure-domain {0,1,2,3,4}]\r\n                                          [--write-bytes-sec <limit>] [--read-bytes-sec <limit>]\r\n                                          [--read-iops-sec <limit>] [--write-iops-sec <limit>]\r\n                                          [--total-bytes-sec <limit>] [--total-iops-sec <limit>]\r\n                                          <storage-policy>\r\n\n\n--name <name>\n\nA new name for the storage policy\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--replicas <norm>[:<min>]\n\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n\n--write-bytes-sec <limit>\n\nNumber of bytes written per second\n--read-bytes-sec <limit>\n\nNumber of bytes read per second\n--read-iops-sec <limit>\n\nNumber of read operations per second\n--write-iops-sec <limit>\n\nNumber of write operations per second\n--total-bytes-sec <bytes>\n\nTotal number of bytes per second\n--total-iops-sec <iops>\n\nTotal number of I/O operations per second\n<storage-policy>\n\nStorage policy ID or name\n\nFor example, to change the redundancy type for the storage policy mystorpolicy from erasure coding 3+2 to 5+2, run:# vinfra service compute storage-policy set mystorpolicy --encoding 5+2\n",
                "title": "To edit a storage policy"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute storage-policy delete <storage-policy>\r\n\n\n<storage-policy>\n\nStorage policy ID or name\n\nFor example, to delete the storage policy mystorpolicy, run:# vinfra service compute storage-policy delete mystorpolicy\n",
                "title": "To remove a storage policy"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Storage policies tab, click Create storage policy.\n\nIn the Create storage policy window, specify a policy name and select  redundancy settings.\n\n\n\n\n\n\nEnable IOPS limit or Bandwidth limit to set the corresponding limits on the volume.\n\nClick Create.\n\n",
                "title": "To create a storage policy"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Storage policies tab, select a policy from the list.\nOn the policy right pane, click Edit.\nChange the required parameters, and then click Save.\n\nKeep in mind, that changes to the storage policy will affect the redundancy and performance of all the volumes covered by it.\n",
                "title": "To edit a storage policy"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Storage policies tab, select a policy from the list.\nOn the policy right pane, click Delete.\nIn the confirmation window, click Delete policy.\n\n",
                "title": "To remove a storage policy"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-storage-policies.html"
    },
    {
        "title": "Managing security group rules",
        "content": "Managing security group rules\nYou can modify security groups by adding and removing rules. Editing rules is not available. If you need to change the existing rule, remove it and re-create with the required parameters.\nPrerequisites\n\nYou have a security group created, as described in Creating and deleting security groups.\n\nTo add a rule to a security group\n\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click the security group to add a rule to.\nOn the group right pane, click Add in the Inbound or Outbound section to create a rule for incoming or outgoing traffic.\nSpecify the rule parameters:\r\n\t\t\tSelect a protocol from the list or enter a number from 0 to 255.Enter a single port or a port range. Some protocols already have a predefined port range. For example, the port for SSH is 22.Select a predefined subnet CIDR or an existing security group.\nClick the check mark to save the changes.\n\nAs soon as the rule is created, it is applied to all of the virtual machines assigned to the security group.\n\nCommand-line interface\nUse the following command:vinfra service compute security-group rule create [--remote-group <remote-group>]\r\n                                                  [--remote-ip <ip-address>]\r\n                                                  [--ethertype <ethertype>]\r\n                                                  [--protocol <protocol>]\r\n                                                  [--port-range-max <port-range-max>]\r\n                                                  [--port-range-min <port-range-min>]\r\n                                                  (--ingress | --egress)\r\n                                                  <security-group>\n\n--remote-group <remote-group>\n\nRemote security group name or ID\n--remote-ip <ip-address>\n\nRemote IP address block in CIDR notation\n--ethertype <ethertype>\n\nEthertype of network traffic: IPv4 or IPv6\n--protocol <protocol>\n\nIP protocol: tcp, udp, icmp, vrrp and others\n--port-range-max <port-range-max>\n\nThe maximum port number in the port range that satisfies the security group rule\n--port-range-min <port-range-min>\n\nThe minimum port number in the port range that satisfies the security group rule\n--ingress\n\nRule for incoming network traffic\n--egress\n\nRule for outgoing network traffic\n<security-group>\n\nSecurity group name or ID to create the rule in\n\nFor example, to create a rule in the security group mygroup to allow incoming IPv4 network traffic on TCP port 22, run:# vinfra service compute security-group rule create mygroup \\\r\n--ethertype IPv4 --protocol tcp --port-range-max 22 --port-range-min 22 --ingress\r\n+-------------------+--------------------------------------+\r\n| Field             | Value                                |\r\n+-------------------+--------------------------------------+\r\n| description       |                                      |\r\n| direction         | ingress                              |\r\n| ethertype         | IPv4                                 |\r\n| id                | 0f395e2f-a8ab-47f4-b670-64399461393c |\r\n| port_range_max    | 22                                   |\r\n| port_range_min    | 22                                   |\r\n| project_id        | e215189c0472482f93e71d10e1245253     |\r\n| protocol          | tcp                                  |\r\n| remote_group_id   |                                      |\r\n| remote_ip_prefix  |                                      |\r\n| security_group_id | 12e6b260-0b61-4551-8168-3e59602a2433 |\r\n+-------------------+--------------------------------------+\nThis rule will appear in the vinfra service compute security-group rule list output:# vinfra service compute security-group rule list mygroup -c id -c direction -c protocol\r\n+--------------------------------------+-----------+----------+\r\n| id                                   | direction | protocol |\r\n+--------------------------------------+-----------+----------+\r\n| 0f395e2f-a8ab-47f4-b670-64399461393c | ingress   | tcp      |\r\n| a7c65861-df3d-47f2-bec3-089747141936 | egress    |          |\r\n| ce854e2b-537f-4618-bea9-e9ec3d8616ac | egress    |          |\r\n+--------------------------------------+-----------+----------+\r\n\n\nTo remove a rule from a security group\n\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click the required security group.\nOn the group right pane, click the bin icon next to a rule you want to remove.\n\nAs soon as the rule is removed, this change is applied to all of the virtual machines assigned to the security group.\n\nCommand-line interface\nUse the following command:vinfra service compute security-group rule delete <security-group-rule>\r\n\n\n<security-group-rule>\n\nSecurity group rule  ID\n\nFor example, to delete the security group rule with the ID\u00a00f395e2f-a8ab-47f4-b670-64399461393c, run:# vinfra service compute security-group rule delete 0f395e2f-a8ab-47f4-b670-64399461393c\n\nSee also\n\nCreating virtual machines\n\nWhat's next\n\nChanging security group assignment",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute security-group rule create [--remote-group <remote-group>]\r\n                                                  [--remote-ip <ip-address>]\r\n                                                  [--ethertype <ethertype>]\r\n                                                  [--protocol <protocol>]\r\n                                                  [--port-range-max <port-range-max>]\r\n                                                  [--port-range-min <port-range-min>]\r\n                                                  (--ingress | --egress)\r\n                                                  <security-group>\n\n--remote-group <remote-group>\n\nRemote security group name or ID\n--remote-ip <ip-address>\n\nRemote IP address block in CIDR notation\n--ethertype <ethertype>\n\nEthertype of network traffic: IPv4 or IPv6\n--protocol <protocol>\n\nIP protocol: tcp, udp, icmp, vrrp and others\n--port-range-max <port-range-max>\n\nThe maximum port number in the port range that satisfies the security group rule\n--port-range-min <port-range-min>\n\nThe minimum port number in the port range that satisfies the security group rule\n--ingress\n\nRule for incoming network traffic\n--egress\n\nRule for outgoing network traffic\n<security-group>\n\nSecurity group name or ID to create the rule in\n\nFor example, to create a rule in the security group mygroup to allow incoming IPv4 network traffic on TCP port 22, run:# vinfra service compute security-group rule create mygroup \\\r\n--ethertype IPv4 --protocol tcp --port-range-max 22 --port-range-min 22 --ingress\r\n+-------------------+--------------------------------------+\r\n| Field             | Value                                |\r\n+-------------------+--------------------------------------+\r\n| description       |                                      |\r\n| direction         | ingress                              |\r\n| ethertype         | IPv4                                 |\r\n| id                | 0f395e2f-a8ab-47f4-b670-64399461393c |\r\n| port_range_max    | 22                                   |\r\n| port_range_min    | 22                                   |\r\n| project_id        | e215189c0472482f93e71d10e1245253     |\r\n| protocol          | tcp                                  |\r\n| remote_group_id   |                                      |\r\n| remote_ip_prefix  |                                      |\r\n| security_group_id | 12e6b260-0b61-4551-8168-3e59602a2433 |\r\n+-------------------+--------------------------------------+\nThis rule will appear in the vinfra service compute security-group rule list output:# vinfra service compute security-group rule list mygroup -c id -c direction -c protocol\r\n+--------------------------------------+-----------+----------+\r\n| id                                   | direction | protocol |\r\n+--------------------------------------+-----------+----------+\r\n| 0f395e2f-a8ab-47f4-b670-64399461393c | ingress   | tcp      |\r\n| a7c65861-df3d-47f2-bec3-089747141936 | egress    |          |\r\n| ce854e2b-537f-4618-bea9-e9ec3d8616ac | egress    |          |\r\n+--------------------------------------+-----------+----------+\r\n\n",
                "title": "To add a rule to a security group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute security-group rule delete <security-group-rule>\r\n\n\n<security-group-rule>\n\nSecurity group rule  ID\n\nFor example, to delete the security group rule with the ID\u00a00f395e2f-a8ab-47f4-b670-64399461393c, run:# vinfra service compute security-group rule delete 0f395e2f-a8ab-47f4-b670-64399461393c\n",
                "title": "To remove a rule from a security group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click the security group to add a rule to.\nOn the group right pane, click Add in the Inbound or Outbound section to create a rule for incoming or outgoing traffic.\nSpecify the rule parameters:\r\n\t\t\tSelect a protocol from the list or enter a number from 0 to 255.Enter a single port or a port range. Some protocols already have a predefined port range. For example, the port for SSH is 22.Select a predefined subnet CIDR or an existing security group.\nClick the check mark to save the changes.\n\nAs soon as the rule is created, it is applied to all of the virtual machines assigned to the security group.\n",
                "title": "To add a rule to a security group"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > Security groups tab, click the required security group.\nOn the group right pane, click the bin icon next to a rule you want to remove.\n\nAs soon as the rule is removed, this change is applied to all of the virtual machines assigned to the security group.\n",
                "title": "To remove a rule from a security group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-security-group-rules.html"
    },
    {
        "title": "Managing the compute storage",
        "content": "Managing the compute storage\nIn Virtuozzo Hybrid Infrastructure, the compute storage comprises volumes provisioned to virtual machines. The provisioned storage space can exceed available physical space. Rules for storing volumes can be set via storage policies. Storage policies also allow you to set different performance levels and redundancy modes for VM volumes.\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-the-compute-storage.html"
    },
    {
        "title": "Managing targets",
        "content": "Managing targets\nPrerequisites\n\nA target group is created, as described in Creating target groups.\n\nTo add a target to a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\nOn the Targets tab, click Add target. The Create target wizard will open.\n\nOn Nodes, select nodes to add to the target group. On these nodes, iSCSI targets will run. You can only choose nodes with network interfaces that are assigned the iSCSI traffic type. It is recommended to have at least two nodes in the target group to achieve high availability. If you plan to use multiple iSCSI initiators, you should have as many nodes in the target group. The optimal way is to create a single target per node.\nIf the node network interfaces are not configured, click the cogwheel icon, select the networks as required, and then click Apply.\n\nOn Targets, select iSCSI interfaces to add to the target group. You can choose from a list of network interfaces that are assigned the iSCSI traffic type. If you plan to use multiple iSCSI initiators, you should select as many interfaces per node. One interface can be added to multiple target groups, although it may reduce performance.\n\nOn Summary, review the target details. You can go back to change them if necessary. Click Next.\n\nThe created target will appear on the Targets tab.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group target create --node <node> --ip <ip> <target-group> <name>\n\n--node <node>\n\nNode ID or hostname\n--ip <ip>\n\nTarget IP address\n<target-group>\n\nTarget group name or ID\n<name>\n\nTarget name\n\nFor example, to add the target target3 to the target group tg1 that will run on the node node003 with the IP address 10.10.10.13, run:# vinfra service block-storage target-group target create --node node003 --ip 10.10.10.13 tg1 target3\nThe added target will appear in the vinfra service block-storage target-group target list output:# vinfra service block-storage target-group target list tg1\r\n+------------------+----------------------------------+---------+-----------------------+\r\n| node_id          | iqn                              | state   | portals               |\r\n+------------------+----------------------------------+---------+-----------------------+\r\n| 7a8f9a2f-fd11<\u00e2\u0080\u00a6> | iqn.2014-06.com.vstorage:target1 | offline | - ipaddr: 10.10.10.11 |\r\n|                  |                                  |         |   port: 3260          |\r\n| a32ba24a-2473<\u00e2\u0080\u00a6> | iqn.2014-06.com.vstorage:target2 | offline | - ipaddr: 10.10.10.12 |\r\n|                  |                                  |         |   port: 3260          |\r\n| 2cef3925-51dc<\u00e2\u0080\u00a6> | iqn.2014-06.com.vstorage:target3 | offline | - ipaddr: 10.10.10.13 |\r\n|                  |                                  |         |   port: 3260          |\r\n+------------------+----------------------------------+---------+-----------------------+\r\n\n\nTo start or stop all targets in a target group\n\nAdmin panel\n\n Open Storage services > Block storage > Target groups. \nClick the ellipsis icon of the desired target group, and then click Start targets or Stop targets.\n\nCommand-line interface\nUse the following commands:\n\nTo start all targets in your target group:vinfra service block-storage target-group start <target-group>\n\nTo stop all targets in your target group:vinfra service block-storage target-group stop <target-group>\n\nTo delete a target from a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\nOn the Targets tab, click the ellipsis button of the desired target, and then click Delete.\nIf the target has active connections, select Force.\nClick Delete in the confirmation window.\n\nIf you delete a target on the Active/Optimized path (indicated in LUN details), said path will switch to another target.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group target delete [--force] <target-group> <target>\n\n--force\n\nForcibly remove a target\n<target-group>\n\nTarget group name or ID\n<target>\n\nTarget IQN\n\nFor example, to remove the target with the IQN iqn.2014-06.com.vstorage:target3 from the target group tg1, run:# vinfra service block-storage target-group target delete tg1 iqn.2014-06.com.vstorage:target3\n\nSee also\n\nManaging volumes\n\nRestricting access to target groups\n\nMonitoring block storage\n\nDeleting target groups",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group target create --node <node> --ip <ip> <target-group> <name>\n\n--node <node>\n\nNode ID or hostname\n--ip <ip>\n\nTarget IP address\n<target-group>\n\nTarget group name or ID\n<name>\n\nTarget name\n\nFor example, to add the target target3 to the target group tg1 that will run on the node node003 with the IP address 10.10.10.13, run:# vinfra service block-storage target-group target create --node node003 --ip 10.10.10.13 tg1 target3\nThe added target will appear in the vinfra service block-storage target-group target list output:# vinfra service block-storage target-group target list tg1\r\n+------------------+----------------------------------+---------+-----------------------+\r\n| node_id          | iqn                              | state   | portals               |\r\n+------------------+----------------------------------+---------+-----------------------+\r\n| 7a8f9a2f-fd11<\u00e2\u0080\u00a6> | iqn.2014-06.com.vstorage:target1 | offline | - ipaddr: 10.10.10.11 |\r\n|                  |                                  |         |   port: 3260          |\r\n| a32ba24a-2473<\u00e2\u0080\u00a6> | iqn.2014-06.com.vstorage:target2 | offline | - ipaddr: 10.10.10.12 |\r\n|                  |                                  |         |   port: 3260          |\r\n| 2cef3925-51dc<\u00e2\u0080\u00a6> | iqn.2014-06.com.vstorage:target3 | offline | - ipaddr: 10.10.10.13 |\r\n|                  |                                  |         |   port: 3260          |\r\n+------------------+----------------------------------+---------+-----------------------+\r\n\n",
                "title": "To add a target to a target group"
            },
            {
                "example": "\nCommand-line interface\nUse the following commands:\n\n\nTo start all targets in your target group:vinfra service block-storage target-group start <target-group>\n\n\nTo stop all targets in your target group:vinfra service block-storage target-group stop <target-group>\n\n\n",
                "title": "To start or stop all targets in a target group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group target delete [--force] <target-group> <target>\n\n--force\n\nForcibly remove a target\n<target-group>\n\nTarget group name or ID\n<target>\n\nTarget IQN\n\nFor example, to remove the target with the IQN iqn.2014-06.com.vstorage:target3 from the target group tg1, run:# vinfra service block-storage target-group target delete tg1 iqn.2014-06.com.vstorage:target3\n",
                "title": "To delete a target from a target group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\n\n\n\n\nOn the Targets tab, click Add target. The Create target wizard will open.\n\nOn Nodes, select nodes to add to the target group. On these nodes, iSCSI targets will run. You can only choose nodes with network interfaces that are assigned the iSCSI traffic type. It is recommended to have at least two nodes in the target group to achieve high availability. If you plan to use multiple iSCSI initiators, you should have as many nodes in the target group. The optimal way is to create a single target per node.\nIf the node network interfaces are not configured, click the cogwheel icon, select the networks as required, and then click Apply.\n\n\n\n\n\n\nOn Targets, select iSCSI interfaces to add to the target group. You can choose from a list of network interfaces that are assigned the iSCSI traffic type. If you plan to use multiple iSCSI initiators, you should select as many interfaces per node. One interface can be added to multiple target groups, although it may reduce performance.\n\n\n\n\n\nOn Summary, review the target details. You can go back to change them if necessary. Click Next.\n\nThe created target will appear on the Targets tab.\n",
                "title": "To add a target to a target group"
            },
            {
                "example": "\nAdmin panel\n\n Open Storage services > Block storage > Target groups. \nClick the ellipsis icon of the desired target group, and then click Start targets or Stop targets.\n\n",
                "title": "To start or stop all targets in a target group"
            },
            {
                "example": "\nAdmin panel\n\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\n\n\n\n\nOn the Targets tab, click the ellipsis button of the desired target, and then click Delete.\nIf the target has active connections, select Force.\nClick Delete in the confirmation window.\n\nIf you delete a target on the Active/Optimized path (indicated in LUN details), said path will switch to another target.\n",
                "title": "To delete a target from a target group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-targets.html"
    },
    {
        "title": "Managing the infrastructure",
        "content": "Managing the infrastructure\nThis section outlines common administrator's tasks for the infrastructure: the license, networks, nodes, security, and the self service.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-the-infrastructure.html"
    },
    {
        "title": "Managing traffic types",
        "content": "Managing traffic types\nYou can manage three groups of traffic types: two default groups, exclusive and regular, created with the infrastructure and custom traffic types created by users. The available actions depend on the traffic type group.\nPrerequisites\n\nA clear understanding of the concept Traffic types.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-traffic-types.html"
    },
    {
        "title": "Managing virtual machines",
        "content": "Managing virtual machines\nEach virtual machine (VM) is an independent system with an independent set of virtual hardware. Its main features are the following:\n\nA virtual machine resembles and works like a regular computer. It has its own virtual hardware. Software applications can run in virtual machines without any modifications or adjustment.\nVirtual machine configuration can be changed easily, for example, by adding new virtual disks or memory.\nAlthough virtual machines share physical hardware resources, they are fully isolated from each other (file system, processes, sysctl variables) and the compute node.\nA virtual machine can run any supported guest operating system.\n\nThe following table lists the current virtual machine configuration limits:\n\nResource\nLimit\n\nRAM\n1 TiB\n\nCPU\n64 virtual CPUs\n\nStorage\n15 volumes, 512 TiB each\n\nNetwork\n15 NICs\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-virtual-machines.html"
    },
    {
        "title": "Managing virtual routers",
        "content": "Managing virtual routers\nVirtual routers provide L3 services such as routing and Source Network Address Translation (SNAT) between virtual and physical networks, or different virtual networks:\n\nA virtual router between virtual and physical networks provides access to public networks, such as the Internet, for VMs connected to this virtual network.\nA virtual router between different virtual networks provides network communication for VMs connected to these virtual networks.\n\nA virtual router has two types of ports:\n\nAn external gateway that is connected to a physical network.\nAn internal port that is connected to a virtual network.\n\nWith virtual routers, you can do the following:\n\nCreate virtual routers\nChange external or internal router interfaces\nCreate, edit, and delete static routes\nChange a router name\nDelete a router\n\nLimitations\n\nA router can only connect networks that have IP management enabled.\n\nYou can delete a virtual router if no floating IP addresses are associated with any network it is connected to.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-virtual-routers.html"
    },
    {
        "title": "Managing VPN connections",
        "content": "Managing VPN connections\nWith Virtual Private Network (VPN) as a service, self-service users can extend virtual networks across public networks, such as the Internet. To connect two or more remote endpoints, VPNs use virtual connections tunneled through physical networks. To secure VPN communication, the traffic that flows between remote endpoints is encrypted. The VPN implementation uses the Internet Key Exchange (IKE) and IP Security (IPsec) protocols to establish secure VPN connections and is based on the strongSwan IPsec solution.\nVPN as a service can be used to establish a Site-to-Site VPN connection between a virtual network configured in Virtuozzo Hybrid Infrastructure and any other network with a VPN gateway that uses the IPsec and IKE protocols. With VPN as a service, you can connect the following workloads:\n\nOn-premises workloads with workloads hosted in Virtuozzo Hybrid Infrastructure\nWorkloads hosted in other clouds with workloads hosted in Virtuozzo Hybrid Infrastructure\nWorkloads hosted in different Virtuozzo Hybrid Infrastructure clusters\n\nAdditionally, VPN as a service provides high availability to VPN connections in clusters with enabled HA. If a node that hosts a virtual router fails, a VPN connection re-initiates after the virtual router relocates to a healthy node.\nVPN connections are created and managed by self-service users, as described in \"Managing VPN connections\" in the Self-Service Guide. In the admin panel, you can view VPN connection details and delete VPN connections.\nLimitations\n\nCurrently, we support only Site-to-Site VPN connections. Point-to-Site VPN connections are not supported.\nVPN connections cannot be tunneled through IPv6 and dual-stack physical networks.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\n\nTo view the details of a VPN connection\n\nAdmin panel\nOn the Compute > Network > VPN screen, click a VPN connection to open its right pane. \n\nCommand-line interface\nUse the following command:vinfra service compute vpn connection show <connection>\r\n\n\n<connection>\n\nVPN connection ID or name\n\nFor example, to view the details of the VPN connection vpn1, run:# vinfra service compute vpn connection show vpn1\r\n+-------------------+--------------------------------------+\r\n| Field             | Value                                |\r\n+-------------------+--------------------------------------+\r\n| dpd               | action: hold                         |\r\n|                   | interval: 30                         |\r\n|                   | timeout: 120                         |\r\n| id                | 9848fd7c-ac1c-4412-bf8d-7616b13a3d03 |\r\n| ikepolicy_id      | 1d70c833-4a8b-455b-9a1b-a86a61159123 |\r\n| initiator         | bi-directional                       |\r\n| ipsecpolicy_id    | 2e1edf17-2874-41ba-9faa-0cb879d09c97 |\r\n| local_ep_group_id | cc8959d8-7274-44b3-b76c-373b19b1ca32 |\r\n| local_id          |                                      |\r\n| mtu               | 1500                                 |\r\n| name              | vpn1                                 |\r\n| peer_address      | 10.136.18.134                        |\r\n| peer_ep_group_id  | deb02fcd-6e24-46e8-b3db-bf41b9ec2564 |\r\n| peer_id           | 10.136.18.134                        |\r\n| project_id        | bba7c2edf544432c9177e2b63b755e10     |\r\n| route_mode        | static                               |\r\n| router_id         | 1da614a7-3fe7-42e0-9494-864d1e890135 |\r\n| status            | ACTIVE                               |\r\n| vpnservice_id     | 01a4ee33-2192-4575-9b01-629144093712 |\r\n+-------------------+--------------------------------------+\n\nTo delete a VPN connection\n\nAdmin panel\n\nOn the Compute > Network > VPN screen, click a VPN connection.\n On the right pane, click Delete.\nClick Delete in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service compute vpn connection delete <connection>\r\n\n\n<connection>\n\nVPN connection ID or name\n\nFor example, to delete the VPN connection vpn1, run:# vinfra service compute vpn connection delete vpn1\r\n\n\nSee also\n\nManaging compute networks\n\nManaging virtual routers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute vpn connection show <connection>\r\n\n\n<connection>\n\nVPN connection ID or name\n\nFor example, to view the details of the VPN connection vpn1, run:# vinfra service compute vpn connection show vpn1\r\n+-------------------+--------------------------------------+\r\n| Field             | Value                                |\r\n+-------------------+--------------------------------------+\r\n| dpd               | action: hold                         |\r\n|                   | interval: 30                         |\r\n|                   | timeout: 120                         |\r\n| id                | 9848fd7c-ac1c-4412-bf8d-7616b13a3d03 |\r\n| ikepolicy_id      | 1d70c833-4a8b-455b-9a1b-a86a61159123 |\r\n| initiator         | bi-directional                       |\r\n| ipsecpolicy_id    | 2e1edf17-2874-41ba-9faa-0cb879d09c97 |\r\n| local_ep_group_id | cc8959d8-7274-44b3-b76c-373b19b1ca32 |\r\n| local_id          |                                      |\r\n| mtu               | 1500                                 |\r\n| name              | vpn1                                 |\r\n| peer_address      | 10.136.18.134                        |\r\n| peer_ep_group_id  | deb02fcd-6e24-46e8-b3db-bf41b9ec2564 |\r\n| peer_id           | 10.136.18.134                        |\r\n| project_id        | bba7c2edf544432c9177e2b63b755e10     |\r\n| route_mode        | static                               |\r\n| router_id         | 1da614a7-3fe7-42e0-9494-864d1e890135 |\r\n| status            | ACTIVE                               |\r\n| vpnservice_id     | 01a4ee33-2192-4575-9b01-629144093712 |\r\n+-------------------+--------------------------------------+\n",
                "title": "To view the details of a VPN connection"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute vpn connection delete <connection>\r\n\n\n<connection>\n\nVPN connection ID or name\n\nFor example, to delete the VPN connection vpn1, run:# vinfra service compute vpn connection delete vpn1\r\n\n",
                "title": "To delete a VPN connection"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nOn the Compute > Network > VPN screen, click a VPN connection to open its right pane. \n",
                "title": "To view the details of a VPN connection"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Network > VPN screen, click a VPN connection.\n On the right pane, click Delete.\nClick Delete in the confirmation window.\n\n",
                "title": "To delete a VPN connection"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-vpn-connections.html"
    },
    {
        "title": "Migrating virtual machines",
        "content": "Migrating virtual machines\nVirtual machine migration helps facilitate cluster upgrades and workload balancing between compute nodes. Virtuozzo Hybrid Infrastructure allows you to perform two types of migration:\n\nCold migration for stopped virtual machines\nHot migration for running virtual machines (allows you to avoid VM downtime)\n\nFor both migration types, a virtual machine is migrated between compute nodes using shared storage, so no block device migration takes place.\nHot migration consists of the following steps:\n\nAll VM memory is copied to the destination node, while the virtual machine keeps running on the source node. If a VM memory page changes, it is copied again.\nWhen only a few memory pages are left to copy, the VM is stopped on the source node, the remaining pages are transferred, and the VM is restarted on the destination node.\n\nLarge virtual machines with write-intensive workloads write to memory faster than memory changes can be transferred to the destination node, thus preventing migration from converging. For such VMs, the auto-converge mechanism is used. When a lack of convergence is detected during live migration, the VM\u00e2\u0080\u0099s vCPU execution speed is throttled down, which also slows down writing to the VM memory. Initially, the virtual machine\u00e2\u0080\u0099s vCPU is throttled by 20 percent and then by 10 percent during each iteration. This process continues until writing to the VM memory slows down enough for migration to complete or the VM vCPU is throttled by 99 percent.\nLimitations\n\nVirtual machines are created with the host CPU model, by default. Having compute nodes with different CPUs may lead to live migration issues. To avoid them, you can manually set the CPU model for all new VMs, as described in Setting virtual machine CPU model. Alternatively, you can create a placement for each group of compute nodes with the same CPU model by using the instructions in Managing placements for compute nodes.\n\nMigration is not supported for suspended virtual machines.\n\nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.\n\nTo migrate a virtual machine\n\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click a VM to migrate.\nClick the ellipsis button next to the VM, and then select Migrate.\n\nIn the new window, specify the destination node:\n\nAuto. Automatically select the optimal destination among cluster nodes, based on available CPU and RAM resources.\nSelect the destination node manually from the drop-down list.\n\nBy default, running VMs are migrated live. You can change the migration mode to offline by selecting the Cold migration check box. A VM will be stopped and restarted on the destination node after migration.\n\nClick Migrate to reserve resources on the destination node and start migration.\n\nThe admin panel will show the migration progress.\n\nCommand-line interface\nUse the following command:vinfra service compute server migrate [--cold] [--node <node>] <server>\r\n\n\n--cold\n\nPerform cold migration. If not set, the migration type is determined automatically.\n--node <node>\n\nDestination node ID or hostname\n<server>\n\nVirtual machine ID or name\n\nFor example, to start migration of the virtual machine myvm to the compute node node003.vstoragedomain, run:# vinfra service compute server migrate myvm --node node003\n\nSee also\n\nManaging virtual machine power state\n\nMonitoring virtual machines\n\nTroubleshooting virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server migrate [--cold] [--node <node>] <server>\r\n\n\n--cold\n\nPerform cold migration. If not set, the migration type is determined automatically.\n--node <node>\n\nDestination node ID or hostname\n<server>\n\nVirtual machine ID or name\n\nFor example, to start migration of the virtual machine myvm to the compute node node003.vstoragedomain, run:# vinfra service compute server migrate myvm --node node003\n",
                "title": "To migrate a virtual machine"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click a VM to migrate.\nClick the ellipsis button next to the VM, and then select Migrate.\n\nIn the new window, specify the destination node:\n\nAuto. Automatically select the optimal destination among cluster nodes, based on available CPU and RAM resources.\nSelect the destination node manually from the drop-down list.\n\n\n\n\n\n\n\nBy default, running VMs are migrated live. You can change the migration mode to offline by selecting the Cold migration check box. A VM will be stopped and restarted on the destination node after migration.\n\nClick Migrate to reserve resources on the destination node and start migration.\n\nThe admin panel will show the migration progress.\n",
                "title": "To migrate a virtual machine"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/migrating-virtual-machines.html"
    },
    {
        "title": "Managing volume snapshots",
        "content": "Managing volume snapshots\nYou can save the current state of a VM file system or user data by creating a snapshot of a volume. A snapshot of a boot volume may be useful, for example, before updating VM software. If anything goes wrong, you will be able to revert the VM to a working state at any time. A snapshot of a data volume can be used for backing up user data and testing purposes.\nPrerequisites\n\nTo create a consistent snapshot of a running VM\u00e2\u0080\u0099s volume, the guest tools must be installed in the VM, as described in Installing guest tools. The QEMU guest agent included in the guest tools image automatically quiesces the filesystem during snapshotting.\n\nTo create a snapshot of a volume\n\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click a volume.\n\nIn the volume right pane, switch to Snapshots, and then click Create snapshot.\n\nCommand-line interface\nUse the following command:vinfra service compute volume snapshot create [--description <description>]\r\n                                              --volume <volume> <volume-snapshot-name>\r\n\n\n--description <description>\n\nVolume snapshot description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--volume <volume>\n\nVolume ID or name\n<volume-snapshot-name>\n\nVolume snapshot name\n\nFor example, to create a snapshot mysnapshot of the volume myvolume, run:# vinfra service compute volume snapshot create mysnapshot --volume myvolume\r\n+-------------+--------------------------------------+\r\n| Field       | Value                                |\r\n+-------------+--------------------------------------+\r\n| created_at  | 2019-04-30T13:12:54.297629+00:00     |\r\n| description |                                      |\r\n| id          | 3fdfe5d6-8bd2-4bf5-8599-a9cef50e5b71 |\r\n| metadata    | {}                                   |\r\n| name        | mysnapshot                           |\r\n| project_id  | fd0ae61496d04ef6bb637bc3167b7eaf     |\r\n| size        | 8                                    |\r\n| status      | creating                             |\r\n| volume_id   | 92dc3bd7-713d-42bf-83cd-4de40c24fed9 |\r\n+-------------+--------------------------------------+\r\n\nThe new snapshot will appear in the vinfra service compute volume snapshot list output:# vinfra service compute volume snapshot list -c id -c name -c size -c status\r\n+--------------------------------------+------------+-----------+\r\n| id                                   | name       | status    |\r\n+--------------------------------------+------------+-----------+\r\n| 3fdfe5d6-8bd2-4bf5-8599-a9cef50e5b71 | mysnapshot | available |\r\n+--------------------------------------+------------+-----------+\r\n\n\nTo manage a volume snapshot\n\nAdmin panel\nSelect a volume and open the Snapshots tab on its right pane.\n\nYou can do the following:\n\nCreate a new volume from the snapshot.\nCreate a template from the snapshot.\n\nDiscard all changes that have been made to the volume since the snapshot was taken. This action is available only for VMs with the \"Shut down\" and \"Shelved offloaded\" statuses.\n\nAs each volume has only one snapshot branch, all snapshots created after the snapshot you are reverting to will be deleted. If you want to save a subsequent snapshot before reverting, create a volume or an image from it first.\n\nChange the snapshot name and description.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\nReset the snapshot stuck in an \"Error\" state or transitional state to the \"Available\" state. \nRemove the snapshot.\n\nTo perform these actions, click the ellipsis button next to a snapshot, and then click the corresponding action.\n\nCommand-line interface\nUse the following commands:\n\nTo revert a volume to the snapshot, use vinfra service compute volume snapshot revert. For example:# vinfra service compute volume snapshot revert mynewsnapshot\n\nTo create a template from the snapshot, use vinfra service compute volume snapshot upload-to-image. For example:# vinfra service compute volume snapshot upload-to-image --name myvm-image mysnapshot\r\n\n\nTo create a new volume from the snapshot, use vinfra service compute volume create. For example:# vinfra service compute volume create myvolume2 --snapshot mysnapshot --storage-policy default --size 8\n\nTo change the snapshot name and description, use vinfra service compute volume snapshot set. For example:# vinfra service compute volume snapshot set mysnapshot --name mynewsnapshot \\\r\n--description \"My new snapshot\"\r\n\n\nTo reset the snapshot stuck in an \"Error\" state or transitional state to the \"Available\" state, use vinfra service compute volume snapshot reset-state. For example:# vinfra service compute volume snapshot reset-state mysnapshot\n\nTo remove the snapshot, use vinfra service compute volume snapshot delete. For example:# vinfra service compute volume snapshot delete mynewsnapshot\n\nSee also\n\nAttaching and detaching volumes\n\nResizing volumes\n\nChanging the storage policy for volumes\n\nCloning volumes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute volume snapshot create [--description <description>]\r\n                                              --volume <volume> <volume-snapshot-name>\r\n\n\n--description <description>\n\n\nVolume snapshot description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--volume <volume>\n\nVolume ID or name\n<volume-snapshot-name>\n\nVolume snapshot name\n\nFor example, to create a snapshot mysnapshot of the volume myvolume, run:# vinfra service compute volume snapshot create mysnapshot --volume myvolume\r\n+-------------+--------------------------------------+\r\n| Field       | Value                                |\r\n+-------------+--------------------------------------+\r\n| created_at  | 2019-04-30T13:12:54.297629+00:00     |\r\n| description |                                      |\r\n| id          | 3fdfe5d6-8bd2-4bf5-8599-a9cef50e5b71 |\r\n| metadata    | {}                                   |\r\n| name        | mysnapshot                           |\r\n| project_id  | fd0ae61496d04ef6bb637bc3167b7eaf     |\r\n| size        | 8                                    |\r\n| status      | creating                             |\r\n| volume_id   | 92dc3bd7-713d-42bf-83cd-4de40c24fed9 |\r\n+-------------+--------------------------------------+\r\n\nThe new snapshot will appear in the vinfra service compute volume snapshot list output:# vinfra service compute volume snapshot list -c id -c name -c size -c status\r\n+--------------------------------------+------------+-----------+\r\n| id                                   | name       | status    |\r\n+--------------------------------------+------------+-----------+\r\n| 3fdfe5d6-8bd2-4bf5-8599-a9cef50e5b71 | mysnapshot | available |\r\n+--------------------------------------+------------+-----------+\r\n\n",
                "title": "To create a snapshot of a volume"
            },
            {
                "example": "\nCommand-line interface\nUse the following commands:\n\n\nTo revert a volume to the snapshot, use vinfra service compute volume snapshot revert. For example:# vinfra service compute volume snapshot revert mynewsnapshot\n\n\nTo create a template from the snapshot, use vinfra service compute volume snapshot upload-to-image. For example:# vinfra service compute volume snapshot upload-to-image --name myvm-image mysnapshot\r\n\n\n\nTo create a new volume from the snapshot, use vinfra service compute volume create. For example:# vinfra service compute volume create myvolume2 --snapshot mysnapshot --storage-policy default --size 8\n\n\nTo change the snapshot name and description, use vinfra service compute volume snapshot set. For example:# vinfra service compute volume snapshot set mysnapshot --name mynewsnapshot \\\r\n--description \"My new snapshot\"\r\n\n\n\nTo reset the snapshot stuck in an \"Error\" state or transitional state to the \"Available\" state, use vinfra service compute volume snapshot reset-state. For example:# vinfra service compute volume snapshot reset-state mysnapshot\n\n\nTo remove the snapshot, use vinfra service compute volume snapshot delete. For example:# vinfra service compute volume snapshot delete mynewsnapshot\n\n\n",
                "title": "To manage a volume snapshot"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click a volume.\n\nIn the volume right pane, switch to Snapshots, and then click Create snapshot.\n\n\n\n\n\n\n",
                "title": "To create a snapshot of a volume"
            },
            {
                "example": "\nAdmin panel\nSelect a volume and open the Snapshots tab on its right pane.\n\n\n\n\nYou can do the following:\n\nCreate a new volume from the snapshot.\nCreate a template from the snapshot.\n\nDiscard all changes that have been made to the volume since the snapshot was taken. This action is available only for VMs with the \"Shut down\" and \"Shelved offloaded\" statuses.\n\nAs each volume has only one snapshot branch, all snapshots created after the snapshot you are reverting to will be deleted. If you want to save a subsequent snapshot before reverting, create a volume or an image from it first.\n\n\n\n\n\nChange the snapshot name and description.\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\nReset the snapshot stuck in an \"Error\" state or transitional state to the \"Available\" state. \nRemove the snapshot.\n\nTo perform these actions, click the ellipsis button next to a snapshot, and then click the corresponding action.\n",
                "title": "To manage a volume snapshot"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-volume-snapshots.html"
    },
    {
        "title": "Managing volumes",
        "content": "Managing volumes\nPrerequisites\n\nA block volume is created, as described in Creating volumes.\nThe volume is attached to the target group by following the instructions in Attaching volumes to target groups.\n\nTo set a read/write limit for a volume attached as a LUN\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\nOn the LUNs tab, click the desired LUN to open its details, then click the pencil icon in the Limits field.\n\nIn the Set LUN limit window, enter limit values, and then click Save.\n\nThe set limits will be shown in LUN details.\n\nCommand-line interface\nUse the following command:vinfra service block-storage volume set [--read-ops-limit <iops>] [--write-ops-limit <iops>]\r\n                                        [--read-bps-limit <MiB/s>] [--write-bps-limit <MiB/s>] <volume>\n\n--read-ops-limit <iops>\n\nNumber of read operations per second\n--write-ops-limit <iops>\n\nNumber of write operations per second\n--read-bps-limit <MiB/s>\n\nNumber of mebibytes read per second\n--write-bps-limit <MiB/s>\n\nNumber of mebibytes written per second\n<volume>\n\nVolume name or ID\n\nFor example, to limit the IOPS and throughput of the volume vol1 to 100 IOPS and 100 MiB/s, run:# vinfra service block-storage volume set vol1 --read-ops-limit 100 --read-bps-limit 100 \\\r\n--write-ops-limit 100 --write-bps-limit 100\nYou can check that the limits are successfully applied in the vinfra service block-storage volume show output:# vinfra service block-storage volume show vol1\r\n+---------------+--------------------------------------+\r\n| Field         | Value                                |\r\n+---------------+--------------------------------------+\r\n| grp_id        | 1b661da7-cb4d-4b81-9f09-f8ad54fa631e |\r\n| grp_name      | tg1                                  |\r\n| id            | 9841d72f-5d68-4659-82d5-cd96cf1031b6 |\r\n| limits        | read:                                |\r\n|               |   bps: 104857600                     |\r\n|               |   ops: 100                           |\r\n|               | write:                               |\r\n|               |   bps: 104857600                     |\r\n|               |   ops: 100                           |\r\n| lun           | 0                                    |\r\n| name          | vol1                                 |\r\n| serial        | cd96cf1031b6                         |\r\n| size          | 107374182400                         |\r\n| used_size     | 1048576                              |\r\n| volume_params | failure_domain: host                 |\r\n|               | redundancy:                          |\r\n|               |   m: 1                               |\r\n|               |   type: raid1                        |\r\n|               | tier: 0                              |\r\n+---------------+--------------------------------------+\r\n\n\nTo detach a volume from a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\nOn the LUNs tab, click the ellipsis button of the desired LUN, and then click Detach.\n\nAlternatively, you can open Storage services > Block storage > Volumes, click the ellipsis icon of the desired volume, and then click Detach.\n\nCommand-line interface\nUse the following command:vinfra service block-storage target-group volume detach <target-group> <volume>\n\n<target-group>\n\nTarget group name or ID\n<volume>\n\nVolume name or ID\n\nFor example, to detach the volume vol1 from the target group tg1, run:# vinfra service block-storage target-group volume detach tg1 vol1\n\nTo delete a volume that is not attached to a target group\n\nAdmin panel\n\nOpen Storage services > Block storage > Volumes. \nClick the ellipsis icon of the desired volume, and then click Delete.\n\nCommand-line interface\nUse the following command:vinfra service block-storage volume delete <volume>\n\n<volume>\n\nVolume name or ID\n\nFor example, to delete the volume vol1, run:# vinfra service block-storage volume delete vol1\n\nSee also\n\nManaging targets\n\nRestricting access to target groups\n\nMonitoring block storage\n\nDeleting target groups",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage volume set [--read-ops-limit <iops>] [--write-ops-limit <iops>]\r\n                                        [--read-bps-limit <MiB/s>] [--write-bps-limit <MiB/s>] <volume>\n\n--read-ops-limit <iops>\n\nNumber of read operations per second\n--write-ops-limit <iops>\n\nNumber of write operations per second\n--read-bps-limit <MiB/s>\n\nNumber of mebibytes read per second\n--write-bps-limit <MiB/s>\n\nNumber of mebibytes written per second\n<volume>\n\nVolume name or ID\n\nFor example, to limit the IOPS and throughput of the volume vol1 to 100 IOPS and 100 MiB/s, run:# vinfra service block-storage volume set vol1 --read-ops-limit 100 --read-bps-limit 100 \\\r\n--write-ops-limit 100 --write-bps-limit 100\nYou can check that the limits are successfully applied in the vinfra service block-storage volume show output:# vinfra service block-storage volume show vol1\r\n+---------------+--------------------------------------+\r\n| Field         | Value                                |\r\n+---------------+--------------------------------------+\r\n| grp_id        | 1b661da7-cb4d-4b81-9f09-f8ad54fa631e |\r\n| grp_name      | tg1                                  |\r\n| id            | 9841d72f-5d68-4659-82d5-cd96cf1031b6 |\r\n| limits        | read:                                |\r\n|               |   bps: 104857600                     |\r\n|               |   ops: 100                           |\r\n|               | write:                               |\r\n|               |   bps: 104857600                     |\r\n|               |   ops: 100                           |\r\n| lun           | 0                                    |\r\n| name          | vol1                                 |\r\n| serial        | cd96cf1031b6                         |\r\n| size          | 107374182400                         |\r\n| used_size     | 1048576                              |\r\n| volume_params | failure_domain: host                 |\r\n|               | redundancy:                          |\r\n|               |   m: 1                               |\r\n|               |   type: raid1                        |\r\n|               | tier: 0                              |\r\n+---------------+--------------------------------------+\r\n\n",
                "title": "To set a read/write limit for a volume attached as a LUN"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage target-group volume detach <target-group> <volume>\n\n<target-group>\n\nTarget group name or ID\n<volume>\n\nVolume name or ID\n\nFor example, to detach the volume vol1 from the target group tg1, run:# vinfra service block-storage target-group volume detach tg1 vol1\n",
                "title": "To detach a volume from a target group"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service block-storage volume delete <volume>\n\n<volume>\n\nVolume name or ID\n\nFor example, to delete the volume vol1, run:# vinfra service block-storage volume delete vol1\n",
                "title": "To delete a volume that is not attached to a target group"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\n\n\n\n\nOn the LUNs tab, click the desired LUN to open its details, then click the pencil icon in the Limits field.\n\nIn the Set LUN limit window, enter limit values, and then click Save.\n\n\n\n\n\n\nThe set limits will be shown in LUN details.\n",
                "title": "To set a read/write limit for a volume attached as a LUN"
            },
            {
                "example": "\nAdmin panel\n\n\nOpen Storage services > Block storage > Target groups, click the name of the desired target group to open it.\n\n\n\n\n\nOn the LUNs tab, click the ellipsis button of the desired LUN, and then click Detach.\n\nAlternatively, you can open Storage services > Block storage > Volumes, click the ellipsis icon of the desired volume, and then click Detach.\n",
                "title": "To detach a volume from a target group"
            },
            {
                "example": "\nAdmin panel\n\nOpen Storage services > Block storage > Volumes. \nClick the ellipsis icon of the desired volume, and then click Delete.\n\n",
                "title": "To delete a volume that is not attached to a target group"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/managing-volumes.html"
    },
    {
        "title": "Monitoring",
        "content": "Monitoring\nVirtuozzo Hybrid Infrastructure uses the Prometheus monitoring system to monitor performance and availability of the storage cluster, infrastructure nodes, and the deployed services. It also generates alerts, which you can configure to be sent as notifications via email.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring.html"
    },
    {
        "title": "Monitoring backup storage",
        "content": "Monitoring backup storage\nAfter you create backup storage, you can monitor it on the Storage services > Backup storage > Overview screen. The charts show the following information:\n\nNodes. The chart shows the number and availability of  nodes in the backup storage cluster.\nPerformance. The chart shows the read and write activity  of backup storage services over time.\nGeo-replication. The chart shows the geo-replication speed and backlog, which is the amount of data waiting to be replicated. If the geo-replication backlog does not decrease over time, it means the data cannot be replicated fast enough. The reason may be insufficient network transfer speed, and you may need to check or upgrade your network.\nAppend latency. The chart shows the time spent on processing requests from backup agents to the storage.\n\nAppend throttle. If the chart is not empty, it means the underlying storage lacks free space and the backup storage is throttling user requests to slow down the data flow.\nTwo thresholds, soft and hard, are set on 85% and 93% of used storage space, accordingly. When the soft threshold is reached, backup storage starts to throttle write operations. Throttling intensity depends on consumed space and increases until the hard threshold is reached. When the used space passes the hard threshold, throttling works with maximum intensity.\n\nObject storage. The chart shows the object storage speed and backlog, which is the amount of data waiting to be uploaded to public cloud. If the object storage backlog does not decrease over time, it means the data cannot be uploaded fast enough. The reason may be insufficient network transfer speed, and you may need to check or upgrade your network.\n\nYou can also monitor backups storage nodes. To do this, go to Storage services > Backup storage > Nodes and click the required node. On the right pane, the Overview tab displays the performance statistics:\n\nCPU/RAM: CPU usage in percent over time, and RAM usage, in GiB over time\nSuccessful/Failed request rate: the number of successful and failed append requests per second\nEgress/Ingress request rate: the number of read and write requests per second\nThroughput: the amount of data read from or written to the backup storage per second\nRequest latency: the time spent on processing requests\n\nAdvanced Backup Gateway monitoring via Grafana\nFor advanced monitoring of the Backup Gateway cluster, go to the Monitoring > Dashboard screen, and then click Grafana dashboard. A separate browser tab will open with preconfigured Grafana dashboards, two of which are dedicated to Acronis Backup Gateway. To see a detailed description for each chart, click the i icon on its left corner.\nOn the Acronis Backup Gateway dashboard, you need to pay attention to the following charts:\n\nAvailability. Any time period during which the gateways have not been available will be highlighted in red. In this case, you will need to look into logs on the nodes with the failed service and report a problem. To see the Backup Gateway log, use the following command:# zstdcat /var/log/abgw/abgw.log.zst\r\n\n\nMigration/Replication throughput. The migration chart should be displayed during migration or if the cluster serves as master in a geo-replication configuration. The replication chart should mirror the ingress bandwidth chart.\n\nMigration/replication backlog. The migration chart should decrease over time. The replication chart should be near zero, high values indicate network issues.\n\nRate limiting/ingress throttling. If the chart is not empty, it means the underlying storage lacks free space and the Backup Gateway is throttling user requests to slow down the data flow. Add more storage space to the cluster to solve the issue. For more information, refer to https://kb.acronis.com/content/62823.\n\nNew client connections. A high rate of failed connections due to SSL certificate verification problems on the chart means that clients uploaded an invalid certificate chain.\n\nIO watchdog timeouts. If the chart is not empty, it means the underlying storage is not healthy and cannot deliver the required performance.\n\nTo see the charts for a particular client request, file, and I/O operation, select them from the drop-down menus above. A high rate of failed requests or operations and high latencies on these charts indicate that the Backup Gateway experiences issues that need to be reported. For example, you can check charts for the \u00e2\u0080\u009cAppend\u00e2\u0080\u009d request:\n\nThe Append rate chart displays the backup data flow from backup agents to the storage in operations per second (one operation equals one big block of backup data; blocks can be of various size).\nThe Append latency chart shows the time spent on processing requests and should average several tens of milliseconds with peak values below one second.\n\nThe Acronis Backup Gateway Details dashboard is intended for low-level troubleshooting by the support team. To monitor a particular node, client request, file, and I/O operation, select them from the drop-down menus above. On the dashboard, you can make sure the Event loop inactivity chart is empty. Otherwise, the Backup Gateway is not healthy on this node and the issue needs to be reported.\n\nSee also\n\nManaging backup storage\n\nBackup storage metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-backup-storage.html"
    },
    {
        "title": "Monitoring cluster objects via SNMP",
        "content": "Monitoring cluster objects via SNMP\nYou can monitor cluster objects via the Simple Network Management Protocol (SNMP). The implementation conforms to the same Structure of Management Information (SMI) rules as the data in the standard SNMP context: all objects are organized in a tree; each object identifier (OID) is a series of integers corresponding to tree nodes and separated by dots.\nGeneral information:\n\nThe OID of the root subtree with all of the objects you can monitor is 1.3.6.1.4.1.8072.161.1.\nThe VSTORAGE-MIB.txt information base file is required to monitor the objects. You can download the file at https://<admin_panel_IP>:8888/api/v2/snmp/mibs/?x-session-id=0.\n\nSNMP monitoring overview\n\nEnable SNMP access.\nAccess cluster information objects with SNMP tools, for example, Net-SNMP or Zabbix.\nIf you need to listen to SNMP traps, configure settings and send a test SNMP trap.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-cluster-objects-via-snmp.html"
    },
    {
        "title": "Monitoring compute nodes",
        "content": "Monitoring compute nodes\nYou can monitor the compute node status on the Compute > Nodes screen. Nodes in the compute cluster can have the following statuses:\n\nHealthy\nThe node operates normally.\nConfiguring\n\nThe node configuration (the default CPU model for VMs or the compute role) is changing.\nFenced\n\nThe node has become available after a failure, it is now fenced from scheduling new VMs on it.\nCritical\n\nThe node has encountered a critical problem and is not operational.\n\nTo check compute services on a node\n\nAdmin panel\nOn the Infrastructure > Nodes screen, click the line with a compute node. On the right pane, the Compute services tab provides information about deployed compute controller and worker services on the node. Healthy compute services are highlighted on the tab in green, failed services in red, and disabled services for a fenced node in yellow.\n\nCommand-line interface\nUse the following command:vinfra service compute node show <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to view the details of the compute node node001, run:# vinfra service compute node show node001\r\n+----------------+------------------------------------------+\r\n| Field          | Value                                    |\r\n+----------------+------------------------------------------+\r\n| fenced_reason  |                                          |\r\n| host           | node001.vstoragedomain                   |\r\n| host_ip        | 192.168.128.113                          |\r\n| hypervisor     | hypervisor_type: QEMU                    |\r\n|                | id: f36f9331-11a8-43c9-a90b-dbda9bdf9a00 |\r\n|                | is_evacuating: false                     |\r\n|                | state: up                                |\r\n|                | status: enabled                          |\r\n|                | vms: 0                                   |\r\n| id             | 52565ca3-5893-8f6b-62ce-2f07b175b549     |\r\n| in_maintenance | False                                    |\r\n| orig_hostname  | node001                                  |\r\n| placements     | []                                       |\r\n| roles          | - controller                             |\r\n|                | - compute                                |\r\n| services       | - name: cinder-scheduler                 |\r\n|                |   state: healthy                         |\r\n|                | - name: cinder-volume                    |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-dhcp-agent               |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-l3-agent                 |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-metadata-agent           |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-openvswitch-agent        |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-compute                     |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-conductor                   |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-scheduler                   |\r\n|                |   state: healthy                         |\r\n| state          | healthy                                  |\r\n+----------------+------------------------------------------+\n\nTo view compute node details\n\nAdmin panel\nOn the Compute > Nodes screen, click a compute node. You can view the following compute node information:\n\nVirtual CPU and RAM reservations:\n\nReserved for the system and storage services\nProvisioned to virtual machines located on the node\nFree virtual CPUs and RAM left on the node\n\nThe number of virtual CPUs is a product of the number of physical CPUs on a node and the overcommitment ratio. The amount of RAM is a product of the amount of physical RAM on a node and the overcommitment ratio. To learn more about physical CPU and RAM\u00a0reservations for system and storage services, refer to Server requirements.\n\nHosted virtual machines and their resource consumption\n\nCommand-line interface\nUse the following command:vinfra service compute node show <node> --with-stats\n\n<node>\n\nNode ID or hostname\n--with-stats\n\nGet node information with statistics\n\nFor example, to view the details of the compute node node001, run:# vinfra service compute node show node001 --with-stats\r\n+----------------+------------------------------------------+\r\n| Field          | Value                                    |\r\n+----------------+------------------------------------------+\r\n| fenced_reason  |                                          |\r\n| host           | node001.vstoragedomain                   |\r\n| host_ip        | 192.168.128.113                          |\r\n| hypervisor     | hypervisor_type: QEMU                    |\r\n|                | id: f36f9331-11a8-43c9-a90b-dbda9bdf9a00 |\r\n|                | is_evacuating: false                     |\r\n|                | state: up                                |\r\n|                | status: enabled                          |\r\n|                | vms: 0                                   |\r\n| id             | 52565ca3-5893-8f6b-62ce-2f07b175b549     |\r\n| in_maintenance | False                                    |\r\n| orig_hostname  | node001                                  |\r\n| placements     | []                                       |\r\n| roles          | - controller                             |\r\n|                | - compute                                |\r\n| services       | - name: cinder-scheduler                 |\r\n|                |   state: healthy                         |\r\n|                | - name: cinder-volume                    |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-dhcp-agent               |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-l3-agent                 |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-metadata-agent           |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-openvswitch-agent        |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-compute                     |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-conductor                   |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-scheduler                   |\r\n|                |   state: healthy                         |\r\n| state          | healthy                                  |\r\n| statistics     | compute:                                 |\r\n|                |   block_capacity: 0                      |\r\n|                |   block_usage: 0                         |\r\n|                |   cpu_usage: 0.0                         |\r\n|                |   vcpus: 0                               |\r\n|                |   vcpus_free: 8                          |\r\n|                |   vm_mem_capacity: 3731456000.0          |\r\n|                |   vm_mem_free: 3731456000.0              |\r\n|                |   vm_mem_reserved: 0                     |\r\n|                |   vm_mem_usage: 0                        |\r\n|                | datetime: '2023-01-10T13:04:18.280858'   |\r\n|                | physical:                                |\r\n|                |   cpu_cores: 4                           |\r\n|                |   cpu_usage: 14.212499999994177          |\r\n|                |   mem_free: 534638592                    |\r\n|                |   mem_total: 25110126592                 |\r\n|                |   swap_free: 0                           |\r\n|                |   swap_total: 0                          |\r\n|                |   vcpus_total: 32                        |\r\n|                | reserved:                                |\r\n|                |   cpus: 3                                |\r\n|                |   memory: 21378670592                    |\r\n|                |   vcpus: 24                              |\r\n+----------------+------------------------------------------+\n\nSee also\n\nMonitoring virtual machines\n\nMonitoring load balancers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute node show <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to view the details of the compute node node001, run:# vinfra service compute node show node001\r\n+----------------+------------------------------------------+\r\n| Field          | Value                                    |\r\n+----------------+------------------------------------------+\r\n| fenced_reason  |                                          |\r\n| host           | node001.vstoragedomain                   |\r\n| host_ip        | 192.168.128.113                          |\r\n| hypervisor     | hypervisor_type: QEMU                    |\r\n|                | id: f36f9331-11a8-43c9-a90b-dbda9bdf9a00 |\r\n|                | is_evacuating: false                     |\r\n|                | state: up                                |\r\n|                | status: enabled                          |\r\n|                | vms: 0                                   |\r\n| id             | 52565ca3-5893-8f6b-62ce-2f07b175b549     |\r\n| in_maintenance | False                                    |\r\n| orig_hostname  | node001                                  |\r\n| placements     | []                                       |\r\n| roles          | - controller                             |\r\n|                | - compute                                |\r\n| services       | - name: cinder-scheduler                 |\r\n|                |   state: healthy                         |\r\n|                | - name: cinder-volume                    |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-dhcp-agent               |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-l3-agent                 |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-metadata-agent           |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-openvswitch-agent        |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-compute                     |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-conductor                   |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-scheduler                   |\r\n|                |   state: healthy                         |\r\n| state          | healthy                                  |\r\n+----------------+------------------------------------------+\n",
                "title": "To check compute services on a node"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute node show <node> --with-stats\n\n<node>\n\nNode ID or hostname\n--with-stats\n\nGet node information with statistics\n\nFor example, to view the details of the compute node node001, run:# vinfra service compute node show node001 --with-stats\r\n+----------------+------------------------------------------+\r\n| Field          | Value                                    |\r\n+----------------+------------------------------------------+\r\n| fenced_reason  |                                          |\r\n| host           | node001.vstoragedomain                   |\r\n| host_ip        | 192.168.128.113                          |\r\n| hypervisor     | hypervisor_type: QEMU                    |\r\n|                | id: f36f9331-11a8-43c9-a90b-dbda9bdf9a00 |\r\n|                | is_evacuating: false                     |\r\n|                | state: up                                |\r\n|                | status: enabled                          |\r\n|                | vms: 0                                   |\r\n| id             | 52565ca3-5893-8f6b-62ce-2f07b175b549     |\r\n| in_maintenance | False                                    |\r\n| orig_hostname  | node001                                  |\r\n| placements     | []                                       |\r\n| roles          | - controller                             |\r\n|                | - compute                                |\r\n| services       | - name: cinder-scheduler                 |\r\n|                |   state: healthy                         |\r\n|                | - name: cinder-volume                    |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-dhcp-agent               |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-l3-agent                 |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-metadata-agent           |\r\n|                |   state: healthy                         |\r\n|                | - name: neutron-openvswitch-agent        |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-compute                     |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-conductor                   |\r\n|                |   state: healthy                         |\r\n|                | - name: nova-scheduler                   |\r\n|                |   state: healthy                         |\r\n| state          | healthy                                  |\r\n| statistics     | compute:                                 |\r\n|                |   block_capacity: 0                      |\r\n|                |   block_usage: 0                         |\r\n|                |   cpu_usage: 0.0                         |\r\n|                |   vcpus: 0                               |\r\n|                |   vcpus_free: 8                          |\r\n|                |   vm_mem_capacity: 3731456000.0          |\r\n|                |   vm_mem_free: 3731456000.0              |\r\n|                |   vm_mem_reserved: 0                     |\r\n|                |   vm_mem_usage: 0                        |\r\n|                | datetime: '2023-01-10T13:04:18.280858'   |\r\n|                | physical:                                |\r\n|                |   cpu_cores: 4                           |\r\n|                |   cpu_usage: 14.212499999994177          |\r\n|                |   mem_free: 534638592                    |\r\n|                |   mem_total: 25110126592                 |\r\n|                |   swap_free: 0                           |\r\n|                |   swap_total: 0                          |\r\n|                |   vcpus_total: 32                        |\r\n|                | reserved:                                |\r\n|                |   cpus: 3                                |\r\n|                |   memory: 21378670592                    |\r\n|                |   vcpus: 24                              |\r\n+----------------+------------------------------------------+\n",
                "title": "To view compute node details"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nOn the Infrastructure > Nodes screen, click the line with a compute node. On the right pane, the Compute services tab provides information about deployed compute controller and worker services on the node. Healthy compute services are highlighted on the tab in green, failed services in red, and disabled services for a fenced node in yellow.\n",
                "title": "To check compute services on a node"
            },
            {
                "example": "\nAdmin panel\nOn the Compute > Nodes screen, click a compute node. You can view the following compute node information:\n\n\nVirtual CPU and RAM reservations:\n\nReserved for the system and storage services\nProvisioned to virtual machines located on the node\nFree virtual CPUs and RAM left on the node\n\nThe number of virtual CPUs is a product of the number of physical CPUs on a node and the overcommitment ratio. The amount of RAM is a product of the amount of physical RAM on a node and the overcommitment ratio. To learn more about physical CPU and RAM\u00a0reservations for system and storage services, refer to Server requirements.\n\nHosted virtual machines and their resource consumption\n\n",
                "title": "To view compute node details"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-compute-nodes.html"
    },
    {
        "title": "Monitoring block storage",
        "content": "Monitoring block storage\nAfter creating a target group, you can monitor it on the Overview tab. The charts show the read and write I/O activity and latency across all LUNs attached to the target group.\n\nAdvanced iSCSI monitoring via Grafana\nFor advanced monitoring of target groups, go to the Monitoring > Dashboard screen, and then click Grafana dashboard. A separate browser tab will open with preconfigured Grafana dashboards. Two of them are dedicated to the iSCSI service. To see a detailed description for each chart, click i in the chart\u00e2\u0080\u0099s top left corner.\nOn the iSCSI overview dashboard, note the following charts:\n\niSCSI availability. The chart shows target availability. Time periods when the targets have not been available will be highlighted in red. In this case, check /var/log/vstorage/iscsi/vstorage-target-monitor.log on the nodes with the failed service and report a problem.\nLatency. The chart shows the time spent on read and write I/O operations across all iSCSI LUNs. It should average a few dozens of milliseconds with peak values below 1s.\n\nThe iSCSI details dashboard is intended for troubleshooting by the technical support team. To monitor a particular target group, target, session, or LUN, select it from a drop-down list above.\n\nSee also\n\nManaging block storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-block-storage.html"
    },
    {
        "title": "Modifying QoS policy rules",
        "content": "Modifying QoS policy rules\nYou can modify QoS policy rules at runtime. Changes will take effect on all ports to which the policy is applied.\nPrerequisites\n\nA QoS policy is created, as described in Creating QoS policies.\n\nTo edit a policy rule\nSpecify new parameter values, the policy name, and its rule ID with openstack network qos rule set. For example:# openstack --insecure network qos policy show policy1\r\n+-------+---------------------------------------------------------------+\r\n| Field | Value                                                         |\r\n+-------+---------------------------------------------------------------+\r\n| <\u00e2\u0080\u00a6>   |                                                               |\r\n| name  | policy1                                                       |\r\n| rules | [{u'max_kbps': 3000, u'direction': u'ingress',                |\r\n|       |   u'qos_policy_id': u'8e2511c9-7db5-456c-b8ee-939f7729d981',  |\r\n|       |   u'type': u'bandwidth_limit', u'max_burst_kbps': 2400,       |\r\n|       |   u'id': u'6f036f09-d952-420d-986b-27c7eb14b2da'}]            |\r\n| <\u00e2\u0080\u00a6>   |                                                               |\r\n+-------+---------------------------------------------------------------+\r\n# openstack --insecure network qos rule set --max-kbps 2000 --max-burst-kbits 1600 \\--ingress policy1 6f036f09-d952-420d-986b-27c7eb14b2da\nSee also\n\nSetting the default QoS policy\n\nAssigning QoS policies\n\nUnassigning QoS policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/modifying-qos-policy-rules.html"
    },
    {
        "title": "Monitoring file storage",
        "content": "Monitoring file storage\nFor advanced monitoring of NFS nodes, services and shares, go to the Monitoring > Dashboard screen, and then click Grafana dashboard. A separate browser tab will open with preconfigured Grafana dashboards. Three of them are dedicated to the NFS service. To see a detailed description for each chart, click i in the chart\u00e2\u0080\u0099s top left corner.\nOn the NFS overview dashboard, note the following charts:\n\nNFS servers availability. The chart shows the availability of NFS hosts. Time periods when the hosts are unavailable will be highlighted in red. In this case, check /var/log/ganesha/ganesha.log and /var/log/ostor/ostorfs.log on these nodes, and report a problem.\nNFS services availability. The chart shows the availability of FS and OS services used by NFS. Time periods when the services are unavailable will be highlighted in red. In this case, check /var/log/ostor/FS-* and /var/log/ostor/OS-* on the corresponding nodes, and report a problem.\nLatency. The chart shows the average latency of read and write I/O operations across all NFS shares.\nIOPS. The chart shows the total numbers of read and write I/O operations, along with their average I/O operations per second across all NFS shares.\nBandwidth. The chart shows the total amount of data read from, or written to, all NFS shares per second.\n\nThe NFS details dashboard is intended for monitoring particular nodes, volumes, or NFS file operations.\n\nThe Object Storage FS details dashboard is intended for monitoring data on particular file services.\nSee also\n\nManaging file storage",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-file-storage.html"
    },
    {
        "title": "Monitoring load balancers",
        "content": "Monitoring load balancers\nTo monitor a load balancer\n\nAdmin panel\nOn the Compute > Network > Load balancers tab, select the required load balancer and open the Overview tab. The following charts are available:\n\nMembers state\n\nThe total number of members in the balancing pools grouped by status: \u00e2\u0080\u009cHealthy\u00e2\u0080\u009d, \u00e2\u0080\u009cUnhealthy\u00e2\u0080\u009d, \u00e2\u0080\u009cError\u00e2\u0080\u009d, and \u00e2\u0080\u009cDisabled\u00e2\u0080\u009d.\nNetwork\n\nIncoming and outgoing network traffic.\nActive connections\n\nThe number of active connections.\nError requests\n\nThe number of error requests.\n\nCommand-line interface\nUse the following command:vinfra service compute load-balancer stats <load-balancer>\r\n\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to view the statistics for the load balancer mylbaas, run:# vinfra service compute load-balancer stats mylbaas\r\n+-------+-------------------------------------------------------+\r\n| Field | Value                                                 |\r\n+-------+-------------------------------------------------------+\r\n| stats | active_connections: 0                                 |\r\n|       | bytes_in: 0                                           |\r\n|       | bytes_out: 0                                          |\r\n|       | listeners: null                                       |\r\n|       | loadbalancer_id: 17cfa86f-c374-4ca3-8cd6-f638a5234fe7 |\r\n|       | request_errors: 0                                     |\r\n|       | total_connections: 0                                  |\r\n+-------+-------------------------------------------------------+\r\n\n\nSee also\n\nMonitoring compute nodes\n\nMonitoring virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute load-balancer stats <load-balancer>\r\n\n\n<load-balancer>\n\nLoad balancer ID or name\n\nFor example, to view the statistics for the load balancer mylbaas, run:# vinfra service compute load-balancer stats mylbaas\r\n+-------+-------------------------------------------------------+\r\n| Field | Value                                                 |\r\n+-------+-------------------------------------------------------+\r\n| stats | active_connections: 0                                 |\r\n|       | bytes_in: 0                                           |\r\n|       | bytes_out: 0                                          |\r\n|       | listeners: null                                       |\r\n|       | loadbalancer_id: 17cfa86f-c374-4ca3-8cd6-f638a5234fe7 |\r\n|       | request_errors: 0                                     |\r\n|       | total_connections: 0                                  |\r\n+-------+-------------------------------------------------------+\r\n\n",
                "title": "To monitor a load balancer"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nOn the Compute > Network > Load balancers tab, select the required load balancer and open the Overview tab. The following charts are available:\n\nMembers state\n\nThe total number of members in the balancing pools grouped by status: \u00e2\u0080\u009cHealthy\u00e2\u0080\u009d, \u00e2\u0080\u009cUnhealthy\u00e2\u0080\u009d, \u00e2\u0080\u009cError\u00e2\u0080\u009d, and \u00e2\u0080\u009cDisabled\u00e2\u0080\u009d.\nNetwork\n\nIncoming and outgoing network traffic.\nActive connections\n\nThe number of active connections.\nError requests\n\nThe number of error requests.\n\n",
                "title": "To monitor a load balancer"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-load-balancers.html"
    },
    {
        "title": "Monitoring the cluster with Zabbix",
        "content": "Monitoring the cluster with Zabbix\nPrerequisites\n\nThe SNMP access is enabled, as described in Enabling SNMP access.\n\nTo configure cluster monitoring in Zabbix\n\nOn the Settings > Advanced settings > SNMP tab, click the provided link to download a template for Zabbix.\n\nThe template is compatible with Zabbix 3.x.\n\nIn Zabbix, click Configuration > Templates > Import, and then click Browse.\n\nNavigate to the template, select it, and then click Import.\n\nClick Configuration > Hosts > Create host.\n\nOn the Host tab, do the following:\n\nSpecify the Host name of the management node and its Visible name in Zabbix.\nSpecify vstorage in the New group field.\nRemove the Agent Interfaces section.\nAdd the SNMP interfaces section and specify the management node IP address.\n\nOn the Templates tab, click Select next to the Link new templates field.\n\nIn the Zabbix Server: Templates window, select the Template VStorageSNMP template, and then click Select.\n\nBack on the Templates tab, click the Add link in the Link new templates section. The VStorageSNMP template will appear in the Linked templates group.\n\nHaving configured the host and added its template, click the Add button.\n\nIn a few minutes, the cluster\u00e2\u0080\u0099s SNMP label in the Availability column on the Configuration > Hosts screen will turn green.\n\nTo monitor the cluster parameters in Zabbix\n Open the Monitoring > Latest data screen, set the filter\u00e2\u0080\u0099s Host groups to vstorage, and then click Apply.\nYou can create performance charts on the Configuration > Hosts > <cluster> > Graphs tab and a workplace for them on the Monitoring > Screens tab.\nSee also\n\nAccessing cluster information objects via SNMP\n\nListening to SNMP traps\n\nCluster objects and traps",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-the-cluster-with-zabbix.html"
    },
    {
        "title": "Monitoring infrastructure nodes",
        "content": "Monitoring infrastructure nodes\nNodes added to the infrastructure are listed on the Infrastructure > Nodes screen. If the storage cluster has not been created yet, you will only see nodes with the Unassigned status.\n\nA node can have one of the following statuses:\n\nHealthy\n\nAll the storage services on the node are running.\nUnhealthy\n\nOne or more storage services on the node have failed.\nIn maintenance\n\nThe node is in maintenance mode. It does not participate in new chunk allocation.\nRecovering\n\nThe configuration of deployed services and infrastructure is being recovered on the node.\nRecovery failed\n\nThe recovery process has failed on the node.\nIn progress\n\nThe node is deploying, entering, or exiting maintenance. Nodes in this state cannot be managed.\nUnassigned\n\nThe node is not assigned to a storage cluster.\n\nTo view the node details\n\nAdmin panel\nOpen the Infrastructure > Nodes screen and click the required node line. On the right pane, the Overview tab includes the node details, such as the node ID and hostname, its status and location, assigned IP addresses, deployed services, and the number of disks and network interfaces.\n\nCommand-line interface\nUse the following command:vinfra node show <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to view the details of the node node003, run:# vinfra node show node003\r\n+---------------+--------------------------------------+\r\n| Field         | Value                                |\r\n+---------------+--------------------------------------+\r\n| has_kvm       | True                                 |\r\n| host          | node003.vstoragedomain               |\r\n| id            | c4d14337-0863-4a67-9dbd-f19c3e49e114 |\r\n| is_assigned   | True                                 |\r\n| is_in_ha      | False                                |\r\n| is_installing | False                                |\r\n| is_online     | True                                 |\r\n| is_primary    | False                                |\r\n| is_virt       | False                                |\r\n| maintenance   |                                      |\r\n| orig_hostname | node003                              |\r\n| roles         | cses:                                |\r\n|               |   active: 4                          |\r\n|               |   being_released: 0                  |\r\n|               | mdses:                               |\r\n|               |   avail: 1                           |\r\n|               |   being_released: 0                  |\r\n|               |   is_master: true                    |\r\n| tasks         |                                      |\r\n+---------------+--------------------------------------+\nIn the command output, the node details include the node ID, hostname, the number and status of storage services. Also, you can check whether the node is joined to the storage cluster and HA configuration, and if it is online or in the maintenance mode.\n\nTo view all processes on nodes\nGo to the Monitoring > Dashboard screen, and then click Grafana dashboard. A separate browser tab will open with preconfigured Grafana dashboards. Open the Process details dashboard, to see a list of processes on your infrastructure nodes. The charts on this dashboard show CPU and memory usage for each process. The displayed data can be filtered per node or process group.",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node show <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to view the details of the node node003, run:# vinfra node show node003\r\n+---------------+--------------------------------------+\r\n| Field         | Value                                |\r\n+---------------+--------------------------------------+\r\n| has_kvm       | True                                 |\r\n| host          | node003.vstoragedomain               |\r\n| id            | c4d14337-0863-4a67-9dbd-f19c3e49e114 |\r\n| is_assigned   | True                                 |\r\n| is_in_ha      | False                                |\r\n| is_installing | False                                |\r\n| is_online     | True                                 |\r\n| is_primary    | False                                |\r\n| is_virt       | False                                |\r\n| maintenance   |                                      |\r\n| orig_hostname | node003                              |\r\n| roles         | cses:                                |\r\n|               |   active: 4                          |\r\n|               |   being_released: 0                  |\r\n|               | mdses:                               |\r\n|               |   avail: 1                           |\r\n|               |   being_released: 0                  |\r\n|               |   is_master: true                    |\r\n| tasks         |                                      |\r\n+---------------+--------------------------------------+\nIn the command output, the node details include the node ID, hostname, the number and status of storage services. Also, you can check whether the node is joined to the storage cluster and HA configuration, and if it is online or in the maintenance mode.\n",
                "title": "To view the node details"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nOpen the Infrastructure > Nodes screen and click the required node line. On the right pane, the Overview tab includes the node details, such as the node ID and hostname, its status and location, assigned IP addresses, deployed services, and the number of disks and network interfaces.\n",
                "title": "To view the node details"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-infrastructure-nodes.html"
    },
    {
        "title": "Monitoring object storage",
        "content": "Monitoring object storage\nYou can monitor the S3 cluster and its components on the Storage services > S3 > Overview screen with the following charts:\n\nAvailability of NS, OS, and GW services. If an S3 GW service has the \u00e2\u0080\u009cfailed\u00e2\u0080\u009d status, most probably the node hosting it is down. It is not critical for the S3 cluster: high availability of the S3 service is based on the DNS records. If the DNS records are properly configured, the S3 service remains fully accessible via S3 clients. On the other hand, if an OS or NS service fails, it is critical: the whole S3 cluster cannot operate normally. If you see that some of the NS or OS services are offline, but all of the cluster nodes are healthy, and the network with the OSTOR private traffic type is working well, contact the technical support team. You can also refer to the Grafana dashboards to find out the failure causes.\nOperation rate. The chart shows the overall cluster load by S3 users\u00e2\u0080\u0099 requests, including all operation types.\nRequest failure rate. The requests are generated by users or their applications. Some requests cannot be processed: for example, they may request nonexistent objects, or mismatch the access rights, or use unsupported features (refer to Supported Amazon S3 features). So, it is normal if the error rate makes up a small proportion of the total operations rate. However, it can also indicate that the S3 application used for access is not working properly. In addition, if the S3 cluster is open for public access, it might be scanned by Internet crawlers. In this case, the error spikes would reflect all the issues with their mismatching access rights. It is not a critical issue for the cluster though.\nBandwidth. The chart shows the overall cluster load by the S3 users\u00e2\u0080\u0099 requests.\nPUT latency and GET latency. These values are measured from the time the last byte of the user request was received until the time the first byte of the response was sent.\n\nAdvanced S3 monitoring via Grafana\nFor advanced monitoring of the S3 cluster, go to the Storage services > S3 > Overview screen, and then click Grafana dashboard. A separate browser tab will open with preconfigured Grafana dashboards. To see a detailed description for each chart, click the i icon in its left corner.\nFor the detailed monitoring of the OS and NS services, use the Object Storage overview, Object Storage OS details, and Object Storage NS details dashboards. Filter the data by nodes or volumes to detect the ones with abnormal service usage. Note the Task delays chart: it shows the proportion of time wasted on waiting for CPU, for available memory (reclaim), for memory transfer from swap (swap in), and for I/O completion.\nThe S3 overview dashboard shows primarily the S3 GW service information. Here, you can monitor the object storage and S3 interface with the following charts:\n\nS3 gateways availability, NS services availability, and OS services availability. The charts show the information on the corresponding S3 services. Time periods when the services are unavailable are highlighted in red.\nGET latency and PUT latency. The charts show the average latency and 95th, 99th, and max latency percentiles of S3 GET and PUT requests. This value is measured from the time the last byte of the request was received until the time the first byte of the response was sent.\nBandwidth. The chart shows the total amount of read or write operations passing through all S3 gateways per second.\nOperation rates. The chart shows the total number of GET, PUT, LIST, and DELETE S3 operations per second across all S3 gateways.\n\nThe S3 geo-replication overview dashboard is intended for monitoring data replicated in multiple geographically distributed datacenters:\n\nReplication backlog and Replication queue depth are the most important charts here. If the values are growing constantly, the replication efficiency is falling. It means that the cluster receives more data than it sends.\nLocal S3 error rate and Remote S3 error rate help locate connection problems. A small number of errors is possible if the clusters are replicated over the Internet with unstable latency.\n\nSee also\n\nManaging object storage\n\nObject storage metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-object-storage.html"
    },
    {
        "title": "Monitoring node performance",
        "content": "Monitoring node performance\nTo monitor the performance of an infrastructure node\n Open the Infrastructure > Nodes screen and click the required node line. On the right pane, the Monitoring tab displays the performance statistics:\n\nCPU/RAM: CPU usage in percent over time, and RAM usage, in GiB over time\nNetwork: the rate of transmitted (TX) and received (RX) traffic over time\nDisks read: node read activity over time\nDisks write: node write activity over time\nPhysical space: the current usage of the node physical space\n\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\nFor advanced monitoring, click Grafana dashboard.\nSee also\n\nMonitoring node disks\n\nMonitoring node network interfaces",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-node-performance.html"
    },
    {
        "title": "Monitoring the cluster remotely",
        "content": "Monitoring the cluster remotely\nYou can monitor your storage cluster remotely via the built-in Prometheus and Alertmanager monitoring toolkit. The built-in Prometheus server stores collected information for seven days. If you want to store metric values for a longer period, use an external Prometheus server. Alertmanager handles alerts generated by the Prometheus alerting rules. It can also be configured to send notifications to external systems, such as PagerDuty.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-the-cluster-remotely.html"
    },
    {
        "title": "Monitoring node network interfaces",
        "content": "Monitoring node network interfaces\nTo check the network interface status\nGo to the Infrastructure > Nodes screen and click the node name. Open the Network interfaces tab to see a list of all network interfaces on the node with their statuses.\nA network interface can have the following statuses:\n\nConnected\n\nThe network adapter is plugged in and up on the node.\nDisconnected\n\nThe network adapter is disconnected.\nDisabled\n\nThe network adapter is down on the node.\nWarning\n\nThe network adapter is not in the full duplex mode, has low speed, or incorrect settings.\n\nTo display network interface details\n\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOpen the Network interfaces tab, click a network interface, and then go to the Overview tab.\n\nNetwork interface details include the interface status, type, assigned network, MTU, MAC, and IP addresses. You can also check the interface speed, as well as transmit (TX) and receive (RX) rates, in packets per second.\n\nCommand-line interface\nUse the following command:vinfra node iface show [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nFor example, to view the details of the network interface bond0.362 on the node node003, run:# vinfra node iface show bond0.362 --node node003\r\n+-----------------------+--------------------------------------+\r\n| Field                 | Value                                |\r\n+-----------------------+--------------------------------------+\r\n| built_on              | bond0                                |\r\n| dhcp4                 |                                      |\r\n| dhcp4_enabled         | False                                |\r\n| dhcp6                 |                                      |\r\n| dhcp6_enabled         | False                                |\r\n| duplex                | full                                 |\r\n| gw4                   |                                      |\r\n| gw6                   |                                      |\r\n| ignore_auto_routes_v4 | True                                 |\r\n| ignore_auto_routes_v6 | True                                 |\r\n| ipv4                  | - 192.168.0.15/24                    |\r\n| ipv6                  | []                                   |\r\n| mac_addr              | 0c:42:a1:0d:f4:ac                    |\r\n| mtu                   | 9000                                 |\r\n| multicast             | True                                 |\r\n| name                  | bond0.362                            |\r\n| network               | e4347c48-2a93-4495-9221-0036d4b7fd2c |\r\n| node_id               | c4d14337-0863-4a67-9dbd-f19c3e49e114 |\r\n| plugged               | True                                 |\r\n| rx_bytes              | 132795090298899                      |\r\n| rx_dropped            | 0                                    |\r\n| rx_errors             | 0                                    |\r\n| rx_overruns           | 0                                    |\r\n| rx_packets            | 11992910723                          |\r\n| speeds                | current: 50000                       |\r\n|                       | max: 50000                           |\r\n| state                 | up                                   |\r\n| tag                   | 362                                  |\r\n| tx_bytes              | 97570120705648                       |\r\n| tx_dropped            | 0                                    |\r\n| tx_errors             | 0                                    |\r\n| tx_overruns           | 0                                    |\r\n| tx_packets            | 10744028252                          |\r\n| type                  | vlan                                 |\r\n+-----------------------+--------------------------------------+\r\n\nIn the command output, network interface details include the interface state, type, assigned network, MTU, MAC and IP addresses, etc. You can also check the interface speed, as well as transmit (TX) and receive (RX) rates, in packets per second.\n\nTo monitor performance  of a network interface\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOpen the Network interfaces tab, click a network interface, and then take a look at the charts on the Monitoring tab.\n\nWhen monitoring network performance, keep in mind that if the Errors charts are not empty, the network is experiencing issues and requires attention. For advanced monitoring, click Grafana dashboard.\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\nSee also\n\nMonitoring node performance\n\nMonitoring node disks",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node iface show [--node <node>] <iface>\r\n\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<iface>\n\nNetwork interface name\n\nFor example, to view the details of the network interface bond0.362 on the node node003, run:# vinfra node iface show bond0.362 --node node003\r\n+-----------------------+--------------------------------------+\r\n| Field                 | Value                                |\r\n+-----------------------+--------------------------------------+\r\n| built_on              | bond0                                |\r\n| dhcp4                 |                                      |\r\n| dhcp4_enabled         | False                                |\r\n| dhcp6                 |                                      |\r\n| dhcp6_enabled         | False                                |\r\n| duplex                | full                                 |\r\n| gw4                   |                                      |\r\n| gw6                   |                                      |\r\n| ignore_auto_routes_v4 | True                                 |\r\n| ignore_auto_routes_v6 | True                                 |\r\n| ipv4                  | - 192.168.0.15/24                    |\r\n| ipv6                  | []                                   |\r\n| mac_addr              | 0c:42:a1:0d:f4:ac                    |\r\n| mtu                   | 9000                                 |\r\n| multicast             | True                                 |\r\n| name                  | bond0.362                            |\r\n| network               | e4347c48-2a93-4495-9221-0036d4b7fd2c |\r\n| node_id               | c4d14337-0863-4a67-9dbd-f19c3e49e114 |\r\n| plugged               | True                                 |\r\n| rx_bytes              | 132795090298899                      |\r\n| rx_dropped            | 0                                    |\r\n| rx_errors             | 0                                    |\r\n| rx_overruns           | 0                                    |\r\n| rx_packets            | 11992910723                          |\r\n| speeds                | current: 50000                       |\r\n|                       | max: 50000                           |\r\n| state                 | up                                   |\r\n| tag                   | 362                                  |\r\n| tx_bytes              | 97570120705648                       |\r\n| tx_dropped            | 0                                    |\r\n| tx_errors             | 0                                    |\r\n| tx_overruns           | 0                                    |\r\n| tx_packets            | 10744028252                          |\r\n| type                  | vlan                                 |\r\n+-----------------------+--------------------------------------+\r\n\nIn the command output, network interface details include the interface state, type, assigned network, MTU, MAC and IP addresses, etc. You can also check the interface speed, as well as transmit (TX) and receive (RX) rates, in packets per second.\n",
                "title": "To display network interface details"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOpen the Network interfaces tab, click a network interface, and then go to the Overview tab.\n\nNetwork interface details include the interface status, type, assigned network, MTU, MAC, and IP addresses. You can also check the interface speed, as well as transmit (TX) and receive (RX) rates, in packets per second.\n",
                "title": "To display network interface details"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-node-network-interfaces.html"
    },
    {
        "title": "Multitenancy",
        "content": "Multitenancy\nVirtuozzo Hybrid Infrastructure uses the administrative hierarchy of domains and projects (tenants) with Role-Based Access Control (RBAC) to manage virtual objects of the compute cluster, such as virtual machines, volumes, and virtual networks. A domain is an isolated container of projects  and users with assigned roles. Each project and user can only belong to one domain. A project is an isolated container of virtual objects with defined limits for virtual resources, such as vCPU, RAM, storage and floating IP addresses, and assigned users. A role is global and defines all of the possible tasks the user may perform at the level of the entire infrastructure, a specific domain, or project.\nAccording to these levels, there are three user roles in Virtuozzo Hybrid Infrastructure: a system administrator, a domain administrator, and a project member. The following chart shows typical users with these roles working at service providers and enterprises, along with their workspaces: admin or self-service panels.\n\nA system administrator can perform system administration tasks, depending on the assigned permissions and has access to the admin panel. This role also enables user and project management in the admin panel. Additionally, a system administrator with domain permissions can manage the Default domain in the self-service panel. \nSystem administrators are usually infrastructure administrators of an SP or MSP, or the main IT department of an enterprise, depending on your business case.\n\nA domain administrator is in charge of its domain in the self-service panel. A domain administrator can be assigned only to one domain and can manage virtual objects in all projects within this domain. This role also enables user and project management in the self-service panel. \nDomain administrators are usually system administrators of an SP's or MSP's client, or the IT department of an enterprise subsidiary, depending on your business case.\n\nA project member acts as a project administrator in a specific domain in the self-service panel. A project member can be assigned to multiple projects and can manage virtual objects in them.\nProject members are usually end users of an SP's or MSP's client, or end users in an enterprise, depending on your business case.\n\nSuch an implementation provides an administrative environment with its own users and virtual objects, and ensures their isolation from other users and virtual objects.\nSee also\n\nConfiguring multitenancy\n\nManaging domains, users, and projects",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/multitenancy.html"
    },
    {
        "title": "Monitoring node disks",
        "content": "Monitoring node disks\nLimitations\n\nYou cannot monitor performance of shingled magnetic recording (SMR) disks.\n\nTo monitor performance of a node disk\n\nGo to the Infrastructure > Nodes screen and click the node name. \nOn the Disks tab, click a node disk, and then take a look at the charts on the Monitoring tab.\n\nThe disk charts display its current usage, average latency, and read/write activity. For advanced monitoring, click Grafana dashboard.\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\nTo view the service details\n\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk, and then go to the Service tab.\n\nService properties differ depending on the disk role:\n\nService properties\nStorage\nMetadata\nMetadata+Cache\nCache\n\nStatus\n\nStorage service status:\n\nActive\n\nThe service is up and running.\nUnresponsive\n\nThe service stops responding and degrades the cluster performance. The disk is isolated from the cluster I/O.\nInactive\n\nThe service is temporarily unavailable. A storage service is marked as inactive during its first 5 minutes of inactivity.\n\nOffline\n\nThe service is inactive for more than 5 minutes. After a storage service goes offline, the cluster starts replicating data to restore the chunks that were stored on the affected storage disk.\n\nOut of space\n\nThe disk that runs the service is running out of space.\nReleasing\n\nThe service is being released.\nFailed\n\nThe service is running but a problem has occurred with the storage disk.\nRelease failed\n\nThe service failed to be released.\nMaintenance\n\nThe node that hosts the service is in the maintenance mode.\nUnknown\n\nThe state of the service is unknown.\n\nMetadata service status:\n\nAvailable\n\nThe service is online.\nSyncing\n\nThe service is syncing the cluster metadata.\nUnavailable\n\nThe service is offline.\n\n\u00e2\u0080\u0094\n\nSystemd\nShows the state of vstorage-csd.<cluster_name>.<CS_ID>.service\n\nShows the state of vstorage-mdsd.<cluster_name>.<MDS_ID>.service\n\n\u00e2\u0080\u0094\n\nTier\n\nShows the assigned storage tier\n\n\u00e2\u0080\u0094\n\nShows tiers that are being cached\n\nService ID\n\nStorage service ID\n\nMetadata service ID\n\n\u00e2\u0080\u0094\n\nUsage\n\nSpace usage on the disk\n\nCaching\n\nEnabled/Disabled\n\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\nCache location\n\nShows the SSD disk where this disk's write cache is saved to.\n\nDisplayed if caching is enabled.\n\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\nChecksumming\nEnabled/Disabled\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\nEncryption\nEnabled/Disabled\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\nCommand-line interface\nUse the following command:vinfra node disk show [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname\n<disk>\n\nDisk ID or device name (default: node001.vstoragedomain)\n\nFor example, to view the details of the disk nvme0n1 attached to the node node003, run:# vinfra node disk show nvme0n1 --node node003\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| Field              | Value                                                                                    |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| being_assigned     | False                                                                                    |\r\n| being_released     | False                                                                                    |\r\n| device             | nvme0n1                                                                                  |\r\n| disk_status        | ok                                                                                       |\r\n| encryption         |                                                                                          |\r\n| form_factor        |                                                                                          |\r\n| id                 | B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                                     |\r\n| is_blink_available | False                                                                                    |\r\n| is_blinking        | False                                                                                    |\r\n| issues             | []                                                                                       |\r\n| lun_id             |                                                                                          |\r\n| model              | INTEL SSDPE2KX020T8                                                                      |\r\n| node_id            | e40195d1-64b8-4117-85f3-00bb5d7a1db6                                                     |\r\n| nvme               | True                                                                                     |\r\n| physical_size      | 2000398934016                                                                            |\r\n| protocol           | name: NVMe                                                                               |\r\n|                    | speed: null                                                                              |\r\n| role               | cs                                                                                       |\r\n| rpm                |                                                                                          |\r\n| serial_number      | PHLJ950101C02P0BGN                                                                       |\r\n| service_id         | 1091                                                                                     |\r\n| service_params     | fail_messages: null                                                                      |\r\n|                    | journal_data_size: 270532608                                                             |\r\n|                    | journal_disk_id: B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                    |\r\n|                    | journal_path: /vstorage/dc7aea32/journal/journal-cs-6aa56a11-70e6-4fd3-be4c-bf7fcd65e5d6 |\r\n|                    | journal_type: inner_cache                                                                |\r\n|                    | repo_dir: /vstorage/dc7aea32/cs                                                          |\r\n|                    | systemd: active                                                                          |\r\n|                    | tier: 0                                                                                  |\r\n| service_status     | active                                                                                   |\r\n| smart_status       | passed                                                                                   |\r\n| space              | size: 1968848437248                                                                      |\r\n|                    | used: 1540324716544                                                                      |\r\n| tasks              |                                                                                          |\r\n| temperature        | 36.0                                                                                     |\r\n| type               | ssd                                                                                      |\r\n| zoned              |                                                                                          |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n\nIn the command output, service properties differ depending on the disk role:\n\nService properties\ncs\n\nmds\n\nmds-journal\n\njournal\n\nservice_id\n\nStorage service ID\n\nMetadata service ID\n\n\u00e2\u0080\u0094\n\nservice_params\n\njournal_data_size\n\nSize of cached data for the storage service\njournal_disk_id\n\nCache disk ID\njournal_path\n\nPath to the directory with the write journal\njournal_type\n\nCache type used for the storage service:\n\nno_cache\n\ninner_cache\n\nexternal_cache\n\nrepo_dir\n\nPath to the repository with the storage service\nsystemd\n\nShows the state of vstorage-csd.<cluster_name>.<CS_ID>.service\ntier\n\nShows the assigned storage tier\n\nrepo_dir\n\nPath to the repository with the metadata service\nsystemd\n\nShows the state of vstorage-mdsd.<cluster_name>.<MDS_ID>.service\n\n\u00e2\u0080\u0094\n\nservice_status\n\nStorage service status:\n\nactive\n\nThe service is up and running.\nill\n\nThe service stops responding and degrades the cluster performance. The disk is isolated from the cluster I/O.\ninactive\n\nThe service is temporarily unavailable. A storage service is marked as inactive during its first 5 minutes of inactivity.\noffline\n\nThe service is inactive for more than 5 minutes. After a storage service goes offline, the cluster starts replicating data to restore the chunks that were stored on the affected storage disk.\nno space\n\nThe disk that runs the service is running out of space.\nreleasing\n\nThe service is being released.\nfailed\n\nThe service is running but a problem has occurred with the storage disk.\nfailed rel\n\nThe service failed to be released.\nmaintenance\n\nThe node that hosts the service is in the maintenance mode.\nunknown\n\nThe state of the service is unknown.\n\nMetadata service status:\n\navail\n\nThe service is online.\nstale\n\nThe service is syncing the cluster metadata.\nunavail\n\nThe service is offline.\n\n\u00e2\u0080\u0094\n\nTo view the disk details\n\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk, and then go to the Disk tab.\n\nDisk properties include the drive name, state, type, physical capacity, disk protocol, model, serial number, S.M.A.R.T. status, and temperature. A disk can have the following states:\n\nHealthy\n\nThe disk is functioning normally.\nUnavailable\n\nThe disk is powered down or disconnected.\nFailed\n\nThe disk has failed or S.M.A.R.T. reported an error. You need to replace the disk.\n\nCommand-line interface\nUse the following command:vinfra node disk show [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname\n<disk>\n\nDisk ID or device name (default: node001.vstoragedomain)\n\nFor example, to view the details of the disk nvme0n1 attached to the node node003, run:# vinfra node disk show nvme0n1 --node node003\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| Field              | Value                                                                                    |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| being_assigned     | False                                                                                    |\r\n| being_released     | False                                                                                    |\r\n| device             | nvme0n1                                                                                  |\r\n| disk_status        | ok                                                                                       |\r\n| encryption         |                                                                                          |\r\n| form_factor        |                                                                                          |\r\n| id                 | B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                                     |\r\n| is_blink_available | False                                                                                    |\r\n| is_blinking        | False                                                                                    |\r\n| issues             | []                                                                                       |\r\n| lun_id             |                                                                                          |\r\n| model              | INTEL SSDPE2KX020T8                                                                      |\r\n| node_id            | e40195d1-64b8-4117-85f3-00bb5d7a1db6                                                     |\r\n| nvme               | True                                                                                     |\r\n| physical_size      | 2000398934016                                                                            |\r\n| protocol           | name: NVMe                                                                               |\r\n|                    | speed: null                                                                              |\r\n| role               | cs                                                                                       |\r\n| rpm                |                                                                                          |\r\n| serial_number      | PHLJ950101C02P0BGN                                                                       |\r\n| service_id         | 1091                                                                                     |\r\n| service_params     | fail_messages: null                                                                      |\r\n|                    | journal_data_size: 270532608                                                             |\r\n|                    | journal_disk_id: B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                    |\r\n|                    | journal_path: /vstorage/dc7aea32/journal/journal-cs-6aa56a11-70e6-4fd3-be4c-bf7fcd65e5d6 |\r\n|                    | journal_type: inner_cache                                                                |\r\n|                    | repo_dir: /vstorage/dc7aea32/cs                                                          |\r\n|                    | systemd: active                                                                          |\r\n|                    | tier: 0                                                                                  |\r\n| service_status     | active                                                                                   |\r\n| smart_status       | passed                                                                                   |\r\n| space              | size: 1968848437248                                                                      |\r\n|                    | used: 1540324716544                                                                      |\r\n| tasks              |                                                                                          |\r\n| temperature        | 36.0                                                                                     |\r\n| type               | ssd                                                                                      |\r\n| zoned              |                                                                                          |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n\nIn the command output, the disk properties include the device name, disk status, type, physical size, protocol, model, serial number, S.M.A.R.T. status, temperature, etc. iSCSi disks also have its LUN ID.\n\nTo check storage disks with enabled caching\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk with the Cache role, and then go to the Cache for disks tab.\n\nThe tab lists all of the storage disks that are being cached on the current disk.\nTo have the disk blink its activity LED\n\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk.\nOn the disk right pane, click Blink.\n\nTo have the disk stop blinking, click Unblink.\n\nCommand-line interface\nUse the following commands:\n\nTo start blinking the specified disk bay:vinfra node disk blink on [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<disk>\n\nDisk ID or device name\n\nFor example, to start blinking the disk sda on the node node005, run:# vinfra node disk blink on sda --node node005\n\nTo stop blinking the specified disk bay:vinfra node disk blink off [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<disk>\n\nDisk ID or device name\n\nFor example, to stop blinking the disk sda on the node node005, run:# vinfra node disk blink off sda --node node005\n\nSee also\n\nMonitoring node performance\n\nMonitoring node network interfaces",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node disk show [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname\n<disk>\n\nDisk ID or device name (default: node001.vstoragedomain)\n\nFor example, to view the details of the disk nvme0n1 attached to the node node003, run:# vinfra node disk show nvme0n1 --node node003\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| Field              | Value                                                                                    |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| being_assigned     | False                                                                                    |\r\n| being_released     | False                                                                                    |\r\n| device             | nvme0n1                                                                                  |\r\n| disk_status        | ok                                                                                       |\r\n| encryption         |                                                                                          |\r\n| form_factor        |                                                                                          |\r\n| id                 | B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                                     |\r\n| is_blink_available | False                                                                                    |\r\n| is_blinking        | False                                                                                    |\r\n| issues             | []                                                                                       |\r\n| lun_id             |                                                                                          |\r\n| model              | INTEL SSDPE2KX020T8                                                                      |\r\n| node_id            | e40195d1-64b8-4117-85f3-00bb5d7a1db6                                                     |\r\n| nvme               | True                                                                                     |\r\n| physical_size      | 2000398934016                                                                            |\r\n| protocol           | name: NVMe                                                                               |\r\n|                    | speed: null                                                                              |\r\n| role               | cs                                                                                       |\r\n| rpm                |                                                                                          |\r\n| serial_number      | PHLJ950101C02P0BGN                                                                       |\r\n| service_id         | 1091                                                                                     |\r\n| service_params     | fail_messages: null                                                                      |\r\n|                    | journal_data_size: 270532608                                                             |\r\n|                    | journal_disk_id: B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                    |\r\n|                    | journal_path: /vstorage/dc7aea32/journal/journal-cs-6aa56a11-70e6-4fd3-be4c-bf7fcd65e5d6 |\r\n|                    | journal_type: inner_cache                                                                |\r\n|                    | repo_dir: /vstorage/dc7aea32/cs                                                          |\r\n|                    | systemd: active                                                                          |\r\n|                    | tier: 0                                                                                  |\r\n| service_status     | active                                                                                   |\r\n| smart_status       | passed                                                                                   |\r\n| space              | size: 1968848437248                                                                      |\r\n|                    | used: 1540324716544                                                                      |\r\n| tasks              |                                                                                          |\r\n| temperature        | 36.0                                                                                     |\r\n| type               | ssd                                                                                      |\r\n| zoned              |                                                                                          |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n\nIn the command output, service properties differ depending on the disk role:\n\n\n\n\n\n\n\n\nService properties\ncs\n\nmds\n\nmds-journal\n\njournal\n\n\n\n\n\nservice_id\n\n\nStorage service ID\n\n\nMetadata service ID\n\n\u00e2\u0080\u0094\n\n\nservice_params\n\n\n\njournal_data_size\n\nSize of cached data for the storage service\njournal_disk_id\n\nCache disk ID\njournal_path\n\nPath to the directory with the write journal\njournal_type\n\n\nCache type used for the storage service:\n\nno_cache\n\ninner_cache\n\nexternal_cache\n\n\n\nrepo_dir\n\nPath to the repository with the storage service\nsystemd\n\nShows the state of vstorage-csd.<cluster_name>.<CS_ID>.service\ntier\n\nShows the assigned storage tier\n\n\n\n\nrepo_dir\n\nPath to the repository with the metadata service\nsystemd\n\nShows the state of vstorage-mdsd.<cluster_name>.<MDS_ID>.service\n\n\n\u00e2\u0080\u0094\n\n\nservice_status\n\n\nStorage service status:\n\nactive\n\nThe service is up and running.\nill\n\nThe service stops responding and degrades the cluster performance. The disk is isolated from the cluster I/O.\ninactive\n\nThe service is temporarily unavailable. A storage service is marked as inactive during its first 5 minutes of inactivity.\noffline\n\nThe service is inactive for more than 5 minutes. After a storage service goes offline, the cluster starts replicating data to restore the chunks that were stored on the affected storage disk.\nno space\n\nThe disk that runs the service is running out of space.\nreleasing\n\nThe service is being released.\nfailed\n\nThe service is running but a problem has occurred with the storage disk.\nfailed rel\n\nThe service failed to be released.\nmaintenance\n\nThe node that hosts the service is in the maintenance mode.\nunknown\n\nThe state of the service is unknown.\n\n\n\nMetadata service status:\n\navail\n\nThe service is online.\nstale\n\nThe service is syncing the cluster metadata.\nunavail\n\nThe service is offline.\n\n\n\u00e2\u0080\u0094\n\n\n\n",
                "title": "To view the service details"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node disk show [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname\n<disk>\n\nDisk ID or device name (default: node001.vstoragedomain)\n\nFor example, to view the details of the disk nvme0n1 attached to the node node003, run:# vinfra node disk show nvme0n1 --node node003\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| Field              | Value                                                                                    |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n| being_assigned     | False                                                                                    |\r\n| being_released     | False                                                                                    |\r\n| device             | nvme0n1                                                                                  |\r\n| disk_status        | ok                                                                                       |\r\n| encryption         |                                                                                          |\r\n| form_factor        |                                                                                          |\r\n| id                 | B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                                     |\r\n| is_blink_available | False                                                                                    |\r\n| is_blinking        | False                                                                                    |\r\n| issues             | []                                                                                       |\r\n| lun_id             |                                                                                          |\r\n| model              | INTEL SSDPE2KX020T8                                                                      |\r\n| node_id            | e40195d1-64b8-4117-85f3-00bb5d7a1db6                                                     |\r\n| nvme               | True                                                                                     |\r\n| physical_size      | 2000398934016                                                                            |\r\n| protocol           | name: NVMe                                                                               |\r\n|                    | speed: null                                                                              |\r\n| role               | cs                                                                                       |\r\n| rpm                |                                                                                          |\r\n| serial_number      | PHLJ950101C02P0BGN                                                                       |\r\n| service_id         | 1091                                                                                     |\r\n| service_params     | fail_messages: null                                                                      |\r\n|                    | journal_data_size: 270532608                                                             |\r\n|                    | journal_disk_id: B9F2C34F-19CF-4133-A3AF-A1440BE837AD                                    |\r\n|                    | journal_path: /vstorage/dc7aea32/journal/journal-cs-6aa56a11-70e6-4fd3-be4c-bf7fcd65e5d6 |\r\n|                    | journal_type: inner_cache                                                                |\r\n|                    | repo_dir: /vstorage/dc7aea32/cs                                                          |\r\n|                    | systemd: active                                                                          |\r\n|                    | tier: 0                                                                                  |\r\n| service_status     | active                                                                                   |\r\n| smart_status       | passed                                                                                   |\r\n| space              | size: 1968848437248                                                                      |\r\n|                    | used: 1540324716544                                                                      |\r\n| tasks              |                                                                                          |\r\n| temperature        | 36.0                                                                                     |\r\n| type               | ssd                                                                                      |\r\n| zoned              |                                                                                          |\r\n+--------------------+------------------------------------------------------------------------------------------+\r\n\nIn the command output, the disk properties include the device name, disk status, type, physical size, protocol, model, serial number, S.M.A.R.T. status, temperature, etc. iSCSi disks also have its LUN ID.\n",
                "title": "To view the disk details"
            },
            {
                "example": "\nCommand-line interface\nUse the following commands:\n\n\nTo start blinking the specified disk bay:vinfra node disk blink on [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<disk>\n\nDisk ID or device name\n\nFor example, to start blinking the disk sda on the node node005, run:# vinfra node disk blink on sda --node node005\n\n\nTo stop blinking the specified disk bay:vinfra node disk blink off [--node <node>] <disk>\r\n\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<disk>\n\nDisk ID or device name\n\nFor example, to stop blinking the disk sda on the node node005, run:# vinfra node disk blink off sda --node node005\n\n\n",
                "title": "To have the disk blink its activity LED"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk, and then go to the Service tab.\n\nService properties differ depending on the disk role:\n\n\n\n\n\n\n\n\nService properties\nStorage\nMetadata\nMetadata+Cache\nCache\n\n\n\n\nStatus\n\nStorage service status:\n\nActive\n\nThe service is up and running.\nUnresponsive\n\nThe service stops responding and degrades the cluster performance. The disk is isolated from the cluster I/O.\nInactive\n\n\nThe service is temporarily unavailable. A storage service is marked as inactive during its first 5 minutes of inactivity.\n\nOffline\n\n\nThe service is inactive for more than 5 minutes. After a storage service goes offline, the cluster starts replicating data to restore the chunks that were stored on the affected storage disk.\n\nOut of space\n\nThe disk that runs the service is running out of space.\nReleasing\n\nThe service is being released.\nFailed\n\nThe service is running but a problem has occurred with the storage disk.\nRelease failed\n\nThe service failed to be released.\nMaintenance\n\nThe node that hosts the service is in the maintenance mode.\nUnknown\n\nThe state of the service is unknown.\n\n\n\nMetadata service status:\n\nAvailable\n\nThe service is online.\nSyncing\n\nThe service is syncing the cluster metadata.\nUnavailable\n\nThe service is offline.\n\n\n\u00e2\u0080\u0094\n\n\nSystemd\nShows the state of vstorage-csd.<cluster_name>.<CS_ID>.service\n\nShows the state of vstorage-mdsd.<cluster_name>.<MDS_ID>.service\n\n\u00e2\u0080\u0094\n\n\nTier\n\nShows the assigned storage tier\n\n\u00e2\u0080\u0094\n\nShows tiers that are being cached\n\n\n\nService ID\n\nStorage service ID\n\n\nMetadata service ID\n\n\u00e2\u0080\u0094\n\n\nUsage\n\nSpace usage on the disk\n\n\n\nCaching\n\nEnabled/Disabled\n\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\n\nCache location\n\nShows the SSD disk where this disk's write cache is saved to.\n\nDisplayed if caching is enabled.\n\n\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\n\nChecksumming\nEnabled/Disabled\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\n\nEncryption\nEnabled/Disabled\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\u00e2\u0080\u0094\n\n\n\n",
                "title": "To view the service details"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk, and then go to the Disk tab.\n\nDisk properties include the drive name, state, type, physical capacity, disk protocol, model, serial number, S.M.A.R.T. status, and temperature. A disk can have the following states:\n\nHealthy\n\nThe disk is functioning normally.\nUnavailable\n\nThe disk is powered down or disconnected.\nFailed\n\nThe disk has failed or S.M.A.R.T. reported an error. You need to replace the disk.\n\n",
                "title": "To view the disk details"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the node name.\nOn the Disks tab, click a node disk.\nOn the disk right pane, click Blink.\n\nTo have the disk stop blinking, click Unblink.\n",
                "title": "To have the disk blink its activity LED"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-node-disks.html"
    },
    {
        "title": "Monitoring virtual machines",
        "content": "Monitoring virtual machines\nTo monitor a virtual machine\n\nAdmin panel\n Select a virtual machine and open the Monitoring tab. The following performance charts are available for virtual machines:\n\nCPU / RAM\n\nCPU and RAM usage by the VM.\nNetwork\n\nIncoming and outgoing network traffic.\nStorage read/write\n\nAmount of data read and written by the VM.\nRead/write latency\n\nRead and write latency. Hovering the mouse cursor over a point on the chart, you can also see the average and maximum latency for that moment, as well as the 95 and 99 percentiles.\n\nAveraged values are calculated every five minutes.\n\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\n\nCommand-line interface\nUse the following command:vinfra service compute server stat <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to view the statistics for the virtual machine myvm, run:# vinfra service compute server stat myvm\r\n+----------+----------------------------------+\r\n| Field    | Value                            |\r\n+----------+----------------------------------+\r\n| datetime | 2019-05-29T11:39:46.429000+00:00 |\r\n| metrics  | block_capacity: 1073741824       |\r\n|          | block_usage: 268435456           |\r\n|          | cpu_usage: 1                     |\r\n|          | mem_usage: 149876736             |\r\n+----------+----------------------------------+\r\n\n\nSee also\n\nMonitoring compute nodes\n\nMonitoring load balancers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server stat <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to view the statistics for the virtual machine myvm, run:# vinfra service compute server stat myvm\r\n+----------+----------------------------------+\r\n| Field    | Value                            |\r\n+----------+----------------------------------+\r\n| datetime | 2019-05-29T11:39:46.429000+00:00 |\r\n| metrics  | block_capacity: 1073741824       |\r\n|          | block_usage: 268435456           |\r\n|          | cpu_usage: 1                     |\r\n|          | mem_usage: 149876736             |\r\n+----------+----------------------------------+\r\n\n",
                "title": "To monitor a virtual machine"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n Select a virtual machine and open the Monitoring tab. The following performance charts are available for virtual machines:\n\nCPU / RAM\n\nCPU and RAM usage by the VM.\nNetwork\n\nIncoming and outgoing network traffic.\nStorage read/write\n\nAmount of data read and written by the VM.\nRead/write latency\n\nRead and write latency. Hovering the mouse cursor over a point on the chart, you can also see the average and maximum latency for that moment, as well as the 95 and 99 percentiles.\n\n\nAveraged values are calculated every five minutes.\n\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\n",
                "title": "To monitor a virtual machine"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-virtual-machines.html"
    },
    {
        "title": "Monitoring the storage cluster",
        "content": "Monitoring the storage cluster\nTo view the storage cluster status\n\nAdmin panel\nClick the cluster name at the bottom of the left menu. The status can be one of the following:\n\nHealthy\n\nAll cluster components are active and operate normally.\nUnavailable\n\nNot enough information about the cluster state (for example, because the cluster is inaccessible).\nDegraded\n\nSome of the cluster components are inactive or inaccessible. The cluster is trying to heal itself, data replication is scheduled or in progress.\nError\n\nThe cluster has too many inactive services and automatic replication is disabled. If the cluster enters this state, troubleshoot the nodes or contact the support team.\n\nCommand-line interface\nUse the following command:vinfra cluster overview\nFor example, to view the status of the cluster cluster1, take a look at this line from the command output:\r\n+-------------------+-------------------------+\r\n| Field             | Value                   |\r\n+-------------------+-------------------------+\r\n| ...               | ...                     |\r\n| status            | healthy                 |\r\n| ...               | ...                     |\r\n+-------------------+-------------------------+\r\n\n\nTo view the storage cluster statistics\n\nAdmin panel\nGo to the Monitoring > Dashboard screen:\n\nTo view the storage cluster statistics in full screen, click Fullscreen mode. \nTo exit the fullscreen mode, press Esc or click Exit fullscreen mode.\n\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\n\nCommand-line interface\nUse the following command:vstorage -c <cluster_name> top\nFor example, to view the general information about the cluster cluster1, take a look at this section from the command output:Cluster 'cluster1': healthy\r\nSpace: [OK] allocatable 11.9TB of 57.3TB, free 13.0TB of 57.3TB\r\nMDS nodes: 3 of 3, epoch uptime: 13d  3h\r\nCS nodes:  32 of 32 (32 avail, 0 inactive, 0 offline)\r\nLicense: ACTIVE (expiration: 01/01/2100, capacity: 500TB, used: 21.2TB)\r\nReplication:  1 norm,  1 limit\r\nIO:       read 26.2MB/s (1.9Kop/s), write  426MB/s (11Kops/s)\r\n\n\nCluster\n\nOverall status of the cluster:\n\nhealthy\n\nAll chunk servers in the cluster are active.\nunknown\n\nThere is not enough information about the cluster state (for example, because the master MDS server was elected a while ago).\ndegraded\n\nSome of the chunk servers in the cluster are inactive.\nfailure\n\nThe cluster has too many inactive chunk servers; the automatic replication is disabled.\nSMART warning\n\nOne or more physical disks attached to cluster nodes are in pre-failure condition.\n\nSpace\n\nAmount of disk space in the cluster:\n\nfree\n\nFree physical disk space in the cluster.\nallocatable\n\nAmount of logical disk space available for storing data. Allocatable disk space is calculated on the basis of the current replication parameters and free disk space on chunk servers. It may also be limited by license.\n\nMDS nodes\n\nNumber of active MDS servers as compared to the total number of MDS servers configured for the cluster.\n\nepoch time\n\nTime elapsed since the MDS master server election.\n\nCS nodes\n\nNumber of active chunk servers as compared to the total number of chunk servers configured for the cluster. In parentheses, you can see the additional information on these chunk servers:\n\n avail\nActive chunk servers that are currently up and running in the cluster.\ninactive\n\nInactive chunk servers that are temporarily unavailable. A chunk server is marked as inactive during its first 5 minutes of inactivity.\noffline\n\nOffline chunk servers that have been inactive for more than 5 minutes. A chunk server changes its state to offline after 5 minutes of inactivity. Once the state is changed to offline, the cluster starts replicating data to restore the chunks that were stored on the offline chunk server.\n\nLicense\n\nKey number under which the license is registered on the Key Authentication server and license state.\nReplication\n\nReplication settings. The normal number of chunk replicas and the limit after which a chunk gets blocked until recovered.\nIO\n\nDisk I/O activity in the cluster:\n\nSpeed of read and write I/O operations, in bytes per second\nNumber of read and write I/O operations per second\n\nTo view more details about the storage cluster\nGo to the Monitoring > Dashboard screen, and then click Grafana dashboard. \nA separate browser tab will open with preconfigured Grafana dashboards where you can manage existing dashboards, create new ones, share them between users, configure alerting, etc. The dashboards use the Prometheus data source. Its metrics are stored for seven days. If you want to increase this retention period, you can configure it manually. For more information, refer to Grafana documentation.\n\nSee also\n\nCore storage metrics\n\nConfiguring retention policy for Prometheus metrics",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster overview\nFor example, to view the status of the cluster cluster1, take a look at this line from the command output:\r\n+-------------------+-------------------------+\r\n| Field             | Value                   |\r\n+-------------------+-------------------------+\r\n| ...               | ...                     |\r\n| status            | healthy                 |\r\n| ...               | ...                     |\r\n+-------------------+-------------------------+\r\n\n",
                "title": "To view the storage cluster status"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vstorage -c <cluster_name> top\nFor example, to view the general information about the cluster cluster1, take a look at this section from the command output:Cluster 'cluster1': healthy\r\nSpace: [OK] allocatable 11.9TB of 57.3TB, free 13.0TB of 57.3TB\r\nMDS nodes: 3 of 3, epoch uptime: 13d  3h\r\nCS nodes:  32 of 32 (32 avail, 0 inactive, 0 offline)\r\nLicense: ACTIVE (expiration: 01/01/2100, capacity: 500TB, used: 21.2TB)\r\nReplication:  1 norm,  1 limit\r\nIO:       read 26.2MB/s (1.9Kop/s), write  426MB/s (11Kops/s)\r\n\n\nCluster\n\n\nOverall status of the cluster:\n\nhealthy\n\nAll chunk servers in the cluster are active.\nunknown\n\nThere is not enough information about the cluster state (for example, because the master MDS server was elected a while ago).\ndegraded\n\nSome of the chunk servers in the cluster are inactive.\nfailure\n\nThe cluster has too many inactive chunk servers; the automatic replication is disabled.\nSMART warning\n\nOne or more physical disks attached to cluster nodes are in pre-failure condition.\n\n\nSpace\n\n\nAmount of disk space in the cluster:\n\nfree\n\nFree physical disk space in the cluster.\nallocatable\n\nAmount of logical disk space available for storing data. Allocatable disk space is calculated on the basis of the current replication parameters and free disk space on chunk servers. It may also be limited by license.\n\n\nMDS nodes\n\n\nNumber of active MDS servers as compared to the total number of MDS servers configured for the cluster.\n\nepoch time\n\nTime elapsed since the MDS master server election.\n\n\nCS nodes\n\n\nNumber of active chunk servers as compared to the total number of chunk servers configured for the cluster. In parentheses, you can see the additional information on these chunk servers:\n\n avail\nActive chunk servers that are currently up and running in the cluster.\ninactive\n\nInactive chunk servers that are temporarily unavailable. A chunk server is marked as inactive during its first 5 minutes of inactivity.\noffline\n\nOffline chunk servers that have been inactive for more than 5 minutes. A chunk server changes its state to offline after 5 minutes of inactivity. Once the state is changed to offline, the cluster starts replicating data to restore the chunks that were stored on the offline chunk server.\n\n\nLicense\n\nKey number under which the license is registered on the Key Authentication server and license state.\nReplication\n\nReplication settings. The normal number of chunk replicas and the limit after which a chunk gets blocked until recovered.\nIO\n\n\nDisk I/O activity in the cluster:\n\nSpeed of read and write I/O operations, in bytes per second\nNumber of read and write I/O operations per second\n\n\n\n",
                "title": "To view the storage cluster statistics"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nClick the cluster name at the bottom of the left menu. The status can be one of the following:\n\nHealthy\n\nAll cluster components are active and operate normally.\nUnavailable\n\nNot enough information about the cluster state (for example, because the cluster is inaccessible).\nDegraded\n\nSome of the cluster components are inactive or inaccessible. The cluster is trying to heal itself, data replication is scheduled or in progress.\nError\n\nThe cluster has too many inactive services and automatic replication is disabled. If the cluster enters this state, troubleshoot the nodes or contact the support team.\n\n\n\n\n\n",
                "title": "To view the storage cluster status"
            },
            {
                "example": "\nAdmin panel\nGo to the Monitoring > Dashboard screen:\n\nTo view the storage cluster statistics in full screen, click Fullscreen mode. \nTo exit the fullscreen mode, press Esc or click Exit fullscreen mode.\n\nThe default time interval for the charts is twelve hours. To zoom into a particular time interval, select the internal with the mouse; to reset zoom, double-click any chart.\n",
                "title": "To view the storage cluster statistics"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-the-storage-cluster.html"
    },
    {
        "title": "Monitoring the compute cluster",
        "content": "Monitoring the compute cluster\nAfter you create the compute cluster, you can monitor its status and statistics. Additionally, you can monitor separate compute nodes, virtual machines, and load balancers.\nTo view the compute cluster status\nClick the cluster name at the bottom of the left menu. It can be one of the following:\n\nHealthy\n\nAll compute cluster components and nodes operate normally.\nConfiguring\n\nThe compute cluster configuration (the default CPU model for VMs or the number of compute nodes) is changing.\nWarning\n\nThe compute cluster operates normally but some issues have been detected.\nCritical\n\nThe compute cluster has encountered a critical problem and is not operational.\n\nTo view the compute cluster statistics\n\nAdmin panel\nGo to the Compute > Overview screen, which has the following charts:\n\nThe Reserved vCPUs chart displays vCPU reservations in the compute cluster. A vCPU reservation is a guarantee on vCPUs for a service or virtual machine.\n\nThe following statistics are available:\n\nTotal\n\nThe total number of virtual CPUs in the compute cluster. It is a product of the total number of physical CPUs on all compute nodes and the overcommitment ratio.\nSystem\n\nThe number of virtual CPUs reserved for the system and storage services on all nodes in the compute cluster. To learn more about CPU reservations for different services, refer to Server requirements.\nVMs\n\nThe number of virtual CPUs provisioned for all virtual machines in the compute cluster.\nFree\n\nThe number of free virtual CPUs on all nodes in the compute cluster.\nFenced\n\nThe number of virtual CPUs on all fenced nodes in the compute cluster.\nOvercommitment ratio\n\nThe ratio of the number of virtual CPUs to physical.\nThe parameter is set in /etc/kolla/nova-compute/nova.conf. You can change it by using the command vinfra service compute set --nova-compute-cpu-allocation-ratio <value> (refer to Changing parameters in OpenStack configuration files).\n\nA similar chart is available for each individual node in the compute cluster.\n\nThe Reserved RAM chart displays RAM reservations in the compute cluster. A RAM reservation is a guarantee on RAM for a service or virtual machine.\n\nThe following statistics are available:\n\nTotal\n\nThe total amount of RAM on all nodes in the compute cluster. It is a product of the total amount of physical RAM on all compute nodes and the overcommitment ratio.\nSystem\n\nThe amount of RAM reserved for the system and storage services on all nodes in the compute cluster. To learn more about RAM reservations for different services, refer to Server requirements.\nYou can view RAM reservation details for all of the cluster nodes in the vinfra node ram-reservation list output.\n\nVMs\n\nThe amount of RAM provisioned for all virtual machines in the compute cluster.\nFree\n\nThe amount of free RAM on all nodes in the compute cluster.\nFenced\n\nThe amount of RAM on all fenced nodes in the compute cluster.\nOvercommitment ratio\n\nThe ratio of the amount of maximum reserved RAM to physical.\nThe parameter is set in /etc/kolla/nova-compute/nova.conf. You can change it by using the command vinfra service compute set --nova-compute-ram-allocation-ratio <value> (refer to Configuring memory for virtual machines).\n\nA similar chart is available for each individual node in the compute cluster.\n\nThe Provisioned storage chart shows usage of storage space by the compute cluster.\n\nThe following statistics are available:\n\nTotal\n\nThe total size of volumes provisioned in the compute cluster.\nUsed\n\nThe amount of storage space actually occupied by data in all volumes provisioned in the compute cluster.\nFree\n\nThe amount of unused space in all volumes provisioned in the compute cluster.\n\nThe VMs status chart shows the total number of virtual machines in the compute cluster and groups them by status.\n\nThe VM\u00a0status can be the following:\n\nRunning\n\nThe number of virtual machines that are up and running.\nIn progress\n\nThe number of virtual machines that are in a transitional state: building, restarting, migrating, etc.\nStopped\n\nThe number of virtual machines that are suspended or powered off.\nError\n\nThe number of virtual machines that have failed. You can reset the state for such VMs to their last stable state.\n\nTo see a full list of virtual machines filtered by the chosen status, click the number next to the status icon.\n\nThe Top VMs chart lists the virtual machines with the highest resource consumption sorted by vCPU, RAM, or Storage in descending order.\n\nTo switch between lists, click the desired resource. To see a full list of virtual machines in the compute cluster, click Show all.\n\nThe Alerts chart lists all of the alerts related to the compute cluster sorted by severity.\n\nAlerts include the following:\n\nCritical\n\nThe compute cluster has encountered a critical problem and is unmanageable. For example, an API service on all of the management nodes has failed or one of the compute agents is down. In this case, contact the technical support team.\nWarning\n\nThe compute cluster is experiencing resource shortage or may become unmanageable. For example, an API service on one of the management nodes has failed or some resource has exceeded 95% of its allocation limit.\nInfo\n\nThe compute cluster is experiencing issues that may lead to resource shortage. For example, some resource has exceeded 80% of its allocation limit.\n\nTo see a full list of alerts, click Show all.\n\nCommand-line interface\nUse the following command:# vinfra service compute stat\r\n+----------+----------------------------------------------+\r\n| Field    | Value                                        |\r\n+----------+----------------------------------------------+\r\n| compute  | block_capacity: 172874997760                 |\r\n|          | block_usage: 22953803776                     |\r\n|          | cpu_allocation_ratio: 8                      |\r\n|          | cpu_usage: 13.3                              |\r\n|          | ram_allocation_ratio: 1.0                    |\r\n|          | vcpus: 7                                     |\r\n|          | vcpus_free: 33                               |\r\n|          | vm_mem_capacity: 35526971392                 |\r\n|          | vm_mem_free: 22105198592                     |\r\n|          | vm_mem_reserved: 13421772800                 |\r\n|          | vm_mem_usage: 10906963968                    |\r\n| datetime | 2022-10-05T13:21:12.447758                   |\r\n| fenced   | physical_cpu_cores: 0                        |\r\n|          | physical_cpu_usage: 0                        |\r\n|          | physical_mem_total: 0                        |\r\n|          | reserved_memory: 0                           |\r\n|          | vcpus: 0                                     |\r\n|          | vm_mem_capacity: 0                           |\r\n| physical | block_capacity: 810773667840                 |\r\n|          | block_free: 713247113216                     |\r\n|          | cpu_cores: 12                                |\r\n|          | cpu_usage: 25.8                              |\r\n|          | mem_total: 75331031040                       |\r\n|          | vcpus_total: 96                              |\r\n| reserved | cpus: 7                                      |\r\n|          | memory: 39804059648                          |\r\n|          | vcpus: 56                                    |\r\n| servers  | count: 5                                     |\r\n|          | error: 0                                     |\r\n|          | in_progress: 0                               |\r\n|          | running: 4                                   |\r\n|          | stopped: 1                                   |\r\n|          | top:                                         |\r\n|          |   disk:                                      |\r\n|          |   - id: f3f522ac-05f7-4849-827d-d787a77edd56 |\r\n|          |     name: k8s2-kvgcdwapwxbh-node-0           |\r\n|          |     size: 6345576448                         |\r\n|          |   - id: f53a6885-7740-4bf7-9765-4c2200b38c2f |\r\n|          |     name: k8s3-dgl3edvjcbf3-node-0           |\r\n|          |     size: 6133764096                         |\r\n|          |   - id: 9a182bc6-54b7-47d9-9553-fcdf712d5a22 |\r\n|          |     name: k8s2-kvgcdwapwxbh-master-0         |\r\n|          |     size: 5385080832                         |\r\n|          |   - id: 38de311b-f46b-4abe-a55b-2c04f677182e |\r\n|          |     name: k8s3-dgl3edvjcbf3-master-0         |\r\n|          |     size: 4925804544                         |\r\n|          |   - id: b5d6bb82-6137-4748-b94a-c1056d4cd8c9 |\r\n|          |     name: vm1                                |\r\n|          |     size: 163577856                          |\r\n|          |   memory:                                    |\r\n|          |   - id: 38de311b-f46b-4abe-a55b-2c04f677182e |\r\n|          |     name: k8s3-dgl3edvjcbf3-master-0         |\r\n|          |     size: 3264253952                         |\r\n|          |   - id: 9a182bc6-54b7-47d9-9553-fcdf712d5a22 |\r\n|          |     name: k8s2-kvgcdwapwxbh-master-0         |\r\n|          |     size: 3132764160                         |\r\n|          |   - id: f53a6885-7740-4bf7-9765-4c2200b38c2f |\r\n|          |     name: k8s3-dgl3edvjcbf3-node-0           |\r\n|          |     size: 2259763200                         |\r\n|          |   - id: f3f522ac-05f7-4849-827d-d787a77edd56 |\r\n|          |     name: k8s2-kvgcdwapwxbh-node-0           |\r\n|          |     size: 2250182656                         |\r\n|          |   - id: b5d6bb82-6137-4748-b94a-c1056d4cd8c9 |\r\n|          |     name: vm1                                |\r\n|          |     size: 0                                  |\r\n|          |   vcpus:                                     |\r\n|          |   - count: 0.55                              |\r\n|          |     id: 38de311b-f46b-4abe-a55b-2c04f677182e |\r\n|          |     name: k8s3-dgl3edvjcbf3-master-0         |\r\n|          |   - count: 0.51                              |\r\n|          |     id: 9a182bc6-54b7-47d9-9553-fcdf712d5a22 |\r\n|          |     name: k8s2-kvgcdwapwxbh-master-0         |\r\n|          |   - count: 0.29                              |\r\n|          |     id: f53a6885-7740-4bf7-9765-4c2200b38c2f |\r\n|          |     name: k8s3-dgl3edvjcbf3-node-0           |\r\n|          |   - count: 0.24                              |\r\n|          |     id: f3f522ac-05f7-4849-827d-d787a77edd56 |\r\n|          |     name: k8s2-kvgcdwapwxbh-node-0           |\r\n|          |   - count: 0                                 |\r\n|          |     id: b5d6bb82-6137-4748-b94a-c1056d4cd8c9 |\r\n|          |     name: vm1                                |\r\n+----------+----------------------------------------------+\n\nTo view more details about the compute cluster\nGo to the Monitoring > Dashboard screen, and then click Grafana dashboard. A separate browser tab will open with preconfigured Grafana dashboards.\nThe Compute service status dashboard shows the status of the compute services and agents on all of the compute nodes. You can sort the displayed services per hostname, service name, and service status.\n\nFor the detailed monitoring of the compute resource allocation, use the Compute resource allocation dashboard. The charts on this dashboard show the usage of vCPUs, memory, storage space per storage policy, and floating IP addresses.  You can view usage statistics for all domains and projects, or filter the data per specific domain or project.\n\nTo monitor the compute API requests, use the Compute service API details dashboard. The charts on this dashboard show the rate of successful and failed requests, as well as the 95th and 99th percentiles of response time, per 10-minute intervals. You can filter the displayed requests per compute service. The most important charts here are those of error request rate and response time. If you see spikes on them, you need to check the status of the corresponding services.\n\nThe RabbitMQ nodes, RabbitMQ messages, and RabbitMQ clients dashboards are intended for troubleshooting the RabbitMQ cluster by the support team. The PostgreSQL overview dashboard shows information about the PostgreSQL database size and replication status, as well as other database details. To see a detailed description for each chart, click the i icon in its left corner.\n\nSee also\n\nManaging the compute cluster",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:# vinfra service compute stat\r\n+----------+----------------------------------------------+\r\n| Field    | Value                                        |\r\n+----------+----------------------------------------------+\r\n| compute  | block_capacity: 172874997760                 |\r\n|          | block_usage: 22953803776                     |\r\n|          | cpu_allocation_ratio: 8                      |\r\n|          | cpu_usage: 13.3                              |\r\n|          | ram_allocation_ratio: 1.0                    |\r\n|          | vcpus: 7                                     |\r\n|          | vcpus_free: 33                               |\r\n|          | vm_mem_capacity: 35526971392                 |\r\n|          | vm_mem_free: 22105198592                     |\r\n|          | vm_mem_reserved: 13421772800                 |\r\n|          | vm_mem_usage: 10906963968                    |\r\n| datetime | 2022-10-05T13:21:12.447758                   |\r\n| fenced   | physical_cpu_cores: 0                        |\r\n|          | physical_cpu_usage: 0                        |\r\n|          | physical_mem_total: 0                        |\r\n|          | reserved_memory: 0                           |\r\n|          | vcpus: 0                                     |\r\n|          | vm_mem_capacity: 0                           |\r\n| physical | block_capacity: 810773667840                 |\r\n|          | block_free: 713247113216                     |\r\n|          | cpu_cores: 12                                |\r\n|          | cpu_usage: 25.8                              |\r\n|          | mem_total: 75331031040                       |\r\n|          | vcpus_total: 96                              |\r\n| reserved | cpus: 7                                      |\r\n|          | memory: 39804059648                          |\r\n|          | vcpus: 56                                    |\r\n| servers  | count: 5                                     |\r\n|          | error: 0                                     |\r\n|          | in_progress: 0                               |\r\n|          | running: 4                                   |\r\n|          | stopped: 1                                   |\r\n|          | top:                                         |\r\n|          |   disk:                                      |\r\n|          |   - id: f3f522ac-05f7-4849-827d-d787a77edd56 |\r\n|          |     name: k8s2-kvgcdwapwxbh-node-0           |\r\n|          |     size: 6345576448                         |\r\n|          |   - id: f53a6885-7740-4bf7-9765-4c2200b38c2f |\r\n|          |     name: k8s3-dgl3edvjcbf3-node-0           |\r\n|          |     size: 6133764096                         |\r\n|          |   - id: 9a182bc6-54b7-47d9-9553-fcdf712d5a22 |\r\n|          |     name: k8s2-kvgcdwapwxbh-master-0         |\r\n|          |     size: 5385080832                         |\r\n|          |   - id: 38de311b-f46b-4abe-a55b-2c04f677182e |\r\n|          |     name: k8s3-dgl3edvjcbf3-master-0         |\r\n|          |     size: 4925804544                         |\r\n|          |   - id: b5d6bb82-6137-4748-b94a-c1056d4cd8c9 |\r\n|          |     name: vm1                                |\r\n|          |     size: 163577856                          |\r\n|          |   memory:                                    |\r\n|          |   - id: 38de311b-f46b-4abe-a55b-2c04f677182e |\r\n|          |     name: k8s3-dgl3edvjcbf3-master-0         |\r\n|          |     size: 3264253952                         |\r\n|          |   - id: 9a182bc6-54b7-47d9-9553-fcdf712d5a22 |\r\n|          |     name: k8s2-kvgcdwapwxbh-master-0         |\r\n|          |     size: 3132764160                         |\r\n|          |   - id: f53a6885-7740-4bf7-9765-4c2200b38c2f |\r\n|          |     name: k8s3-dgl3edvjcbf3-node-0           |\r\n|          |     size: 2259763200                         |\r\n|          |   - id: f3f522ac-05f7-4849-827d-d787a77edd56 |\r\n|          |     name: k8s2-kvgcdwapwxbh-node-0           |\r\n|          |     size: 2250182656                         |\r\n|          |   - id: b5d6bb82-6137-4748-b94a-c1056d4cd8c9 |\r\n|          |     name: vm1                                |\r\n|          |     size: 0                                  |\r\n|          |   vcpus:                                     |\r\n|          |   - count: 0.55                              |\r\n|          |     id: 38de311b-f46b-4abe-a55b-2c04f677182e |\r\n|          |     name: k8s3-dgl3edvjcbf3-master-0         |\r\n|          |   - count: 0.51                              |\r\n|          |     id: 9a182bc6-54b7-47d9-9553-fcdf712d5a22 |\r\n|          |     name: k8s2-kvgcdwapwxbh-master-0         |\r\n|          |   - count: 0.29                              |\r\n|          |     id: f53a6885-7740-4bf7-9765-4c2200b38c2f |\r\n|          |     name: k8s3-dgl3edvjcbf3-node-0           |\r\n|          |   - count: 0.24                              |\r\n|          |     id: f3f522ac-05f7-4849-827d-d787a77edd56 |\r\n|          |     name: k8s2-kvgcdwapwxbh-node-0           |\r\n|          |   - count: 0                                 |\r\n|          |     id: b5d6bb82-6137-4748-b94a-c1056d4cd8c9 |\r\n|          |     name: vm1                                |\r\n+----------+----------------------------------------------+\n",
                "title": "To view the compute cluster statistics"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nGo to the Compute > Overview screen, which has the following charts:\n\n\nThe Reserved vCPUs chart displays vCPU reservations in the compute cluster. A vCPU reservation is a guarantee on vCPUs for a service or virtual machine.\n\nThe following statistics are available:\n\nTotal\n\nThe total number of virtual CPUs in the compute cluster. It is a product of the total number of physical CPUs on all compute nodes and the overcommitment ratio.\nSystem\n\nThe number of virtual CPUs reserved for the system and storage services on all nodes in the compute cluster. To learn more about CPU reservations for different services, refer to Server requirements.\nVMs\n\nThe number of virtual CPUs provisioned for all virtual machines in the compute cluster.\nFree\n\nThe number of free virtual CPUs on all nodes in the compute cluster.\nFenced\n\nThe number of virtual CPUs on all fenced nodes in the compute cluster.\nOvercommitment ratio\n\n\nThe ratio of the number of virtual CPUs to physical.\nThe parameter is set in /etc/kolla/nova-compute/nova.conf. You can change it by using the command vinfra service compute set --nova-compute-cpu-allocation-ratio <value> (refer to Changing parameters in OpenStack configuration files).\n\n\n\n\n\n\nA similar chart is available for each individual node in the compute cluster.\n\n\n\n\nThe Reserved RAM chart displays RAM reservations in the compute cluster. A RAM reservation is a guarantee on RAM for a service or virtual machine.\n\nThe following statistics are available:\n\nTotal\n\nThe total amount of RAM on all nodes in the compute cluster. It is a product of the total amount of physical RAM on all compute nodes and the overcommitment ratio.\nSystem\n\n\nThe amount of RAM reserved for the system and storage services on all nodes in the compute cluster. To learn more about RAM reservations for different services, refer to Server requirements.\nYou can view RAM reservation details for all of the cluster nodes in the vinfra node ram-reservation list output.\n\nVMs\n\nThe amount of RAM provisioned for all virtual machines in the compute cluster.\nFree\n\nThe amount of free RAM on all nodes in the compute cluster.\nFenced\n\nThe amount of RAM on all fenced nodes in the compute cluster.\nOvercommitment ratio\n\n\nThe ratio of the amount of maximum reserved RAM to physical.\nThe parameter is set in /etc/kolla/nova-compute/nova.conf. You can change it by using the command vinfra service compute set --nova-compute-ram-allocation-ratio <value> (refer to Configuring memory for virtual machines).\n\n\n\n\n\n\nA similar chart is available for each individual node in the compute cluster.\n\n\n\n\nThe Provisioned storage chart shows usage of storage space by the compute cluster.\n\nThe following statistics are available:\n\nTotal\n\nThe total size of volumes provisioned in the compute cluster.\nUsed\n\nThe amount of storage space actually occupied by data in all volumes provisioned in the compute cluster.\nFree\n\nThe amount of unused space in all volumes provisioned in the compute cluster.\n\n\n\n\n\n\n\n\n\nThe VMs status chart shows the total number of virtual machines in the compute cluster and groups them by status.\n\nThe VM\u00a0status can be the following:\n\nRunning\n\nThe number of virtual machines that are up and running.\nIn progress\n\nThe number of virtual machines that are in a transitional state: building, restarting, migrating, etc.\nStopped\n\nThe number of virtual machines that are suspended or powered off.\nError\n\nThe number of virtual machines that have failed. You can reset the state for such VMs to their last stable state.\n\n\n\n\n\nTo see a full list of virtual machines filtered by the chosen status, click the number next to the status icon.\n\n\n\n\nThe Top VMs chart lists the virtual machines with the highest resource consumption sorted by vCPU, RAM, or Storage in descending order.\n\nTo switch between lists, click the desired resource. To see a full list of virtual machines in the compute cluster, click Show all.\n\n\n\n\n\n\n\n\nThe Alerts chart lists all of the alerts related to the compute cluster sorted by severity.\n\nAlerts include the following:\n\nCritical\n\nThe compute cluster has encountered a critical problem and is unmanageable. For example, an API service on all of the management nodes has failed or one of the compute agents is down. In this case, contact the technical support team.\nWarning\n\nThe compute cluster is experiencing resource shortage or may become unmanageable. For example, an API service on one of the management nodes has failed or some resource has exceeded 95% of its allocation limit.\nInfo\n\nThe compute cluster is experiencing issues that may lead to resource shortage. For example, some resource has exceeded 80% of its allocation limit.\n\nTo see a full list of alerts, click Show all.\n\n\n\n\n",
                "title": "To view the compute cluster statistics"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/monitoring-the-compute-cluster.html"
    },
    {
        "title": "Network requirements",
        "content": "Network requirements\nThe general network requirements are the following:\n\nAll network interfaces on a node must be assigned IP addresses that belong to different subnets. A network interface can be a VLAN-tagged logical interface, an untagged bond, or an Ethernet link.\nThe network for internal traffic can be non-routable, with minimum 10 Gbit/s bandwidth.\nNodes are added to clusters by their IP addresses, not FQDNs. Changing the IP address of a node in the cluster will remove that node from the cluster. If you plan to use DHCP in a cluster, make sure that IP addresses are bound to the MAC addresses of the nodes\u00e2\u0080\u0099 network interfaces.\nEach node must have Internet access so that updates can be installed.\nNetwork time synchronization is required for correct statistics. It is enabled by default via the chronyd service. If you want to use ntpdate or ntpd, stop and disable chronyd first.\n\nSee also\n\nNetwork recommendations\n\nNetwork ports",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/network-requirements.html"
    },
    {
        "title": "Network ports",
        "content": "Network ports\nPorts that will be opened on cluster nodes depend on services that will run on the node and traffic types associated with them. Before enabling a specific service on a cluster node, you need to assign the respective traffic type to a network this node is connected to. Assigning a traffic type to a network configures a firewall on nodes connected to this network, opens specific ports on node network interfaces, and sets the necessary iptables rules.\nThe table below lists all the required ports and services associated with them:\n\nService\nTraffic type\nPort\nTraffic direction\nDescription\n\nWeb control panel\nAdmin panel1 Ports for this traffic type must only be open on management nodes.\nTCP 8888\nInbound\nExternal access to the admin panel.\n\nSelf-service panel\nTCP 8800\nInbound\nExternal access to the self-service panel.\n\nManagement\nInternal management\nall ports of a subnet\nInbound/outbound\nInternal cluster management and transfers\r\nof node monitoring data to the admin panel.\n\nMetadata service\nStorage\nall ports of a subnet\nInbound/outbound\nInternal communication between MDS services,\r\nas well as with chunks services and clients.\n\nChunk service\nall ports of a subnet\nInbound/outbound\nInternal communication with MDS services and\r\nclients.\n\nClient\nall ports of a subnet\nInbound/outbound\nInternal communication with MDS and chunk\r\nservices.\n\nBackup Gateway\nBackup (ABGW) public\nTCP 40440, 44445\nInbound\nExternal data exchange with Acronis Cyber Protect\r\nagents and Acronis Cyber Protect Cloud.\n\nBackup (ABGW) private\nall ports of a subnet\nInbound/outbound\nInternal management of and data exchange\r\nbetween multiple backup storage services.\n\n\u00e2\u0080\u0094\nTCP 8443\nOutbound\nData control for Acronis Cyber Protect agents and Management server\n\n\u00e2\u0080\u0094\nTCP 9877\nOutbound\nRegistration with Acronis Cyber Protect Management server in on-premises installations\n\niSCSI\niSCSI\nTCP 3260\nInbound\nExternal data exchange with the iSCSI\r\naccess point.\n\nS3\nS3 public\nTCP 80, 443\nInbound\nExternal data exchange with the S3 access\r\npoint.\n\nOSTOR private\nall ports of a subnet\nInbound/outbound\nInternal data exchange between multiple S3\r\nservices.\n\nNFS\nNFS\nTCP/UDP 111, 892,\r\n2049\nInbound\nExternal data exchange with the NFS access\r\npoint.\n\nOSTOR private\nall ports of a subnet\nInbound/outbound\nInternal data exchange between multiple NFS\r\nservices.\n\nCompute\nCompute API2 Ports for this traffic type must only be open on management nodes.\n\u00a0\n\u00a0\nExternal access to standard OpenStack API\r\nendpoints:\n\nTCP 5000\nInbound\nIdentity API v3\n\nTCP 6080\nInbound\nnoVNC Websocket Proxy\n\nTCP 8004\nInbound\nOrchestration Service API v1\n\nTCP 8041\nInbound\nGnocchi API (billing metering service)\n\nTCP 8774\nInbound\nCompute API\n\nTCP 8776\nInbound\nBlock Storage API v3\n\nTCP 8780\nInbound\nPlacement API\n\nTCP 9292\nInbound\nImage Service API v2\n\nTCP 9313\nInbound\nKey Manager API v1\n\nTCP 9513\nInbound\nContainer Infrastructure Management API\r\n(Kubernetes service)\n\nTCP 9696\nInbound\nNetworking API v2\n\nTCP 9888\nInbound\nOctavia API v2 (load balancer service)\n\nVM public\nL2 layer\nInbound/outbound\nExternal data exchange between VMs and public networks.\n\nVM private\nUDP 4789\nInbound/outbound\nNetwork traffic between VMs in compute virtual networks.\n\nTCP 15900\u00e2\u0080\u009316900\nInbound/outbound\nVNC console traffic.\n\nVM backups\nTCP 49300\u00e2\u0080\u009365535\nInbound/outbound3 Outbound traffic is only used for the Acronis Disaster Recovery (DR) hybrid deployment.\nExternal access to NBD endpoints.\n\n\u00e2\u0080\u0094\nUDP 500, 4500\nOutbound\nVPN as a Service\n\nSSH\nSSH\nTCP 22\nInbound\nRemote access to nodes via SSH.\n\nSNMP\nSNMP4 Ports for this traffic type must only be open on management nodes.\nUDP 161\nInbound\nExternal access to storage cluster\r\nmonitoring statistics via the SNMP protocol.\n\nDNS\n\u00e2\u0080\u0094\nTCP/UDP 53\nOutbound\nDNS name resolution.\n\nNTP\n\u00e2\u0080\u0094\nUDP 123\nOutbound\nTime syncronization.\n\nSee also\n\nNetwork requirements\n\nNetwork recommendations",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/network-ports.html"
    },
    {
        "title": "Network requirements and recommendations",
        "content": "Network requirements and recommendations\nWhen planning the network for your cluster, ensure that it meets the general network requirements and recommendations. Additional network requirements depend on the services you will deploy.\n\nSee also\n\nHardware recommendations\n\nServer requirements\n\nDisk requirements\n\nAdmin panel requirements",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/network-requirements-and-recommendations.html"
    },
    {
        "title": "Network recommendations",
        "content": "Network recommendations\nRecommendations for network hardware\n\nNetwork latency dramatically reduces cluster performance. Use quality network equipment with low latency links. Do not use consumer-grade network switches.\nDo not use desktop network adapters like Intel EXPI9301CTBLK or Realtek 8129 as they are not designed for heavy load and may not support full-duplex links. Also use non-blocking Ethernet switches.\n\nWe recommend using NVIDIA Mellanox ConnectX-5 adapters for the RDMA mode. If you want to use other adapters in the RDMA mode, contact the technical support team for recommendations.\n\nIf you use NVIDIA Mellanox network adapters and AMD Epyc Rome CPU together on physical nodes, ensure that SR-IOV is properly enabled. Otherwise, this may lead to data loss and performance degradation.\nTo enable SR-IOV\n\nEnable SR-IOV in BIOS.\n\nEnable IOMMU on the node: \n\nIn the /etc/default/grub file, locate the GRUB_CMDLINE_LINUX line, and then add the iommu=pt kernel parameter. The resulting line may look as follows:GRUB_CMDLINE_LINUX=\"crashkernel=auto tcache.enabled=0 quiet iommu=pt\"\n\nRegenerate the GRUB configuration file by running:# grub2-mkconfig -o /boot/grub2/grub.cfg\n\nThe default location is different on a UEFI-based system.\n\nWe do not recommend using the BNX2X driver for Broadcom-based network adapters, such as BCM57840 NetXtreme II 10/20-Gigabit Ethernet / HPE FlexFabric 10Gb 2-port 536FLB Adapter. This driver limits MTU to 3616, which affects the cluster performance. Ensure that the BNXT driver is used instead.\n\nRDMA is not supported for the compute service. Therefore, the compute and storage networks must be physically separated on different NICs. If you use the recommended approach with bonded network interfaces, you should have one network card with two bonded network interfaces for the storage network and one network card with two bonded network interfaces for the compute network. To learn how to use a compute trunk network, refer to Connecting virtual switches to trunk interfaces.\n\nRecommendations for network security\n\nUse separate networks (and, ideally albeit optionally, separate network adapters) for internal and public traffic. Doing so will prevent public traffic from affecting cluster I/O performance and also prevent possible denial-of-service attacks from the outside.\nTo avoid intrusions, Virtuozzo Hybrid Infrastructure should be on a dedicated internal network inaccessible from outside.\nEven though cluster nodes have the necessary iptables rules configured, we recommend using an external firewall for untrusted public networks, such as the Internet.\n\nRecommendations for network performance\n\nUse one 1 Gbit/s link per each two HDDs on the node (rounded up). For one or two HDDs on a node, two bonded network interfaces are still recommended for high network availability. The reason for this recommendation is that 1 Gbit/s Ethernet networks can deliver 110-120 MB/s of throughput, which is close to sequential I/O performance of a single disk. Since several disks on a server can deliver higher throughput than a single 1 Gbit/s Ethernet link, networking may become a bottleneck.\nFor maximum sequential I/O performance, use one 1 Gbit/s link per each hard drive or one 10+ Gbit/s link per node. Even though I/O operations are most often random in real-life scenarios, sequential I/O is important in backup scenarios.\nFor maximum overall performance, we recommend using 25 or 40 Gbit/s network adapters. Using 10 Gbit/s adapters is also possible, but not recommended.\nIt is not recommended to configure 1 Gbit/s network adapters to use non-default MTUs (for example, 9000-byte jumbo frames). Such settings require additional configuration of switches and often lead to human error. 10+ Gbit/s network adapters, on the other hand, need to be configured to use jumbo frames to achieve full performance. You will need to configure the same MTU value on each router and switch on the network (refer to your network equipment manuals), as well as on each node\u00e2\u0080\u0099s network card, bond, or VLAN. The MTU value is set to 1500 by default. \n\nNetwork recommendations for clients\nThe following table lists the maximum network performance a client can get with the specified network interface. The recommendation for clients is to use 10 Gbps network hardware between any two cluster nodes and minimize network latencies, especially if SSD disks are used.\n\nMaximum client network performance\r\n            \n\nStorage network interface\nNode max. I/O\nVM max. I/O (replication)\nVM max. I/O (erasure coding)\n\n1 Gbps\n100 MB/s\n100 MB/s\n70 MB/s\n\n2 x 1 Gbps\n~175 MB/s\n100 MB/s\n~130 MB/s\n\n3 x 1 Gbps\n~250 MB/s\n100 MB/s\n~180 MB/s\n\n10 Gbps\n1 GB/s\n1 GB/s\n700 MB/s\n\n2 x 10 Gbps\n1.75 GB/s\n1 GB/s\n1.3 GB/s\n\nSee also\n\nNetwork requirements\n\nNetwork ports",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/network-recommendations.html"
    },
    {
        "title": "No redundancy",
        "content": "No redundancy\n\nDanger of data loss!\n\nWithout redundancy, singular chunks are stored on failure domains, one per failure domain. If the node or disk fails, the data may be lost. Having no redundancy is highly not recommended no matter the scenario, unless you only want to evaluate Virtuozzo Hybrid Infrastructure on a single server.\nSee also\n\nRedundancy by replication\n\nRedundancy by erasure coding\n\nRedundancy modes\n\nFailure domains\n\nStorage tiers\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/no-redundancy.html"
    },
    {
        "title": "Obtaining Linux templates",
        "content": "Obtaining Linux templates\nAs all Linux guests have OpenSSH Server preinstalled by default, you only need to make sure a Linux template has cloud-init installed.\nThe easiest way to get a Linux template with cloud-init installed is to obtain it from its official repository or build one with the diskimage-builder tool. You can also create a Linux template from an existing boot volume.\nLimitations\n\nThe disk image is created with only the root user that has neither password nor SSH keys. You can use the user data and cloud-init methods to perform initial configuration tasks on VMs that will be deployed from the disk image, for example, create custom user accounts. For more options to customize a VM during boot, refer to the cloud-init documentation.\n\nTo build a Linux template\n\nInstall the diskimage-builder package:# yum install diskimage-builder\r\n\n\nFor the RHEL 7 guest OS, download the cloud image from the Red Hat Customer Portal (login required) and execute:# export DIB_LOCAL_IMAGE=<path_to_rhel7_image>\r\n\n\nExecute the disk-image-create command to build a disk image with installed cloud-init for the desired Linux guest. For example:# disk-image-create vm centos7 -t qcow2 -o centos7\r\n\nwhere\n\ncentos7 is the name of a guest OS. Can be one of the following: centos6, centos7, debian, rhel7, or ubuntu.\nBy default, using the ubuntu element will create a disk image for Ubuntu 16.04. To build the Ubuntu 18.04 disk image, add the DIB_RELEASE=bionic to the command: DIB_RELEASE=bionic disk-image-create vm ubuntu -t qcow2 -o ubuntu18.\n\n-o sets the name for the resulting disk image file.\n\nUpload the created disk image by using the vinfra tool to the compute cluster:# vinfra service compute image create centos7-image --os-distro centos7 \\\r\n--disk-format qcow2 --file centos7.qcow2\r\n\nwhere\n\ncentos7-image is the name of a new image.\ncentos7 is the OS distribution. Can be one of the following: centos6, centos7, debian9, rhel7, ubuntu16.04, and ubuntu18.04.\ncentos7.qcow2 is the QCOW2-image created on step 3.\n\nTo deploy a virtual machine from the uploaded template\n\nCreate the user-data configuration file with a custom user account:# cat <<EOF > user-data\r\n#cloud-config\r\nuser: myuser\r\npassword: password\r\nchpasswd: {expire: False}\r\nssh_pwauth: True\r\nEOF\r\n\nwhere myuser is the name of a custom user and password is a password for the account.\n\nLaunch the deployment of a VM from the disk image by using the configuration file as user data:# vinfra service compute server create centos7-vm --flavor medium \\\r\n--network public --user-data user-data --volume source=image,\\id=centos7-image,size=10\r\n\nwhere\n\ncentos7-vm is the name of a new VM.\nuser-data is the configuration file created in step 5.\ncentos7-image is the image added to the compute cluster in step 4.\n\nWhat's next\n\nEnabling logging for virtual machines\n\nCreating templates\n\nCreating virtual machines\n\nRescuing virtual machines\n\nManaging images",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/obtaining-linux-templates.html"
    },
    {
        "title": "Object storage requirements",
        "content": "Object storage requirements\n\nGeneral requirements are listed in General requirements.\n\nNote the additional requirements for object storage:\n\nTo be able to deploy and work with object storage, run Virtuozzo Hybrid Infrastructure on physical servers.\nKeep in mind that object storage uses more space than is occupied by all S3 objects. This happens because the S3 service also stores internal metadata about objects and their distribution across object servers. This metadata usually requires 0.5\u00e2\u0080\u009311 Object metadata might use more space if all objects in the S3 cluster occupy less than 100 KB, or if an S3 user additionally sets metadata when uploading an object. percent of the space used by S3 data. Moreover, since version 4.6, Virtuozzo Hybrid Infrastructure provides backups for object metadata, which increase the metadata size by additional 0.5 percent. The backups have automatic retention and do not require any assistance from a system administrator. The metadata and backups use the same redundancy scheme that is configured for the S3 cluster.\n\nObjects storage reserves RAM and CPU cores depending on the number of S3 nodes and taking into account a possible node failure. Each S3 node runs the hostd daemon, an S3 gateway, up to 10 object services (OS), and up to 10 name services (NS). However, the entire S3 cluster cannot host more than 24 OS and 16 NS services.\nThe reserved RAM size on an S3 node is calculated by using the following formula:HOSTD * 256 MB + S3GW * 256 MB + (total_OS * 256 MB + total_NS * 512 MB) / (S3_nodes_number - nodes_that_can_fail_number)\nThe reserved CPU cores on an S3 node are calculated by using the following formula:S3GW * 1 core + (total_OS * 0.1 cores + total_NS * 0.2 cores) / (S3_nodes_number - nodes_that_can_fail_number)\nFor example, the S3 cluster of five nodes runs 24 object services, 16 name services, and may lose one node without data loss. In this case, the reserved RAM size on each S3 node will be calculated as 256 MB + 256 MB + (24 * 256 MB + 16 * 512 MB) / (5 - 1), that is 4096 MB or 4 GB. Also, the reserved CPU cores will be calculated as 1 core + (24 * 0.1 cores + 16 * 0.2 cores) / (5 - 1), that is 2.4 cores.\n\nTo better understand how to calculate the hardware configuration for object storage, consider the following examples with RAM and CPU reservations.\nExample 1. If you have 3 nodes (1 system+metadata disk and 5 storage disks) and want to use 3 replicas redundancy mode with the host failure domain, refer to the table below for the calculations. Note that three nodes are used for the management node high availability, and each of them meets the requirements for the management node.\n\n3 nodes for the S3 cluster                    \n\nService\nManagement nodes\n\nSystem\n4.5 GB,\t3.3 cores\n\nStorage services\n\n5 storage disks, 1 metadata on system disk (each takes 0.5 GB and 0.2 cores), that is 3 GB and 1.2 cores in total\n\nS3\n7.7 GB, 3.8 cores\n\nService reservations\n15.2 GB of RAM and 8.3 cores\n\nMinimum hardware configuration\n16 GB of RAM and 8  cores\n\nRecommended hardware configuration\n32 GB of RAM and 16  cores\n\nExample 2. If you have 5 nodes (1 system+metadata disk, 1 SSD cache disk, 10 storage disks) and want to use them for the S3 cluster, refer to the table below for the calculations. Note that three nodes are used for the management node high availability, and each of them meets the requirements for the management node.\n\n5 nodes for the S3 cluster\n\nService\nManagement nodes (nodes 1-3)\nSecondary nodes (4-5)\n\nSystem\n4.5 GB,\t3.3 cores\n1.5 GB,\t1.1 cores\n\nStorage services\n10 storage disks, 1 metadata on system disk, 1 cache disk (each takes 0.5 GB and 0.2 cores), that is 6 GB and 2.4 cores in total\n10 storage disks, 1 metadata on system disk, 1 cache disk (each takes 0.5 GB and 0.2 cores), that is 6 GB and 2.4 cores in total\n\nS3\n4 GB, 2.4 cores\n4 GB, 2.4 cores\n\nService reservations\n12.6 GB of RAM and 8.1 cores\n9.6 GB of RAM and 5.9 cores\n\nMinimum hardware configuration\n16 GB of RAM and 8 cores\n12 GB of RAM and 6  cores\n\nRecommended hardware configuration\n48 GB of RAM and 16  cores\n\r\n48 GB of RAM and 16  cores\n\nSee also\n\nNetwork requirements\n\nProvisioning object storage space",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/object-storage-requirements.html"
    },
    {
        "title": "Physical space chart",
        "content": "Physical space chart\n\nAdmin panel\nThe Physical space chart shows the current usage of physical space in the entire storage cluster and on each particular tier. The used space includes the space occupied by all data chunks and their replicas, plus the space occupied by any other data.\n\nCommand-line interface\nUse the following command:vinfra cluster overview\nFor example, to view the physical space usage in the cluster cluster1, take a look at these lines from the command output:\r\n+-------------------+-------------------------+\r\n| Field             | Value                   |\r\n+-------------------+-------------------------+\r\n| ...               | ...                     |\r\n| tiers             | - id: 0                 |\r\n|                   |   phys_space:           |\r\n|                   |     free: 611533008896  |\r\n|                   |     total: 675644723200 |\r\n|                   |     used: 64111714304   |\r\n+-------------------+-------------------------+\r\n\n\nHow the physical space is calculated\nThe total physical disk space is a total of all the disk space on all storage disks on the same tier. The used physical space is a total of all the user data on the storage disks of the same tier, considering the redundancy mode. The free disk space is the total physical space minus the used physical space.\nTo better understand how physical disk space is calculated, consider the following example:\n\n\u00a0\nUsed/Total (Free), GiB\n\nTier 0,\r\n3+2 encoding\n(67% overhead)\n\nTier 1,\r\n2 replicas\n(100% overhead)\n\nTier 2,\r\nno redundancy\n\nNode 1\n334/1024 (690)\n134/512 (378)\n50/256 (206)\n\nNode 2\n334/1024 (690)\n133/512 (379)\n50/256 (206)\n\nNode 3\n334/1024 (690)\n133/512 (379)\n\u00a0\n\nNode 4\n334/1024 (690)\n\u00a0\n\u00a0\n\nNode 5\n334/1024 (690)\n\u00a0\n\u00a0\n\nReported\r\nsummary\n1670/5120 (3450)\n400/1536 (1136)\n100/512 (412)\n\nThe cluster has ten disks with the storage role: five 1024 GiB disks are assigned to tier 0, three 512 GiB disks to tier 1, and two 256 GiB disk to tier 2. There is no other data on the disks (for example, system files). Tier 0 stores 1000 GiB of user data in the 3+2 encoding mode. Tier 1 stores 200 GiB of user data in the 2 replicas mode. Tier 2 stores 100 GB of user data with no redundancy.\nNo matter what redundancy mode is used, the cluster attempts to spread data chunks evenly across disks of the same tier.\nIn this example, the physical disk space on each tier is reported as follows:\n\nOn tier 0, the total disk space is 5120 GiB, the used disk space is 1670 GiB, and the free disk space is 3450 GiB.\nOn tier 1, the total disk space is 1536 GiB, the used disk space is 400 GiB, and the free disk space is 1136 GiB.\nOn tier 2, the total disk space is 512 GiB, the used disk space is 100 GiB, and the free disk space is 456 GiB.\n\nSee also\n\nI/O activity charts\n\nServices chart\n\nChunks chart\n\nLogical space chart",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/physical-space-chart.html"
    },
    {
        "title": "Preparing boot volumes",
        "content": "Preparing boot volumes\nThe instruction for preparing a boot volume differs depending on the source type.\nTo make a boot volume from an existing virtual machine\n\nAdmin panel\n\nShut down the VM which boot volume you want to use.\nOn the VM right panel, go to the Properties section, and then click the disk marked as Bootable.\nOn the volume right pane, click Clone.\n\nIn the Clone volume window, specify a volume name, size, and storage policy. Click Clone.\n\nCommand-line interface\n\nShut down the VM which boot volume you want to use. For example:# vinfra service compute server stop myvm\n\nFind out the boot volume's ID. For example:# vinfra service compute server show myvm\r\n+---------------+--------------------------------------------+\r\n| Field         | Value                                      |\r\n+---------------+--------------------------------------------+\r\n| config_drive  |                                            |\r\n| created       | 2021-06-10T08:55:53Z                       |\r\n| description   |                                            |\r\n| fault         |                                            |\r\n| flavor        | disk: 0                                    |\r\n|               | ephemeral: 0                               |\r\n|               | extra_specs: {}                            |\r\n|               | original_name: tiny                        |\r\n|               | ram: 512                                   |\r\n|               | swap: 0                                    |\r\n|               | vcpus: 1                                   |\r\n| ha_enabled    | True                                       |\r\n| host          | amigai-ac-ve0.vstoragedomain               |\r\n| host_status   | UP                                         |\r\n| id            | 6d0fc132-7ea7-41f0-81ca-a4a2b2a2c893       |\r\n| key_name      |                                            |\r\n| metadata      | {}                                         |\r\n| name          | myvm                                       |\r\n| networks      | - id: bd17c207-5291-4096-be6a-0a8a4bf67792 |\r\n|               |   ipam_enabled: true                       |\r\n|               |   ips:                                     |\r\n|               |   - 192.168.128.100                        |\r\n|               |   mac_addr: fa:16:3e:6b:6c:83              |\r\n|               |   name: private                            |\r\n|               |   spoofing_protection: true                |\r\n| orig_hostname | amigai-ac-ve0                              |\r\n| placements    | []                                         |\r\n| power_state   | SHUTDOWN                                   |\r\n| project_id    | dfd99654b8c94b939b638f94abb2ad73           |\r\n| status        | SHUTOFF                                    |\r\n| task_state    |                                            |\r\n| updated       | 2021-06-15T11:24:05Z                       |\r\n| user_data     |                                            |\r\n| vm_state      | stopped                                    |\r\n| volumes       | - delete_on_termination: false             |\r\n|               |   id: 49be1057-c026-494f-b85d-e013728d41bd |\r\n|               | - delete_on_termination: false             |\r\n|               |   id: eca9f679-7e35-4768-ad20-9bcb6af6fd59 |\r\n+---------------+--------------------------------------------+\r\n\nThe first volume in the output is the boot one.\n\nClone the boot volume specifying the name of the new volume. For example:# vinfra service compute volume clone 49be1057-c026-494f-b85d-e013728d41bd \\\r\n--name cloned_volume\n\nThe cloned volume will appear in the vinfra service compute volume list output:# vinfra service compute volume list\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| id               | name                    | size | status    | os-vol-host-attr:host     |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| 14f4053e-cff5<\u00e2\u0080\u00a6> | cloned_volume           | 1    | available | node003.vstoragedomain<\u00e2\u0080\u00a6> |\r\n| 504078c7-9035<\u00e2\u0080\u00a6> | myvm/cirros/Boot volume | 1    | in-use    | node002.vstoragedomain<\u00e2\u0080\u00a6> |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n\n\nTo make a boot volume from a template\n\nAdmin panel\n\nGo to the Compute > Storage > Images tab, and then click the required image.\nOn the image panel, click Create volume.\n\nIn the Create volume window, specify the volume name, size, and select a storage policy.\n\nClick Create.\n\nThe new volume will appear on the Compute > Storage > Volumes tab.\n\nCommand-line interface\nUse the following command:vinfra service compute volume create [--description <description>] [--image <image>]\r\n                                     --storage-policy <storage_policy> --size <size-gb> <volume-name>\r\n\n\n--description <description>\n\nVolume description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n--image <image>\n\nSource compute image ID or name\n--storage-policy <storage_policy>\n\nStorage policy ID or name\n--size <size-gb>\n\nVolume size, in gigabytes\n<volume-name>\n\nVolume name\n\nFor example, to create a volume called cirros_volume with the size of 1 GB and the default storage policy from the cirros image, run:# vinfra service compute volume create cirros_volume --image cirros --storage-policy default --size 1\nThe new volume will appear in the vinfra service compute volume list output:# vinfra service compute volume list\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| id               | name                    | size | status    | os-vol-host-attr:host     |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| 232d09db-bc75<\u00e2\u0080\u00a6> | cirros_volume           | 1    | available | node003.vstoragedomain<\u00e2\u0080\u00a6> |\r\n| 14f4053e-cff5<\u00e2\u0080\u00a6> | cloned_volume           | 1    | available | node003.vstoragedomain<\u00e2\u0080\u00a6> |\r\n| 504078c7-9035<\u00e2\u0080\u00a6> | myvm/cirros/Boot volume | 1    | in-use    | node002.vstoragedomain<\u00e2\u0080\u00a6> |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n\n\nWhat's next\n\nCreating virtual machines\n\nManaging images",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nShut down the VM which boot volume you want to use. For example:# vinfra service compute server stop myvm\n\n\nFind out the boot volume's ID. For example:# vinfra service compute server show myvm\r\n+---------------+--------------------------------------------+\r\n| Field         | Value                                      |\r\n+---------------+--------------------------------------------+\r\n| config_drive  |                                            |\r\n| created       | 2021-06-10T08:55:53Z                       |\r\n| description   |                                            |\r\n| fault         |                                            |\r\n| flavor        | disk: 0                                    |\r\n|               | ephemeral: 0                               |\r\n|               | extra_specs: {}                            |\r\n|               | original_name: tiny                        |\r\n|               | ram: 512                                   |\r\n|               | swap: 0                                    |\r\n|               | vcpus: 1                                   |\r\n| ha_enabled    | True                                       |\r\n| host          | amigai-ac-ve0.vstoragedomain               |\r\n| host_status   | UP                                         |\r\n| id            | 6d0fc132-7ea7-41f0-81ca-a4a2b2a2c893       |\r\n| key_name      |                                            |\r\n| metadata      | {}                                         |\r\n| name          | myvm                                       |\r\n| networks      | - id: bd17c207-5291-4096-be6a-0a8a4bf67792 |\r\n|               |   ipam_enabled: true                       |\r\n|               |   ips:                                     |\r\n|               |   - 192.168.128.100                        |\r\n|               |   mac_addr: fa:16:3e:6b:6c:83              |\r\n|               |   name: private                            |\r\n|               |   spoofing_protection: true                |\r\n| orig_hostname | amigai-ac-ve0                              |\r\n| placements    | []                                         |\r\n| power_state   | SHUTDOWN                                   |\r\n| project_id    | dfd99654b8c94b939b638f94abb2ad73           |\r\n| status        | SHUTOFF                                    |\r\n| task_state    |                                            |\r\n| updated       | 2021-06-15T11:24:05Z                       |\r\n| user_data     |                                            |\r\n| vm_state      | stopped                                    |\r\n| volumes       | - delete_on_termination: false             |\r\n|               |   id: 49be1057-c026-494f-b85d-e013728d41bd |\r\n|               | - delete_on_termination: false             |\r\n|               |   id: eca9f679-7e35-4768-ad20-9bcb6af6fd59 |\r\n+---------------+--------------------------------------------+\r\n\nThe first volume in the output is the boot one.\n\n\nClone the boot volume specifying the name of the new volume. For example:# vinfra service compute volume clone 49be1057-c026-494f-b85d-e013728d41bd \\\r\n--name cloned_volume\n\n\nThe cloned volume will appear in the vinfra service compute volume list output:# vinfra service compute volume list\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| id               | name                    | size | status    | os-vol-host-attr:host     |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| 14f4053e-cff5<\u00e2\u0080\u00a6> | cloned_volume           | 1    | available | node003.vstoragedomain<\u00e2\u0080\u00a6> |\r\n| 504078c7-9035<\u00e2\u0080\u00a6> | myvm/cirros/Boot volume | 1    | in-use    | node002.vstoragedomain<\u00e2\u0080\u00a6> |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n\n",
                "title": "To make a boot volume from an existing virtual machine"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute volume create [--description <description>] [--image <image>]\r\n                                     --storage-policy <storage_policy> --size <size-gb> <volume-name>\r\n\n\n--description <description>\n\n\nVolume description\n\nA description should not contain any personally identifiable information or sensitive business data.\n\n\n--image <image>\n\nSource compute image ID or name\n--storage-policy <storage_policy>\n\nStorage policy ID or name\n--size <size-gb>\n\nVolume size, in gigabytes\n<volume-name>\n\nVolume name\n\nFor example, to create a volume called cirros_volume with the size of 1 GB and the default storage policy from the cirros image, run:# vinfra service compute volume create cirros_volume --image cirros --storage-policy default --size 1\nThe new volume will appear in the vinfra service compute volume list output:# vinfra service compute volume list\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| id               | name                    | size | status    | os-vol-host-attr:host     |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n| 232d09db-bc75<\u00e2\u0080\u00a6> | cirros_volume           | 1    | available | node003.vstoragedomain<\u00e2\u0080\u00a6> |\r\n| 14f4053e-cff5<\u00e2\u0080\u00a6> | cloned_volume           | 1    | available | node003.vstoragedomain<\u00e2\u0080\u00a6> |\r\n| 504078c7-9035<\u00e2\u0080\u00a6> | myvm/cirros/Boot volume | 1    | in-use    | node002.vstoragedomain<\u00e2\u0080\u00a6> |\r\n+------------------+-------------------------+------+-----------+---------------------------+\r\n\n",
                "title": "To make a boot volume from a template"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nShut down the VM which boot volume you want to use.\nOn the VM right panel, go to the Properties section, and then click the disk marked as Bootable.\nOn the volume right pane, click Clone.\n\nIn the Clone volume window, specify a volume name, size, and storage policy. Click Clone.\n\n\n\n\n\n\n",
                "title": "To make a boot volume from an existing virtual machine"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Compute > Storage > Images tab, and then click the required image.\nOn the image panel, click Create volume.\n\nIn the Create volume window, specify the volume name, size, and select a storage policy.\n\n\n\n\n\nClick Create.\n\nThe new volume will appear on the Compute > Storage > Volumes tab.\n",
                "title": "To make a boot volume from a template"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/preparing-boot-volumes.html"
    },
    {
        "title": "Performing a failover",
        "content": "Performing a failover\nIf the primary cluster becomes unavailable, you can perform a manual failover by promoting the secondary cluster to primary. This operation will switch the configuration of the secondary cluster, including its DNS name, to the configuration of the primary one. Failover of the primary cluster can be performed in the following cases:\n\nThe current primary cluster is completely non-operational and isolated from the Internet and any backup agents.\nBackup agents are unable to communicate with the current primary cluster.\nThe DNS name of the primary cluster has been reconfigured to its IP addresses.\n\nPromoting the secondary cluster to primary is an irreversible operation that will invalidate all data on the primary cluster. Use it only in case of emergency.\n\nPrerequisites\n\nGeo-replication is enabled, as described in Enabling geo-replication.\nThe secondary cluster can reach itself via its own domain name on TCP port 44445.\n\nTo perform a failover\n\nAdmin panel\n\nOn the secondary cluster, go to Storage services > Backup storage > Geo-replication, and then click Promote to primary.\n\nClick Failover in the confirmation window.\nReconfigure the DNS records with the IP addresses of the former secondary cluster for each registration.\n\nIf the current primary cluster is still operational, forcibly delete all of its registrations, and then perform a failover.\n\nCommand-line interface\nRun the following command:# vinfra service backup geo-replication secondary promote-to-primary\n\nSee also\n\nMonitoring backup storage\n\nImporting registrations to the secondary cluster\n\nDisabling geo-replication",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nRun the following command:# vinfra service backup geo-replication secondary promote-to-primary\n",
                "title": "To perform a failover"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\n\nOn the secondary cluster, go to Storage services > Backup storage > Geo-replication, and then click Promote to primary.\n\n\n\n\n\nClick Failover in the confirmation window.\nReconfigure the DNS records with the IP addresses of the former secondary cluster for each registration.\n\nIf the current primary cluster is still operational, forcibly delete all of its registrations, and then perform a failover.\n",
                "title": "To perform a failover"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/performing-a-failover.html"
    },
    {
        "title": "Placement modes",
        "content": "Placement modes\nPlacements for compute nodes have two modes: hard and soft. The hard mode is the default one. To better understand how these modes work, let's consider an example when a system administrator creates placements and assigns them to images, flavors, and nodes as follows:\n\nIn the figure above:\n\nPlacement1 in the hard mode is assigned to the Win2k19 image and the nodes Node1, Node2, and Node4.\nPlacement2 in the soft mode is assigned to the Win10 image and the nodes Node5, Node6, and Node7.\nPlacement3 in the soft mode is assigned to the Medium flavor and the nodes Node4, Node6, and Node7.\nPlacement4 in the hard mode is assigned to the Large flavor and the nodes Node1, Node3, Node5, and Node6.\nThe Linux image, the Small flavor, and node Node8 have no placements assigned.\n\nWhen a user starts creating virtual machines, they inherit placements from the selected images and flavors. Depending on its placement mode, a virtual machine can be placed on different nodes.\nVirtual machines with the hard placement mode\nWith the hard placement mode, a virtual machine is placed on a node that has exactly the same placements as the virtual machine.\n\nIn the figure above:\n\nVM1 is created from the Win2k19 image with Placement1 and the Large flavor with Placement4. The VM inherits Placement1 and Placement4 in the hard mode. VM1 can be placed only on Node1 because only this node has both Placement1 and Placement4 assigned.\nVM2 is created from the Win2k19 image with Placement1 and the Small flavor without placements. The VM inherits Placement1 in the hard mode. VM2 can be placed only on Node2 because only this node has single Placement1 assigned.\nVM3 is created from the Linux image without placements and with the Large flavor with Placement4. The VM inherits Placement4 in the hard mode. VM3 can be placed only on Node3 because only this node has single Placement4 assigned.\n\nVirtual machines with the soft placement mode\nWith the soft placement mode, a VM is placed on a node that has at least the same placements as the VM.\n\nIn the figure above:\n\nVM4 is created from the Win10 image with Placement2 and the Medium flavor with Placement3. The VM inherits Placement2 and Placement3 in the soft mode. VM4 can be placed on Node6 and Node7 because these nodes have both Placement2 and Placement3 assigned.\nVM5 is created from the Win10 image with Placement2 and the Small flavor without placements. The VM inherits Placement2 in the soft mode. VM5 can be placed Node5, Node6, and Node7 because all of these nodes have Placement2 assigned.\nVM6 is created from the Linux image without placements and with the Medium flavor with Placement3. The VM inherits Placement3 in the soft mode. VM6 can be placed on Node4, Node6, and Node7 because all of these nodes have Placement3 assigned.\n\nVirtual machines with both placement modes and without placements\nA VM can have placements in the both soft and hard modes. In this case, VM placements use the soft mode, that is a VM is placed on a node that has at least the same placements as the VM.\nWhen a VM has no placements assigned, it can be placed either on a node with placements in the soft mode or on a node that is not added to any placements\n\nIn the figure above:\n\nVM7 is created from the Win2k19 image with Placement1 and the Medium flavor with Placement3. The VM inherits Placement1 in the hard mode and Placement3 in the soft mode. VM7 can be placed on Node4 because only this node has both Placement1 and Placement3 assigned.\nVM8 is created from the Win10 image with Placement2 and the Large flavor with Placement4. The VM inherits Placement4 in the hard mode and Placement2 in the soft mode. VM7 can be placed on Node5 and Node6 because these nodes have both Placement2 and Placement4 assigned.\nVM9 is created from the Linux image without placements and the Small flavor without placements. The VM inherits no placements. VM6 can be placed on Node7 because this node has placements in the soft mode. Also, VM6 can be placed on Node8 because this node has no placements assigned.\n\nWhat's next\n\nCreating placements",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/placement-modes.html"
    },
    {
        "title": "Performing node maintenance",
        "content": "Performing node maintenance\nWhenever you need to perform service operations on a cluster node, place it in the maintenance mode. When you do so, the node stops allocating new chunks of storage data, but continues to handle I/O operations for the core storage services such as MDS, CS, and cache. They will not, however, be used to allocate new data, so placing the node in maintenance may reduce the free space in the storage cluster. All storage services on the node will continue serving data unless the node goes offline. Other node\u00e2\u0080\u0099s services (compute, Backup Gateway, iSCSI, S3, and NFS) can either be relocated or left as is during maintenance. \nIf a service cannot be evacuated from the node for some reason, entering maintenance will be halted. You will need to decide on how to proceed: exit or force maintenance.\nOnce the node is in the maintenance mode, you can shut it down and perform service operations on it. Once you are done, power on the node and return it to operation in the admin panel.\nIf needed, you can place more nodes in the maintenance mode, one at a time. The cluster can tolerate multiple nodes in maintenance simultaneously as long as the other cluster nodes have enough resources to accommodate evacuated workloads. This also applies during a cluster update, which can be performed when the cluster already has nodes in maintenance. If nodes that have entered the maintenance mode are online, they are updated together with the other cluster nodes, but do not exit maintenance after the update is complete. If a node goes offline while in maintenance, the cluster becomes degraded, thus blocking an update. You can wait until this node is up again or release it from the cluster.\nLimitations\n\nThe last management node cannot be placed in the maintenance mode.\nYou cannot evacuate a service from the last operational node where this service is deployed. For example, VMs that are hosted on the last compute node cannot be relocated from this node and will be ignored during maintenance.\nSuspended VMs cannot be relocated from the node and will be ignored.\nNodes in maintenance can only be returned to operation or released.\n\nPrerequisites\n\nA clear understanding of cluster self-healing, which is explained in \u00d0\u00a1luster rebuilding.\nYou have five MDS services in the storage cluster. In this case, when a node running the MDS service is shut down during maintenance, the cluster can survive the failure of another node.\nIf the node hosts virtual machines, the other compute nodes have enough resources to accommodate these VMs.\nIf the node hosts iSCSI targets, the iSCSI initiators are configured to use multiple IP addresses from the same target group.\nIf the node runs an S3 gateway, its IP addresses are removed from the DNS records of the S3 access points. Otherwise, some of the S3 clients may experience connection timeouts.\n\nTo place a node in the maintenance mode\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the line with the desired node.\nOn the node right pane, click Enter maintenance.\n\nIn the Enter maintenance window, select to Evacuate or Ignore the following workloads during maintenance:\n\nBlock storage. iSCSI target groups are highly available, with multiple targets running on different nodes. When the node enters maintenance, the target it hosts is stopped and the preferred path is moved to another node in the target group within 60 seconds. Thus, the service is not interrupted during maintenance.\nCompute. Evacuating virtual machines from the node means migrating them live one by one to other compute nodes. If you select to ignore them, they will continue running until you reboot or shut down the node. In this case, they will be stopped, resulting in downtime. They also will not be started automatically once the node is up again.\nS3. You can evacuate S3 services from this node to other nodes in the S3 cluster or ignore them. In the latter case, they will continue running until you reboot or shut down the node, resulting in downtime. They will be started automatically once the node is up again.\nNFS. You can evacuate NFS services from this node to other nodes in the NFS cluster or ignore them. In the latter case, they will continue running until you reboot or shut down the node, resulting in downtime. They will be started automatically once the node is up again.\nABGW. This service is highly available, with multiple instances spread across different nodes. Placing this node in the maintenance mode will stop one of the instances, but the others will continue working, so the service will not be interrupted.\n\nStart data healing. Cluster self-healing is an automatic restoration of storage cluster data that becomes unavailable when a storage node (or a disk) goes offline. If this happens during maintenance, self-healing is delayed (for 30 minutes by default) to save cluster resources. If the node goes back online before the delay ends, self-healing is not necessary.\nYou can manually configure the replication timeout by setting the mds.wd.offline_tout_mnt parameter, in milliseconds, with the vstorage -c <cluster_name> set-config command.\n\nIf the node has non-redundant chunks of data, you will see the Relocate non-redundant data option. Select it to move the non-redundant data to other storage nodes. Otherwise, this data will become unavailable if the node goes offline. The data may also be temporarily moved to another tier if the current one is full.\nClick Enter.\n\nCommand-line interface\nUse the following commands:\n\nStart maintenance precheck for a node. For example:# vinfra node maintenance precheck node001\r\n\n\nView the maintenance status. For example:# vinfra node maintenance status node001\r\n+-----------+------------------------------------------+\r\n| Field     | Value                                    |\r\n+-----------+------------------------------------------+\r\n| node_id   | c3b2321a-7c12-8456-42ce-8005ff937e12     |\r\n| params    | alua_mode: suspend                       |\r\n|           | compute_mode: evacuate                   |\r\n|           | iscsi_mode: evacuate                     |\r\n|           | nfs_mode: evacuate                       |\r\n|           | s3_mode: evacuate                        |\r\n|           | storage_mode: suspend                    |\r\n| precheck  | flow: completed                          |\r\n|           | id: c15bf919-9a81-45b1-8fef-5f626e68f957 |\r\n|           | result:                                  |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources: null                        |\r\n|           |   service: node                          |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: false                   |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources: null                        |\r\n|           |   service: iscsi                         |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources:                             |\r\n|           |     failed: []                           |\r\n|           |   service: alua                          |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources: null                        |\r\n|           |   service: compute                       |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: false                   |\r\n|           |   relocation_is_possible: null           |\r\n|           |   resources: null                        |\r\n|           |   service: nfs                           |\r\n|           |   service_is_available: false            |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: false          |\r\n|           |   resources: null                        |\r\n|           |   service: s3                            |\r\n|           |   service_is_available: true             |\r\n|           | state: success                           |\r\n|           | updated_at: '2021-11-01T11:09:41.331926' |\r\n| resources |                                          |\r\n| state     | completed                                |\r\n| task      |                                          |\r\n+-----------+------------------------------------------+\nThe output above shows that the node has relocatable iSCSI and compute services, as well as the S3 service that cannot be relocated.\n\nStart the node maintenance by running the command:vinfra node maintenance start [--iscsi-mode <mode>] [--compute-mode <mode>]\r\n                              [--s3-mode <mode>] [--storage-mode <mode>]\r\n                              [--alua-mode <mode>] [--nfs-mode <mode>] <node>\r\n\n\n--iscsi-mode <mode>\n\nIgnore ISCSI evacuation during maintenance (ignore).\n--compute-mode <mode>\n\nIgnore compute evacuation during maintenance (ignore).\n--s3-mode <mode>\n\nIgnore S3 evacuation during maintenance (ignore).\n--storage-mode <mode>\n\nIgnore storage evacuation during maintenance (ignore).\n--alua-mode <mode>\n\nIgnore Block Storage target groups during maintenance (ignore).\n--nfs-mode <mode>\n\nIgnore NFS evacuation during maintenance (ignore).\n<node>\n\nNode ID or hostname\n\nFor example, to start maintenance for the node node001 without evacuating its S3 service, run:# vinfra node maintenance start node001 --s3-mode ignore\n\nTo proceed after entering maintenance halted\n\nOn the Infrastructure > Nodes screen, click the line with the desired node.\nOn the node right pane, click Enter maintenance.\nSelect the desired action:Select Exit maintenance to return all services on the node  to their normal state.Select Force maintenance to stop the services that could not be evacuated  during node reboot or shutdown.\n Click Continue.\n\nTo return a node to operation\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the line with the desired node.\nOn the node right pane, click Exit maintenance.\nIn the confirmation window, click Yes.\n\nCommand-line interface\nUse the following command:vinfra node maintenance stop <node> [--ignore-compute]\n\n<node>\n\nNode ID or hostname\n--ignore-compute\n\nIgnore compute resources while returning a node to operation\n\nFor example, to stop maintenance for the node node001, run:# vinfra node maintenance stop node001\n\nSee also\n\nInstalling updates",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following commands:\n\n\nStart maintenance precheck for a node. For example:# vinfra node maintenance precheck node001\r\n\n\n\nView the maintenance status. For example:# vinfra node maintenance status node001\r\n+-----------+------------------------------------------+\r\n| Field     | Value                                    |\r\n+-----------+------------------------------------------+\r\n| node_id   | c3b2321a-7c12-8456-42ce-8005ff937e12     |\r\n| params    | alua_mode: suspend                       |\r\n|           | compute_mode: evacuate                   |\r\n|           | iscsi_mode: evacuate                     |\r\n|           | nfs_mode: evacuate                       |\r\n|           | s3_mode: evacuate                        |\r\n|           | storage_mode: suspend                    |\r\n| precheck  | flow: completed                          |\r\n|           | id: c15bf919-9a81-45b1-8fef-5f626e68f957 |\r\n|           | result:                                  |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources: null                        |\r\n|           |   service: node                          |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: false                   |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources: null                        |\r\n|           |   service: iscsi                         |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources:                             |\r\n|           |     failed: []                           |\r\n|           |   service: alua                          |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: true           |\r\n|           |   resources: null                        |\r\n|           |   service: compute                       |\r\n|           |   service_is_available: true             |\r\n|           | - has_resources: false                   |\r\n|           |   relocation_is_possible: null           |\r\n|           |   resources: null                        |\r\n|           |   service: nfs                           |\r\n|           |   service_is_available: false            |\r\n|           | - has_resources: true                    |\r\n|           |   relocation_is_possible: false          |\r\n|           |   resources: null                        |\r\n|           |   service: s3                            |\r\n|           |   service_is_available: true             |\r\n|           | state: success                           |\r\n|           | updated_at: '2021-11-01T11:09:41.331926' |\r\n| resources |                                          |\r\n| state     | completed                                |\r\n| task      |                                          |\r\n+-----------+------------------------------------------+\nThe output above shows that the node has relocatable iSCSI and compute services, as well as the S3 service that cannot be relocated.\n\n\nStart the node maintenance by running the command:vinfra node maintenance start [--iscsi-mode <mode>] [--compute-mode <mode>]\r\n                              [--s3-mode <mode>] [--storage-mode <mode>]\r\n                              [--alua-mode <mode>] [--nfs-mode <mode>] <node>\r\n\n\n--iscsi-mode <mode>\n\nIgnore ISCSI evacuation during maintenance (ignore).\n--compute-mode <mode>\n\nIgnore compute evacuation during maintenance (ignore).\n--s3-mode <mode>\n\nIgnore S3 evacuation during maintenance (ignore).\n--storage-mode <mode>\n\nIgnore storage evacuation during maintenance (ignore).\n--alua-mode <mode>\n\nIgnore Block Storage target groups during maintenance (ignore).\n--nfs-mode <mode>\n\nIgnore NFS evacuation during maintenance (ignore).\n<node>\n\nNode ID or hostname\n\nFor example, to start maintenance for the node node001 without evacuating its S3 service, run:# vinfra node maintenance start node001 --s3-mode ignore\n\n\n",
                "title": "To place a node in the maintenance mode"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node maintenance stop <node> [--ignore-compute]\n\n<node>\n\nNode ID or hostname\n--ignore-compute\n\nIgnore compute resources while returning a node to operation\n\nFor example, to stop maintenance for the node node001, run:# vinfra node maintenance stop node001\n",
                "title": "To return a node to operation"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the line with the desired node.\nOn the node right pane, click Enter maintenance.\n\nIn the Enter maintenance window, select to Evacuate or Ignore the following workloads during maintenance:\n\nBlock storage. iSCSI target groups are highly available, with multiple targets running on different nodes. When the node enters maintenance, the target it hosts is stopped and the preferred path is moved to another node in the target group within 60 seconds. Thus, the service is not interrupted during maintenance.\nCompute. Evacuating virtual machines from the node means migrating them live one by one to other compute nodes. If you select to ignore them, they will continue running until you reboot or shut down the node. In this case, they will be stopped, resulting in downtime. They also will not be started automatically once the node is up again.\nS3. You can evacuate S3 services from this node to other nodes in the S3 cluster or ignore them. In the latter case, they will continue running until you reboot or shut down the node, resulting in downtime. They will be started automatically once the node is up again.\nNFS. You can evacuate NFS services from this node to other nodes in the NFS cluster or ignore them. In the latter case, they will continue running until you reboot or shut down the node, resulting in downtime. They will be started automatically once the node is up again.\nABGW. This service is highly available, with multiple instances spread across different nodes. Placing this node in the maintenance mode will stop one of the instances, but the others will continue working, so the service will not be interrupted.\n\n\n\n\n\n\n\nStart data healing. Cluster self-healing is an automatic restoration of storage cluster data that becomes unavailable when a storage node (or a disk) goes offline. If this happens during maintenance, self-healing is delayed (for 30 minutes by default) to save cluster resources. If the node goes back online before the delay ends, self-healing is not necessary.\nYou can manually configure the replication timeout by setting the mds.wd.offline_tout_mnt parameter, in milliseconds, with the vstorage -c <cluster_name> set-config command.\n\nIf the node has non-redundant chunks of data, you will see the Relocate non-redundant data option. Select it to move the non-redundant data to other storage nodes. Otherwise, this data will become unavailable if the node goes offline. The data may also be temporarily moved to another tier if the current one is full.\nClick Enter.\n\n",
                "title": "To place a node in the maintenance mode"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the line with the desired node.\nOn the node right pane, click Exit maintenance.\nIn the confirmation window, click Yes.\n\n",
                "title": "To return a node to operation"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/performing-node-maintenance.html"
    },
    {
        "title": "Preparing nodes for GPU virtualization",
        "content": "Preparing nodes for GPU virtualization\nFor vGPU to work, enable it on the node by installing the NVIDIA kernel module, and then enable IOMMU. However, if you want to virtualize a GPU that was previously detached from the node for GPU passthrough, you need to additionally modify the GRUB configuration file.\nTo enable vGPU on a node\n\nOn the node with the physical GPU, do one of the following:\n\nIf the physical GPU is attached to the node\n\nBlacklist the Nouveau driver:# rmmod nouveau\r\n# echo -e \"blacklist nouveau\\noptions nouveau modeset=0\" > /usr/lib/modprobe.d/nouveau.conf\r\n# echo -e \"blacklist nouveau\\noptions nouveau modeset=0\" > /etc/modprobe.d/nouveau.conf\n\nIf the physical GPU is detached from the node\n\nIn the /etc/sysconfig/grub file, locate the GRUB_CMDLINE_LINUX line, and then delete pci-stub.ids=<gpu_vid>:<gpu_pid>. For example, for a GPU with the VID and PID 10de:1eb8, delete pci-stub.ids=10de:1eb8, and check the resulting file:# cat /etc/sysconfig/grub | grep CMDLINE\r\nGRUB_CMDLINE_LINUX=\"crashkernel=auto tcache.enabled=0 quiet iommu=pt rd.driver.blacklist=nouveau nouveau.modeset=0\"\n\nRegenerate the GRUB configuration file.\n\nOn a BIOS-based system, run:# /usr/sbin/grub2-mkconfig -o /etc/grub2.cfg\n\nOn a UEFI-based system, run:# /usr/sbin/grub2-mkconfig -o /etc/grub2-efi.cfg\n\nReboot the node to apply the changes:# reboot\n\nInstall the kernel-devel package:# dnf install kernel-devel\n\nInstall the vGPU KVM kernel module from the NVIDIA GRID package:# bash NVIDIA-Linux-x86_64-xxx.xx.xx-vgpu-kvm*.run\n\nRe-create the Linux boot image by running:# dracut -f\n\nRun the pci-helper.py enable-iommu script to enable IOMMU on the node:# /usr/libexec/vstorage-ui-agent/bin/pci-helper.py enable-iommu\nThe script works for both Intel and AMD processors.\n\nReboot the node to apply the changes:# reboot\n\nYou can check that IOMMU is successfully enabled in the dmesg output:# dmesg | grep -e DMAR -e IOMMU\r\n[    0.000000] DMAR: IOMMU enabled\nTo check that a GPU card is vGPU enabled\nList all graphics cards on the node and obtain their PCI addresses:# lspci -D | grep NVIDIA\r\n0000:01:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\r\n0000:81:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\n0000:01:00.0 and \r\n0000:81:00.0 are the PCI addresses of the graphics cards.\nCheck that the graphics card is vGPU enabled:ls /sys/bus/pci/devices/0000\\:01:00.0/mdev_supported_types\r\nnvidia-222  nvidia-223  nvidia-224  nvidia-225  nvidia-226  nvidia-227  nvidia-228  nvidia-229  nvidia-230  nvidia-231\r\nnvidia-232  nvidia-233  nvidia-234  nvidia-252  nvidia-319  nvidia-320  nvidia-321\nFor a vGPU-enabled card, the directory contains a list of supported vGPU types. A vGPU type is a vGPU configuration that defines the vRAM size, maximum resolution, maximum number of supported vGPUs, and other parameters.\nWhat's next\n\nEnabling PCI passthrough and vGPU support",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/preparing-nodes-for-gpu-virtualization.html"
    },
    {
        "title": "Protecting data during a power outage",
        "content": "Protecting data during a power outage\nTo protect Virtuozzo Hybrid Infrastructure against power outages, it is recommended to use an Uninterruptable Power Supply (UPS) for all servers and network switches.\nAdditionally, you can prevent data loss and preserve data integrity by using enterprise-grade SSD drives. Unlike HDD drives, enterprise-grade SSD drives can properly handle power loss events. Enterprise-grade SSD drives that operate correctly usually have the power loss protection property in their technical specification. Some of the market names for this technology are Enhanced Power Loss Data Protection (Intel), Cache Power Protection (Samsung), Power-Failure Support (Kingston), and Complete Power Fail Protection (OCZ).\nIt is also recommended to ensure that all storage devices that will be added to your cluster can flush data from cache to disk in case of a power outage. You can check the data flushing capabilities of your disks, as explained in Checking disk data flushing capabilities.\nSee also\n\nQuantity of disks per node\n\nHDD/SSD configuration\n\nServer requirements\n\nNetwork requirements and recommendations",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/protecting-data-during-a-power-outage.html"
    },
    {
        "title": "Preparing nodes for SR-IOV",
        "content": "Preparing nodes for SR-IOV\nTo use SR-IOV capabilities, check that the network adapter that you want to pass through supports them, and then enable IOMMU. For NVIDIA Mellanox network adapters, you need to additionally enable SR-IOV in firmware.\nTo check that a network adapter supports SR-IOV\nList all network adapters on a node and obtain their VID and PID:# lspci -nnD | grep Ethernet\r\n0000:00:03.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017]\r\n0000:00:04.0 Ethernet controller [0200]: Mellanox Technologies MT27800 Family [ConnectX-5] [15b3:1017]\n[15b3:1017] is the VID and PID of the network adapter.\nCheck that the chosen network adapter supports SR-IOV by using its VID and PID:# lspci -vv -d 15b3:1017 | grep SR-IOV\r\nCapabilities: [180 v1] Single Root I/O Virtualization (SR-IOV)\r\nCapabilities: [180 v1] Single Root I/O Virtualization (SR-IOV)\nTo enable IOMMU on a node\nRun the pci-helper.py script, and then reboot the node to apply the changes:# /usr/libexec/vstorage-ui-agent/bin/pci-helper.py enable-iommu\r\n# reboot\nThe script works for both Intel and AMD processors.\nYou can check that IOMMU is successfully enabled in the dmesg output:# dmesg | grep -e DMAR -e IOMMU\r\n[    0.000000] DMAR: IOMMU enabled\nTo enable SR-IOV in firmware for NVIDIA Mellanox network adapters\n\nDownload Mellanox Firmware Tools (MFT) from the official website and extract the archive on the node. For example:# wget https://www.mellanox.com/downloads/MFT/mft-4.17.0-106-x86_64-rpm.tgz\r\n# tar -xvzf mft-4.17.0-106-x86_64-rpm.tgz\n\nInstall the package, and then start Mellanox Software Tools (MST):# yum install rpm-build\r\n# . mft-4.17.0-106-x86_64-rpm/install.sh\r\n# mst start\n\nDetermine the MST device path:# mst status\n\nQuery the current configuration:# mlxconfig -d /dev/mst/mt4119_pciconf0 q\r\n...\r\nConfigurations:\r\n...\r\n         NUM_OF_VFS            4               # Number of activated VFs\r\n         SRIOV_EN              True(1)         # SR-IOV is enabled\r\n...\n\nSet the desired values, if necessary. For example, to increase the number of virtual functions to 8, run:# mlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=8\n\nReboot the node to apply the changes.\n\nWhat's next\n\nEnabling PCI passthrough and vGPU support",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/preparing-nodes-for-sr-iov.html"
    },
    {
        "title": "Preparing templates",
        "content": "Preparing templates\nYou may need to create a template in these cases:\n\nTo rescue a virtual machine\nTo create a VM accessible via SSH\nTo create a VM customizable with user data\n\nPreparation overview\n\nInstall cloud-init and OpenSSH Server in the virtual machine.\n\nEnable logging for virtual machines that will be created from the template.\n\nConvert the VM boot volume to the template.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/preparing-templates.html"
    },
    {
        "title": "Prometheus metrics",
        "content": "Prometheus metrics\nVirtuozzo Hybrid Infrastructure uses three types of metrics in Prometheus:\n\nCounter metrics (usually, with the \"_total\" suffix) are cumulative and increase over time\nGauge metrics show fluctuating values\nHistogram metrics are cumulative and store measurements in different buckets depending on the measurement value:Metrics with the \"_bucket\" suffix show the current value per bucketMetrics with the \"_sum\" suffix show the total sum of all values per bucketMetrics with the \"_count\" suffix show the number of stored measurements per bucket",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/prometheus-metrics.html"
    },
    {
        "title": "Providing access to the self-service portal",
        "content": "Providing access to the self-service portal\nThe self-service panel is a web-based control panel that allows end users to manage virtual objects, such as virtual machines, volumes, virtual networks, and other, in isolated administrative environments.\nLimitations\n\nThe default system administrator cannot log in to the self-service portal.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\nSelf-service projects and users are created, as described in Configuring multitenancy.\nThe needed compute images are shared between all projects or the permission to upload images is granted to self-service users.\nThe compute cluster has at least one physical network.\n\nTo be able to access the self-service panel\n\nAdmin panel\nOpen TCP port 8800 on the management node by doing the following:\n\nOn the Infrastructure > Networks screen, click Edit.\nAdd the Self-service panel traffic type to your public network by selecting the corresponding check box.\nClick Save to apply changes.\n\nYou can now access the self-service panel at http://<admin_panel_IP_address>:8800. Use the domain name and user credentials to log in. If high availability for the management node is enabled, log in into the self-service panel by using the virtual address for the admin panel: http://<admin_panel_virtual_IP_address>:8800. You can also use the link in the Panel URLs field on the Settings > System settings > Self-service portal screen.\n\nCommand-line interface\nOpen TCP port 8800 on the management node by adding the Self-service panel traffic type to your public network. For example, run:# vinfra cluster network set Public --add-traffic-types \"Self-service panel\"\nYou can now access the self-service panel at http://<admin_panel_IP_address>:8800. Use the domain name and user credentials to log in. If high availability for the management node is enabled, log in into the self-service panel by using the virtual address for the admin panel: http://<admin_panel_virtual_IP_address>:8800.\n\nSee also\n\nConfiguring the self-service panel",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nOpen TCP port 8800 on the management node by adding the Self-service panel traffic type to your public network. For example, run:# vinfra cluster network set Public --add-traffic-types \"Self-service panel\"\nYou can now access the self-service panel at http://<admin_panel_IP_address>:8800. Use the domain name and user credentials to log in. If high availability for the management node is enabled, log in into the self-service panel by using the virtual address for the admin panel: http://<admin_panel_virtual_IP_address>:8800.\n",
                "title": "To be able to access the self-service panel"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nOpen TCP port 8800 on the management node by doing the following:\n\nOn the Infrastructure > Networks screen, click Edit.\nAdd the Self-service panel traffic type to your public network by selecting the corresponding check box.\nClick Save to apply changes.\n\nYou can now access the self-service panel at http://<admin_panel_IP_address>:8800. Use the domain name and user credentials to log in. If high availability for the management node is enabled, log in into the self-service panel by using the virtual address for the admin panel: http://<admin_panel_virtual_IP_address>:8800. You can also use the link in the Panel URLs field on the Settings > System settings > Self-service portal screen.\n\n\n\n\n",
                "title": "To be able to access the self-service panel"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/providing-access-to-the-self-service-portal.html"
    },
    {
        "title": "Preparing the bootable media",
        "content": "Preparing the bootable media\nYou can install Virtuozzo Hybrid Infrastructure  from the following bootable media:\n\nUSB drives, if you can access the server physically\nIntelligent Platform Management Interface (IPMI) virtual drives, if the server  is configured for remote out-of-band management via the IPMI ports\nPreboot execution environment (PXE) server, if you want to boot over a network",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/preparing-the-bootable-media.html"
    },
    {
        "title": "Provisioning load balancers",
        "content": "Provisioning load balancers\nVirtuozzo Hybrid Infrastructure offers load balancing as a service for the compute infrastructure. Load balancing ensures fault tolerance and improves performance of web applications by distributing incoming network traffic across virtual machines from a balancing pool. A load balancer receives and then routes incoming requests to a suitable VM based on a configured balancing algorithm and VM health.\nLoad balancers are created and managed by self-service users, as described in \"Managing load balancers\" in the Self-Service Guide.\r\nHowever, to provide self-service users with this functionality, you need to install the load balancer service in the admin panel.\nLimitations \n\nIn the current version of Virtuozzo Hybrid Infrastructure, the installed service cannot be removed.\n\nFor self-service users to be able to create highly available load balancers, the compute cluster must have at least two nodes.\nIf you install the service after creating a project, load balancers are not automatically enabled in the project quotas.\nThe maximum number of load balancers supported per project is 100.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\n\nTo install the load balancer service\n\nAdmin panel\n\nGo to the Settings > Add-on services screen.\nIn the Load balancer service section, click Install.\n\nCommand-line interface\nRun the following command:# vinfra service compute cluster set --enable-lbaas\n\nWhat's next\n\nManaging load balancers",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nRun the following command:# vinfra service compute cluster set --enable-lbaas\n",
                "title": "To install the load balancer service"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Settings > Add-on services screen.\nIn the Load balancer service section, click Install.\n\n",
                "title": "To install the load balancer service"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/provisioning-load-balancers.html"
    },
    {
        "title": "Provisioning billing metering",
        "content": "Provisioning billing metering\nThe billing metering service collects, stores, and provides usage metrics for resources consumed by end users in their projects. This compute service opens port 8041 and enables two Gnocchi services: gnocchi-api, an HTTP server, and gnocchi-metricd, a metric daemon.\nLimitations\n\nThe metering service will only take into account compute objects created after it has been enabled.\n\nIn the current version of Virtuozzo Hybrid Infrastructure, the installed service cannot be removed.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\nBefore deploying the metering service, ensure that you have tier 0 in your storage.\n\nTo install the billing metering service\n\nAdmin panel\n\nGo to the Settings > Add-on services screen.\nIn the Billing metering service section, click Install.\n\nCommand-line interface\nRun the following command:# vinfra service compute cluster set --enable-metering\r\n\n\nWhat's next\n\nUsing metering for compute resources",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nRun the following command:# vinfra service compute cluster set --enable-metering\r\n\n",
                "title": "To install the billing metering service"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Settings > Add-on services screen.\nIn the Billing metering service section, click Install.\n\n",
                "title": "To install the billing metering service"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/provisioning-billing-metering.html"
    },
    {
        "title": "Quantity of servers",
        "content": "Quantity of servers\nOne of the strongest features of Virtuozzo Hybrid Infrastructure is scalability. The bigger the cluster, the better Virtuozzo Hybrid Infrastructure performs.\nTo achieve the best performance, keep at least 20 percent of the cluster capacity free.\nOne server (evaluation scenario)\nEven though three nodes are recommended even for the minimum configuration, you can start evaluating Virtuozzo Hybrid Infrastructure with just one node and add more nodes later. If you only want to use the Backup Gateway, you can deploy a very basic installation on a single virtual or physical node. Although in this case you may need to provide data redundancy by other means or risk losing user data. \nAt the very least, a storage cluster must have one metadata service and one chunk service running. However, such a configuration will have two key limitations:\n\nJust one MDS will be a single point of failure. If it fails, the entire cluster will stop working.\nJust one CS will be able to store just one chunk replica. If it fails, the data will be lost.\n\nYou can use a single-node installation for  backup storage with the NFS share or public cloud destination.\nThree servers (minimum for high availability)\nThree servers are required to test all of the product features.\nThe minimum configuration ensures that the cluster can survive the failure of one node without data loss.\nThe minimum cluster configuration must have at least three metadata services running. SSD disks can be assigned the system, metadata, and cache roles at the same time, freeing up more disks for the storage role.\nFive and more servers (recommended for high availability and optimal TCO)\nFor a production environment, at least five nodes are required to ensure that the cluster can survive failure of two nodes without data loss. For improved resilience, performance, and fault tolerance it is recommended to create production clusters from at least ten nodes.\nA production-ready cluster can be created from just five nodes by using the recommended hardware. However, it is recommended to enter production with at least ten nodes if you are aiming to achieve significant performance advantages over direct-attached storage (DAS) or improved recovery times.\nThe recommended cluster configuration must have five metadata services running and dedicated disks for write cache.\nSee also\n\nGeneral requirements\n\nHDD/SSD configuration",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/quantity-of-servers.html"
    },
    {
        "title": "Quantity of disks per node",
        "content": "Quantity of disks per node\nEach management node must have at least two disks (one for system and metadata, one for storage). Each secondary node must have at least two disks (one for system, one for storage). It is recommended to have at least three but not more than five metadata disks in a cluster.\nThe more disks per node, the lower the CAPEX. As an example, a cluster created from ten nodes with two disks in each will be less expensive than a cluster created from twenty nodes with one disk in each. \nIn general, a cluster with many nodes and few disks per node offers higher performance, while a cluster with the minimum number of nodes (3) and a lot of disks per node is cheaper. Refer to the following table for more details.\n\nCluster composition recommendations\r\n            \n\nDesign considerations\nMinimum nodes (3),\r\nmany disks per node\nMany nodes, few disks per node\r\n(all-flash configuration)\n\nOptimization\nLower cost.\nHigher performance.\n\nFree disk space to\r\nreserve\nMore space to reserve for\r\ncluster rebuilding, as fewer\r\nhealthy nodes will have to\r\nstore the data from a failed node.\nLess space to reserve for cluster\r\nrebuilding, as more healthy nodes\r\nwill have to store the data\r\nfrom a failed node.\n\nRedundancy\nFewer erasure coding choices.\nMore erasure coding choices.\n\nCluster balance and\r\nrebuilding performance\nWorse balance and slower rebuilding.\nBetter balance and faster\r\nrebuilding.\n\nNetwork capacity\nMore network bandwidth required to\r\nmaintain cluster performance during\r\nrebuilding.\nLess network bandwidth required\r\nto maintain cluster performance\r\nduring rebuilding.\n\nFavorable data type\nCold data (for example, backups).\nHot data (for example, virtual\r\nenvironments).\n\nSample server\r\nconfiguration\nSupermicro SSG-6047R-E1R36L (Intel\r\nXeon E5-2620 v1/v2 CPU, 32 GB RAM,\r\n36 x 12 TB HDDs, a 500 GB system\r\ndisk).\nSupermicro SYS-2028TP-HC0R-SIOM\r\n(4 x Intel E5-2620 v4 CPUs,\r\n4 x 16 GB RAM, 4 x 1.9 TB Samsung\r\nPM1643 SSDs).\n\nTake note of the following:\n\nThese considerations only apply if the failure domain is the host.\nVirtuozzo Hybrid Infrastructure supports hundreds of disks per node. If you plan to use more than 36 disks per node, contact our sales engineers who will help you design a more efficient cluster.\n\nSee also\n\nHDD/SSD configuration\n\nProtecting data during a power outage\n\nChecking disk data flushing capabilities\n\nServer requirements\n\nNetwork requirements and recommendations",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/quantity-of-disks-per-node.html"
    },
    {
        "title": "QoS policy rules",
        "content": "QoS policy rules\nYou can create two rule types to define a QoS policy:\n\nBandwidth limit provides bandwidth limitations on networks, ports or floating IPs. Any VM traffic that exceeds the specified rate will be dropped. \nMinimum bandwidth provides minimum bandwidth guarantees on networks, ports or floating IPs. VM traffic will use no less than the specified bandwidth.\n\nRules of different types can be combined in one QoS policy. For example, you can create a bandwidth limit rule and a minimum bandwidth rule. Additionally, you can add rules of one type to a policy if the traffic direction of each rule is different. For example, you can create two bandwidth limit rules, one for egress traffic and one for ingress traffic.\nLimitations\n\nA QoS policy with minimum bandwidth cannot be applied to entire virtual networks.\n\nTo define a bandwidth limit\nSpecify the following parameters:\n\nmax_kbps: The maximum rate, in Kbps, that the VM can send.\n\nmax_burst_kbps: The maximum amount of data, in kbits, that the port can send in a VM if the token buffer is full. The token buffer replenishes at a max_kbps rate.\n\nIf the burst value is set too low, bandwidth usage will be throttled even with a proper bandwidth limit setting, resulting in a lower than expected bandwidth. \nIf the configured burst value is too high, too few packets could be limited, resulting in a higher than expected bandwidth limit.\n\nIf you omit this parameter, the recommended burst value for TCP traffic will be applied, which defaults to 80% of the bandwidth limit. For example, if the bandwidth limit is  1000 kbps, then a burst value of 800 kbps is enough.\n\ningress or egress: The direction of traffic the rule is applied to. For a VM, ingress indicates download, and egress indicates upload.\n\nTo define minimum  bandwidth\nSpecify the following parameters:\n\nmin-kbps: The minimum bandwidth, in Kbps, guaranteed to a VM.\ningress or egress: The direction of traffic the rule is applied to. For a VM, ingress indicates download, and egress indicates upload.\n\nWhat's next\n\nCreating QoS policies",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/qos-policy-rules.html"
    },
    {
        "title": "Re-adding unassigned nodes",
        "content": "Re-adding unassigned nodes\nNodes removed from the infrastructure can be re-added as unassigned. If the infrastructure is empty, you need to add the management node first, and then proceed to join more nodes. \nLimitations\n\nIf you cleaned up the node during its release, you need to reinstall Virtuozzo Hybrid Infrastructure on the node from scratch.\n\nPrerequisites\n\nThe node is completely removed from the infrastructure, as described in Removing unassigned nodes.\n\nTo re-add the management node to the infrastructure\nLog in to the node via SSH and run the following script:# /usr/libexec/vstorage-ui-agent/bin/register-storage-node.sh -m <MN_IP_address> -x <public_iface>\nwhere\n\n<MN_IP_address> is the management node's IP address in the internal management network \n<public_iface> is the name of the public network interface connected to the admin panel network\n\nYou can obtain the both parameters by using the command ip a.\n To re-add a node to the infrastructure\nLog in to the node via SSH and run the following script:# /usr/libexec/vstorage-ui-agent/bin/register-storage-node.sh -m <MN_IP_address> -t <token>\r\n\nwhere\n\n<MN_IP_address> is the IP address of the private network interface on the node with the admin panel\n<token> is the token obtained in the admin panel or from the vinfra node token show output\n\nWhat's next\n\nDeploying the storage cluster",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/re-adding-unassigned-nodes.html"
    },
    {
        "title": "Reconfiguring virtual machines",
        "content": "Reconfiguring virtual machines\nOnce you create a virtual machine, you can manage its CPU and RAM resources, as well as network interfaces and volumes.\nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/reconfiguring-virtual-machines.html"
    },
    {
        "title": "Releasing nodes from backup storage",
        "content": "Releasing nodes from backup storage\nBackup storage is connected to one specific backup destination. If you need to switch the destination, for example, from a public cloud to a local storage cluster or one public cloud bucket to another, you need to delete the backup storage by releasing all of its nodes from the backup storage cluster and create a new one. Keep in mind that by destroying backup storage you also delete all backup data on the storage cluster and cannot restore it later.\nPrerequisites\n\nThe backup storage cluster is created and registered in the Cloud Management Panel, as described in Provisioning backup storage space.\nAll of the backup storage registrations are deleted, as explained in Deleting registrations.\n\nTo release a node from backup storage\n\nAdmin panel\n\nGo to the Storage services > Backup storage > Nodes screen.\nClick a node to release, and then on the node right pane, click Release. \nClick Release in the confirmation window.\n\nThe backup storage will remain operational until there is at least one node in it.\n\nCommand-line interface\nUse the following command:vinfra service backup node release --nodes <nodes>\r\n\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n\nFor example, to release the node node003 from the backup storage, run:# vinfra service backup node release --nodes node003\n\nTo destroy backup storage\n\nAdmin panel\n\nGo to the Storage services > Backup storage > Nodes screen.\nSelect all of the backup nodes or click the only node in the backup storage cluster, and then click Release. \nEnter Delete in the confirmation window, and then click Delete.\n\nCommand-line interface\nUse the following command:vinfra service backup cluster release\n\nSee also\n\nAdding nodes to backup storage\n\nChanging the redundancy scheme for backup storage",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup node release --nodes <nodes>\r\n\n\n--nodes <nodes>\n\nA comma-separated list of node hostnames or IDs\n\nFor example, to release the node node003 from the backup storage, run:# vinfra service backup node release --nodes node003\n",
                "title": "To release a node from backup storage"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup cluster release\n",
                "title": "To destroy backup storage"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > Backup storage > Nodes screen.\nClick a node to release, and then on the node right pane, click Release. \nClick Release in the confirmation window.\n\nThe backup storage will remain operational until there is at least one node in it.\n",
                "title": "To release a node from backup storage"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Storage services > Backup storage > Nodes screen.\nSelect all of the backup nodes or click the only node in the backup storage cluster, and then click Release. \nEnter Delete in the confirmation window, and then click Delete.\n\n",
                "title": "To destroy backup storage"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/releasing-nodes-from-backup-storage.html"
    },
    {
        "title": "Redundancy by replication",
        "content": "Redundancy by replication\nWith replication, Virtuozzo Hybrid Infrastructure breaks the incoming data stream into 256 MB chunks. Each chunk is replicated and replicas are stored on different failure domains, so that each failure domain has only one replica of a given chunk.\nThe following diagram illustrates the 2 replicas redundancy mode with the host failure domain.\n\nReplication in Virtuozzo Hybrid Infrastructure is similar to the RAID rebuild process, but has two key differences:\n\nReplication in Virtuozzo Hybrid Infrastructure is much faster than that of a typical online RAID 1/5/10 rebuild. The reason is that Virtuozzo Hybrid Infrastructure replicates chunks in parallel, to multiple failure domains.\nThe more storage nodes are in a cluster, the faster the cluster will recover from a disk or node failure.\n\nHigh replication performance minimizes the periods of reduced redundancy for the cluster. Replication performance is affected by:\n\nThe number of available storage nodes. As the replication runs in parallel, the more available replication sources and destinations there are, the faster it is.\nPerformance of storage node disks.\nNetwork performance. All replicas are transferred between failure domains over network. For example, 1 Gbps throughput can be a bottleneck (refer to Network requirements and recommendations).\nDistribution of data in the cluster. Some storage nodes may have much more data to replicate than others and may become overloaded during replication.\nI/O activity in the cluster during replication.\n\nSee also\n\nRedundancy by erasure coding\n\nNo redundancy\n\nRedundancy modes\n\nFailure domains\n\nStorage tiers\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/redundancy-by-replication.html"
    },
    {
        "title": "Redundancy by erasure coding",
        "content": "Redundancy by erasure coding\nWith erasure coding, Virtuozzo Hybrid Infrastructure breaks the incoming data stream into fragments of a certain size, then splits each fragment into a certain number (M) of 1-megabyte pieces and creates a certain number (N) of parity pieces for redundancy. All pieces are distributed among M+N failure domains, that is, one piece per failure domain. On failure domains, pieces are stored in regular chunks of 256 MB but such chunks are not replicated as redundancy is already achieved. The cluster can survive failure of any N failure domains without data loss.\nThe values of M and N are indicated in the names of erasure coding redundancy modes. For example, in the 5+2 mode, the incoming data is broken into 5 MB fragments, each fragment is split into five 1 MB pieces and two more 1 MB parity pieces are added for redundancy. In addition, if N is 2, the data is encoded by using the RAID6 scheme, and if N is greater than 2, erasure codes are used.\nThe diagram below illustrates the 5+2 encoding mode with the host failure domain.\n\nSee also\n\nRedundancy by replication\n\nNo redundancy\n\nRedundancy modes\n\nFailure domains\n\nStorage tiers\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/redundancy-by-erasure-coding.html"
    },
    {
        "title": "Releasing node disks",
        "content": "Releasing node disks\nDuring a disk release, its data is safely migrated to other disks, which takes time. To avoid data loss, wait until data migration is complete.\nLimitations\n\nA graceful release of a storage disk is possible only if the remaining disks in the storage cluster can comply with the configured redundancy scheme. You can, however, release a disk forcibly without data migration, but it will make the cluster degraded and trigger the cluster self-healing.\n\nPrerequisites\n\nBefore replacing a disk with the Metadata, Cache, or Metadata+Cache role, ensure that the same role is assigned to a new disk, and then release the old disk.\nIf you want all disks with the Storage role to be assigned automatically after replacement, automatic configuration of new disks must be enabled, as described in Configuring new storage disks automatically.\n\nTo release a disk from the storage cluster\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node.\nOn the Disks tab, click the disk to replace.\nOn the disk right pane, click Release.\n\n(Optional, highly not recommended) If you do not want to wait for data migration to finish, select Release without data migration.\n\nThere is a risk of data loss. Use this option only if your data is redundant.\n\nClick Release.\n\nWhen the data migration from the disk completes, the disk will be displayed without a role on the Disks tab and can be replaced with a new one.\n\nCommand-line interface\nUse the following command:vinfra node disk release [--force] [--node <node>] <disk>\r\n\n\n--force\n\nRelease without data migration\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<disk>\n\nDisk ID or device name\n\nFor example, to release the role cs from the disk sdc on the node node003, run:# vinfra node disk release sdc --node node003\n\nWhat's next\n\nConfiguring new disks manually",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node disk release [--force] [--node <node>] <disk>\r\n\n\n--force\n\nRelease without data migration\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n<disk>\n\nDisk ID or device name\n\nFor example, to release the role cs from the disk sdc on the node node003, run:# vinfra node disk release sdc --node node003\n",
                "title": "To release a disk from the storage cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node.\nOn the Disks tab, click the disk to replace.\nOn the disk right pane, click Release.\n\n(Optional, highly not recommended) If you do not want to wait for data migration to finish, select Release without data migration.\n\nThere is a risk of data loss. Use this option only if your data is redundant.\n\n\nClick Release.\n\nWhen the data migration from the disk completes, the disk will be displayed without a role on the Disks tab and can be replaced with a new one.\n",
                "title": "To release a disk from the storage cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/releasing-node-disks.html"
    },
    {
        "title": "Redundancy modes",
        "content": "Redundancy modes\nVirtuozzo Hybrid Infrastructure supports a number of modes for each redundancy method. Only predefined redundancy modes are available in the admin panel. The following table illustrates data overhead of various redundancy modes. The first three lines are replication and the rest are erasure coding.\nThe numbers of failure domains listed in the table indicate only the requirements of each redundancy method but not the number of failure domains needed for the Virtuozzo Hybrid Infrastructure cluster. The general recommendation is to always have at least one more failure domain in a cluster than required by the chosen redundancy scheme. For example, a cluster using replication with 3 replicas and the host failure domain should have four nodes, and a cluster that works in the 7+2 erasure coding mode with the disk failure domain should have ten disks. Such a cluster configuration has the following advantages:\n\nThe cluster will not be exposed to additional failures when in the degraded state. With one failure domain down, the cluster may not survive another even single-disk failure without data loss.\nYou will be able to perform maintenance on cluster nodes that may be needed to recover a failed node (for example, for installing software updates).\nIn most cases, the cluster will have enough nodes to rebuild itself. In a cluster with the host failure domain but without a spare node, each replica of user data is distributed to each cluster node for redundancy. If one or two nodes go down, the user data will not be lost, but the cluster will become degraded and will only start self-healing after the failed nodes are back online. During its rebuilding process, the cluster may be exposed to additional failures until all of its nodes are healthy again.\nYou can replace and upgrade a cluster node without adding a new node to the cluster. A graceful release of a storage node is only possible if the remaining nodes in the cluster can comply with the configured redundancy scheme. You can, however, release a node forcibly without data migration, but it will make the cluster degraded and trigger the cluster self-healing.\n\nThe minimum and recommended cluster configurations are described in Quantity of servers.\n\nRedundancy mode\nFailure domains required to store data copies\nHow many failure domains can\r\nfail without data loss\nStorage\r\noverhead, percent\nRaw space needed to\r\nstore 100 GB of data\n\n1 replica\r\n(no redundancy)\n1\n0\n0\n100 GB\n\n2 replicas\n2\n1\n100\n200 GB\n\n3 replicas\n3\n2\n200\n300 GB\n\nEncoding 1+0\r\n(no redundancy)\n1\n0\n0\n100 GB\n\nEncoding 1+1\n2\n1\n100\n200 GB\n\nEncoding 1+2\n3\n2\n200\n300 GB\n\nEncoding 3+1\n4\n1\n33\n133 GB\n\nEncoding 3+2\n5\n2\n67\n167 GB\n\nEncoding 5+2\n7\n2\n40\n140 GB\n\nEncoding 7+2\n9\n2\n29\n129 GB\n\nEncoding 17+3\n20\n3\n18\n118 GB\n\nThe 1+0, 1+1, 1+2, and 3+1 encoding modes are meant for small clusters that have insufficient nodes for other erasure coding modes but will grow in the future. As a redundancy type cannot be changed once chosen (from replication to erasure coding or vice versa), this mode allows you to choose erasure coding even if your cluster is smaller than recommended. Once the cluster has grown, more beneficial redundancy modes can be chosen.\n\nYou select a data redundancy mode when configuring storage services and creating storage volumes for virtual machines. No matter what redundancy mode you select, it is highly recommended to be protected against a simultaneous failure of two nodes, as that happens often in real-life scenarios.\nBy default, all encoding modes, except 1+0 and M+1, allow write operations when one failure domain (for example, a storage node or disk) is inaccessible. The cluster starts working in the read-only mode with disabled write operations in the following cases:\n\nWhen redundancy is 1, that is with the M+1 encoding mode, and one failure domain is inaccessible.\nWhen redundancy is 2, that is with the M+2 encoding mode, and two failure domains are inaccessible.\n\nIf the number of unavailable failure domains is higher than the redundancy factor, then data becomes unavailable even for reading and there is a high risk of data loss. Therefore, it is strongly recommended to use the encoding modes M+2 or M+3.\nSee also\n\nRedundancy by replication\n\nRedundancy by erasure coding\n\nNo redundancy\n\nFailure domains\n\nStorage tiers\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/redundancy-modes.html"
    },
    {
        "title": "Recovering nodes",
        "content": "Recovering nodes\nInformation about storage services is stored on a system disk and may be lost in case of a system disk failure. If this happens, you can recover the system disk and the node configuration by reinstalling the product from an ISO image in the recovery mode. The recovery mode provides basic troubleshooting steps when a node fails to boot.\nDuring the recovery process, the configuration of deployed services and infrastructure is automatically detected and recovered from storage disks.\nLimitations\n\nSingle-node clusters cannot be recovered.\nManagement nodes with disabled high availability cannot be recovered.\nThe configuration of the compute, iSCSI, S3, and NFS services cannot be automatically recovered. For the manual recovery, contact the technical support.\nRecovery is only possible if the node hardware configuration has not been changed.\nThe primary management node can only be recovered during an upgrade.\nRemote iSCSI devices that are attached to cluster nodes as storage disks cannot be recovered. Data stored on such devices will be lost.\n\nTo recover a node\n\nBefore recovering a node, place the node into the maintenance mode to evacuate services and virtual machines from the node. To do this, click Enter maintenance on the node right pane.\nPrepare the bootable media by using the distribution ISO image, as described in Preparing the bootable media.\nAttach the bootable media to the node, and then reboot the node.\nConfigure the node to boot from the chosen media.\n\nReinstall Virtuozzo Hybrid Infrastructure on the node:\n\nOn the welcome screen, click Troubleshooting\u00e2\u0080\u0093>, and then Recover - Node recovery.\nOn step 1, accept the End-User License Agreement by selecting I accept the End-User License Agreement, and then click Next.\nOn step 2, specify the current network configuration for this node, and then click Next.\nOn step 3, choose the correct time zone, and then click Next.\nOn step 4, select No, add it to an existing cluster, and specify the private IP address of the management node and the token, obtained on the Infrastructure > Nodes > Connect node screen. Then, click Next.\nOn step 5, choose the system disk for reinstalling the operating system, and then click Next.\nOn step 6, enter and confirm the password for the root account, and then click Start installation.\n\nOnce the installation is complete and the node is rebooted, the recovery script will be executed automatically. Wait until the node recovery is finished.\n\nIf a node has the iSCSI, S3, or NFS services deployed:\n\nRelease the node from these services.\nReturn the node to operation by exiting the maintenance mode.\nRejoin the node to the relevant services.\n\nIf a node has the compute services deployed:\n\nThe recovery process fails when a network with the VM public traffic type does not include the Internal management traffic type. This happens because this network is not reassigned to a public interface after the product reinstallation. In this case, do the following:\n\nGo to the node's Network interfaces tab in the admin panel.\nAssign the network with the VM public traffic type to your public interface.\nOn the node pane, click Retry recovery.\n\nReturn the node to operation by exiting the maintenance mode.\nGo to Settings > System settings > Management node high availability and destroy the HA configuration.\nOn the Compute > Nodes screen, release the node from the compute cluster, and then add it again.\nRe-create the HA configuration.\n\nIf a node is not included in the compute cluster but is a part of the HA configuration, go to Settings > System settings > Management node high availability and do one of the following:\n\nIf the cluster has spare nodes for replacement, replace the node in HA configuration.\nIf the cluster has no spare nodes for replacement, destroy the HA configuration, and then re-create it.\n\nIf the node recovery fails\n\nOn the node right pane, click Retry recovery to repeat the attempt to recover the node.\nOn the node right pane, click Cancel recovery to wipe out and re-import disks on the node.\n\nSee also\n\nPerforming node maintenance\n\nInstalling updates\n\nTroubleshooting installation",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/recovering-nodes.html"
    },
    {
        "title": "Removing outbound firewall rules",
        "content": "Removing outbound firewall rules\n\nWhen restricting outbound traffic, it is recommend to modify the default outbound rules to use specific IP addresses or subnets, according to your network infrastructure and security policies.\n\nTo remove outbound firewall rules\nUse the following command:vinfra cluster network set --del-outbound-allow-list <rules> <network>\n\n--add-outbound-allow-list <rules>\n\nA comma-separated list of allow rules in the format: <address>:<protocol>:<port>:<description>, where:\n\n<address> is a single IP address (10.10.10.10), address range (10.10.10.0-10.10.10.10), or subnet CIDR (10.10.10.0/32)\n<protocol> can be udp, tcp, or any\n<port> is an integer value (22) or a range (20-22)\n<description> usually contains the name of the service that uses the specified port\n\n<network>\n\nNetwork ID or name\n\nFor example, to remove the rule 0.0.0.0:any:0:Allow all, which allows all outbound traffic, run:# vinfra cluster network set Public --del-outbound-allow-list \"0.0.0.0:any:0:Allow all\"\nIn this case, all attempts to establish connections from the cluster to external endpoints will be blocked.\nWhat's next\n\nListing outbound firewall rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/removing-outbound-firewall-rules.html"
    },
    {
        "title": "Replacing node disks",
        "content": "Replacing node disks\nTo replace a disk with a new disk, you need to release the old one from the storage cluster. If the new disk contains data that was not placed there by Virtuozzo Hybrid Infrastructure, the disk will not be considered suitable for use in the storage cluster.\nAfter the replacement, you will need to assign the role of the released disk to the new one:\n\nThe Storage role will be assigned automatically if you enabled automatic configuration of new disks before a disk failure.\nAll other roles need to be assigned by hand.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/replacing-node-disks.html"
    },
    {
        "title": "Renewing encryption certificates",
        "content": "Renewing encryption certificates\nEncryption certificates, except for CA certificates, are automatically rotated on all cluster nodes upon their expiration, that is, once per year. If a certificate is compromised, you need to replace it manually.\nTo manually renew IPsec certificates\nUse the following command:vinfra node certificate ipsec renew <node>\n\n<node>\n\nNode ID or hostname\n\nFor example, to renew certificates for the node node1, run:vinfra node certificate ipsec renew node1\nSee also\n\nEnabling and disabling data-in-transit encryption\n\nManaging encryption exceptions",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/renewing-encryption-certificates.html"
    },
    {
        "title": "Removing unassigned nodes",
        "content": "Removing unassigned nodes\nTo be completely removed from the infrastructure, the node must not be a part of the storage cluster.\nPrerequisites\n\nThe node is removed from the storage cluster, as described in Releasing nodes from the storage cluster.\n\nTo remove a node from the infrastructure\n\nAdmin panel\n\nSelect the unassigned node on the Infrastructure > Nodes screen, and then click Remove.\n\nFor security purposes, clean up node certificates and identity by deleting the following from the node:# rm -rf /usr/libexec/vstorage-ui-backend/ca\r\n# rm -rf /etc/nginx/ssl\r\n# rm -f /etc/vstorage/host_id\r\n# rm -f /etc/vstorage/vstorage-ui-agent.conf\n\nAfter such a cleanup, the only way to add the node back to the cluster is by re-installing Virtuozzo Hybrid Infrastructure on it from scratch.\n\nCommand-line interface\nUse the following command:vinfra node forget <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to unregister the node node005 from Virtuozzo Hybrid Infrastructure, run:# vinfra node forget node005\n\nSee also\n\nRe-adding unassigned nodes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node forget <node>\r\n\n\n<node>\n\nNode ID or hostname\n\nFor example, to unregister the node node005 from Virtuozzo Hybrid Infrastructure, run:# vinfra node forget node005\n",
                "title": "To remove a node from the infrastructure"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nSelect the unassigned node on the Infrastructure > Nodes screen, and then click Remove.\n\nFor security purposes, clean up node certificates and identity by deleting the following from the node:# rm -rf /usr/libexec/vstorage-ui-backend/ca\r\n# rm -rf /etc/nginx/ssl\r\n# rm -f /etc/vstorage/host_id\r\n# rm -f /etc/vstorage/vstorage-ui-agent.conf\n\nAfter such a cleanup, the only way to add the node back to the cluster is by re-installing Virtuozzo Hybrid Infrastructure on it from scratch.\n\n\n\n",
                "title": "To remove a node from the infrastructure"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/removing-unassigned-nodes.html"
    },
    {
        "title": "Replicating S3 data between datacenters",
        "content": "Replicating S3 data between datacenters\nFor data replication between datacenters, S3 users can use either S3 geo-replication or cross-region replication (CRR):\n\nGeo-replication is designed to improve the distribution of data across geographically distributed data networks. You can enable S3 geo-replication in the admin panel.\nCRR is used to copy objects asynchronously across S3 buckets stored in different clusters and public cloud providers. You can enable CRR by using the Amazon S3-compatible API. For details, refer to the Object Storage Orchestration API Reference.\n\nLimitations\n\nS3 geo-replication and cross-region replication (CRR) are incompatible. With geo-replication enabled, CRR is not available.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/replicating-s3-data-between-datacenters.html"
    },
    {
        "title": "Resizing volumes",
        "content": "Resizing volumes\nYou can change volume size only by increasing it. Volumes can be extended for both running (online resizing) and stopped (offline resizing) virtual machines. Online volume resizing allows users to avoid downtime and enables scaling VM\u00a0storage capacity on the fly without service interruption.\nLimitations\n\nYou cannot shrink volumes.\nDuring volume resizing, the file system inside the guest OS is not extended.\nIf you revert a volume to a snapshot that was taken before the volume extension, the new volume size will be retained.\n\nPrerequisites\n\nA volume is created, as described in Creating and deleting volumes.\n\nTo extend a volume\n\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click a volume.\n Click the pencil icon in the Size field. \nEnter the desired volume capacity, and then click the tick icon.\n\n After the volume is extended, you will need to re-partition the disk inside the guest OS to allocate the added disk space.\n\nCommand-line interface\nUse the following command:vinfra service compute volume extend --size <size_gb> <volume>\r\n\n\n<volume>\n\nVolume ID or name\n\nFor example, to extend the volume myvolume to 16 GB, run:# vinfra service compute volume extend myvolume --size 16\n\nSee also\n\nAttaching and detaching volumes\n\nChanging the storage policy for volumes\n\nCloning volumes\n\nManaging volume snapshots",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute volume extend --size <size_gb> <volume>\r\n\n\n<volume>\n\nVolume ID or name\n\nFor example, to extend the volume myvolume to 16 GB, run:# vinfra service compute volume extend myvolume --size 16\n",
                "title": "To extend a volume"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Storage > Volumes tab, click a volume.\n Click the pencil icon in the Size field. \nEnter the desired volume capacity, and then click the tick icon.\n\n After the volume is extended, you will need to re-partition the disk inside the guest OS to allocate the added disk space.\n",
                "title": "To extend a volume"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/resizing-volumes.html"
    },
    {
        "title": "Restoring default outbound firewall rules",
        "content": "Restoring default outbound firewall rules\nLimitations\n\nWhen resetting to defaults, your custom outbound firewall rules will be discarded.\n\nTo restore the default outbound firewall rules\nUse the following command:vinfra cluster network set --restore-default-outbound-allow-list <network>\n\n<network>\n\nNetwork ID or name\n\nFor example, to restore the default outbound allow rules for the Public network, run:vinfra cluster network set Public --restore-default-outbound-allow-list\r\n\nSee also\n\nDefault outbound firewall rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/restoring-default-outbound-firewall-rules.html"
    },
    {
        "title": "Restricting access to target groups",
        "content": "Restricting access to target groups\nYou can restrict access to entire target groups (and all volumes attached to them) by using ACL-based authorization as well as password-based authentication (CHAP).\nPrerequisites\n\nA target group is created, as described in Creating target groups.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/restricting-access-to-target-groups.html"
    },
    {
        "title": "Rescuing virtual machines",
        "content": "Rescuing virtual machines\nIf a VM experiences boot problems, you can send it to the rescue mode to access its boot volume. When a VM in the \u00e2\u0080\u009cActive\u00e2\u0080\u009d state is sent to the rescue mode, it is shut down softly first. Once the VM is in the rescue mode, you can connect to it via SSH or via the console. Its previous boot disk is now attached as a secondary one. You can mount the disk and repair it.\nLimitations\n\nThe rescue mode can use ISO images for booting both Linux and Wndows virtual machines and QCOW2 images (templates) for booting Linux VMs. For instructions on making templates, refer to Preparing templates.\nYou can send a VM to the rescue mode only if its current status is \u00e2\u0080\u009cActive\u00e2\u0080\u009d or \u00e2\u0080\u009cShut down\u00e2\u0080\u009d.\nThere are only three actions available for the VM in the rescue mode: Console, Exit rescue mode, and Delete.\nIf a rescue image has cloud-init installed, then the VM booted from it can be accessed with the same SSH key that was used for its creation.\n\nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.\n\nTo put a virtual machine to the rescue mode\n\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click the required VM on the list.\nOn the VM right pane, click the ellipsis button on the toolbar. Then, click Enter rescue mode.\n\nIn the Enter rescue mode window, select an image to rescue the VM with. By default, the initial image used for creating the VM is selected. Click Enter.\n\nThe machine status changes to \u00e2\u0080\u009cRescue\u00e2\u0080\u009d. \n\nCommand-line interface\nUse the following command:vinfra service compute server rescue [--image <image>] <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n--image <image>\n\nBoot from image ID or name\n\nFor example, to send the myvm virtual machine to the rescue mode with the cirros image, run:# vinfra service compute server rescue myvm --image cirros\n\nTo return a virtual machine to normal operation\n\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click the required VM on the list.\nOn the VM right pane, click Exit rescue mode.\nIn the Exit rescue mode window, click Exit. The VM will be automatically rebooted.\n\nThe VM status changes to \u00e2\u0080\u009cActive\u00e2\u0080\u009d and it boots from the original root disk.\n\nIf the VM status changes to \u00e2\u0080\u009cError\u00e2\u0080\u009d when exiting the rescue mode, you can reset its status with the Reset state action. The VM should then return to the \u00e2\u0080\u009cRescue\u00e2\u0080\u009d status again.\n\nCommand-line interface\nUse the following command:vinfra service compute server unrescue <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to stop the rescue mode for the myvm virtual machine, run:# vinfra service compute server unrescue myvm\n\nTo exit the rescue mode for a Windows VM\nThere might be an issue of exiting the rescue mode for a Windows VM. If in the rescue mode you set the original system disk online, its ID becomes the same as that of the rescue disk. Then, when you try to exit the rescue mode, the boot loader cannot find the proper boot disk. To resolve the ID conflict, follow the steps:\n\nWith the VM in the rescue mode, open the Disk Management window and note the numbers of the original system disk (offline) and the rescue disk (online). Set the original system disk to Online.\n\nTo edit the boot configuration, enter the following command in the Command Prompt window:> bcdedit /store <the original system disk name>:\\boot\\bcd\r\n\n\nReview the output and check that the rescue disk is the target for objects in the output (partition=<the rescue disk name>).\nIf the objects do not point to drive C, fix it with the following commands:> bcdedit /store <the original system disk name>:\\boot\\bcd \\\r\n/set {default} osdevice partition=<the rescue disk name>:\r\n> bcdedit /store <the original system disk name>:\\boot\\bcd \\\r\n/set {default} device partition=<the rescue disk name>:\r\n> bcdedit /store <the original system disk name>:\\boot\\bcd \\\r\n/set {bootmgr} device partition=<the rescue disk name>:\r\n> bcdedit /store <the original system disk name>:\\boot\\bcd \\\r\n/set {memdiag} device partition=<the rescue disk name>:\r\n\n\nTo view the available disks, enter the following commands in the command line:> DISKPART\r\n> LIST DISK\r\n\nMatch the disk number and name to those displayed in the Disk Management window.\n\nTo get the ID of the rescue disk, run the following commands:> SELECT DISK <the rescue disk number>\r\n> UNIQUEID DISK\r\n\nRecord the disk ID, you will need it later.\n\nChange this ID by using the following command:> UNIQUEID DISK id=<any hex value of 8 characters>\r\n\nMake sure that the value has changed with the UNIQUEID DISK command.\n\nAssign the ID that you recorded previusly to the original system disk:> SELECT DISK <the original system disk number>\r\n> UNIQUEID DISK id=<the recorded disk ID>\r\n\nMake sure that the value has changed with the UNIQUEID DISK command.\n\nYou should now be able to exit the rescue mode.\n\nSee also\n\nConnecting to virtual machines\n\nManaging virtual machine power state\n\nAttaching ISO images to virtual machines\n\nTroubleshooting virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server rescue [--image <image>] <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n--image <image>\n\nBoot from image ID or name\n\nFor example, to send the myvm virtual machine to the rescue mode with the cirros image, run:# vinfra service compute server rescue myvm --image cirros\n",
                "title": "To put a virtual machine to the rescue mode"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server unrescue <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to stop the rescue mode for the myvm virtual machine, run:# vinfra service compute server unrescue myvm\n",
                "title": "To return a virtual machine to normal operation"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click the required VM on the list.\nOn the VM right pane, click the ellipsis button on the toolbar. Then, click Enter rescue mode.\n\nIn the Enter rescue mode window, select an image to rescue the VM with. By default, the initial image used for creating the VM is selected. Click Enter.\n\n\n\n\n\n\nThe machine status changes to \u00e2\u0080\u009cRescue\u00e2\u0080\u009d. \n",
                "title": "To put a virtual machine to the rescue mode"
            },
            {
                "example": "\nAdmin panel\n\nOn the Compute > Virtual machines > Virtual machines tab, click the required VM on the list.\nOn the VM right pane, click Exit rescue mode.\nIn the Exit rescue mode window, click Exit. The VM will be automatically rebooted.\n\nThe VM status changes to \u00e2\u0080\u009cActive\u00e2\u0080\u009d and it boots from the original root disk.\n\nIf the VM status changes to \u00e2\u0080\u009cError\u00e2\u0080\u009d when exiting the rescue mode, you can reset its status with the Reset state action. The VM should then return to the \u00e2\u0080\u009cRescue\u00e2\u0080\u009d status again.\n\n",
                "title": "To return a virtual machine to normal operation"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/rescuing-virtual-machines.html"
    },
    {
        "title": "Securing OpenStack API traffic with SSL",
        "content": "Securing OpenStack API traffic with SSL\nTraffic to and from the public endpoint that listens to OpenStack API requests can be secured with an SSL certificate. However, as domain names are not used by default, the certificate will need a subjectAltName field containing the aforementioned management node IP address. If it does not have such a field, you will need to modify the public endpoint to use a domain name that you have a certificate for.\nLimitations\n\nOnly one SSL certificate can be added and applied for both the admin panel and OpenStack API.\n\nTo secure public OpenStack API traffic with SSL\n\nIn the admin panel, upload the SSL certificate and private key on the Settings > System settings > SSL certificate screen.\n\nOn the client side, place the CA certificate file to the operating system\u00e2\u0080\u0099s trusted bundle:# cp ca.pem /etc/pki/ca-trust/source/anchors/\r\n# update-ca-trust extract\r\n\nAlternatively, you can append the --os-cacert ca.pem option to each OpenStack client call.\n\nIf your certificate does not have the subjectAltName field, modify all public endpoints to use the domain name for which you have the certificate for, as described in Setting a DNS name for the compute API. This domain name must resolve to the management node IP address (or to its virtual IP address if high availability is enabled).\n\nIn your OpenRC script, change OS_AUTH_URL to the same domain name and remove all parameters related to insecure access. For example:export OS_PROJECT_DOMAIN_NAME=Default\r\nexport OS_USER_DOMAIN_NAME=Default\r\nexport OS_PROJECT_NAME=admin\r\nexport OS_USERNAME=admin\r\nexport OS_PASSWORD=<ADMIN_PASSWORD>\r\nexport OS_AUTH_URL=https://<DOMAIN_NAME>:5000/v3\r\nexport OS_IDENTITY_API_VERSION=3\r\n\n\nNow you can run OpenStack commands without the --insecure option.\nWhat's next\n\nConfiguring multitenancy",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/securing-openstack-api-traffic-with-ssl.html"
    },
    {
        "title": "Scaling the storage cluster",
        "content": "Scaling the storage cluster\nAfter deploying the storage cluster, you can expand its storage capacity at any time by adding more storage disks (vertical scaling) or increasing the number of storage nodes (horizontal scaling). You can also replace storage disks with disks of larger size by following the instructions in Replacing node disks.\nTo better understand the difference between vertical and horizontal scaling, let\u00e2\u0080\u0099s have a look at the following scenarios:\n\nVertical scaling. The cluster has five nodes with 12 hard drive slots each. One disk is used for system and metadata, and 9 disks are used for storage on tier 0. Backup storage is deployed on top of the storage cluster with the 3+2 encoding mode. You can expand the storage capacity of the backup storage by adding two more disks to each node. As a result, the storage capacity will increase by 2/9.\nHorizontal scaling. The cluster has five nodes with 12 hard drive slots each. One disk is used for system and metadata, and 11 disks are used for storage on tier 0. Backup storage is deployed on top of the storage cluster with the 3+2 encoding mode. You can expand the storage capacity and throughput of the backup storage by adding two more nodes of the same size (that is, with 12 disks). As a result, the storage capacity will increase by 2/5. Additionally, to maximize the storage efficiency, you can update the encoding mode to 5+2, as described in Changing the redundancy scheme for backup storage.\n\nBefore you add new disks and nodes, consider the following recommendations for their sizing:\n\nIt is recommended for a storage tier to have an equal number of disks per node. Then, the data will be spread more evenly among nodes. For more information, refer to Logical space chart.\nHaving the same-size disks helps distribute the loads more evenly. Inside a cluster, the disk usage is proportional to the disk size. For example, if you have a disk of 10 \u00d0\u00a2B and a disk of 2 TB, a 50% cluster load will use 5 \u00d0\u00a2B and 1 TB, respectively.\n\nLimitations\n\nYou can assign a role to a disk only if its size is greater than 1 GiB.\nYou can assign an additional role to a system disk only if its size is at least 100 GiB.\nIt is recommended to assign the System and Metadata roles to either an SSD disk or different HDDs. Assigning both of these roles to the same HDD disk will result in mediocre performance suitable only for cold data (for example, archiving).\nThe System role cannot be combined with the Cache and Metadata+Cache roles. The reason is that the I/O generated by the operating system and applications would contend with the I/O generated by journaling, thus negating its performance benefits.\nYou can use shingled magnetic recording (SMR) HDDs only with the Storage role and only if the node has an SSD disk with the Cache role.\nYou cannot use SMR and standard disks in the same tier.\nYou cannot assign roles to system and non-system disks at a time.\n\nPrerequisites\n\nThe storage cluster is created, as described in Deploying the storage cluster.\n\nTo add disks to the storage cluster\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node.\nOn the Disks tab, click the new disk without a role.\nOn the disk right pane, click Assign role.\n\nIn the Assign role window, select a disk role, that is how you want to use the disk:\n\n[Only for SSD drives] To store write cache\n\nSelect the Cache role.\nSelect a storage tier that you want to cache.\n\nFor storage disks to use cache, the Cache role must be assigned before the Storage role. You can also assign both of these roles to disks at the same time, and the system will configure the cache disk first.\n\nTo store data\n\nSelect the Storage role.\nSelect a storage tier where to store your data. To make better use of data redundancy, do not assign all of the disks on a node to the same tier. Instead, make sure that each tier is evenly distributed across the cluster.\n\nEnable data caching and checksumming:\n\nEnable SSD caching and checksumming. Available and recommended only for nodes with SSDs.\nEnable checksumming (default). Recommended for nodes with HDDs as it provides better reliability.\nDisable checksumming. Not recommended for production. For an evaluation or testing environment, you can disable checksumming for nodes with HDDs, to provide better performance.\n\nTo store cluster metadata\n\nSelect the Metadata role.\n\nIt is recommended to have only one disk with the Metadata role per node and maximum five such disks in a cluster.\n\n[Only for SSD drives] To store both metadata and write cache\n\nSelect the Metadata+Cache role.\nSelect a storage tier that you want to cache.\n\nClick Assign.\n\nCommand-line interface\nUse the following command:vinfra node disk assign --disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]\r\n                        [--node <node>]\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n\nFor example, to assign the role cs to the disk sdc on the node node003, run:# vinfra node disk assign --disk sdc:cs --node node003\nYou can view the node's disk configuration in the vinfra node disk list output:# vinfra node disk list --node node003\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| id                                   | device | type | role       | disk_status | used    | size     | physical_size | service_id | service_status |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| 2A006CA5-732F-4E17-8FB0-B82CE0F28DB2 | sdc    | hdd  | cs         | ok          | 11.2GiB | 125.8GiB | 128.0GiB      | 1026       | active         |\r\n| 642A7162-B66C-4550-9FB2-F06866FB7EA1 | sdb    | hdd  | cs         | ok          | 8.7GiB  | 125.8GiB | 128.0GiB      | 1025       | active         |\r\n| 45D38CD2-3B94-4F0F-8864-9D51F716D3B1 | sda    | hdd  | mds-system | ok          | 21.0GiB | 125.9GiB | 128.0GiB      | 1          | avail          |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n\n\nTo add nodes to the storage cluster\n\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click an unassigned node.\nOn the node right pane, click Join to cluster.\n\nIn the Join node to storage cluster window, check the default disk configuration. If it is correct, proceed to join the node to the storage cluster.\nAlso, you can assign roles to your disks manually or use Disk actions to work with the disks. Alternatively, you can copy the disk configuration from another node by clicking Copy configuration from and selecting the desired node.\n\nOnce you finish configuring the disks, click Join, to add the node to the storage cluster.\n\nCommand-line interface\nUse the following command:vinfra node join [--disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]] <node>\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n<node>\n\nNode ID or hostname\n\nFor example, to add the node node002 to the storage cluster and assign roles to disks: mds-system to sda, cs to sdb and sdc, run:# vinfra node join f59dabdb-bd1c-4944-8af2-26b8fe9ff8d4 --disk sda:mds-system \\\r\n--disk sdb:cs --disk sdc:cs\nThe added node will appear in the vinfra node list output:# vinfra node list\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| id           | host         | is_primary | is_online | is_assigned | is_in_ha |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| 09bb6b8<...> | node001<...> | True       | True      | True        | False    |\r\n| 187edb1<...> | node002<...> | False      | True      | True        | False    |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n\n\nSee also\n\nMonitoring the storage cluster\n\nManaging infrastructure nodes",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node disk assign --disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]\r\n                        [--node <node>]\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n--node <node>\n\nNode ID or hostname (default: node001.vstoragedomain)\n\nFor example, to assign the role cs to the disk sdc on the node node003, run:# vinfra node disk assign --disk sdc:cs --node node003\nYou can view the node's disk configuration in the vinfra node disk list output:# vinfra node disk list --node node003\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| id                                   | device | type | role       | disk_status | used    | size     | physical_size | service_id | service_status |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n| 2A006CA5-732F-4E17-8FB0-B82CE0F28DB2 | sdc    | hdd  | cs         | ok          | 11.2GiB | 125.8GiB | 128.0GiB      | 1026       | active         |\r\n| 642A7162-B66C-4550-9FB2-F06866FB7EA1 | sdb    | hdd  | cs         | ok          | 8.7GiB  | 125.8GiB | 128.0GiB      | 1025       | active         |\r\n| 45D38CD2-3B94-4F0F-8864-9D51F716D3B1 | sda    | hdd  | mds-system | ok          | 21.0GiB | 125.9GiB | 128.0GiB      | 1          | avail          |\r\n+--------------------------------------+--------+------+------------+-------------+---------+----------+---------------+------------+----------------+\r\n\n",
                "title": "To add disks to the storage cluster"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra node join [--disk <disk>:<role>[:<key=value,\u00e2\u0080\u00a6>]] <node>\r\n\n\n--disk <disk>:<role> [:<key=value,\u00e2\u0080\u00a6>]\n\n\nDisk configuration in the format:\n\n<disk>: disk device ID or name\n<role>: disk role (cs, mds, journal, mds-journal, mds-system, cs-system, system)\ncomma-separated key=value pairs with keys (optional):tier: disk tier (0, 1, 2 or 3)journal-tier: journal (cache) disk tier (0, 1, 2 or 3)journal-type: journal (cache) disk type (no_cache, inner_cache or external_cache)journal-disk: journal (cache) disk ID or device namebind-address: bind IP address for the metadata service\n\nExample: sda:cs:tier=0,journal-type=inner_cache. This option can be used multiple times.\n\n<node>\n\nNode ID or hostname\n\nFor example, to add the node node002 to the storage cluster and assign roles to disks: mds-system to sda, cs to sdb and sdc, run:# vinfra node join f59dabdb-bd1c-4944-8af2-26b8fe9ff8d4 --disk sda:mds-system \\\r\n--disk sdb:cs --disk sdc:cs\nThe added node will appear in the vinfra node list output:# vinfra node list\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| id           | host         | is_primary | is_online | is_assigned | is_in_ha |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n| 09bb6b8<...> | node001<...> | True       | True      | True        | False    |\r\n| 187edb1<...> | node002<...> | False      | True      | True        | False    |\r\n+--------------+--------------+------------+-----------+-------------+----------+\r\n\n",
                "title": "To add nodes to the storage cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click the name of the node.\nOn the Disks tab, click the new disk without a role.\nOn the disk right pane, click Assign role.\n\nIn the Assign role window, select a disk role, that is how you want to use the disk:\n\n\n[Only for SSD drives] To store write cache\n\n\nSelect the Cache role.\nSelect a storage tier that you want to cache.\n\n\nFor storage disks to use cache, the Cache role must be assigned before the Storage role. You can also assign both of these roles to disks at the same time, and the system will configure the cache disk first.\n\n\n\n\n\nTo store data\n\n\nSelect the Storage role.\nSelect a storage tier where to store your data. To make better use of data redundancy, do not assign all of the disks on a node to the same tier. Instead, make sure that each tier is evenly distributed across the cluster.\n\nEnable data caching and checksumming:\n\nEnable SSD caching and checksumming. Available and recommended only for nodes with SSDs.\nEnable checksumming (default). Recommended for nodes with HDDs as it provides better reliability.\nDisable checksumming. Not recommended for production. For an evaluation or testing environment, you can disable checksumming for nodes with HDDs, to provide better performance.\n\n\n\n\n\n\n\nTo store cluster metadata\n\nSelect the Metadata role.\n\nIt is recommended to have only one disk with the Metadata role per node and maximum five such disks in a cluster.\n\n\n\n\n\n[Only for SSD drives] To store both metadata and write cache\n\n\nSelect the Metadata+Cache role.\nSelect a storage tier that you want to cache.\n\n\n\n\n\n\n\n\nClick Assign.\n\n",
                "title": "To add disks to the storage cluster"
            },
            {
                "example": "\nAdmin panel\n\nOn the Infrastructure > Nodes screen, click an unassigned node.\nOn the node right pane, click Join to cluster.\n\nIn the Join node to storage cluster window, check the default disk configuration. If it is correct, proceed to join the node to the storage cluster.\nAlso, you can assign roles to your disks manually or use Disk actions to work with the disks. Alternatively, you can copy the disk configuration from another node by clicking Copy configuration from and selecting the desired node.\n\nOnce you finish configuring the disks, click Join, to add the node to the storage cluster.\n\n",
                "title": "To add nodes to the storage cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/scaling-the-storage-cluster.html"
    },
    {
        "title": "Services chart",
        "content": "Services chart\n\nAdmin panel\nOn the Services chart, you can monitor two types of services:\n\nMetadata services (MDS). The number of all disks with the metadata role. Ensure that at least three MDSes are running at all times.\nChunk services (CS). The number of all disks with the storage role.\n\nTypical statistics may look like this:\n\nIf some of the chunk services were unresponsive or in maintenance for some time, these time periods will be highlighted in orange on the chart. For failed metadata and chunk services, the color will be red.\n\nCommand-line interface\nUse the following command:vinfra cluster overview\nFor example, to view the information about the storage services in the cluster cluster1, take a look at these lines from the command output:\r\n+-------------------+-------------------------+\r\n| Field             | Value                   |\r\n+-------------------+-------------------------+\r\n| ...               | ...                     |\r\n| cs                | failed: 0               |\r\n|                   | slow: 0                 |\r\n|                   | total: 5                |\r\n| ...               | ...                     |\r\n| mds               | failed: 0               |\r\n|                   | total: 3                |\r\n+-------------------+-------------------------+\r\n\n\nSee also\n\nI/O activity charts\n\nChunks chart\n\nPhysical space chart\n\nLogical space chart",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/services-chart.html"
    },
    {
        "title": "Setting memory parameters per node",
        "content": "Setting memory parameters per node\nTo change per-node memory parameters\nUse the following command:vinfra memory-policy vstorage-services per-node change [--guarantee <guarantee>] [--swap <swap>]\r\n                                                       [--cache-ratio <cache-ratio> --cache-minimum <cache-minimum>\r\n                                                       --cache-maximum <cache-maximum>] --node <node>\n\n--guarantee <guarantee>\n\nGuarantee, in bytes\n--swap <swap>\n\nSwap size, in bytes, or -1 if unlimited\n--cache-ratio <cache-ratio>\n\nCache ratio from 0 to 1 inclusive\n--cache-minimum <cache-minimum>\n\nMinimum cache, in bytes\n--cache-maximum <cache-maximum>\n\nMaximum cache, in bytes\n--node <node>\n\nNode ID or hostname\n\nFor example, to set the storage memory parameters for the node node001, run:# vinfra memory-policy vstorage-services per-node change --guarantee 8796093022208 --swap 1099511627776 --cache-ratio 0.5 \\\r\n--cache-minimum 1099511627776 --cache-maximum 3298534883328 --node node001\nThis command sets the memory parameters as follows:\n\nThe memory guarantee to 8 GB\nThe swap size to 1 GB\nThe page cache limits: the minimum to 1 GB, the maximum to 3 GB, and the cache ratio to 0.5\n\nYou can view the updated per-node memory parameters in the vinfra memory-policy vstorage-services per-node show output:# vinfra memory-policy vstorage-services per-node show --node node001\r\n+-----------+-------------------------+\r\n| Field     | Value                   |\r\n+-----------+-------------------------+\r\n| cache     | maximum: 13194139533312 |\r\n|           | minimum: 8796093022208  |\r\n|           | ratio: 0.7              |\r\n| guarantee | 8796093022208           |\r\n| swap      | 1099511627776           |\r\n+-----------+-------------------------+\r\n\nTo reset per-node parameters to default\nUse the following command:vinfra memory-policy vstorage-services per-node reset [--guarantee] [--swap] [--cache] --node <node>\n\n--guarantee\n\nReset only the guarantee.\n--swap\n\nReset only the swap size.\n--cache\n\nReset only cache values.\n--node <node>\n\nNode ID or hostname\n\nFor example, to reset the manually configured page cache limits to default for the node node001, run:# vinfra memory-policy vstorage-services per-node reset --cache --node node001\nSee also\n\nSetting memory parameters per cluster",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-memory-parameters-per-node.html"
    },
    {
        "title": "Setting the default QoS policy",
        "content": "Setting the default QoS policy\nYou can set the default QoS policy to a project. It will be automatically assigned to all new networks that you create within the project. Existing networks within the project will not inherit the default QoS policy, you will need to explicitly assign the policy to them.\nLimitations\n\nEach project can have only one default QoS policy. If you want to change the default policy, unset the old default policy first.\n\nPrerequisites\n\nA QoS policy is created, as described in Creating QoS policies.\n\nTo set a QoS policy as default to a project\nUse the --default option with the openstack network qos policy set command. For example:# openstack --insecure network qos policy set --default policy1\nTo unset the default policy from a project\nUse the --no-default option with the openstack network qos policy set command. For example:# openstack --insecure network qos policy set --no-default policy1\nSee also\n\nAssigning QoS policies\n\nModifying QoS policy rules",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-the-default-qos-policy.html"
    },
    {
        "title": "Setting memory parameters per cluster",
        "content": "Setting memory parameters per cluster\nTo change per-cluster memory parameters\nUse the following command:vinfra memory-policy vstorage-services per-cluster change [--guarantee <guarantee>] [--swap <swap>]\r\n                                                          [--cache-ratio <cache-ratio> --cache-minimum <cache-minimum>\r\n                                                          --cache-maximum <cache-maximum>]\r\n\n\n--guarantee <guarantee>\n\nGuarantee, in bytes\n--swap <swap>\n\nSwap size, in bytes, or -1 if unlimited\n--cache-ratio <cache-ratio>\n\nCache ratio from 0 to 1 inclusive\n--cache-minimum <cache-minimum>\n\nMinimum cache, in bytes\n--cache-maximum <cache-maximum>\n\nMaximum cache, in bytes\n\nFor example, to set the storage memory parameters for all nodes in the storage cluster, run:# vinfra memory-policy vstorage-services per-cluster change --guarantee 8796093022208 --swap 1099511627776 \\\r\n--cache-ratio 0.5 --cache-minimum 1099511627776 --cache-maximum 3298534883328\nThis command sets the memory parameters as follows:\n\nThe memory guarantee to 8 GB\nThe swap size to 1 GB\nThe page cache limits: the minimum to 1 GB, the maximum to 3 GB, and the cache ratio to 0.5\n\nYou can view the updated per-cluster memory parameters in the vinfra memory-policy vstorage-services per-cluster show output:# vinfra memory-policy vstorage-services per-cluster show\r\n+-----------+------------------------+\r\n| Field     | Value                  |\r\n+-----------+------------------------+\r\n| cache     | maximum: 3298534883328 |\r\n|           | minimum: 1099511627776 |\r\n|           | ratio: 0.5             |\r\n| guarantee | 8796093022208          |\r\n| swap      | 1099511627776          |\r\n+-----------+------------------------+\r\n\nTo reset per-cluster parameters to default\nUse the following command:vinfra memory-policy vstorage-services per-cluster reset [--guarantee] [--swap] [--cache]\r\n\n\n--guarantee\n\nReset only the guarantee.\n--swap\n\nReset only the swap size.\n--cache\n\nReset only cache values.\n\nFor example, to reset the manually configured page cache limits to default for all nodes in the storage cluster, run:# vinfra memory-policy vstorage-services per-cluster reset --cache\nSee also\n\nSetting memory parameters per node",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-memory-parameters-per-cluster.html"
    },
    {
        "title": "Setting up a PXE server",
        "content": "Setting up a PXE server\nYou will need to install and configure the following components:\n\nTFTP server. This is a machine that allows your servers to boot and install Virtuozzo Hybrid Infrastructure over the network. Any machine that can run Linux and is accessible over network can be a TFTP server.\nDHCP server. This is a standard DHCP machine serving TCP/IP settings to computers on your network.\n\nHTTP server. This is a machine serving Virtuozzo Hybrid Infrastructure installation files over a network.\nYou can also share Virtuozzo Hybrid Infrastructure distribution over a network via FTP (for example, with vsftpd) or NFS.\n\nTo install PXE components\nRun the following command:# yum install tftp-server syslinux httpd dhcp\r\n\nYou can also use servers that already exist in your infrastructure. For example, skip httpd and dhcp if you already have the HTTP and DHCP servers.\nTo set up the TFTP server\nFollow this instruction to configure the TFTP server for BIOS-based systems. For information on configuring it for EFI-based systems, refer to the Red Hat Enterprise Linux Installation Guide.\n\nOn the server, open the /etc/xinetd.d/tftp file, and edit it as follows:service tftp{disable         = nosocket_type     = dgramprotocol        = udpwait            = yesuser            = rootserver          = /usr/sbin/in.tftpdserver_args     = -v -s /tftpbootper_source      = 11cps             = 100 2flags           = IPv4}\nOnce you are done, save the file.\n\nCreate the /tftpboot directory and copy the following files to it: vmlinuz, initrd.img, menu.c32, pxelinux.0.\nThese files are necessary to start installation. You can find the first two in the /images/pxeboot directory of the Virtuozzo Hybrid Infrastructure distribution. The last two files are located in the syslinux directory (usually /usr/share/syslinux or /usr/lib/syslinux).\n\nCreate the /tftpboot/pxelinux.cfg directory and make the default file in it.# mkdir /tftpboot/pxelinux.cfg\r\n# touch /tftpboot/pxelinux.cfg/default\n\nAdd the following lines to default:default menu.c32\r\nprompt 0timeout 100ontimeout INSTALLmenu title Boot Menulabel INSTALL        menu label Install        kernel vmlinuz        append initrd=initrd.img ip=dhcp\r\n\nFor detailed information on parameters you can specify in this file, refer to the documentation for syslinux.\n\nRestart the xinetd service:# /etc/init.d/xinetd restart\r\n\n\nIf necessary, configure the firewall to allow access to the TFTP server (on port 69 by default).\nWhen running the TFTP server, you might get the \u00e2\u0080\u009cPermission denied\u00e2\u0080\u009d error. In this case, you may try to fix the problem by running the following command: # restorecon -Rv /tftboot/.\n\nTo set up the DHCP server\nAdd the following strings to the dhcpd.conf file, which is usually located in the /etc or /etc/dhcp directory:next-server <PXE_server_IP_address>;filename \"/pxelinux.0\";\r\n\nTo configure a DHCP server for installation on EFI-based systems, specify filename \"/bootx64.efi\" instead of filename \"/pxelinux.0\" in the dhcpd.conf file, where /bootx64.efi is the directory to which you copied the EFI boot images when setting up the TFTP server.\nTo make the  distribution files available on the HTTP server\n\nSet up an HTTP server (or configure the one you already have).\nCopy the contents of the distribution image to a directory on the HTTP server (for example, /var/www/html/distrib).\n\nOn the PXE server, specify the path to the  installation files in the append line of the /tftpboot/pxelinux.cfg/default file.\nFor EFI-based systems, the file you need to edit has the name of /tftpboot/pxelinux.cfg/efidefault or /tftpboot/pxelinux.cfg/<PXE_server_IP_address>.\nAssuming that the HTTP server is at 198.123.123.198, the installation files are in /var/www/html/distrib/, and DocumentRoot is set to /var/www/html, the default file may look like this:default menu.c32prompt 0timeout 100ontimeout INSTALLmenu title Boot Menulabel INSTALL        menu label Install        kernel vmlinuz        append initrd=initrd.img ip=dhcp inst.repo=http://198.123.123.198/distrib\n\nWhat's next\n\nInstalling in the attended mode\n\nInstalling in the unattended mode",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-up-a-pxe-server.html"
    },
    {
        "title": "Sending email notifications",
        "content": "Sending email notifications\nVirtuozzo Hybrid Infrastructure can send automatic email notifications about errors, warnings, and alerts.\nPrerequisites\n\nThe management node must be able to access the SMTP server.\n\nTo set up email notifications\n\nAdmin panel\n\nGo to Settings > System settings > Email notifications and turn on the toggle switch Enable email notification.\n\nSpecify the SMTP server details:\n\nIn User account and User password, the credentials of the notification sender registered on the SMTP server.\nIn SMTP server, the DNS name of the SMTP server, either public (for example, smtp.gmail.com) or the one in your organization.\nIn SMTP port, a custom  SMTP port that the server uses.\nIn Security, the security protocol of the SMTP server.\n\nSelect Errors, Warnings, and/or Information, to be notified about these alerts.\n\nSpecify the sender and recipient details:\n\nIn From and Sender name, the notification sender\u00e2\u0080\u0099s email and name.\nIn To , enter one or more notification recipients\u00e2\u0080\u0099 emails, comma separated.\n\nTo send a test email, click Test.\nClick Save to apply your changes.\n\nCommand-line interface\nUse the following command:vinfra cluster settings email-notifications set [--user-account <user-account>]\r\n                                                [--user-password <user-password>]\r\n                                                --smtp-server <smtp-server>\r\n                                                --smtp-port <smtp-port>\r\n                                                --security {SSL,STARTTLS}\r\n                                                --severity {error,warning,info}\r\n                                                [--from <from>] [--sender-name <sender-name>]\r\n                                                [--email-recipients-list <email-recipients-list>]\n\n--user-account <user-account>\n\nUser account of the notification sender\n--user-password <user-password>\n\nPassword for the user account\n--smtp-server <smtp-server>\n\nDNS name of the SMTP server\n--smtp-port <smtp-port>\n\nSMTP port used by the SMTP server\n--security {SSL,STARTTLS}\n\nSecurity protocol of the SMTP server\n--severity {error,warning,info}\n\nSeverity type of email notifications. This option can be used multiple times.\n--from <from>\n\nEmail address of the notification sender\n--sender-name <sender-name>\n\nNotification sender name\n--email-recipients-list <email-recipients-list>\n\nA comma-separated list of notification recipients' emails\n\nFor example, to set up email notification from notifier@example.com to user1@example.com, user2@example.com, and user3@example.com, run:# vinfra cluster settings email-notifications set --user-account notifier@example.com --user-password ****** \\\r\n--smtp-server smtp.example.com --smtp-port 587 --security SSL --severity warning --severity error \\\r\n--from notifier@example.com --sender-name \"Event notifier\" --email-recipients-list user1@example.com,user2@example.com,user3@example.com\nYou can view the email notifications settings in the vinfra cluster settings email-notifications show output:# vinfra cluster settings email-notifications show\r\n+-----------------------+------------------------------------+\r\n| Field                 | Value                              |\r\n+-----------------------+------------------------------------+\r\n| notification_settings | alerts_severities:                 |\r\n|                       | - warning                          |\r\n|                       | - error                            |\r\n|                       | email_recipients_list:             |\r\n|                       | - user1@example.com                |\r\n|                       | - user2@example.com                |\r\n|                       | - user3@example.com                |\r\n|                       | from: notifier@example.com         |\r\n|                       | sender_name: Event notifier        |\r\n| smtp_settings         | account_name: notifier@example.com |\r\n|                       | connection_security: SSL           |\r\n|                       | port: 587                          |\r\n|                       | smtp_server: smtp.example.com      |\r\n+-----------------------+------------------------------------+\n\nTo disable email notifications\n\nAdmin panel\n\nGo to Settings > System settings > Email notifications and turn off the toggle switch Enable email notification.\nClick Save to apply your changes.\n\nCommand-line interface\nUse the following command:vinfra cluster settings email-notifications disable\n\nSee also\n\nViewing alerts\n\nViewing audit log\n\nMonitoring infrastructure nodes\n\nMonitoring cluster objects via SNMP",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster settings email-notifications set [--user-account <user-account>]\r\n                                                [--user-password <user-password>]\r\n                                                --smtp-server <smtp-server>\r\n                                                --smtp-port <smtp-port>\r\n                                                --security {SSL,STARTTLS}\r\n                                                --severity {error,warning,info}\r\n                                                [--from <from>] [--sender-name <sender-name>]\r\n                                                [--email-recipients-list <email-recipients-list>]\n\n--user-account <user-account>\n\nUser account of the notification sender\n--user-password <user-password>\n\nPassword for the user account\n--smtp-server <smtp-server>\n\nDNS name of the SMTP server\n--smtp-port <smtp-port>\n\nSMTP port used by the SMTP server\n--security {SSL,STARTTLS}\n\nSecurity protocol of the SMTP server\n--severity {error,warning,info}\n\nSeverity type of email notifications. This option can be used multiple times.\n--from <from>\n\nEmail address of the notification sender\n--sender-name <sender-name>\n\nNotification sender name\n--email-recipients-list <email-recipients-list>\n\nA comma-separated list of notification recipients' emails\n\nFor example, to set up email notification from notifier@example.com to user1@example.com, user2@example.com, and user3@example.com, run:# vinfra cluster settings email-notifications set --user-account notifier@example.com --user-password ****** \\\r\n--smtp-server smtp.example.com --smtp-port 587 --security SSL --severity warning --severity error \\\r\n--from notifier@example.com --sender-name \"Event notifier\" --email-recipients-list user1@example.com,user2@example.com,user3@example.com\nYou can view the email notifications settings in the vinfra cluster settings email-notifications show output:# vinfra cluster settings email-notifications show\r\n+-----------------------+------------------------------------+\r\n| Field                 | Value                              |\r\n+-----------------------+------------------------------------+\r\n| notification_settings | alerts_severities:                 |\r\n|                       | - warning                          |\r\n|                       | - error                            |\r\n|                       | email_recipients_list:             |\r\n|                       | - user1@example.com                |\r\n|                       | - user2@example.com                |\r\n|                       | - user3@example.com                |\r\n|                       | from: notifier@example.com         |\r\n|                       | sender_name: Event notifier        |\r\n| smtp_settings         | account_name: notifier@example.com |\r\n|                       | connection_security: SSL           |\r\n|                       | port: 587                          |\r\n|                       | smtp_server: smtp.example.com      |\r\n+-----------------------+------------------------------------+\n",
                "title": "To set up email notifications"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster settings email-notifications disable\n",
                "title": "To disable email notifications"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to Settings > System settings > Email notifications and turn on the toggle switch Enable email notification.\n\nSpecify the SMTP server details:\n\nIn User account and User password, the credentials of the notification sender registered on the SMTP server.\nIn SMTP server, the DNS name of the SMTP server, either public (for example, smtp.gmail.com) or the one in your organization.\nIn SMTP port, a custom  SMTP port that the server uses.\nIn Security, the security protocol of the SMTP server.\n\n\n\n\n\n\nSelect Errors, Warnings, and/or Information, to be notified about these alerts.\n\nSpecify the sender and recipient details:\n\nIn From and Sender name, the notification sender\u00e2\u0080\u0099s email and name.\nIn To , enter one or more notification recipients\u00e2\u0080\u0099 emails, comma separated.\n\n\n\n\n\n\nTo send a test email, click Test.\nClick Save to apply your changes.\n\n",
                "title": "To set up email notifications"
            },
            {
                "example": "\nAdmin panel\n\nGo to Settings > System settings > Email notifications and turn off the toggle switch Enable email notification.\nClick Save to apply your changes.\n\n",
                "title": "To disable email notifications"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/sending-email-notifications.html"
    },
    {
        "title": "Server requirements",
        "content": "Server requirements\nThe hardware requirements and recommended number of servers in a cluster depend on the services you will deploy in your cluster.\n\nSee also\n\nHardware recommendations\n\nQuantity of servers\n\nDisk requirements\n\nNetwork requirements and recommendations\n\nAdmin panel requirements",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/server-requirements.html"
    },
    {
        "title": "Setting up networks",
        "content": "Setting up networks\nBy default, you have two preconfigured networks named Public and Private, according to their type. They can be considered templates, which you can customize to create the desired (recommended) configuration.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-up-networks.html"
    },
    {
        "title": "Setting up user authentication and authorization",
        "content": "Setting up user authentication and authorization\nVirtuozzo Hybrid Infrastructure allows you to authenticate users in specific NFS shares via Kerberos and authorize them to access specific NFS exports inside these shares via LDAP.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-up-user-authentication-and-authorization.html"
    },
    {
        "title": "Shelving virtual machines",
        "content": "Shelving virtual machines\nYou can unbind a stopped VM from the node it is hosted on and release its reserved resources such as CPU and RAM. A shelved VM remains bootable and retains its configuration, including the IP addresses. \nPrerequisites\n\nVirtual machines are created, as described in Creating virtual machines.\n\nTo shelve a virtual machine\n\nAdmin panel\n\nClick the desired virtual machine.\nIf the VM is stopped, click Shelve on its right pane.\nIf the VM is running or suspended, click Shut down or Power off on its right pane, and then select Shelve virtual machine in the confirmation window.\n\nCommand-line interface\nUse the following command:vinfra service compute server shelve <server>\r\n\n\n<server>\n\nVirtual machine ID or name.\n\nFor example, to shelve the virtual machine myvm, run:# vinfra service compute server shelve myvm\n\nTo spawn a shelved VM on a node with enough resources to host it\n\nAdmin panel\n\nClick a shelved virtual machine.\nOn the VM right pane, click Unshelve.\n\nCommand-line interface\nUse the following command:vinfra service compute server unshelve <server>\r\n\n\n<server>\n\nVirtual machine ID or name.\n\nFor example, to unshelve the virtual machine myvm, run:# vinfra service compute server unshelve myvm\n\nSee also\n\nManaging virtual machine power state\n\nTroubleshooting virtual machines\n\nDeleting virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server shelve <server>\r\n\n\n<server>\n\nVirtual machine ID or name.\n\nFor example, to shelve the virtual machine myvm, run:# vinfra service compute server shelve myvm\n",
                "title": "To shelve a virtual machine"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server unshelve <server>\r\n\n\n<server>\n\nVirtual machine ID or name.\n\nFor example, to unshelve the virtual machine myvm, run:# vinfra service compute server unshelve myvm\n",
                "title": "To spawn a shelved VM on a node with enough resources to host it"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nClick the desired virtual machine.\nIf the VM is stopped, click Shelve on its right pane.\nIf the VM is running or suspended, click Shut down or Power off on its right pane, and then select Shelve virtual machine in the confirmation window.\n\n",
                "title": "To shelve a virtual machine"
            },
            {
                "example": "\nAdmin panel\n\nClick a shelved virtual machine.\nOn the VM right pane, click Unshelve.\n\n",
                "title": "To spawn a shelved VM on a node with enough resources to host it"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/shelving-virtual-machines.html"
    },
    {
        "title": "Setting virtual machine CPU model",
        "content": "Setting virtual machine CPU model\nVirtual machines are created with the host CPU model by default. If nodes in the compute cluster have different CPUs, live migration of VMs between compute nodes may not work or applications inside VMs that depend on particular CPUs may not function properly. To avoid this, you can find out which CPU model offers compatibility across all nodes in the compute cluster and manually set it as the compute cluster default. In this case, however, the compute cluster CPU model will be the least advanced one and compute nodes will lose CPU capabilities of a more advanced processor.\nYou can view the list of supported CPU models by running the command vinfra service compute show.\nLimitations:\n\nChanging CPU model affects only new VMs, that is, those created after the change.\n\nPrerequisites\n\nThe compute cluster is created, as described in Creating the compute cluster.\n\nTo set the compute cluster CPU model\n\nDetermine baseline CPU models for the compute cluster. For example:# vinfra service compute baseline-cpu\r\n+------------------------+----------------------+\r\n| Field                  | Value                |\r\n+------------------------+----------------------+\r\n| All selected nodes     | models:              |\r\n|                        | - SandyBridge        |\r\n|                        | patched: true        |\r\n| node001.vstoragedomain | models:              |\r\n|                        | - Cascadelake-Server |\r\n|                        | - SandyBridge        |\r\n|                        | - Skylake-Client     |\r\n|                        | - Skylake-Server     |\r\n|                        | - Broadwell          |\r\n|                        | - Haswell            |\r\n|                        | - Broadwell-noTSX    |\r\n|                        | - Haswell-noTSX      |\r\n|                        | patched: true        |\r\n| node002.vstoragedomain | models:              |\r\n|                        | - SandyBridge        |\r\n|                        | - Nehalem            |\r\n|                        | - IvyBridge          |\r\n|                        | - Broadwell          |\r\n|                        | - Haswell            |\r\n|                        | - Broadwell-noTSX    |\r\n|                        | - Haswell-noTSX      |\r\n|                        | patched: true        |\r\n| node003.vstoragedomain | models:              |\r\n|                        | - SandyBridge        |\r\n|                        | - Skylake-Client     |\r\n|                        | - Skylake-Server     |\r\n|                        | - Nehalem            |\r\n|                        | - IvyBridge          |\r\n|                        | patched: true        |\r\n+------------------------+----------------------+\nThe command will print the most compatible CPU model across all compute nodes. In the example, it is SandyBridge.\n\nSet this CPU model for the compute cluster. For example:# vinfra service compute set --cpu-model SandyBridge\n\nSee also\n\nConfiguring CPU features for virtual machines\n\nWhat's next\n\nSetting a DNS name for the compute API",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/setting-virtual-machine-cpu-model.html"
    },
    {
        "title": "Signing in through identity providers",
        "content": "Signing in through identity providers\nAfter connecting to an identity provider, you can obtain the direct link to the self-service login page and share it with federated users. On this login page, federated users need to click Sign in with <identity_provider> to be redirected to their identity provider for authentication. Upon successful authentication, federated users are redirected back to the self-service panel.\n\nIf the Implicit Flow is used for authorization, the AD FS authorization endpoint must support the Form Post Response Mode.\n\nIf federated users are added to a domain group within the Default domain with the System administrator permissions, they will also be able to log in to the admin panel through their identity provider.\nPrerequisites\n\nIdentity providers are added to the admin panel, as described in Adding  identity providers.\n\nTo share the self-service panel URL with federated users\n\nOn the Projects and users screen, click the required domain.\nSwitch to the Settings > Identity provider screen, and then click the arrow icon next to the identity provider, to expand the detailed information.\nIn Self-service panel URL, copy the direct link to the self-service panel, and then share it with the federated users.\n\nSee also\n\nEditing and deleting identity providers",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/signing-in-through-identity-providers.html"
    },
    {
        "title": "Shutting down and starting up the cluster",
        "content": "Shutting down and starting up the cluster\nPrerequisites\n\nIf you have the compute cluster deployed, all running virtual machines must be stopped and no VMs should be in a transitional state. \nIf you have the NFS or block storage cluster deployed, iSCSI LUNs and NFS exports must be unmounted on the client\u00e2\u0080\u0099s side before stopping the services. Otherwise, shutting down the cluster may result in data loss.\n\nTo shut down the entire cluster\n\nShut down cluster nodes that are not running metadata services. To distinguish such nodes, go to Infrastructure > Nodes and look for the nodes without the Metadata service. On each of these nodes, do one of the following:\n\nIf you can access the node remotely, execute:# shutdown -h now\r\n\n\nIf you can access the node physically, briefly press the power button once\n\nShut down cluster nodes with metadata services by using the command from the previous step.\n\nTo start up the cluster\n\nBoot the nodes with the Metadata or/and Admin panel services. \nTurn on the other cluster nodes.\nCheck the storage and compute cluster statuses before starting working with Virtuozzo Hybrid Infrastructure.\n\nSee also\n\nMonitoring the storage cluster\n\nMonitoring the compute cluster",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/shutting-down-and-starting-up-the-cluster.html"
    },
    {
        "title": "Storage policies",
        "content": "Storage policies\nIn Virtuozzo Hybrid Infrastructure, the common unit of data is a volume. When you create a volume, you need to define its redundancy mode, tier, and failure domain. These parameters make up a storage policy defining how redundant a volume must be and where it needs to be located.\nTo better understand a storage policy, let\u00e2\u0080\u0099s have a look at its main components (tiers, failure domains, and redundancy), for a sample scenario. For example, you have three nodes with a number of storage nodes: fast SSDs and high-capacity HDDs. Node 1 has only SSDs; nodes 2 and 3 have both SSDs and HDDs. You want to export storage space via iSCSI and S3, so you need to define a suitable storage policy for each workload.\n\nThe first parameter, tier, defines a group of disks united by criteria (drive type, as a rule) tailored to a specific storage workload. For this sample scenario, you can group your SSD drives into tier 2, and HDD drives into tier 3. You can assign a disk to a tier when creating a storage cluster or adding nodes to it. Note that only nodes 2 and 3 have HDDs and will be used for tier 3. The first node\u00e2\u0080\u0099s SSDs cannot be used for tier 3.\nThe second parameter, failure domain, defines a scope within which a set of storage services can fail in a correlated manner. The default failure domain is host. Each data chunk is copied to different storage nodes, just one copy per node. If a node fails, the data is still accessible from the healthy nodes. A disk can also be a failure domain, though it is only relevant for one-node clusters. As you have three nodes in this scenario, we recommend choosing the host failure domain.\nThe third parameter, redundancy, should be configured to fit the available disks and tiers. In our example, you have three nodes: all of them have SSDs on tier 2. So, if you select tier 2 in your storage policy, you can use the three nodes for 1, 2, or 3 replicas. But only two of your nodes have HDDs on tier 3. So, if you select tier 3 in your storage policy, you can only store 1 or 2 replicas on the two nodes. In both cases, you can also use encoding, but in this scenario, let\u00e2\u0080\u0099s stick to replication: 3 replicas for SSDs and 2 replicas for HDDs.\n\nTo sum it up, the resulting storage policies are:\n\nSee also\n\nData redundancy\n\nFailure domains\n\nStorage tiers\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/storage-policies.html"
    },
    {
        "title": "Storage cluster architecture",
        "content": "Storage cluster architecture\nThe fundamental component of Virtuozzo Hybrid Infrastructure is a storage cluster, a group of physical servers interconnected by the network. The core storage comprises server disks, which are assigned one or more roles. Typically, each server in the cluster runs core storage services that correspond to the following disk roles:\n\nMetadata\n\nMetadata nodes run metadata services (MDS), store cluster metadata, and control how user files are split into chunks and where these chunks are located. Metadata nodes also ensure that chunks have the required amount of replicas. Finally, they log all important events that happen in the cluster. To provide system reliability, Virtuozzo Hybrid Infrastructure uses the Paxos consensus algorithm. It guarantees fault-tolerance if the majority of nodes running metadata services are healthy. \nTo ensure high availability of metadata in a production environment, metadata services must be run on at least three cluster nodes. In this case, if one metadata service fails, the remaining two will still be controlling the cluster. However, it is recommended to have maximum five metadata services in a cluster, to ensure that the cluster can survive simultaneous failure of two nodes and without data loss.\nThe primary metadata node is the master node in the metadata quorum. If the master MDS fails, another available MDS is selected as master.\n\nStorage\n\nStorage nodes run chunk services (CS), store all data in the form of fixed-size chunks, and provide access to these chunks. All data chunks are replicated and the replicas are kept on different storage nodes to achieve high availability of data. If one of the storage nodes fails, the remaining healthy storage nodes continue providing the data chunks that were stored on the failed node. The storage role can only be assigned to a server with disks of a certain capacity.\nStorage nodes can also benefit from data caching and checksumming:\n\nData caching improves cluster performance by placing frequently accessed data on an SSD.\n\nData checksumming generates checksums each time some data in the cluster is modified. When this data is then read, a new checksum is computed and compared with the old checksum. If the two are not identical, a read operation is performed again, thus providing better data reliability and integrity.\nIf a node has an SSD, it will be automatically configured to keep checksums when you add a node to a cluster. This is the recommended setup. However, if a node does not have an SSD drive, checksums will be stored on a rotational disk by default. It means that this disk will have to handle double the I/O, because for each data read/write operation there will be a corresponding checksum read/write operation. For this reason, you may want to disable checksumming on nodes without SSDs to gain performance at the expense of checksums. This can be especially useful for hot data storage.\n\nSupplementary roles:\n\nCache\r\n                        \n\nBoosts chunk read/write performance by creating write caches on selected solid-state drives (SSDs). It is also recommended to use such SSDs for metadata. The use of write journals may more than double the write speed in the cluster.\n\nSystem\r\n                        \n\nOne disk per node that is reserved for the operating system and unavailable for data storage.\nNote the following:\n\nThe System role cannot be unassigned from a disk. \n\nIf a physical server has a system disk with the capacity greater than 100 GB, that disk can be additionally assigned the Metadata or Storage role.\nIt is recommended to assign the System+Metadata role to an SSD. Assigning both of these roles to an HDD will result in mediocre performance suitable only for cold data (for example, archiving).\nThe System role cannot be combined with the Cache and Metadata+Cache roles. The reason is that the I/O generated by the operating system and applications would contend with the I/O generated by journaling, thus negating its performance benefits.\n\nAlong with the core storage services, servers run storage access points that allow top-level virtualization and storage services to access the storage cluster.\n\nIn addition, a server joined to the storage cluster can run neither metadata nor chunk services. In this case, the node will run only storage access points and serve as a storage cluster client.\nSee also\n\nStorage cache architecture",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/storage-cluster-architecture.html"
    },
    {
        "title": "Storage tiers",
        "content": "Storage tiers\nIn Virtuozzo Hybrid Infrastructure terminology, tiers are disk groups that allow you to organize storage workloads based on your criteria. For example, you can use tiers to separate workloads produced by different tenants. Or you can have a tier of fast SSDs for service or virtual environment workloads and a tier of high-capacity HDDs for backup storage.\nManual data migration between tiers (default)\nWhen required, you can manually migrate data from one tier to another. To start data migration, ensure that the target tier has enough free space, and then select the desired tier in the settings of the current storage policy.\nAutomatic data migration between tiers\nAutomatic data migration between tiers works in the inter-tier data allocation mode and is disabled by default. In this mode, data is automatically migrated to a lower tier if the current tier is full. To enable the inter-tier data allocation mode, use vstorage -c <cluster_name> set-config mds.alloc.strict_tier=0 command.\nWhen assigning disks to tiers (which you can do at any time), have in mind that faster storage drives should be assigned to higher tiers. For example, you can use tier 0 for backups and other cold data (CS without SSD cache); tier 1 for virtual environments\u00e2\u0080\u0094a lot of cold data but fast random writes (CS with SSD cache); and tier 2 for hot data (CS on SSD), caches, specific disks, and such.\nThis recommendation is related to how Virtuozzo Hybrid Infrastructure works with storage space in the inter-tier data allocation mode. If a storage tier runs out of free space, Virtuozzo Hybrid Infrastructure will attempt to temporarily use the space of the lower tiers down to the lowest. If the lowest tier also becomes full, Virtuozzo Hybrid Infrastructure will attempt to use a higher one. If you add more storage to the original tier later, the data, temporarily stored elsewhere, will be moved to the tier where it should have been stored originally. For example, if you try to write data to tier 2 and it is full, Virtuozzo Hybrid Infrastructure will attempt to write that data to tier 1, then to tier 0. If you add more storage to tier 2 later, the aforementioned data, now stored on the tier 1 or 0, will be moved back to tier 2, where it was meant to be stored originally.\nSee also\n\nStorage policies\n\nData redundancy\n\nFailure domains\n\n\u00d0\u00a1luster rebuilding",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/storage-tiers.html"
    },
    {
        "title": "Storage cache architecture",
        "content": "Storage cache architecture\nThe terms \"cache\" and \"journal\" are sometimes used interchangeably. In the storage cluster, however, cache refers to a fast hardware device (for example, SSD- or NVMe-based) that is used to store the chunk service journal. A journal, in its turn, is a buffer that is used by the chunk service and stored in a cache device. As multiple chunk services can share the same cache device, one cache can contain multiple journals.\nAs such, caching does not count as an additional storage tier in the cluster. Instead, each cache device can be associated with multiple chunk services that are assigned to different tiers, and used to store data journals.\nBy default, the chunk service stores its journal on the same device as its data. This configuration is called \"inner cache.\" In order to use a fast cache device, the chunk service must be configured to use an \"external cache.\"\n\nIf you use the \"inner cache\" configuration, it is recommended to keep the default journal size of 256 MB.\n\nReads and writes behavior\nIn the storage cluster, cache is mainly used for writing data: when new data is ingested in the system, it is temporarily stored in the cache. As a cache device is faster than a capacity one, writing data on the cache device improves performance. For a certain amount of time, data only exists in the cache, with remote replicas on the other cluster nodes, if remote replication is configured. During this time, all read operations hit the cache and benefit from the performance boost, as well. When the cache is reclaimed and data is removed from it, all subsequent read operations are redirected to a capacity device.\nThe journal is used as a ring buffer: it stores data until there is a need to reclaim space and make room for new data. When this happens, data is offloaded to a capacity device in a first-in, first-out fashion (FIFO).\nCaching benefits\nCaching helps to significantly improve write speed and write latency, with only a slight increase in the system cost. Systems with cache can benefit from high capacity of low-cost and low-performance hard disk devices (HDD), while providing fast write access by using flash devices, such as solid-state drives (SSD) or non-volatile memory devices (NVMe). Though the cost of cache devices is higher than that of capacity devices, only a few cache devices are needed, thereby making the overall system cost generally low. Moreover, the performance boost often justifies such an upgrade.\nYou can benefit from using cache in the following scenarios:\n\n\"Hot\" data storage\nRandom writes\nSmaller block sizes or smaller files\nDatabases and environments with several clients/threads\n\nOn the other hand, scenarios that usually have small advantage when using cache include:\n\n\"Cold\" data storage\nConstant throughput workloads, such as video surveillance recording\nSequential writes of very large files\nRead-intensive workloads\n\nIn these cases, you might use an all-HDD solution, which will provide the same performance at a lower cost; or an all-flash solution, if your aim is to increase performance.\nSee also\n\nStorage cluster architecture\n\nCache configuration",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/storage-cache-architecture.html"
    },
    {
        "title": "Switching between GPU passthrough and vGPU",
        "content": "Switching between GPU passthrough and vGPU\nIf you have already enabled GPU passthrough for the compute cluster but want to use vGPU instead, or vice versa, you will need to reconfigure the node with the physical GPU and the compute cluster.\nPrerequisites\n\nThe compute cluster is reconfigured for GPU passthrough or vGPU support, as described in Enabling PCI passthrough and vGPU support.\n\nTo reconfigure the compute cluster from GPU passthrough to vGPU support\n\nOn the node with the physical GPU, find out the service that is associated with the GPU. For example:# systemctl | grep stub\r\n  pcistub-0000:01:00.0.service       loaded active exited    Bind device to pci-stub driver\n\nDisable this service. For example:# systemctl disable pcistub-0000:01:00.0.service\n\nReboot the node to apply your changes:# reboot\n\nModify the configuration file:\n\nChange the device_type from generic to pgpu\nSpecify the GPU's PCI address in device\nRemove the alias field\nAdd the desired vgpu_type\n\nAs a result, your configuration file config.yaml may look as follows:- node_id: c3b2321a-7c12-8456-42ce-8005ff937e12\r\n  devices:\r\n    - device_type: pgpu\r\n      device: \"0000:01:00.0\"\r\n      vgpu_type: nvidia-224\n\nPass the configuration file to the vinfra service compute set command. For example:# vinfra service compute set --pci-passthrough-config config.yaml\n\nTo reconfigure the compute cluster from vGPU support to GPU passthrough\n\nRemove the vGPU-related information from the configuration file config.yaml. For example, you may need to remove these lines:- device_type: pgpu\r\n  device: \"0000:01:00.0\"\r\n  vgpu_type: nvidia-224\n\nReconfigure the compute cluster by using the updated configuration file config.yaml. For example:# vinfra service compute set --pci-passthrough-config config.yaml\n\nOn the node with the physical GPU, run the pci-helper.py script to assign the pci-stub driver to the GPU at its PCI address. For example:# /usr/libexec/vstorage-ui-agent/bin/pci-helper.py bind-to-stub 0000:01:00.0\n\nAdd the GPU card to the configuration file as a generic device. For example:- device_type: generic\r\n  device: 1b36:0100\r\n  alias: gpu\n\nPass the configuration file to the vinfra service compute set command. For example:# vinfra service compute set --pci-passthrough-config config.yaml\n\nWhat's next\n\nCreating virtual machines with physical GPUs\n\nCreating virtual machines with virtual GPUs\n\nCreating virtual machines with different vGPU types",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/switching-between-gpu-passthrough-and-vgpu.html"
    },
    {
        "title": "Supported authentication schemes",
        "content": "Supported authentication schemes\nThe following authentication schemes are supported by the Virtuozzo Hybrid Infrastructure implementation of the Amazon S3 protocol:\n\nSignature Version 2\n\nSignature Version 4\n\nThe following authentication methods are supported by the Virtuozzo Hybrid Infrastructure implementation of the Amazon S3 protocol:\n\nUsing the authorization header\n\nTransferring payload in a single chunk\n\nUsing query parameters\n\nBrowser-based uploads using POST\n\nThe following authentication method is not supported:\n\nTransferring payload in multiple chunks\n\nSee also\n\nSupported Amazon S3 REST operations\n\nSupported Amazon request headers\n\nSupported Amazon response headers\n\nSupported Amazon error response headers\n\nAmazon S3 features supported by bucket policies\n\nSupported Amazon S3 object expiration actions",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/supported-authentication-schemes.html"
    },
    {
        "title": "Troubleshooting installation",
        "content": "Troubleshooting installation\nThis chapter describes ways to troubleshoot installation of Virtuozzo Hybrid Infrastructure.\nInstalling in basic graphics mode\nIf the installer cannot load the correct driver for your graphics card, you can try to install Virtuozzo Hybrid Infrastructure in the basic graphics mode. In this mode, however, you may experience issues with the user interface. For example, some of its elements may not fit the screen.\nThe installation process itself is the same as in the default graphics mode.\nTo select basic graphics mode\nOn the welcome screen, select Troubleshooting\u00e2\u0080\u0093>, then Install in basic graphics mode.\nBooting into recovery mode\nIf a node fails to boot, you can recover the system disk and the node configuration by reinstalling the product from an ISO image in the recovery mode.\nDuring the recovery process, the configuration of deployed services and infrastructure is automatically detected and recovered from storage disks.\nTo enter the recovery mode\nOn the welcome screen, select Troubleshooting\u00e2\u0080\u0093>, then Recover - Node recovery.\nBooting into rescue mode\nIf you experience problems with your system, you can boot into the rescue mode to troubleshoot these problems. Once you are in the rescue mode, your Virtuozzo Hybrid Infrastructure installation is mounted under /mnt/sysimage. You can go to this directory and make the necessary changes to your system.\nTo enter the rescue mode\n\nBoot your system from the  distribution image.\nOn the welcome screen, click Troubleshooting\u00e2\u0080\u0093>, then Rescue system.\nOnce Virtuozzo Hybrid Infrastructure boots into the emergency mode, press Ctrl+D to load the rescue environment.\nIn the rescue environment, you can select one of the following options:Continue (press 1): mount the Virtuozzo Hybrid Infrastructure installation in read and write mode under /mnt/sysimage.Read-only mount (press 2): mount the Virtuozzo Hybrid Infrastructure installation in read-only mode under /mnt/sysimage.Skip to shell (press 3): load shell, if your file system cannot be mounted, for example, when it is corrupted.Quit (Reboot) (press 4): reboot the server.\nUnless you press 4, a shell prompt will appear. In it, run chroot /mnt/sysimage to make the Virtuozzo Hybrid Infrastructure installation the root environment. Now you can run commands and try to fix the problems you are experiencing.\nAfter you fix the problem, run exit to exit the chrooted environment, and then run reboot to restart the system.\n\nWhat's next\n\nInstalling in the attended mode",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/troubleshooting-installation.html"
    },
    {
        "title": "Traffic types",
        "content": "Traffic types\nTo balance and optimize networking in Virtuozzo Hybrid Infrastructure, you can assign different types of traffic to separate networks. Assigning a traffic type to a network means that a firewall is configured on nodes connected to this network, specific ports are opened on node network interfaces, and the necessary iptables rules are set. For example, nodes connected to a network with only the S3 public traffic type will accept incoming connections only on ports 80 and 443.\nThe next three subsections describe all of the traffic types that can be assigned to networks.\nExclusive traffic types\nExclusivity means that such a traffic type can be added only to one network.\n\nInternal management\n\nInternal cluster management and transfers of node monitoring data to the admin panel. Without this traffic type, the administrator cannot control and monitor the cluster. The cluster, however, continues working. Uses any available port.\nStorage\n\nInternal transfers of data chunks, high availability service heartbeats, as well as data self-healing. This is the most critical traffic type that defines storage performance and enables cluster high availability. Uses any available port.\nOSTOR private\n\nInternal data exchange between multiple S3/NFS services. Uses any available port.\nBackup (ABGW) private\n\nInternal management of and data exchange between multiple backup storage services. Uses any available port.\nVM private\n\nNetwork traffic between VMs in private virtual networks and VNC console traffic. Virtual networks are implemented as VXLAN, overlay networking fully isolated on L2. Opens UDP port 4789 and TCP ports from 15900 to 16900.\nCompute API\n\nExternal access to standard OpenStack API endpoints. Opens the following ports:\n\nTCP 5000\u00e2\u0080\u0094Identity API v3\nTCP 6080\u00e2\u0080\u0094noVNC Websocket Proxy\nTCP 8004\u00e2\u0080\u0094Orchestration Service API v1\nTCP 8041\u00e2\u0080\u0094Gnocchi API (billing metering service)\nTCP 8774\u00e2\u0080\u0094Compute API\nTCP 8776\u00e2\u0080\u0094Block Storage API v3\nTCP 8780\u00e2\u0080\u0094Placement API\nTCP 9292\u00e2\u0080\u0094Image Service API v2\nTCP 9313\u00e2\u0080\u0094Key Manager API v1\nTCP 9513\u00e2\u0080\u0094Container Infrastructure Management API (Kubernetes service)\nTCP 9696\u00e2\u0080\u0094Networking API v2\nTCP 9888\u00e2\u0080\u0094Octavia API v2 (load balancer service)\n\nVM backups\n\nExternal access to NBD endpoints. Third-party backup management systems can pull VM backups by using this traffic type. To be able to access backup agents installed in virtual machines, assign this traffic type along with VM public. Opens TCP ports from 49300 to 65535.\n\nRegular traffic types\nRegular traffic types traffic types can be added to multiple networks.\n\nS3 public\n\nExternal data exchange with the S3 access point. Uses TCP ports 80 and 443.\niSCSI\n\nExternal data exchange with the iSCSI access point. Uses TCP port 3260.\nNFS\n\nExternal data exchange with the NFS access point. Uses TCP/UDP ports 111, 892, and 2049.\nBackup (ABGW) public\n\nExternal data exchange with Acronis Cyber Protect agents and Acronis Cyber Protect Cloud. Uses TCP ports 40440 and 44445.\nAdmin panel\n\nExternal access to the admin panel. Uses TCP port 8888.\nVM public\n\nExternal data exchange between VMs and public networks (for example, the Internet). When a node network interface is assigned to a network with this traffic type, an Open vSwitch bridge is created on that network interface.\nSSH\n\nRemote access to nodes via SSH. Uses TCP port 22.\nSNMP\n\nExternal access to storage cluster monitoring statistics via the SNMP protocol. Opens UDP port 161.\nSelf-service panel\n\nExternal access to the self-service panel. Opens TCP port 8800.\n\nCustom traffic types\nCustom traffic types are created by system administrators to open required TCP ports.\nSee also\n\nManaging traffic types",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/traffic-types.html"
    },
    {
        "title": "Transferring volumes between projects",
        "content": "Transferring volumes between projects\nThere is no direct way to migrate a virtual machine between different projects. However, you can transfer the VM boot volume, and then create a new VM from it. You can transfer both boot and non-boot volumes to projects within different domains.\nLimitations\n\nYou can only transfer volumes with the \"Available\" status.\nTransferring volumes that have snapshots breaks the snapshots.\n\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\nYou have login credentials for the source and destination projects.\nIf you want to transfer a boot volume that is attached to a VM, clone this volume first, as described in Cloning volumes.\nIf you want to transfer a non-boot volume that is attached to a VM, detach it first, as described in Attaching and detaching volumes.\n\nTo transfer a volume between two projects\r\n\r\n\n\nLog in to the source project by changing the environment variables to the project credentials. For example:export OS_PROJECT_DOMAIN_NAME=domain1\r\nexport OS_USER_DOMAIN_NAME=domain1\r\nexport OS_PROJECT_NAME=project1\r\nexport OS_USERNAME=user1\r\nexport OS_PASSWORD=password\n\nList all volumes within your project to find out the ID of the volume you want to transfer:# openstack --insecure volume list\r\n+--------------------------------------+-------------------+-----------+------+\r\n| ID                                   | Name              | Status    | Size |\r\n+--------------------------------------+-------------------+-----------+------+\r\n| 2c8386fa-331b-4ba8-9e4c-de690969a4c8 | win10/Boot volume | available |   64 |\r\n+--------------------------------------+-------------------+-----------+------+\n\nCreate a transfer request by specifying the ID of the chosen volume. For example:# openstack --insecure volume transfer request create c0d4cf0e-48e3-417d-b6fc-f1fb36571c5f\r\n+------------+--------------------------------------+\r\n| Field      | Value                                |\r\n+------------+--------------------------------------+\r\n| auth_key   | 75fcf37d56f40182                     |\r\n| created_at | 2022-04-27T09:00:11.776511           |\r\n| id         | b9b835a3-ed41-489a-9552-483fae33c549 |\r\n| name       | None                                 |\r\n| volume_id  | c0d4cf0e-48e3-417d-b6fc-f1fb36571c5f |\r\n+------------+--------------------------------------+\r\n\nSave the request id and auth-key from the command output, to accept the transfer in the other project.\n\nLog in to the destination project by changing the environment variables to the project credentials. For example:export OS_PROJECT_DOMAIN_NAME=domain1\r\nexport OS_USER_DOMAIN_NAME=domain1\r\nexport OS_PROJECT_NAME=project2\r\nexport OS_USERNAME=user2\r\nexport OS_PASSWORD=password\n\nAccept the transfer request by specifying the request ID and authorization key. For example:# openstack --insecure volume transfer request accept --auth-key 75fcf37d56f40182 \\\r\nb9b835a3-ed41-489a-9552-483fae33c549\n\nOnce the volume is moved to the other project, you can create a virtual machine from it.\nSee also\n\nResizing volumes\n\nChanging the storage policy for volumes\n\nManaging volume snapshots\n\nWhat's next\n\nCreating virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/transferring-volumes-between-projects.html"
    },
    {
        "title": "Troubleshooting virtual machines",
        "content": "Troubleshooting virtual machines\nIf a virtual machine fails to deploy\nReview the error message on the VM right pane. One of the possible root causes is that compute nodes lack free RAM or CPU resources to host the VM.\nIf a virtual machine is in the error state\n\nAdmin panel\nExamine the VM history in the History tab on the VM right pane. The event log will contain all of the VM management operations performed by users in the user or command-line interface. You can expand each log entry to view operation details by clicking the arrow icon next to it. The details include the operation name, date and time, status, initiator, and request ID.\n\nCommand-line interface\nUse the following command:vinfra service compute server event list [--long] --server <server>\n\n--long\n\nEnable access and listing of all fields of objects\n--server <server>\n\nVirtual machine ID or name\n\nFor example, to view the event log of the virtual machine myvm, run:# vinfra service compute server event list --server myvm\r\n+--------------+------------------+----------------------+----------------+---------------------+-------------+----------+---------+\r\n| project_id   | server_id        | request_id           | action         | start_time          | user_id     | username | status  |\r\n+--------------+------------------+----------------------+----------------+---------------------+-------------+----------+---------+\r\n| 231f2740c<\u00e2\u0080\u00a6> | 0ad154ad-aa52<\u00e2\u0080\u00a6> | req-fbdacb0a-55f3<\u00e2\u0080\u00a6> | live-migration | 2022-07-22T11:43<\u00e2\u0080\u00a6> | effbd0f0<\u00e2\u0080\u00a6> | admin    | Error   |\r\n| 231f2740c<\u00e2\u0080\u00a6> | 0ad154ad-aa52<\u00e2\u0080\u00a6> | req-8145f911-02a6<\u00e2\u0080\u00a6> | reboot         | 2022-07-22T09:11<\u00e2\u0080\u00a6> | effbd0f0<\u00e2\u0080\u00a6> | admin    | Success |\r\n| 42166f19e<\u00e2\u0080\u00a6> | 0ad154ad-aa52<\u00e2\u0080\u00a6> | req-aff9b796-0378<\u00e2\u0080\u00a6> | create         | 2022-07-21T17:48<\u00e2\u0080\u00a6> | 646f5793<\u00e2\u0080\u00a6> | admin1   | Success |\r\n+--------------+------------------+----------------------+----------------+---------------------+-------------+----------+---------+\nThe details of a particular event are shown in the vinfra service compute server event show output:# vinfra service compute server event show --server myvm req-fbdacb0a-55f3-4670-8af0-3465f2abd64f\r\n+------------+------------------------------------------+\r\n| Field      | Value                                    |\r\n+------------+------------------------------------------+\r\n| action     | live-migration                           |\r\n| project_id | 231f2740c2704c6c8c06d0626d51346e         |\r\n| request_id | req-fbdacb0a-55f3-4670-8af0-3465f2abd64f |\r\n| server_id  | 0ad154ad-aa52-4b23-a780-7ecc96adcbb8     |\r\n| start_time | 2022-07-22T11:43:51.172315Z              |\r\n| status     | Error                                    |\r\n| user_id    | effbd0f037714ff7886586d34e4d2a95         |\r\n| username   | admin                                    |\r\n+------------+------------------------------------------+\r\n\n\nIf a virtual machine is stuck in a failed or transitional state\n\nAdmin panel\n Reset the VM to its last stable state: active, shut down or shelved:\n\n Click the stuck VM.\nOn the VM right pane, click Reset state.\n\nCommand-line interface\nUse the following command:vinfra service compute server reset-state [--state-error] <server>\r\n\n\n--state-error\n\nReset virtual machine to \u00e2\u0080\u0098ERROR\u00e2\u0080\u0099 state\n<server>\n\nVirtual machine ID or name\n\nFor example, to reset the transitional state of the virtual machine myvm to the previous one, run:# vinfra service compute server reset-state myvm\n\nIf a virtual machine is stuck with the \u00e2\u0080\u009cPowering off\u00e2\u0080\u009d task state\nIn this case, a VM will have the \u00e2\u0080\u009cActive (Powering off)\u00e2\u0080\u009d status on its right pane.\nYou can cancel this task by using the following command:vinfra service compute server cancel-stop <server>\r\n\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to cancel shutdown for the virtual machine myvm if it has the \u00e2\u0080\u009cpowering-off\u00e2\u0080\u009d task state, run:# vinfra service compute server cancel-stop myvm\nIf a virtual machine fails to boot\n\nAdmin panel\nExamine the VM console log by clicking Download console log on the VM right pane. The log will contain log messages only if logging is enabled inside the VM.\n\nCommand-line interface\nUse the following command:vinfra service compute server log <server>\t\t\t\t\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to print the log of the virtual machine myvm to the file myvm.log, run:# vinfra service compute server log myvm > myvm.log\t\t\t\nThe log will contain log messages only if logging is enabled inside the VM, otherwise the log will be empty. \n\nSee also\n\nEnabling logging for virtual machines\n\nManaging virtual machine power state\n\nRunning commands in virtual machines without network connectivity\n\nMonitoring virtual machines\n\nRescuing virtual machines\n\nDeleting virtual machines",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server event list [--long] --server <server>\n\n--long\n\nEnable access and listing of all fields of objects\n--server <server>\n\nVirtual machine ID or name\n\nFor example, to view the event log of the virtual machine myvm, run:# vinfra service compute server event list --server myvm\r\n+--------------+------------------+----------------------+----------------+---------------------+-------------+----------+---------+\r\n| project_id   | server_id        | request_id           | action         | start_time          | user_id     | username | status  |\r\n+--------------+------------------+----------------------+----------------+---------------------+-------------+----------+---------+\r\n| 231f2740c<\u00e2\u0080\u00a6> | 0ad154ad-aa52<\u00e2\u0080\u00a6> | req-fbdacb0a-55f3<\u00e2\u0080\u00a6> | live-migration | 2022-07-22T11:43<\u00e2\u0080\u00a6> | effbd0f0<\u00e2\u0080\u00a6> | admin    | Error   |\r\n| 231f2740c<\u00e2\u0080\u00a6> | 0ad154ad-aa52<\u00e2\u0080\u00a6> | req-8145f911-02a6<\u00e2\u0080\u00a6> | reboot         | 2022-07-22T09:11<\u00e2\u0080\u00a6> | effbd0f0<\u00e2\u0080\u00a6> | admin    | Success |\r\n| 42166f19e<\u00e2\u0080\u00a6> | 0ad154ad-aa52<\u00e2\u0080\u00a6> | req-aff9b796-0378<\u00e2\u0080\u00a6> | create         | 2022-07-21T17:48<\u00e2\u0080\u00a6> | 646f5793<\u00e2\u0080\u00a6> | admin1   | Success |\r\n+--------------+------------------+----------------------+----------------+---------------------+-------------+----------+---------+\nThe details of a particular event are shown in the vinfra service compute server event show output:# vinfra service compute server event show --server myvm req-fbdacb0a-55f3-4670-8af0-3465f2abd64f\r\n+------------+------------------------------------------+\r\n| Field      | Value                                    |\r\n+------------+------------------------------------------+\r\n| action     | live-migration                           |\r\n| project_id | 231f2740c2704c6c8c06d0626d51346e         |\r\n| request_id | req-fbdacb0a-55f3-4670-8af0-3465f2abd64f |\r\n| server_id  | 0ad154ad-aa52-4b23-a780-7ecc96adcbb8     |\r\n| start_time | 2022-07-22T11:43:51.172315Z              |\r\n| status     | Error                                    |\r\n| user_id    | effbd0f037714ff7886586d34e4d2a95         |\r\n| username   | admin                                    |\r\n+------------+------------------------------------------+\r\n\n",
                "title": "If a virtual machine is in the error state"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server reset-state [--state-error] <server>\r\n\n\n--state-error\n\nReset virtual machine to \u00e2\u0080\u0098ERROR\u00e2\u0080\u0099 state\n<server>\n\nVirtual machine ID or name\n\nFor example, to reset the transitional state of the virtual machine myvm to the previous one, run:# vinfra service compute server reset-state myvm\n",
                "title": "If a virtual machine is stuck in a failed or transitional state"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute server log <server>\t\t\t\t\n\n<server>\n\nVirtual machine ID or name\n\nFor example, to print the log of the virtual machine myvm to the file myvm.log, run:# vinfra service compute server log myvm > myvm.log\t\t\t\nThe log will contain log messages only if logging is enabled inside the VM, otherwise the log will be empty. \n",
                "title": "If a virtual machine fails to boot"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\nExamine the VM history in the History tab on the VM right pane. The event log will contain all of the VM management operations performed by users in the user or command-line interface. You can expand each log entry to view operation details by clicking the arrow icon next to it. The details include the operation name, date and time, status, initiator, and request ID.\n",
                "title": "If a virtual machine is in the error state"
            },
            {
                "example": "\nAdmin panel\n Reset the VM to its last stable state: active, shut down or shelved:\n\n Click the stuck VM.\nOn the VM right pane, click Reset state.\n\n",
                "title": "If a virtual machine is stuck in a failed or transitional state"
            },
            {
                "example": "\nAdmin panel\nExamine the VM console log by clicking Download console log on the VM right pane. The log will contain log messages only if logging is enabled inside the VM.\n",
                "title": "If a virtual machine fails to boot"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/troubleshooting-virtual-machines.html"
    },
    {
        "title": "Troubleshooting node disks",
        "content": "Troubleshooting node disks\nThe S.M.A.R.T. status of all disks is monitored by the smartctl tool installed along with Virtuozzo Hybrid Infrastructure. Run every 10 minutes, the tool polls all disks attached to nodes, including journaling SSDs and system disks, and reports the results to the management node. The tool checks disk health according to S.M.A.R.T. attributes. If it finds a disk in the pre-failure condition, it generates an alert. The pre-failure condition means that at least one of these S.M.A.R.T. attributes is not zero:\n\nReallocated Sector Count\nReallocated Event Count\nCurrent Pending Sector Count\nUncorrectable Sector Count\n\nAdditionally, slow disk and CS analyzers calculate disk health according to the average I/O latency over time. When the disk I/O latency reaches the predefined threshold, the disk health is considered to be 0%. In this case, an alert is generated and the disk is marked as unresponsive.\nLimitations\n\nFor the S.M.A.R.T. tool to work, the S.M.A.R.T. functionality must be enabled in the node\u00e2\u0080\u0099s BIOS.\n\nPrerequisites\n\nA clear understanding of how disk health is calculated, described in Calculating disk health.\n\nTo troubleshoot an unresponsive disk\n\nGo to the Infrastructure > Nodes screen and click the name of a node that hosts an unresponsive storage disk.\nOn the Disks tab, click the storage disk, and then go to the Service tab, to view the warning message.\nCheck the disk connectivity, S.M.A.R.T. status, and dmesg output on the node.\n\nAfter fixing the issue, click Mark as healthy to change the disk status to Healthy. If however the problem persists, it is recommended to replace the disk before its failure. If you recover such a disk, it may decrease the cluster performance and increase I/O latency.\nTo troubleshoot a failed disk\n\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the name of a node that hosts a failed service.\nOn the Disks tab, click the failed disk, and then go to the Service tab, to view the error message.\nClick Get diagnostic information to investigate the smartctl and dmesg outputs.\n\nCommand-line interface\n\nFind out the device name of a failed disk on the node from the vinfra node disk list output:# vinfra node disk list --node node003\r\n+-------------+---------+------+--------+-------------+----------+----------+---------------+------------+----------------+\r\n| id          | device  | type | role   | disk_status | used     | size     | physical_size | service_id | service_status |\r\n+-------------+---------+------+--------+-------------+----------+----------+---------------+------------+----------------+\r\n| 36972905<\u00e2\u0080\u00a6> | nvme1n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1090       | failed         |\r\n| B9F2C34F<\u00e2\u0080\u00a6> | nvme0n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1091       | active         |\r\n| A8E05CCA<\u00e2\u0080\u00a6> | nvme2n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1086       | active         |\r\n| D6E421E0<\u00e2\u0080\u00a6> | nvme3n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1087       | active         |\r\n| md126       | md126   | ssd  | system | ok          | 364.2MiB | 989.9MiB | 1022.0MiB     |            |                |\r\n| md127       | md127   | ssd  | system | ok          | 104.4GiB | 187.1GiB | 190.2GiB      |            |                |\r\n+-------------+---------+------+--------+-------------+----------+----------+---------------+------------+----------------+\r\n\nOn the node node003, the storage disc nvme1n1 is reported as failed.\n\nInvestigate the smartctl and dmesg outputs for the failed disk. For example:# vinfra node disk show diagnostic-info --node node003 nvme1n1 -f yaml\r\n- command: smartctl --all /dev/nvme1n1\r\n  stdout: 'smartctl 7.1 2020-06-20 r5066 [x86_64-linux-3.10.0-1160.41.1.vz7.183.5]\r\n    (local build)\r\n  Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org\r\n\r\n  === START OF INFORMATION SECTION ===\r\n  Model Number:                       INTEL SSDPE2KX020T8\r\n  Serial Number:                      PHLJ9500032H2P0BGN\r\n  Firmware Version:                   VDV10131\r\n  PCI Vendor/Subsystem ID:            0x8086\r\n  IEEE OUI Identifier:                0x5cd2e4\r\n  Total NVM Capacity:                 2,000,398,934,016 [2.00 TB]\r\n  Unallocated NVM Capacity:           0\r\n  Controller ID:                      0\r\n  Number of Namespaces:               1\r\n  Namespace 1 Size/Capacity:          2,000,398,934,016 [2.00 TB]\r\n  Namespace 1 Formatted LBA Size:     512\r\n  Namespace 1 IEEE EUI-64:            5cd2e4 75b0070100\r\n  Local Time is:                      Fri Nov 26 13:32:44 2021 EET\r\n  Firmware Updates (0x02):            1 Slot\r\n  Optional Admin Commands (0x000e):   Format Frmw_DL NS_Mngmt\r\n  Optional NVM Commands (0x0006):     Wr_Unc DS_Mngmt\r\n  Maximum Data Transfer Size:         32 Pages\r\n  Warning  Comp. Temp. Threshold:     70 Celsius\r\n  Critical Comp. Temp. Threshold:     80 Celsius\r\n\r\n  Supported Power States\r\n  St Op     Max   Active     Idle   RL RT WL WT  Ent_Lat  Ex_Lat\r\n   0 +    25.00W       -        -    0  0  0  0        0       0\r\n\r\n  Supported LBA Sizes (NSID 0x1)\r\n  Id Fmt  Data  Metadt  Rel_Perf\r\n   0 +     512       0         2\r\n   1 -    4096       0         0\r\n\r\n  === START OF SMART DATA SECTION ===\r\n  SMART overall-health self-assessment test result: PASSED\r\n\r\n  SMART/Health Information (NVMe Log 0x02)\r\n  Critical Warning:                   0x00\r\n  Temperature:                        34 Celsius\r\n  Available Spare:                    99%\r\n  Available Spare Threshold:          10%\r\n  Percentage Used:                    5%\r\n  Data Units Read:                    550,835,698 [282 TB]\r\n  Data Units Written:                 720,479,182 [368 TB]\r\n  Host Read Commands:                 10,050,305,459\r\n  Host Write Commands:                20,760,365,218\r\n  Controller Busy Time:               1,968\r\n  Power Cycles:                       20\r\n  Power On Hours:                     13,405\r\n  Unsafe Shutdowns:                   16\r\n  Media and Data Integrity Errors:    0\r\n  Error Information Log Entries:      0\r\n  Warning  Comp. Temperature Time:    0\r\n  Critical Comp. Temperature Time:    0\r\n\r\n  Error Information (NVMe Log 0x01, max 64 entries)\r\n  No Errors Logged\r\n  '\r\n- command: dmesg --ctime --kernel --level=emerg,alert,crit,err,warn --facility=kern\r\n    | grep 'nvme1n1'\r\n  stdout: ''\n\nIf you cannot fix the problem, contact the technical support team, as described in Getting technical support.\nWhat's next\n\nReplacing node disks",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\n\n\nFind out the device name of a failed disk on the node from the vinfra node disk list output:# vinfra node disk list --node node003\r\n+-------------+---------+------+--------+-------------+----------+----------+---------------+------------+----------------+\r\n| id          | device  | type | role   | disk_status | used     | size     | physical_size | service_id | service_status |\r\n+-------------+---------+------+--------+-------------+----------+----------+---------------+------------+----------------+\r\n| 36972905<\u00e2\u0080\u00a6> | nvme1n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1090       | failed         |\r\n| B9F2C34F<\u00e2\u0080\u00a6> | nvme0n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1091       | active         |\r\n| A8E05CCA<\u00e2\u0080\u00a6> | nvme2n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1086       | active         |\r\n| D6E421E0<\u00e2\u0080\u00a6> | nvme3n1 | ssd  | cs     | ok          | 1.5TiB   | 1.8TiB   | 1.8TiB        | 1087       | active         |\r\n| md126       | md126   | ssd  | system | ok          | 364.2MiB | 989.9MiB | 1022.0MiB     |            |                |\r\n| md127       | md127   | ssd  | system | ok          | 104.4GiB | 187.1GiB | 190.2GiB      |            |                |\r\n+-------------+---------+------+--------+-------------+----------+----------+---------------+------------+----------------+\r\n\nOn the node node003, the storage disc nvme1n1 is reported as failed.\n\n\nInvestigate the smartctl and dmesg outputs for the failed disk. For example:# vinfra node disk show diagnostic-info --node node003 nvme1n1 -f yaml\r\n- command: smartctl --all /dev/nvme1n1\r\n  stdout: 'smartctl 7.1 2020-06-20 r5066 [x86_64-linux-3.10.0-1160.41.1.vz7.183.5]\r\n    (local build)\r\n  Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org\r\n\r\n  === START OF INFORMATION SECTION ===\r\n  Model Number:                       INTEL SSDPE2KX020T8\r\n  Serial Number:                      PHLJ9500032H2P0BGN\r\n  Firmware Version:                   VDV10131\r\n  PCI Vendor/Subsystem ID:            0x8086\r\n  IEEE OUI Identifier:                0x5cd2e4\r\n  Total NVM Capacity:                 2,000,398,934,016 [2.00 TB]\r\n  Unallocated NVM Capacity:           0\r\n  Controller ID:                      0\r\n  Number of Namespaces:               1\r\n  Namespace 1 Size/Capacity:          2,000,398,934,016 [2.00 TB]\r\n  Namespace 1 Formatted LBA Size:     512\r\n  Namespace 1 IEEE EUI-64:            5cd2e4 75b0070100\r\n  Local Time is:                      Fri Nov 26 13:32:44 2021 EET\r\n  Firmware Updates (0x02):            1 Slot\r\n  Optional Admin Commands (0x000e):   Format Frmw_DL NS_Mngmt\r\n  Optional NVM Commands (0x0006):     Wr_Unc DS_Mngmt\r\n  Maximum Data Transfer Size:         32 Pages\r\n  Warning  Comp. Temp. Threshold:     70 Celsius\r\n  Critical Comp. Temp. Threshold:     80 Celsius\r\n\r\n  Supported Power States\r\n  St Op     Max   Active     Idle   RL RT WL WT  Ent_Lat  Ex_Lat\r\n   0 +    25.00W       -        -    0  0  0  0        0       0\r\n\r\n  Supported LBA Sizes (NSID 0x1)\r\n  Id Fmt  Data  Metadt  Rel_Perf\r\n   0 +     512       0         2\r\n   1 -    4096       0         0\r\n\r\n  === START OF SMART DATA SECTION ===\r\n  SMART overall-health self-assessment test result: PASSED\r\n\r\n  SMART/Health Information (NVMe Log 0x02)\r\n  Critical Warning:                   0x00\r\n  Temperature:                        34 Celsius\r\n  Available Spare:                    99%\r\n  Available Spare Threshold:          10%\r\n  Percentage Used:                    5%\r\n  Data Units Read:                    550,835,698 [282 TB]\r\n  Data Units Written:                 720,479,182 [368 TB]\r\n  Host Read Commands:                 10,050,305,459\r\n  Host Write Commands:                20,760,365,218\r\n  Controller Busy Time:               1,968\r\n  Power Cycles:                       20\r\n  Power On Hours:                     13,405\r\n  Unsafe Shutdowns:                   16\r\n  Media and Data Integrity Errors:    0\r\n  Error Information Log Entries:      0\r\n  Warning  Comp. Temperature Time:    0\r\n  Critical Comp. Temperature Time:    0\r\n\r\n  Error Information (NVMe Log 0x01, max 64 entries)\r\n  No Errors Logged\r\n  '\r\n- command: dmesg --ctime --kernel --level=emerg,alert,crit,err,warn --facility=kern\r\n    | grep 'nvme1n1'\r\n  stdout: ''\n\n\n",
                "title": "To troubleshoot a failed disk"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Infrastructure > Nodes screen and click the name of a node that hosts a failed service.\nOn the Disks tab, click the failed disk, and then go to the Service tab, to view the error message.\nClick Get diagnostic information to investigate the smartctl and dmesg outputs.\n\n",
                "title": "To troubleshoot a failed disk"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/troubleshooting-node-disks.html"
    },
    {
        "title": "Unassigning QoS policies",
        "content": "Unassigning QoS policies\nPrerequisites\n\nA QoS policy is assigned to a resource, as described in Assigning QoS policies.\nEnsure that a a QoS policy is not used by networks, ports, or IP addresses.\n\nTo unassign a QoS policy from a network port\nDetach the network port from the policy with openstack port unset --qos-policy. For example:# openstack --insecure port unset --qos-policy c0ea690f-4993-4467-afd5-5389016a0658\nTo unassign a policy from a floating IP address\nDetach the floating IP from the policy with openstack floating ip unset --qos-policy. For example:# openstack --insecure floating ip unset --qos-policy 866203a2-4e1c-459f-807f-14ed563409f1\n To unassign a policy from a network\nDetach the network from the policy with openstack network set --no-qos-policy. For example:# openstack --insecure network set --no-qos-policy public",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/unassigning-qos-policies.html"
    },
    {
        "title": "Troubleshooting Kubernetes clusters",
        "content": "Troubleshooting Kubernetes clusters\nIf a Kubernetes cluster fails, you can download its configuration file and the log files of its nodes for troubleshooting.\nAdditionally, you can get diagnostic information of Kubernetes services by running commands inside a Kubernetes virtual machine from the compute node this VM reside on. Kubernetes services that you need to check are the following:\n\nkubelet is a node agent that works on each Kubernetes node. It ensures that containers described in pod specification files are healthy and running.\nkube-proxy is a network proxy that runs on each Kubernetes node. It configures network rules that resolve connections to pods from within and without a Kubernetes cluster.\nkube-apiserver is an API server that runs only on master nodes. It validates and configures data for the API objects, such as  pods, services, replication controllers, and others. The API server also assigns pods to nodes and synchronizes pod information with the service configuration.\n\nTo download a Kubernetes configuration file\n\nFind out the ID of the required Kubernetes cluster:# vinfra service compute k8saas list\r\n+--------------------------------------+------+--------+\r\n| id                                   | name | status |\r\n+--------------------------------------+------+--------+\r\n| 834397b9-22d3-486d-afc7-5c0122d6735d | k8s1 | ERROR  |\r\n+--------------------------------------+------+--------+\n\nPrint the configuration of the Kubernetes cluster to a file. For example, to download the kubeconfig of the k8s1 cluster to the file k8s1.kubeconfig, run:# vinfra service compute k8saas config 834397b9-22d3-486d-afc7-5c0122d6735d > k8s1.kubeconfig\n\nTo download logs of a Kubernetes node\n\nFind out the name of the required Kubernetes virtual machine:# vinfra service compute server list\r\n+--------------------------------------+---------------+--------+------------------------+---------------------+\r\n| id                                   | name          | status | host                   | networks            |\r\n+--------------------------------------+---------------+--------+------------------------+---------------------+\r\n| 18fb7436-f1fa-4859-99dd-284cef9edc54 | k8s1-node-0   | ACTIVE | node002.vstoragedomain | - public=10.10.10.2 |\r\n| 66bc8454-efb4-4263-a0e2-523fd8f15bda | k8s1-master-0 | ACTIVE | node001.vstoragedomain | - public=10.10.10.1 |\r\n+--------------------------------------+---------------+--------+------------------------+---------------------+\r\n\n\nPrint the log of the Kubernetes VM to a file. For example, to download the log of the k8s1 master node to the file k8s1-master-0.log, run:# vinfra service compute server log k8s1-master-0 > k8s1-master-0.log\n\nTo run commands inside a Kubernetes node\n\nFind out the Kubernetes VM ID and the hostname of the node it runs on by listing all virtual machines in the compute cluster:# vinfra service compute server list\r\n+--------------------------------------+---------------+--------+------------------------+---------------------+\r\n| id                                   | name          | status | host                   | networks            |\r\n+--------------------------------------+---------------+--------+------------------------+---------------------+\r\n| 18fb7436-f1fa-4859-99dd-284cef9edc54 | k8s1-node-0   | ACTIVE | node002.vstoragedomain | - public=10.10.10.2 |\r\n| 66bc8454-efb4-4263-a0e2-523fd8f15bda | k8s1-master-0 | ACTIVE | node001.vstoragedomain | - public=10.10.10.1 |\r\n+--------------------------------------+---------------+--------+------------------------+---------------------+\r\n\nIn this example, the Kubernetes master node has the ID 66bc8454-efb4-4263-a0e2-523fd8f15bda and resides on the node node001.\n\nLog in to the node that hosts the needed Kubernetes VM, and then inside the VM. For example:# ssh node001.vstoragedomain\r\n[root@node001 ~]# virsh x-exec 66bc8454-efb4-4263-a0e2-523fd8f15bda\n\nNow, you can perform diagnostic checks inside the VM. For example, you may start with finding out what services have failed:[root@k8s1-master-0 /]# systemctl list-units --failed\r\n  UNIT               LOAD   ACTIVE SUB    DESCRIPTION\r\n\u00e2\u0097\u008f kube-proxy.service loaded failed failed kube-proxy via Hyperkube\nTo show more details about the failed service, run:[root@k8s1-master-0 /]# systemctl status kube-proxy\r\n\u00c3\u0097 kube-proxy.service - kube-proxy via Hyperkube\r\n     Loaded: loaded (/etc/systemd/system/kube-proxy.service; enabled; vendor preset: disabled)\r\n     Active: failed (Result: exit-code) since Thu 2022-04-28 11:20:18 UTC; 2min 6s ago\r\n    Process: 4603 ExecStartPre=/bin/mkdir -p /etc/kubernetes/ (code=exited, status=0/SUCCESS)\r\n    Process: 4604 ExecStartPre=/usr/bin/podman rm kube-proxy (code=exited, status=1/FAILURE)\r\n    Process: 4624 ExecStart=/bin/bash -c /usr/bin/podman run --name kube-proxy --log-opt path=/dev/null --privileged --net host --entrypoint /hyperkube --volume /e>\r\n    Process: 115468 ExecStop=/usr/bin/podman stop kube-proxy (code=exited, status=0/SUCCESS)\r\n   Main PID: 4624 (code=exited, status=2)\r\n        CPU: 2min 6.180s\r\n...\r\n\nSee also\n\nTroubleshooting virtual machines\n\nRunning commands in virtual machines without network connectivity",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/troubleshooting-kubernetes-clusters.html"
    },
    {
        "title": "Unlocking user accounts",
        "content": "Unlocking user accounts\nA user account is locked after a number of successive failed login attempts from the same IP address in the admin or self-service panel:\n\n10 failed attempts per hour\n50 failed attempts per day\n100 failed attempts per month\n300 failed attempts per year\n\nA successful login resets the counter. A locked user account is unlocked automatically after the lock expires or can be unlocked manually by a system administrator.\nTo unlock a user\n\nFind the name or ID of a locked user within the domain by using the vinfra tool. For example:# vinfra domain user list --domain domain1 --long\r\n+---------------+--------+-------+-------------------------------------------------+----------------+--------------------+------+\r\n| id            | name   | <...> | locked_ips                                      | role           | system_permissions | tags |\r\n+---------------+--------+-------+-------------------------------------------------+----------------+--------------------+------+\r\n| 646f5793<...> | admin1 | <...> | []                                              | domain_admin   | []                 | []   |\r\n|               |        |       |                                                 |                |                    |      |\r\n| 899bc00d<...> | user2  | <...> | - ip: 10.35.15.27                               | project_member | []                 | []   |\r\n|               |        |       |   lock_expired_at: '2022-09-27T13:17:21.499275' |                |                    |      |\r\n| f0b9c6d0<...> | user1  | <...> | []                                              | project_member | []                 | []   |\r\n|               |        |       |                                                 |                |                    |      |\r\n+---------------+--------+-------+-------------------------------------------------+----------------+--------------------+------+\nIn this command output, the locked user is user2, the locked IP address is 10.35.15.27, and the lock will expire at 13:17 on September 27, 2022.\n\nUnlock the user account by specifying its name or ID. For example:# vinfra domain user unlock --domain domain1 user2\n\nSee also\n\nManaging admin panel users\n\nManaging self-service users",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/unlocking-user-accounts.html"
    },
    {
        "title": "Updating registration certificates",
        "content": "Updating registration certificates\nWhen you register backup storage in Acronis Cyber Protect Cloud or Acronis Cyber Protect, they exchange certificates that are valid for three years. One and a half months before expiration, you will be alerted about the expiring certificate in the admin panel. \nPrerequisites\n\nThe backup storage cluster is created and registered in the Cloud Management Panel, as described in Provisioning backup storage space.\nOne or more registrations are added for the backup storage, as explained in Adding registrations.\nEnsure that two-factor authentication (2FA) is disabled for your partner account. You can also disable it for a specific user within a 2FA-enabled tenant, as described in the Acronis Cyber Protect Cloud documentation, and specify the user credentials.\n\nIf you have enabled login control for the Acronis Cyber Protect Cloud web interface, ensure that the public IP address of your backup storage cluster is specified among the allowed IP addresses, as instructed in the Acronis Cyber Protect Cloud documentation.\n\nTo update the registration certificate\n\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Registrations tab. \nSelect a backup storage registration, and then click Update certificate.\nSpecify the credentials of a partner account in the cloud or of an organization administrator on the local management server.\nClick Update.\n\nThe backup storage service will be automatically restarted and the new certificate will be loaded in five minutes.\n\nCommand-line interface\nUse the following command:vinfra service backup registration renew-certificates --username <username> [--server-cert-only]\r\n                                                      [--stdin] <registration>\n\n--stdin\n\nUse for setting registration password from stdin.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server\n--server-cert-only\n\nUpdate only the server certificate\n<registration>\n\nRegistration ID or name\n\nFor example, to update certificates for the backup storage registration registration1, run:# vinfra service backup registration renew-certificates --username account@example.com \\\r\n--stdin registration1\nSpecify the registration password when prompted.\nThe backup storage service will be automatically restarted and the new certificate will be loaded in five minutes.\n\nSee also\n\nDeleting registrations",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service backup registration renew-certificates --username <username> [--server-cert-only]\r\n                                                      [--stdin] <registration>\n\n--stdin\n\nUse for setting registration password from stdin.\n--username <username>\n\nPartner account in the cloud or of an organization administrator on the local management server\n--server-cert-only\n\nUpdate only the server certificate\n<registration>\n\nRegistration ID or name\n\nFor example, to update certificates for the backup storage registration registration1, run:# vinfra service backup registration renew-certificates --username account@example.com \\\r\n--stdin registration1\nSpecify the registration password when prompted.\nThe backup storage service will be automatically restarted and the new certificate will be loaded in five minutes.\n",
                "title": "To update the registration certificate"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nOn the Storage services > Backup storage screen, go to the Registrations tab. \nSelect a backup storage registration, and then click Update certificate.\nSpecify the credentials of a partner account in the cloud or of an organization administrator on the local management server.\nClick Update.\n\nThe backup storage service will be automatically restarted and the new certificate will be loaded in five minutes.\n",
                "title": "To update the registration certificate"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/updating-registration-certificates.html"
    },
    {
        "title": "Updating Kubernetes clusters",
        "content": "Updating Kubernetes clusters\nWhen a new Kubernetes version becomes available, you can update your Kubernetes cluster to it. An update is non-disruptive for Kubernetes worker nodes, which means that these nodes are updated one by one, with the data availability unaffected. The Kubernetes API will be unavailable during an update, unless high availability is enabled for the master node.\nStarting from a Kubernetes cluster update to version 1.24.3, Kubernetes virtual machines are re-created based on a newer Fedora CoreOS image. Such a rolling update is used to preserve the cluster data. Before starting the update, you need to make sure that the compute cluster has enough resources and quotas for at least one extra VM of the largest flavor used by your Kubernetes cluster. If the master and worker node flavors differ, then you should take into account the largest one of them.\nLimitations\n\nYou cannot update Kubernetes clusters with version 1.15.x to newer versions.\nYou cannot manage Kubernetes clusters in the admin panel during an update.\n\nTo update a Kubernetes cluster\n\nAdmin panel\n\nClick a Kubernetes cluster that is marked with the Update available tag.\nOn the Kubernetes cluster pane, click Update in the Kubernetes version field.\nIn the Update window, select a Kubernetes version to update to and follow the provided link to read about API resources that are deprecated or obsoleted in the selected version. Then, click Update.\n\nIn the confirmation window, click Confirm. The update process will start.\n\nDo not manage Kubernetes virtual machines during the update as it may lead to disruption of the update process and cluster inoperability.\n\nCommand-line interface\nUse the following command:vinfra service compute k8saas upgrade <cluster> <version>\n\n<cluster>\n\nCluster ID or name\n<version>\n\nKubernetes version (v1.25.7, v1.24.3, v1.23.5, and v1.22.2)\n\nFor example, to upgrade the Kubernetes cluster k8s1 to version 1.25.7, run:# vinfra service compute k8saas upgrade k8s1 v1.25.7\n\nSee also\n\nCreating custom load balancer flavors\n\nChanging Kubernetes service parameters\n\nTroubleshooting Kubernetes clusters",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra service compute k8saas upgrade <cluster> <version>\n\n<cluster>\n\nCluster ID or name\n<version>\n\nKubernetes version (v1.25.7, v1.24.3, v1.23.5, and v1.22.2)\n\nFor example, to upgrade the Kubernetes cluster k8s1 to version 1.25.7, run:# vinfra service compute k8saas upgrade k8s1 v1.25.7\n",
                "title": "To update a Kubernetes cluster"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nClick a Kubernetes cluster that is marked with the Update available tag.\nOn the Kubernetes cluster pane, click Update in the Kubernetes version field.\nIn the Update window, select a Kubernetes version to update to and follow the provided link to read about API resources that are deprecated or obsoleted in the selected version. Then, click Update.\n\nIn the confirmation window, click Confirm. The update process will start.\n\nDo not manage Kubernetes virtual machines during the update as it may lead to disruption of the update process and cluster inoperability.\n\n\n\n",
                "title": "To update a Kubernetes cluster"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/updating-kubernetes-clusters.html"
    },
    {
        "title": "Using Alertmanager for notifications",
        "content": "Using Alertmanager for notifications\nTo configure the built-in Alertmanager to send notifications, you need to open a TCP port for the Alertmanager API to be accessible from the outside.\nTo open a port for the Alertmanager API\n\nOn the Infrastructure > Networks screen, click Edit and then Create traffic type.\n\nIn the Create traffic type window, specify a custom name in the Name field and 9093 in the Port field. Then, click Create.\n\nClick Assign to networks next to the Custom traffic types section, and then add the created traffic type to your public network by selecting the corresponding check box.\nClick Save to apply the changes.\n\nYou can now access the Alertmanager API at http://<admin_panel_IP_address>:9093. For more information on configuring Alertmanager, refer to its documentation.\nSee also\n\nUsing external Prometheus for monitoring\n\nConfiguring retention policy for Prometheus metrics\n\nPrometheus metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/using-alertmanager-for-notifications.html"
    },
    {
        "title": "Uninstalling guest tools",
        "content": "Uninstalling guest tools\nIf you find out that the guest tools are incompatible with some software inside a virtual machine, you can uninstall them.\nPrerequisites\n\nThe guest tools are installed inside the virtual machine, as described in Installing guest tools.\n\nTo uninstall guest tools\n\nInside a Windows VM:\n\nRemove the QEMU device drivers from the device manager.\n\nDo not remove the VirtIO/SCSI hard disk driver and NetKVM network driver. Without the former, the VM will not boot; without the latter, the VM will lose network connectivity.\n\nUninstall the QEMU guest agent and guest tools from the list of installed applications.\n\nStop and delete Guest Tools Monitor:> sc stop VzGuestToolsMonitor\r\n> sc delete VzGuestToolsMonitor\r\n\n\nUnregister Guest Tools Monitor from Event Log:> reg delete HKLM\\SYSTEM\\CurrentControlSet\\services\\eventlog\\Application\\\\\r\nVzGuestToolsMonitor\r\n\n\nDelete the autorun registry key for RebootNotifier:> reg delete HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run /v \\\r\nVzRebootNotifier\r\n\n\nDelete the C:\\Program Files\\Qemu-ga\\ directory.\nIf VzGuestToolsMonitor.exe is locked, close all the Event Viewer windows. If it remains locked, restart the eventlog service:> sc stop eventlog\r\n> sc start eventlog\r\n\n\nAfter removing the guest tools, restart the virtual machine.\n\nInside a Linux VM:\n\nRemove the packages:\n\nOn RPM-based systems (CentOS and other):# yum remove dkms-vzvirtio_balloon prl_nettool qemu-guest-agent-vz \\vz-guest-udev\r\n\n\nOn DEB-based systems (Debian and Ubuntu):# apt-get remove vzvirtio-balloon-dkms prl-nettool qemu-guest-agent-vz \\vz-guest-udev\r\n\nIf any of the packages listed above are not installed on your system, the command will fail. In this case, exclude these packages from the command and run it again.\n\nRemove the files:# rm -f /usr/bin/prl_backup /usr/share/qemu-ga/VERSION \\/usr/bin/install-tools \\\r\n/etc/udev/rules.d/90-guest_iso.rules /usr/local/bin/fstrim-static \\/etc/cron.weekly/fstrim\r\n\n\nReload the udev rules:# udevadm control --reload\r\n\n\nAfter removing guest tools, restart the virtual machine.\n\nSee also\n\nManaging virtual machine power state\n\nReconfiguring virtual machines\n\nTroubleshooting virtual machines",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/uninstalling-guest-tools.html"
    },
    {
        "title": "Using application credentials",
        "content": "Using application credentials\nWith application credentials, you can allow your applications to authenticate to OpenStack without embedding user credentials in configuration files. This is especially important in cases when the user's identification is provided by an external system, such as LDAP or a single-sign-on service.\nTo grant an application access to a project, you need to create an application credential with delegated role assignments on this project, and then use the application credential identifier and a secret string for authentication. You can delegate the same role assignments that you have on that project to the application credential or only a subset of your role assignments. This is useful, for example, if you have administrative privileges on a project, but you want to limit the application privileges to read-only.\nTo learn how to create and manage application credentials, refer to the OpenStack documentation.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/using-application-credentials.html"
    },
    {
        "title": "Viewing audit log",
        "content": "Viewing audit log\nIn the audit log, you can view all of the management operations performed by users and their activity events.\nTo view a log entry\n\nAdmin panel\n\nGo to the Monitoring > Audit log screen to view the list of audit log entries. \nClick the required log entry on the list to open its details.\n\nCommand-line interface\nUse the following command:vinfra cluster auditlog show <auditlog>\r\n\n\n<auditlog>\n\nAudit log ID\n\nFor example, to the details of the audit log entry about the storage cluster creation, run:\r\n# vinfra cluster auditlog list\r\n+----+-----------+------------------------+--------------------------+---------------------+\r\n| id | username  | type                   | activity                 | timestamp           |\r\n+----+-----------+------------------------+--------------------------+---------------------+\r\n| 8  | admin     | CreateCluster          | Create cluster           | 2021-09-07T17:39:16 |\r\n| 7  | anonymous | RegisterNewNode        | Register new node        | 2021-09-07T17:39:14 |\r\n| 6  | anonymous | RegisterNewNode        | Register new node        | 2021-09-07T17:39:10 |\r\n| 5  | admin     | GetRegistrationToken   | Get registration token   | 2021-09-07T17:39:08 |\r\n| 4  | admin     | GetRegistrationToken   | Get registration token   | 2021-09-07T17:39:04 |\r\n| 3  | admin     | LoginUser              | User login               | 2021-09-07T17:39:04 |\r\n| 2  | anonymous | UpdateNodeRegistration | Update node registration | 2021-09-07T17:39:02 |\r\n| 1  | anonymous | RegisterNewNode        | Register new node        | 2021-09-07T17:38:54 |\r\n+----+-----------+------------------------+--------------------------+---------------------+\r\n# vinfra cluster auditlog show 8\r\n+--------------+--------------------------------------+\r\n| Field        | Value                                |\r\n+--------------+--------------------------------------+\r\n| activity     | Create cluster                       |\r\n| cluster_id   |                                      |\r\n| cluster_name |                                      |\r\n| component    | Cluster                              |\r\n| details      | - id: node                           |\r\n|              |   name: Node                         |\r\n|              |   value: node001.vstoragedomain      |\r\n| id           | 8                                    |\r\n| message      | Create cluster \"cluster1\"            |\r\n| node_id      | c3b2321a-7c12-8456-42ce-8005ff937e12 |\r\n| result       | success                              |\r\n| task_id      | r-38c61bb2c7144cef                   |\r\n| timestamp    | 2021-09-07T17:39:16                  |\r\n| type         | CreateCluster                        |\r\n| user_id      | c727a901a6444ee1a8ad31e3d5b53b3a     |\r\n| username     | admin                                |\r\n+--------------+--------------------------------------+\n\nSee also\n\nViewing alerts\n\nViewing cluster logs",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster auditlog show <auditlog>\r\n\n\n<auditlog>\n\nAudit log ID\n\nFor example, to the details of the audit log entry about the storage cluster creation, run:\r\n# vinfra cluster auditlog list\r\n+----+-----------+------------------------+--------------------------+---------------------+\r\n| id | username  | type                   | activity                 | timestamp           |\r\n+----+-----------+------------------------+--------------------------+---------------------+\r\n| 8  | admin     | CreateCluster          | Create cluster           | 2021-09-07T17:39:16 |\r\n| 7  | anonymous | RegisterNewNode        | Register new node        | 2021-09-07T17:39:14 |\r\n| 6  | anonymous | RegisterNewNode        | Register new node        | 2021-09-07T17:39:10 |\r\n| 5  | admin     | GetRegistrationToken   | Get registration token   | 2021-09-07T17:39:08 |\r\n| 4  | admin     | GetRegistrationToken   | Get registration token   | 2021-09-07T17:39:04 |\r\n| 3  | admin     | LoginUser              | User login               | 2021-09-07T17:39:04 |\r\n| 2  | anonymous | UpdateNodeRegistration | Update node registration | 2021-09-07T17:39:02 |\r\n| 1  | anonymous | RegisterNewNode        | Register new node        | 2021-09-07T17:38:54 |\r\n+----+-----------+------------------------+--------------------------+---------------------+\r\n# vinfra cluster auditlog show 8\r\n+--------------+--------------------------------------+\r\n| Field        | Value                                |\r\n+--------------+--------------------------------------+\r\n| activity     | Create cluster                       |\r\n| cluster_id   |                                      |\r\n| cluster_name |                                      |\r\n| component    | Cluster                              |\r\n| details      | - id: node                           |\r\n|              |   name: Node                         |\r\n|              |   value: node001.vstoragedomain      |\r\n| id           | 8                                    |\r\n| message      | Create cluster \"cluster1\"            |\r\n| node_id      | c3b2321a-7c12-8456-42ce-8005ff937e12 |\r\n| result       | success                              |\r\n| task_id      | r-38c61bb2c7144cef                   |\r\n| timestamp    | 2021-09-07T17:39:16                  |\r\n| type         | CreateCluster                        |\r\n| user_id      | c727a901a6444ee1a8ad31e3d5b53b3a     |\r\n| username     | admin                                |\r\n+--------------+--------------------------------------+\n",
                "title": "To view a log entry"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Monitoring > Audit log screen to view the list of audit log entries. \nClick the required log entry on the list to open its details.\n\n\n\n\n\n",
                "title": "To view a log entry"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/viewing-audit-log.html"
    },
    {
        "title": "Using volume QoS policies",
        "content": "Using volume QoS policies\nYou can use capacity-based quality of service (QoS) policies  to define dynamic limits of compute volumes. Per-GB QoS policies allow you to manually configure IOPS and bandwidth for volumes depending on their size. For example, with the limits 5 IOPS/GB and 1MB/sec per GB, a 100-GB volume will have the performance parameters of 500 IOPS and 100 MB/s, while a 1-TB volume will have as much as 5000 IOPS and 1000 MB/s. After you extend a volume with per-GB limits, its performance will automatically increase corresponding to its size.\nFor dynamic IOPS and bandwidth configuration, you can limit the following volume parameters:\n\nNumber of read operations per second per GB\nNumber of write operations per second per GB\nTotal number of read and write operations per second per GB\nNumber of bytes read per second per GB\nNumber of bytes written per second per GB\nTotal number of bytes read and written per second per GB\n\nAdditionally, you can control the minimum IOPS and bandwidth values, to guarantee high performance to small volumes.\nPer-GB QoS policies can be set to compute volumes by means of storage policies. You can either create a new storage policy with per-GB limits or add such limits to an existing storage policy.\nLimitations\n\nPer-GB limits are not applied to existing volumes on the fly. To apply them to an existing volume, restart the VM that this volume is attached to.\nYou can limit either total IOPS/bandwidth or read and write IOPS/bandwidth per QoS policy. For example, you cannot set the total-bytes-sec-per-gb parameter together with the write-bytes-sec-per-gb or read-bytes-sec-per-gb parameter.\n\nTo create a per-GB QoS policy\nUse the following command:vinfra service compute storage-policy create --tier {0,1,2,3} (--replicas <norm> | --encoding <M>+<N>)\r\n                                             --failure-domain {0,1,2,3,4}\r\n                                             [--write-bytes-sec-per-gb <limit>] [--write-bytes-sec-per-gb-min <limit>]\r\n                                             [--read-bytes-sec-per-gb <limit>] [--read-bytes-sec-per-gb-min <limit>]\r\n                                             [--write-iops-sec-per-gb <limit>] [--write-iops-sec-per-gb-min <limit>]\r\n                                             [--read-iops-sec-per-gb <limit>] [--read-iops-sec-per-gb-min <limit>]\r\n                                             [--total-bytes-sec-per-gb <limit>] [--total-bytes-sec-per-gb-min <limit>]\r\n                                             [--total-iops-sec-per-gb <limit>] [--total-iops-sec-per-gb-min <limit>]\r\n                                             <name>\r\n\n\n--tier {0,1,2,3}\n\nStorage tier\n--encoding <M>+<N>\n\nStorage erasure encoding mapping in the format:\n\nM: number of data blocks\nN: number of parity blocks\n\n--failure-domain {0,1,2,3,4}\n\nStorage failure domain\n--replicas <norm>[:<min>]\n\nStorage replication mapping in the format:\n\nnorm: number of replicas to maintain\nmin: minimum required number of replicas (optional)\n\n--write-bytes-sec-per-gb <limit>\n\nNumber of bytes written per second per GB\n--write-bytes-sec-per-gb-min <limit>\n\nMinimum number of bytes written per second per GB\n--read-bytes-sec-per-gb <limit>\n\nNumber of bytes read per second per GB\n--read-bytes-sec-per-gb-min <limit>\n\nMinimum number of bytes read per second per GB\n--write-iops-sec-per-gb <limit>\n\nNumber of write operations per second per GB\n--write-iops-sec-per-gb-min <limit>\n\nMinimum number of write operations per second per GB\n--read-iops-sec-per-gb <limit>\n\nNumber of read operations per second per GB\n--read-iops-sec-per-gb-min <limit>\n\nMinimum number of read operations per second per GB\n--total-bytes-sec-per-gb <limit>\n\nTotal number of bytes read and written per second per GB\n--total-bytes-sec-per-gb-min <limit>\n\nMinimum number of bytes read and written per second per GB\n--total-iops-sec-per-gb <limit>\n\nTotal number of read and write operations per second per GB\n--total-iops-sec-per-gb-min <limit>\n\nMinimum number of read and write operations per second per GB\n<name>\n\nStorage policy name\n\nFor example, to create a storage policy myqospolicy with the limit of 100 read and write operations per second per GB and the minimum of 50 read and write operations per second per GB, run:# vinfra service compute storage-policy create myqospolicy --tier 1 --failure-domain 1 --replicas 3 \\\r\n--total-iops-sec-per-gb 100 --total-iops-sec-per-gb-min 50\nTo add per-GB limits to an existing policy\nUse the command vinfra service compute storage-policy set and specify the required value for a parameter that you want to limit. For example, to add the bandwidth limit of 104857600 bytes read and written per second per GB to the storage policy myqospolicy, run:# vinfra service compute storage-policy set myqospolicy --total-bytes-sec-per-gb 104857600\nTo remove per-GB limits from an existing policy\nUse the command vinfra service compute storage-policy set and specify -1 as a value for a parameter that you want to be unlimited. For example, to make unlimited bandwidth for the storage policy myqospolicy, run:# vinfra service compute storage-policy set myqospolicy --total-bytes-sec-per-gb -1\nSee also\n\nManaging storage policies\n\nManaging compute volumes\n\nManaging external storages",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/using-volume-qos-policies.html"
    },
    {
        "title": "Using the command-line interface",
        "content": "Using the command-line interface\nTo manage Virtuozzo Hybrid Infrastructure from console and automate such management tasks, you can use the vinfra command-line tool, which is installed automatically with the product.\nTo get a list of all the supported commands and their descriptions, run vinfra help. For help on a specific command, either run vinfra help <command> or vinfra <command> --help.\nNote that the following operations should not be done from the command line:\n\nSetting custom paths for Virtuozzo Hybrid Infrastructure services, in particular:Creating S3 clusters only in /mnt/vstorage/vols/s3Creating iSCSI targets only in /mnt/vstorage/vols/iscsi\nMounting clusters or change cluster mount options\nConfiguring firewall with firewall-cmd\nRenaming network connections\nManaging metadata and storage services\nManaging partitions, LVMs, or software RAID\nModifying files in /mnt/vstorage/vols and /mnt/vstorage/webcp/backup directories\nSetting encoding or replication of cluster root\n\nProviding credentials to vinfra\nThe vinfra tool requires the following information:\n\nIP address or hostname of the management node (set to backend-api.svc.vstoragedomain by default)\nUser name (admin by default)\nPassword (created during installation of Virtuozzo Hybrid Infrastructure)\nDomain name to authenticate with (Default by default)\nProject ID to authenticate with (admin by default)\n\nThis information can be supplied by using the following command-line parameters with each command:\n\n--vinfra-portal <portal>\n\n--vinfra-username <username>\n\n--vinfra-password <password>\n\n--vinfra-domain <domain>\n\n--vinfra-project <project>\n\nAlternatively, you can supply custom credentials by setting the following environment variables (for example, in your ~/.bash_profile): VINFRA_PORTAL, VINFRA_USERNAME, VINFRA_PASSWORD, VINFRA_DOMAIN, and VINFRA_PROJECT. In this case, you will be able to run the CLI tool without the aforementioned command-line parameters.\nAs you typically run vinfra from the management node as admin, the only variable you usually need to set is the password. For example:# export VINFRA_PASSWORD=12345\r\n\nIf you installed vinfra on a remote machine and/or run it as a different system administartor, you will need to set VINFRA_PORTAL and/or VINFRA_USERNAME on that machine in addition to VINFRA_PASSWORD.\nIn addition, if you want to authenticate within a different project or/and domain, you will need to set two more environment variables: VINFRA_PROJECT and/or VINFRA_DOMAIN.\nManaging vinfra tasks\nThe vinfra tool executes some commands immediately, while for other commands (that may take some time to complete) it creates system tasks that are queued. Examples of actions performed via tasks are creating the storage or compute cluster and adding nodes to it.\nTo keep track of tasks performed by vinfra, use the vinfra task list and vinfra task show commands. For example:# vinfra task list\r\n+----------------+---------+-----------------------------------------+\r\n| task_id        | state   | name                                    |\r\n+----------------+---------+-----------------------------------------+\r\n| 8fc27e7a-<...> | success | backend.tasks.cluster.CreateNewCluster  |\r\n| e61377db-<...> | success | backend.tasks.disks.ApplyDiskRoleTask   |\r\n| a005b748-<...> | success | backend.tasks.node.AddNodeInClusterTask |\r\n+----------------+---------+-----------------------------------------+\r\n# vinfra task show 8fc27e7a-ba73-471d-9134-e351e1137cf4\r\n+---------+----------------------------------------+\r\n| Field   | Value                                  |\r\n+---------+----------------------------------------+\r\n| args    | - stor1                                |\r\n|         | - 7ffa9540-5a20-41d1-b203-e3f349d62565 |\r\n|         | - null                                 |\r\n|         | - null                                 |\r\n| kwargs  | {}                                     |\r\n| name    | backend.tasks.cluster.CreateNewCluster |\r\n| result  | cluster_id: 1                          |\r\n| state   | success                                |\r\n| task_id | 8fc27e7a-ba73-471d-9134-e351e1137cf4   |\r\n+---------+----------------------------------------+\r\n\nWhat's next\n\nSetting up networks",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/using-the-command-line-interface.html"
    },
    {
        "title": "Using network QoS policies",
        "content": "Using network QoS policies\nYou can use quality of service (QoS) policies to guarantee or limit network bandwidth for egress and ingress VM traffic in different projects. QoS policies can be applied to separate network ports and floating IP addresses, as well as to entire networks. In addition,  you can set a QoS policy as default for a project to automatically assign the policy to all new networks created within the project. The default QoS policy will be applied to a network if no other policy is explicitly assigned during the network creation process.\nWhen you assign a policy to a network, all ports connected to this network inherit the policy unless the port has a specific policy assigned to it. The assigned policy is applied to both existing and new virtual machines. Internal network ports, like DHCP, and internal router ports are excluded from network policy application.",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/using-network-qos-policies.html"
    },
    {
        "title": "Viewing outgoing traffic usage",
        "content": "Viewing outgoing traffic usage\nThe bandwidth metric that can show outgoing traffic usage is not available by default. To be able to use it, you need to configure it first. A meter must be created for each new project. The meter only accounts traffic that goes through a virtual router. You cannot measure traffic going to or from ports directly attached to virtual machines.\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo measure outgoing traffic for a project\n\nCreate a meter for a project. For example, to create the meter outgoing_traffic_project1 in the project project1 within the domain domain1, run: # openstack --insecure network meter create outgoing_traffic_project1 --project project1 --project-domain domain1\n\nCreate a rule for the meter to account all traffic leaving the project router. For example:# openstack --insecure network meter rule create outgoing_traffic_project1 --egress --include \\\r\n--remote-ip-prefix 0.0.0.0/0 --project project1 --project-domain domain1\nThe new meter with the type bandwidth will be created and associated with the project project1 in the domain domain1.\n\n List all of the bandwidth metrics. For example:# gnocchi --insecure metric list | grep bandwidth\r\n+--------------------------------------+---------------------+-----------+------+--------------------------------------+\r\n| id                                   | archive_policy/name | name      | unit | resource_id                          |\r\n+--------------------------------------+---------------------+-----------+------+--------------------------------------+\r\n| a7982897-42c3-4586-bb4a-8c1b8d11fcc6 | low                 | bandwidth | B    | 883fb5e5-be97-4fbf-b1bd-8f33e552f214 |\r\n| ac2bf21d-f2b0-4043-9fb1-59e1930e497c | low                 | bandwidth | B    | 7147aba5-829e-49b7-a520-01b300178f6c |\r\n+--------------------------------------+---------------------+-----------+------+--------------------------------------+\nIn this output, resource_id is the meter ID, so you can use it to identify meters.# openstack --insecure network meter list\r\n+--------------------------------------+---------------------------+-------------+--------+\r\n| ID                                   | Name                      | Description | Shared |\r\n+--------------------------------------+---------------------------+-------------+--------+\r\n| 883fb5e5-be97-4fbf-b1bd-8f33e552f214 | outgoing_traffic_project1 |             |        |\r\n| 7147aba5-829e-49b7-a520-01b300178f6c | outgoing_traffic_project2 |             |        |\r\n+--------------------------------------+---------------------------+-------------+--------+\r\n\n\n View the measures by using the metric id. For example:# gnocchi --insecure measures show a7982897-42c3-4586-bb4a-8c1b8d11fcc6 --aggregation sum\r\n+---------------------------+-------------+---------+\r\n| timestamp                 | granularity |   value |\r\n+---------------------------+-------------+---------+\r\n| 2021-10-03T02:40:00+03:00 |       300.0 |  1045.0 |\r\n| 2021-10-03T02:50:00+03:00 |       300.0 |  1907.0 |\r\n| 2021-10-03T02:55:00+03:00 |       300.0 | 15932.0 |\r\n+---------------------------+-------------+---------+\n\nTo exclude outgoing traffic for a network\nCreate a rule for the meter to exclude outgoing traffic for a particular network. For example, for the network with the CIDR 10.0.0.0/24, run:# openstack --insecure network meter rule create outgoing_traffic --egress --exclude \\\r\n--remote-ip-prefix 10.0.0.0/24 --project project1 --project-domain domain1\nSee also\n\nViewing resources, metrics, and measures\n\nChanging retention period for metrics\n\nViewing resource usage per project",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/viewing-outgoing-traffic-usage.html"
    },
    {
        "title": "Viewing cluster logs",
        "content": "Viewing cluster logs\nWhen you encounter a problem in Virtuozzo Hybrid Infrastructure, you can send a problem report, as described in Getting technical support. The report will gather all the logs needed to troubleshoot the problem and send them to the technical support team.\nAlternatively, you can investigate the root cause of your problem by using the logs listed in table below.\n\nService\nLog location\nDescription\n\nMetadata\n/vstorage/mds/logs/mds.log.zst on the storage node\r\nhosting an MDS service\nStorage metadata service events\n\nStorage\n\n/vstorage/<id>/cs/logs/cs.log.zst on the storage node\r\nhosting a CS service\n\nTo find out the log location of a particular CS\r\non the node, run\r\nvstorage -c <cluster_name> list-services -C.\n\nChunk service events\n\nStorage mountpoint\n/var/log/vstorage/<cluster_name>/vstorage-mount.*.blog\r\non any storage node\nSoftware-defined storage mounting on\r\neach node\n\nManagement node\n/var/log/vstorage-ui-backend/messages.log and\r\n/var/log/vstorage-ui-backend/celery*.log on the\r\nmanagement node\nManagement node and admin panel events\n\n/var/log/vstorage-ui-agent/* on any storage node\nAgent controller component events\n\nBackup Gateway\n\n/var/log/abgw/abgw.log*zst on any node in the\r\nBackup Gateway cluster\n\nThe latest log is abgw.log.zst, older ones\r\nare renamed to abgw.log.0.zst, abgw.log.1.zst,\r\netc.\n\nBackup Gateway cluster deployment and\r\nmanagement\n\niSCSI\n/var/log/vstorage/iscsi/vstorage-target.log on any\r\nnode in an iSCSI target group\niSCSI target management\n\n/var/log/vstorage/iscsi/vstorage-target-monitor.log on\r\nany node in an iSCSI target group\niSCSI target monitoring\n\n/var/log/vstorage/iscsi/scst.log.zst on any node in an\r\niSCSI target group\nSCST service logs\n\nS3\n/var/log/ostor/NS-* on the S3 node with NS services\nS3 name server events\n\n/var/log/ostor/OS-* on the S3 node with OS services\nS3 object server events\n\n/var/log/ostor/S3GW-* on the S3 node with GW services\nS3 gateway events\n\n/var/log/nginx/* on any node in the S3 cluster\nnginx service logs\n\n/var/log/ostor/GR-* on the S3 node with GR services\nS3 geo-replicator service events\n\n/var/log/ostor/ACC-* on the S3 node with ACC services\nS3 account control service events\n\n/var/log/ostor/ostorcfgd.log*.zst on the S3 node with the Object storage configuration service (ostor-cfgd.service)\nObject storage configuration service events\n\n/var/log/ostor/ostor-agent.log*.zst on any S3 node\nObject storage host agent service events\n\nNFS\n/var/log/ganesha/ganesha.log and\r\n/var/log/ostor/ostorfs.log.gz on any node in the NFS\r\ncluster\nNFS server events\n\n/var/log/vstorage/vstorage-nfsd.log on any node in\r\nthe NFS cluster\nNFS service events\n\n/var/log/ostor/FS-* on the node hosting an NFS share\nFS service events\n\n/var/log/ostor/OS-* on the node hosting an NFS share\nOS service events\n\n/var/log/ostor/ostorcfgd.log*.zst on the NFS node with the Object storage configuration service (ostor-cfgd.service)\nObject storage configuration service events\n\n/var/log/ostor/ostor-agent.log*.zst on any NFS node\nObject storage host agent service events\n\nCompute\n/var/log/vstorage-ui-backend/ansible.log on the\r\ncontroller node\nCompute cluster and add-on deployment\n\n/var/log/hci/beholder/beholder.log on the controller\r\nnode\nNotifications about all compute\r\nevents, including VM placement\n\n/var/log/hci/nova/* on the compute node hosting a VM\n\nIn case of problems during VM migration, view\r\n/var/log/hci/nova/nova-compute.log on the source\r\nand destination compute nodes.\n\nVirtual machine management\n\n/var/log/hci/neutron/neutron-l3-agent.log on any\r\ncompute node\nVirtual routing events\n\n/var/log/hci/neutron/neutron-openvswitch-agent.log\r\non the compute node hosting a VM\nVM network interface management\n\n/var/log/hci/cinder/* on the controller node\nCompute volume management\n\n/var/log/hci/glance/glance-api.log on the controller\r\nnode\nImage service API requests\n\n/var/log/hci/octavia/octavia-worker.log and\r\n/var/log/hci/octavia/octavia-api.log on the\r\ncontroller node\nLoad balancing service management\n\n/var/log/hci/magnum/magnum-conductor.log,\r\n/var/log/hci/magnum/magnum-api.log, and\r\n/var/log/hci/heat/heat-engine.log on the\r\ncontroller node\nKubernetes service and VM stack\r\ndeployment and management\n\n/var/log/hci/gnocchi/* and\r\n/var/log/hci/ceilometer/* on any compute node\nBilling metering service management\n\nHigh availability\n/var/log/vstorage-ui-backend/ha.log on all management\r\nnodes\nHigh availability management\n\nUpdates\n/var/log/vstorage-ui-backend/software-updates.log on\r\nthe management node\nSoftware update orchestration\n\n/var/log/vstorage-ui-agent/software-updates.log on\r\nany storage node\nSoftware update downloading and\r\ninstallation on each node\n\nTo open log files\nUse the following commands:\n\nfor LOG files:# less <log_file>.log\r\n\n\nfor BLOG files:# blogcat <log_file>.blog | less\r\n\n\nfor GZ files:# zless <log_file>.gz\r\n\n\nfor ZST files:# zstdless <log_file>.zst\r\n\n\nSee also\n\nViewing alerts\n\nViewing audit log",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/viewing-cluster-logs.html"
    },
    {
        "title": "Viewing alerts",
        "content": "Viewing alerts\nAn alert is generated and logged each time one of the following conditions is met or events happen:\n\nA critical issue has happened with a cluster, its components (CS, MDS), disks, nodes, or services.\nThe cluster requires configuration or more resources to build or restore its health.\nThe network requires configuration or is experiencing issues that may affect performance.\nA license has expired.\nThe cluster is about to, or has, run out of available space.\n\nAlerts can be ignored (deleted from the alerts list) or postponed for several hours. Postponed alerts reappear in the list after some time. \nTo view an alert\n\nAdmin panel\n\nGo to the Monitoring > Alerts screen, which lists all of the alerts logged by Virtuozzo Hybrid Infrastructure. \nClick the required alert on the list to open its details.\n\nCommand-line interface\nUse the following command:vinfra cluster alert show <alert>\r\n\n\n<alert>\n\nAlert ID that can be obtained with vinfra cluster alert list\n\nFor example, to view the details of the license alert, run:# vinfra cluster alert list\r\n+----+----------------------------------------------------------+---------------------+----------+---------+\r\n| id | type                                                     | datetime            | severity | enabled |\r\n+----+----------------------------------------------------------+---------------------+----------+---------+\r\n| 8  | High availability for the admin panel must be configured | 2021-09-07T18:38:55 | error    | True    |\r\n| 6  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n| 4  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n| 23 | Disk cache settings are not optimal                      | 2021-09-30T23:46:28 | warning  | True    |\r\n| 1  | License is not loaded                                    | 2021-09-07T18:38:55 | warning  | True    |\r\n| 22 | Configuration warning                                    | 2021-09-30T23:21:32 | warning  | True    |\r\n| 3  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n| 7  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n+----+----------------------------------------------------------+---------------------+----------+---------+\r\n# vinfra cluster alert show 1\r\n+---------------+-----------------------+\r\n| Field         | Value                 |\r\n+---------------+-----------------------+\r\n| _type         | license_isnot_loaded  |\r\n| cluster_id    | 1                     |\r\n| cluster_name  | cluster1              |\r\n| datetime      | 2021-09-07T18:38:55   |\r\n| details       | {}                    |\r\n| enabled       | True                  |\r\n| group         | cluster               |\r\n| host          |                       |\r\n| id            | 1                     |\r\n| message       | License is not loaded |\r\n| node_id       |                       |\r\n| object_id     | None                  |\r\n| orig_hostname |                       |\r\n| severity      | warning               |\r\n| suspended     |                       |\r\n| type          | License is not loaded |\r\n+---------------+-----------------------+\n\nTo ignore an alert\n\nGo to the Monitoring > Alerts screen, and then click the required alert on the list. \nOn the alert right pane, click Ignore.\n\nTo postpone an alert\n\nAdmin panel\n\nGo to the Monitoring > Alerts screen, and then click the required alert on the list. \nOn the alert right pane, click Postpone.\n\nCommand-line interface\nUse the following command:vinfra cluster alert delete <alert>\r\n\n\n<alert>\n\nAlert ID\n\nFor example, to delete the alert with the ID 1 from the log, run:# vinfra cluster alert delete 1\n\nSee also\n\nSending email notifications\n\nViewing audit log\n\nViewing cluster logs",
        "paragraphs": [],
        "cli_examples": [
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster alert show <alert>\r\n\n\n<alert>\n\nAlert ID that can be obtained with vinfra cluster alert list\n\nFor example, to view the details of the license alert, run:# vinfra cluster alert list\r\n+----+----------------------------------------------------------+---------------------+----------+---------+\r\n| id | type                                                     | datetime            | severity | enabled |\r\n+----+----------------------------------------------------------+---------------------+----------+---------+\r\n| 8  | High availability for the admin panel must be configured | 2021-09-07T18:38:55 | error    | True    |\r\n| 6  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n| 4  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n| 23 | Disk cache settings are not optimal                      | 2021-09-30T23:46:28 | warning  | True    |\r\n| 1  | License is not loaded                                    | 2021-09-07T18:38:55 | warning  | True    |\r\n| 22 | Configuration warning                                    | 2021-09-30T23:21:32 | warning  | True    |\r\n| 3  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n| 7  | Network warning                                          | 2021-09-07T18:38:55 | warning  | True    |\r\n+----+----------------------------------------------------------+---------------------+----------+---------+\r\n# vinfra cluster alert show 1\r\n+---------------+-----------------------+\r\n| Field         | Value                 |\r\n+---------------+-----------------------+\r\n| _type         | license_isnot_loaded  |\r\n| cluster_id    | 1                     |\r\n| cluster_name  | cluster1              |\r\n| datetime      | 2021-09-07T18:38:55   |\r\n| details       | {}                    |\r\n| enabled       | True                  |\r\n| group         | cluster               |\r\n| host          |                       |\r\n| id            | 1                     |\r\n| message       | License is not loaded |\r\n| node_id       |                       |\r\n| object_id     | None                  |\r\n| orig_hostname |                       |\r\n| severity      | warning               |\r\n| suspended     |                       |\r\n| type          | License is not loaded |\r\n+---------------+-----------------------+\n",
                "title": "To view an alert"
            },
            {
                "example": "\nCommand-line interface\nUse the following command:vinfra cluster alert delete <alert>\r\n\n\n<alert>\n\nAlert ID\n\nFor example, to delete the alert with the ID 1 from the log, run:# vinfra cluster alert delete 1\n",
                "title": "To postpone an alert"
            }
        ],
        "panel_examples": [
            {
                "example": "\nAdmin panel\n\nGo to the Monitoring > Alerts screen, which lists all of the alerts logged by Virtuozzo Hybrid Infrastructure. \nClick the required alert on the list to open its details.\n\n\n\n\n\n",
                "title": "To view an alert"
            },
            {
                "example": "\nAdmin panel\n\nGo to the Monitoring > Alerts screen, and then click the required alert on the list. \nOn the alert right pane, click Postpone.\n\n",
                "title": "To postpone an alert"
            }
        ],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/viewing-alerts.html"
    },
    {
        "title": "Viewing resource usage per project",
        "content": "Viewing resource usage per project\nTo get usage of compute resources allocated to all virtual machines that belong to a particular project, you can use either the vinfra command or the gnocchi tool. The latter, however, collects resource usage for a specific period of time.\nPrerequisites\n\nTo authorize further OpenStack commands, the OpenStack command-line client must be configured, as outlined in Connecting to OpenStack command-line interface.\n\nTo view resource usage per project via vinfra\nUse the command vinfra service compute quotas show --usage <project_id>. For example:# vinfra service compute quotas show 6ef6f48f01b640ccb8ff53117b830fa3 --usage\r\n+---------------------------------+-------+\r\n| Field                           | Value |\r\n+---------------------------------+-------+\r\n| compute.cores.limit             | 20    |\r\n| compute.cores.used              | 2     |\r\n| compute.ram.limit               | 40960 |\r\n| compute.ram.used                | 4096  |\r\n| k8saas.cluster.limit            | 10    |\r\n| k8saas.cluster.used             | 0     |\r\n| lbaas.loadbalancer.limit        | 10    |\r\n| lbaas.loadbalancer.used         | 0     |\r\n| network.floatingip.limit        | 10    |\r\n| network.floatingip.used         | 0     |\r\n| storage.gigabytes.default.limit | 1024  |\r\n| storage.gigabytes.default.used  | 66    |\r\n+---------------------------------+-------+\r\n\nThe output shows that VMs included in the project with the ID 62af79f31ae5488aa33077d02af48282 were allocated 2 vCPUs, 4 GB of RAM, and 66 GB of disk space.\nTo view resource usage per project via gnocchi\nUse the following commands, for example, for the project with the ID 75521ab61d1f4e9090aac5836c219492 from 12:00 PM July 18, 2021, to 12:00 PM July 19, 2021:\n\nTo aggregate the number of provisioned vCPUs:# gnocchi --insecure aggregates --resource-type instance --needed-overlap 0 \"(aggregate sum (metric vcpus mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the amount of provisioned RAM:# gnocchi --insecure aggregates --resource-type instance --needed-overlap 0 \"(aggregate sum (metric memory mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the total size of provisioned storage space:# gnocchi --insecure aggregates --resource-type volume --needed-overlap 0 \"(aggregate sum (metric volume.size mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the size of provisioned storage space with the storage policy with the ID 10056d2e-6fc9-4f2e-92c2-dbebb1714778:# gnocchi --insecure aggregates --resource-type volume --needed-overlap 0 \\\r\n\"(aggregate sum (metric volume.size.10056d2e-6fc9-4f2e-92c2-dbebb1714778 mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the number of used floating IP addresses:# gnocchi --insecure aggregates --resource-type network --needed-overlap 0 \"(aggregate sum (metric ip.floating mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the size of outgoing network traffic:# gnocchi --insecure aggregates --resource-type network --needed-overlap 0 \"(aggregate sum (metric bandwidth mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the number of load balancers:# gnocchi --insecure aggregates --resource-type loadbalancer --needed-overlap 0 \\\r\n\"(aggregate sum (metric network.services.lb.loadbalancer mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nTo aggregate the number of Kubernetes clusters:\n\nKubernetes clusters may be created with an empty project ID. In this case, specify None for the project_id attribute.\n# gnocchi --insecure aggregates --resource-type coe_cluster --needed-overlap 0 \"(aggregate sum (metric magnum.cluster mean))\" \\\r\n\"project_id=75521ab61d1f4e9090aac5836c219492\" --start 2021-07-18T12:00:00 --stop 2021-07-19T12:00:00\n\nSee also\n\nViewing resources, metrics, and measures\n\nViewing outgoing traffic usage\n\nChanging retention period for metrics",
        "paragraphs": [],
        "cli_examples": [],
        "panel_examples": [],
        "url": "https://docs.virtuozzo.com/virtuozzo_hybrid_infrastructure_6_0_admins_guide/viewing-resource-usage-per-project.html"
    }
]